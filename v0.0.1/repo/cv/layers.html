
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Deep Learning Course" name="description"/>
<link href="https://danibcorr.github.io/unie-deep-learning/v0.0.1/repo/cv/layers.html" rel="canonical"/>
<link href="../../index.html" rel="prev"/>
<link href="models.html" rel="next"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.21" name="generator"/>
<title>Layers - Deep Learning Course</title>
<link href="../../assets/stylesheets/main.2a3383ac.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../assets/_mkdocstrings.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="black" data-md-color-primary="black" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#src.cv.layers.aps">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<div data-md-color-scheme="default" data-md-component="outdated" hidden="">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Deep Learning Course" class="md-header__button md-logo" data-md-component="logo" href="../../index.html" title="Deep Learning Course">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Deep Learning Course
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Layers
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="black" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="black" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="black" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/danibcorr/unie-deep-learning" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    unie-deep-learning
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
        
  
  
    
  
  Home

      </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="layers.html">
          
  
  
    
  
  Computer Vision

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../generative/models.html">
          
  
  
    
  
  Generative

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../nlp/layers.html">
          
  
  
    
  
  Natural Language Processing

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../notebooks/Computer%20Vision/step_1_image_processing.html">
          
  
  
    
  
  Notebooks

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Deep Learning Course" class="md-nav__button md-logo" data-md-component="logo" href="../../index.html" title="Deep Learning Course">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>
</a>
    Deep Learning Course
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/danibcorr/unie-deep-learning" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    unie-deep-learning
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    Home
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
<span class="md-ellipsis">
    Computer Vision
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Layers
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="layers.html">
<span class="md-ellipsis">
    Layers
    
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.aps">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-module"></code> aps
    </span>
</a>
<nav aria-label=" aps" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.aps.AdaptivePolyphaseSampling">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> AdaptivePolyphaseSampling
    </span>
</a>
<nav aria-label=" AdaptivePolyphaseSampling" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.aps.AdaptivePolyphaseSampling.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.lps">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-module"></code> lps
    </span>
</a>
<nav aria-label=" lps" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.lps.LearnablePolyphaseSampling">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> LearnablePolyphaseSampling
    </span>
</a>
<nav aria-label=" LearnablePolyphaseSampling" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.lps.LearnablePolyphaseSampling.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.se">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-module"></code> se
    </span>
</a>
<nav aria-label=" se" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.se.SqueezeExcitation">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> SqueezeExcitation
    </span>
</a>
<nav aria-label=" SqueezeExcitation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.se.SqueezeExcitation.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-module"></code> vit
    </span>
</a>
<nav aria-label=" vit" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.EncoderBlock">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> EncoderBlock
    </span>
</a>
<nav aria-label=" EncoderBlock" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.EncoderBlock.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.FeedForward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> FeedForward
    </span>
</a>
<nav aria-label=" FeedForward" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.FeedForward.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.LayerNormalization">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> LayerNormalization
    </span>
</a>
<nav aria-label=" LayerNormalization" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.LayerNormalization.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.MultiHeadAttention">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> MultiHeadAttention
    </span>
</a>
<nav aria-label=" MultiHeadAttention" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.MultiHeadAttention.attention">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.MultiHeadAttention.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.PatchEmbedding">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> PatchEmbedding
    </span>
</a>
<nav aria-label=" PatchEmbedding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.PatchEmbedding.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.Patches">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Patches
    </span>
</a>
<nav aria-label=" Patches" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.Patches.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.PositionalEncoding">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> PositionalEncoding
    </span>
</a>
<nav aria-label=" PositionalEncoding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.PositionalEncoding.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.ResidualConnection">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> ResidualConnection
    </span>
</a>
<nav aria-label=" ResidualConnection" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.ResidualConnection.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.VisionTransformer">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> VisionTransformer
    </span>
</a>
<nav aria-label=" VisionTransformer" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#src.cv.layers.vit.VisionTransformer.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="models.html">
<span class="md-ellipsis">
    Models
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
<span class="md-ellipsis">
    Generative
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            Generative
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../generative/models.html">
<span class="md-ellipsis">
    Models
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
<span class="md-ellipsis">
    Natural Language Processing
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../nlp/layers.html">
<span class="md-ellipsis">
    Layers
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../nlp/utils.html">
<span class="md-ellipsis">
    Utilities
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
<span class="md-ellipsis">
    Notebooks
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
            Notebooks
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
<span class="md-ellipsis">
    Computer Vision
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_1">
<span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_1_image_processing.html">
<span class="md-ellipsis">
    Step 1 image processing
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_2_convolutional_layer.html">
<span class="md-ellipsis">
    Step 2 convolutional layer
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_3_lenet.html">
<span class="md-ellipsis">
    LeNet
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_4_vgg.html">
<span class="md-ellipsis">
    Step 4 vgg
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_5_resnet.html">
<span class="md-ellipsis">
    Step 5 resnet
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_6_autoencoders.html">
<span class="md-ellipsis">
    Step 6 autoencoders
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_7_transfer_learning.html">
<span class="md-ellipsis">
    Step 7 transfer learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_8_knowledge_distillation.html">
<span class="md-ellipsis">
    Step 8 knowledge distillation
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_9_intepretability.html">
<span class="md-ellipsis">
    Step 9 intepretability
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Computer%20Vision/step_x_normalization.html">
<span class="md-ellipsis">
    Local Reponse Normalization
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
<span class="md-ellipsis">
    Mathematics
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
            Mathematics
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_1_data_structures.html">
<span class="md-ellipsis">
    Comprobar disponibilidad de la GPU
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_2_linear_algebra_calculus.html">
<span class="md-ellipsis">
    Step 2 linear algebra calculus
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_3_mathematics_concepts.html">
<span class="md-ellipsis">
    Cosine Similarity
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_4_artificial_neuron.html">
<span class="md-ellipsis">
    Artificial Neuron
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_5_gradient_descent.html">
<span class="md-ellipsis">
    Example 1
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_6_deep_neural_networks.html">
<span class="md-ellipsis">
    Step 6 deep neural networks
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_7_regularization_techniques.html">
<span class="md-ellipsis">
    Step 7 regularization techniques
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_8_optimizers.html">
<span class="md-ellipsis">
    Step 8 optimizers
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Mathematics/step_9_metrics.html">
<span class="md-ellipsis">
    Step 9 metrics
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
<span class="md-ellipsis">
    Time Series
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
            Time Series
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../notebooks/Time%20Series/step_x_other_techniques.html">
<span class="md-ellipsis">
    Gramian Angular Field
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1><span class="enumerate-headings-plugin enumerate-heading-plugin">1.</span> Layers</h1>
<div class="doc doc-object doc-module">
<h2 class="doc doc-heading" id="src.cv.layers.aps"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code> <code>aps</code>
</h2>
<div class="doc doc-contents first">
<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214</p>
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.aps.AdaptivePolyphaseSampling"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">AdaptivePolyphaseSampling</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes the class with normalization option.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>norm</code>
</td>
<td>
<code><span title="int">int</span> | <span title="float">float</span> | <span title="typing.Literal">Literal</span>['fro', 'nuc', 'inf', '-inf'] | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Normalization type or value, defaults to 2.</p>
</div>
</td>
<td>
<code>2</code>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/aps.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">norm</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"fro"</span><span class="p">,</span> <span class="s2">"nuc"</span><span class="p">,</span> <span class="s2">"inf"</span><span class="p">,</span> <span class="s2">"-inf"</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes the class with normalization option.</span>

<span class="sd">    Args:</span>
<span class="sd">        norm: Normalization type or value, defaults to 2.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.aps.AdaptivePolyphaseSampling.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Processes input tensor to extract dominant polyphase component.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor with shape (B, C, H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>return_index</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, returns index of dominant component.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span> | <span title="tuple">tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor, optionally with index if return_index is True.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/aps.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">return_index</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Processes input tensor to extract dominant polyphase component.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Tensor with shape (B, C, H, W).</span>
<span class="sd">        return_index: If True, returns index of dominant component.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor, optionally with index if return_index is True.</span>
<span class="sd">    """</span>

    <span class="c1"># Tenemos a la entrada un tensor de (B, C, H, W)</span>
    <span class="c1"># El número de componentes polifásicas coincide con el tamaño</span>
    <span class="c1"># de paso elevado al cuadrado, porque nos vemos tanto en la</span>
    <span class="c1"># altura como en la anchura , en total 4</span>
    <span class="n">poly_a</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_b</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_c</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_d</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>

    <span class="c1"># Combinamos las componentes en un solo tensor (B, P, C, H, W)</span>
    <span class="n">polyphase_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">poly_a</span><span class="p">,</span> <span class="n">poly_b</span><span class="p">,</span> <span class="n">poly_c</span><span class="p">,</span> <span class="n">poly_d</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Extraemos las dimensiones</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">polyphase_combined</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># Combinamos los valores de los canales, altura y anchura del tensor</span>
    <span class="n">polyphase_combined_reshaped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">polyphase_combined</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Aplicamos la norma a la última dimensión</span>
    <span class="n">polyphase_norms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">polyphase_combined_reshaped</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Seleccionamos el componente polifásico de mayor orden</span>
    <span class="n">polyphase_max_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">polyphase_norms</span><span class="p">)</span>

    <span class="c1"># Obtenemos el componente polifásico de mayor orden</span>
    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">polyphase_combined</span><span class="p">[:,</span> <span class="n">polyphase_max_norm</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

    <span class="c1"># En el paper existe la opción de devolver el índice</span>
    <span class="k">if</span> <span class="n">return_index</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="n">polyphase_max_norm</span>

    <span class="c1"># En caso contrario solo devolvemos el tensor</span>
    <span class="k">return</span> <span class="n">output_tensor</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<h2 class="doc doc-heading" id="src.cv.layers.lps"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code> <code>lps</code>
</h2>
<div class="doc doc-contents first">
<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001</p>
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.lps.LearnablePolyphaseSampling"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">LearnablePolyphaseSampling</span><span class="p">(</span><span class="n">channel_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes the model with specified channel and hidden sizes.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>channel_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of input channels for the Conv2D layer.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>hidden_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of hidden units for the Conv2D layer.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/lps.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes the model with specified channel and hidden sizes.</span>

<span class="sd">    Args:</span>
<span class="sd">        channel_size: Number of input channels for the Conv2D layer.</span>
<span class="sd">        hidden_size: Number of hidden units for the Conv2D layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Definimos el modelo único para cada componente</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">channel_size</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.lps.LearnablePolyphaseSampling.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Processes input to extract dominant polyphase component.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor with shape (B, C, H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>return_index</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, returns index of dominant component.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span> | <span title="tuple">tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor of dominant component, optionally with index.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/lps.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">return_index</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Processes input to extract dominant polyphase component.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Tensor with shape (B, C, H, W).</span>
<span class="sd">        return_index: If True, returns index of dominant component.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of dominant component, optionally with index.</span>
<span class="sd">    """</span>

    <span class="c1"># Tenemos a la entrada un tensor de (B, C, H, W)</span>
    <span class="c1"># El número de componentes polifásicas coincide con el tamaño</span>
    <span class="c1"># de paso elevado al cuadrado, porque nos vemos tanto en la</span>
    <span class="c1"># altura como en la anchura , en total 4</span>
    <span class="n">poly_a</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_b</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_c</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>
    <span class="n">poly_d</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span> <span class="mi">1</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">]</span>

    <span class="c1"># Combinamos las componentes en un solo tensor (B, P, C, H, W)</span>
    <span class="n">polyphase_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">poly_a</span><span class="p">,</span> <span class="n">poly_b</span><span class="p">,</span> <span class="n">poly_c</span><span class="p">,</span> <span class="n">poly_d</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Utilizamos el modelo basado en convoluciones por cada componente</span>
    <span class="n">_logits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">polyphase</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">polyphase_combined</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">_logits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_model</span><span class="p">(</span><span class="n">polyphase_combined</span><span class="p">[:,</span> <span class="n">polyphase</span><span class="p">,</span> <span class="o">...</span><span class="p">]))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">_logits</span><span class="p">))</span>

    <span class="c1"># Aplicamos la norma a la última dimensión</span>
    <span class="n">polyphase_norms</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Seleccionamos el componente polifásico de mayor orden</span>
    <span class="n">polyphase_max_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">polyphase_norms</span><span class="p">)</span>

    <span class="c1"># Obtenemos el componente polifásico de mayor orden</span>
    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">polyphase_combined</span><span class="p">[:,</span> <span class="n">polyphase_max_norm</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

    <span class="c1"># En el paper existe la opción de devolver el índice</span>
    <span class="k">if</span> <span class="n">return_index</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="n">polyphase_max_norm</span>

    <span class="c1"># En caso contrario solo devolvemos el tensor</span>
    <span class="k">return</span> <span class="n">output_tensor</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<h2 class="doc doc-heading" id="src.cv.layers.se"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code> <code>se</code>
</h2>
<div class="doc doc-contents first">
<p>Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507</p>
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.se.SqueezeExcitation"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">SqueezeExcitation</span><span class="p">(</span><span class="n">channel_size</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Implements Squeeze-and-Excitation (SE) block.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>channel_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of channels in the input tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>ratio</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Reduction factor for the compression layer.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/se.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ratio</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Implements Squeeze-and-Excitation (SE) block.</span>

<span class="sd">    Args:</span>
<span class="sd">        channel_size: Number of channels in the input tensor.</span>
<span class="sd">        ratio: Reduction factor for the compression layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Vamos a crear un modelo Sequential</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">se_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>  <span class="c1"># (B, C, 1, 1)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>  <span class="c1"># (B, C)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">channel_size</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">channel_size</span> <span class="o">//</span> <span class="n">ratio</span>
        <span class="p">),</span>  <span class="c1"># (B, C//ratio)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>  <span class="c1"># (B, C//ratio)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">channel_size</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">channel_size</span>
        <span class="p">),</span>  <span class="c1"># (B, C)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.se.SqueezeExcitation.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Applies attention mechanism to input tensor.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor with shape (B, C, H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor with attention applied, same shape as input.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/se.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Applies attention mechanism to input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Input tensor with shape (B, C, H, W).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor with attention applied, same shape as input.</span>
<span class="sd">    """</span>

    <span class="c1"># Primero podemos obtener el tamaño del tensor de entrada</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># Obtenemos el tensor de aplicar SE</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">se_block</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

    <span class="c1"># Modificamos el shape del tensor para ajustarlo al input</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Aplicamos el producto como mecanismo de atención</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">input_tensor</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<h2 class="doc doc-heading" id="src.cv.layers.vit"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code> <code>vit</code>
</h2>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.EncoderBlock"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">EncoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize encoder block module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of features in input.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>d_ff</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Hidden layer feature dimensions.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>h</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of attention heads.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate for layers.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize encoder block module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: Number of features in input.</span>
<span class="sd">        d_ff: Hidden layer feature dimensions.</span>
<span class="sd">        h: Number of attention heads.</span>
<span class="sd">        dropout_rate: Dropout rate for layers.</span>
<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Parametros</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>

    <span class="c1"># Definicion de las capas</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_attention_layer</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
        <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual_layer_1</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="p">(</span>
        <span class="n">features</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_layer</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span>
        <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual_layer_2</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="p">(</span>
        <span class="n">features</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.EncoderBlock.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Process input tensor through encoder block.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of input tensors.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>mask</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Mask tensor, optional.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after encoder block processing.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Process input tensor through encoder block.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Batch of input tensors.</span>
<span class="sd">        mask: Mask tensor, optional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after encoder block processing.</span>
<span class="sd">    """</span>

    <span class="c1"># Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_layer_1</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_attention_layer</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># Segunda conexión residual con feed-forward</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_layer_2</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">input_tensor</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.FeedForward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize feed-forward neural network.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input and output feature dimensions.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>d_ff</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Hidden layer feature dimensions.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate applied on layers.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize feed-forward neural network.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: Input and output feature dimensions.</span>
<span class="sd">        d_ff: Hidden layer feature dimensions.</span>
<span class="sd">        dropout_rate: Dropout rate applied on layers.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>

    <span class="c1"># Creamos el modelo secuencial</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.FeedForward.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.2.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Process input tensor through feed-forward layers.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of input tensors.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after feed-forward processing.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Process input tensor through feed-forward layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Batch of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after feed-forward processing.</span>
<span class="sd">    """</span>

    <span class="c1"># (B, sequence_length, d_model)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.LayerNormalization"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize layer normalization module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>features</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of features in input.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>eps</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Small value to avoid division by zero.</p>
</div>
</td>
<td>
<code>1e-06</code>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize layer normalization module.</span>

<span class="sd">    Args:</span>
<span class="sd">        features: Number of features in input.</span>
<span class="sd">        eps: Small value to avoid division by zero.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="c1"># Utilizamos un factor alpha para multiplicar el valor de la normalización</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">))</span>
    <span class="c1"># Utilizamos un factor del sesgo para sumar</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.LayerNormalization.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.3.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Apply layer normalization to input embeddings.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_embedding</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of input embeddings.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Normalized embeddings.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embedding</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Apply layer normalization to input embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_embedding: Batch of input embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Normalized embeddings.</span>
<span class="sd">    """</span>

    <span class="c1"># (B, sequence_length, d_model)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">input_embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">input_embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">((</span><span class="n">input_embedding</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)))</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.MultiHeadAttention"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize multi-head attention module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of features in input.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>h</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of attention heads.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate applied on scores.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize multi-head attention module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: Number of features in input.</span>
<span class="sd">        h: Number of attention heads.</span>
<span class="sd">        dropout_rate: Dropout rate applied on scores.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># el tamalo de los embeddings debe ser proporcional al número de cabezas</span>
    <span class="c1"># para realizar la división, por lo que es el resto ha de ser 0</span>
    <span class="k">if</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"d_model ha de ser divisible entre h"</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="c1"># Valore establecidos en el paper</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span>

    <span class="c1"># Parámetros</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_OUTPUT_CONCAT</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.MultiHeadAttention.attention"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.4.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">attention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
<span class="doc doc-labels">
<small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Compute attention scores and output.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>k</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Key tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>q</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Query tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>v</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Value tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>mask</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Mask tensor, optional.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout</code>
</td>
<td>
<code><span title="torch.nn.Dropout">Dropout</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout layer, optional.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
</td>
<td>
<div class="doc-md-description">
<p>Tuple of attention output and scores.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute attention scores and output.</span>

<span class="sd">    Args:</span>
<span class="sd">        k: Key tensor.</span>
<span class="sd">        q: Query tensor.</span>
<span class="sd">        v: Value tensor.</span>
<span class="sd">        mask: Mask tensor, optional.</span>
<span class="sd">        dropout: Dropout layer, optional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of attention output and scores.</span>
<span class="sd">    """</span>

    <span class="c1"># Primero realizamos el producto matricial con la transpuesta</span>
    <span class="c1"># q = (Batch, h, seq_len, d_k)</span>
    <span class="c1"># k.T = (Batch, h, d_k, seq_len)</span>
    <span class="c1"># matmul_q_k = (Batch, h, seq_len, seq_len)</span>
    <span class="n">matmul_q_k</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Luego realizamos el escalado</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">matmul_q_k_scaled</span> <span class="o">=</span> <span class="n">matmul_q_k</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>

    <span class="c1"># El enmascarado es para el decoder, relleno de infinitos</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">matmul_q_k_scaled</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

    <span class="c1"># Obtenemos los scores/puntuación de la atención</span>
    <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">matmul_q_k_scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Aplicamos dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

    <span class="c1"># Multiplicamos por el valor</span>
    <span class="c1"># attention_scores = (Batch, h, seq_len, seq_len)</span>
    <span class="c1"># v = (Batch, h, seq_len, d_k)</span>
    <span class="c1"># Output = (Batch, h, seq_len, d_k)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">attention_scores</span> <span class="o">@</span> <span class="n">v</span><span class="p">),</span> <span class="n">attention_scores</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.MultiHeadAttention.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.4.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Process input tensors through multi-head attention.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>k</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Key tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>q</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Query tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>v</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Value tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>mask</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Mask tensor, optional.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after attention processing.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Process input tensors through multi-head attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        k: Key tensor.</span>
<span class="sd">        q: Query tensor.</span>
<span class="sd">        v: Value tensor.</span>
<span class="sd">        mask: Mask tensor, optional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after attention processing.</span>
<span class="sd">    """</span>

    <span class="c1"># k -&gt; (Batch, seq_len, d_model) igual para el resto</span>
    <span class="n">key_prima</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">query_prima</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="n">value_prima</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="c1"># Cambiamos las dimensiones y hacemos el split de los embedding para cada head</span>
    <span class="c1"># Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)</span>
    <span class="c1"># Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)</span>
    <span class="n">key_prima</span> <span class="o">=</span> <span class="n">key_prima</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">key_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
    <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">query_prima</span> <span class="o">=</span> <span class="n">query_prima</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">query_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">query_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
    <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">value_prima</span> <span class="o">=</span> <span class="n">value_prima</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">value_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value_prima</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
    <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Obtenemos la matriz de atencion y la puntuación</span>
    <span class="c1"># attention = (Batch, h, seq_len, d_k)</span>
    <span class="c1"># attention_scores = (Batch, h, seq_len, seq_len)</span>
    <span class="n">attention</span><span class="p">,</span> <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
        <span class="n">k</span><span class="o">=</span><span class="n">key_prima</span><span class="p">,</span>
        <span class="n">q</span><span class="o">=</span><span class="n">query_prima</span><span class="p">,</span>
        <span class="n">v</span><span class="o">=</span><span class="n">value_prima</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Tenemos que concatenar la información de todas las cabezas</span>
    <span class="c1"># Queremos (Batch, seq_len, d_model)</span>
    <span class="c1"># self.d_k = self.d_model // self.h; d_model = d_k * h</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (Batch, seq_len, h, d_k)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="c1"># Al parecer, contiguous permite evitar errores de memoria</span>
    <span class="n">attention_concat</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_k</span>
    <span class="p">)</span>  <span class="c1"># (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_OUTPUT_CONCAT</span><span class="p">(</span><span class="n">attention_concat</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.PatchEmbedding"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.5</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">PatchEmbedding</span><span class="p">(</span><span class="n">patch_size_height</span><span class="p">,</span> <span class="n">patch_size_width</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize patch embedding module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>patch_size_height</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Height of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>patch_size_width</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Width of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>in_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of input channels.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of the model.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">patch_size_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">patch_size_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize patch embedding module.</span>

<span class="sd">    Args:</span>
<span class="sd">        patch_size_height: Height of each patch.</span>
<span class="sd">        patch_size_width: Width of each patch.</span>
<span class="sd">        in_channels: Number of input channels.</span>
<span class="sd">        d_model: Dimension of the model.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span> <span class="o">=</span> <span class="n">patch_size_height</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span> <span class="o">=</span> <span class="n">patch_size_width</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="c1"># Esta es una de las diferencias con usar transformers en el texto</span>
    <span class="c1"># Aquí usamos FFN en vez de Embedding layer, es una proyección</span>
    <span class="c1"># de los pixeles</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span>
        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span><span class="p">,</span>
        <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.PatchEmbedding.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.5.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Apply linear projection to input tensor.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of image patches as a tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor after linear projection of patches.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Apply linear projection to input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Batch of image patches as a tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after linear projection of patches.</span>
<span class="sd">    """</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.Patches"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.6</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">Patches</span><span class="p">(</span><span class="n">patch_size_height</span><span class="p">,</span> <span class="n">patch_size_width</span><span class="p">,</span> <span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize patch extraction module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>patch_size_height</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Height of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>patch_size_width</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Width of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>img_height</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Height of the input image.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>img_width</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Width of the input image.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If img_height not divisible by patch height.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If img_width not divisible by patch width.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">patch_size_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">patch_size_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">img_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">img_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize patch extraction module.</span>

<span class="sd">    Args:</span>
<span class="sd">        patch_size_height: Height of each patch.</span>
<span class="sd">        patch_size_width: Width of each patch.</span>
<span class="sd">        img_height: Height of the input image.</span>
<span class="sd">        img_width: Width of the input image.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If img_height not divisible by patch height.</span>
<span class="sd">        ValueError: If img_width not divisible by patch width.</span>
<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">img_height</span> <span class="o">%</span> <span class="n">patch_size_height</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"img_height tiene que se divisible entre el patch_size_height"</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">img_width</span> <span class="o">%</span> <span class="n">patch_size_width</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"img_width tiene que se divisible entre el patch_size_width"</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span> <span class="o">=</span> <span class="n">patch_size_height</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span> <span class="o">=</span> <span class="n">patch_size_width</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unfold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Unfold</span><span class="p">(</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span><span class="p">),</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.Patches.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.6.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Extract patches from input tensor.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of images as a tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor with patches from input images.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Extract patches from input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Batch of images as a tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor with patches from input images.</span>
<span class="sd">    """</span>

    <span class="c1"># unfold devuelve (b, c * patch_height * patch_width, num_patches)</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
    <span class="c1"># Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)</span>
    <span class="k">return</span> <span class="n">patches</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.PositionalEncoding"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.7</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize positional encoding module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of the model.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>sequence_length</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Max length of input sequences.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate applied on outputs.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize positional encoding module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: Dimension of the model.</span>
<span class="sd">        sequence_length: Max length of input sequences.</span>
<span class="sd">        dropout_rate: Dropout rate applied on outputs.</span>
<span class="sd">    """</span>

    <span class="c1"># Constructor de la clase</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Definimos los parámetros de la clase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="c1"># Cuando le damos una secuencia de tokens, tenemos que saber</span>
    <span class="c1"># la longitud máxima de la secuencia</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">sequence_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="c1"># Creamos una matriz del positional embedding</span>
    <span class="c1"># (sequence_length, d_model)</span>
    <span class="n">pe_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>

    <span class="c1"># Crear vector de posiciones</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Crear vector de divisores</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Aplicar sin y cos</span>
    <span class="n">pe_matrix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe_matrix</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>

    <span class="c1"># Tenemos que convertirlo a (1, sequence_length, d_model) para</span>
    <span class="c1"># procesarlo por lotes</span>
    <span class="n">pe_matrix</span> <span class="o">=</span> <span class="n">pe_matrix</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"pe_matrix"</span><span class="p">,</span> <span class="n">tensor</span><span class="o">=</span><span class="n">pe_matrix</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.PositionalEncoding.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.7.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Add positional encoding to input embeddings.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_embedding</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of input embeddings.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Embeddings with added positional encoding.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embedding</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Add positional encoding to input embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_embedding: Batch of input embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Embeddings with added positional encoding.</span>
<span class="sd">    """</span>

    <span class="c1"># (B, ..., d_model) -&gt; (B, sequence_length, d_model)</span>
    <span class="c1"># Seleccionamos</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">input_embedding</span> <span class="o">+</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe_matrix</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">input_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.ResidualConnection"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.8</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">ResidualConnection</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize residual connection module.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>features</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of features in input.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate for sublayer output.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize residual connection module.</span>

<span class="sd">    Args:</span>
<span class="sd">        features: Number of features in input.</span>
<span class="sd">        dropout_rate: Dropout rate for sublayer output.</span>
<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.ResidualConnection.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.8.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Apply residual connection to sublayer output.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Original input tensor.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>sublayer</code>
</td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Sublayer module to apply.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor with residual connection applied.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Apply residual connection to sublayer output.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Original input tensor.</span>
<span class="sd">        sublayer: Sublayer module to apply.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor with residual connection applied.</span>
<span class="sd">    """</span>

    <span class="k">return</span> <span class="n">input_tensor</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)))</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="src.cv.layers.vit.VisionTransformer"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.9</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code class="highlight language-python"><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">patch_size_height</span><span class="p">,</span> <span class="n">patch_size_width</span><span class="p">,</span> <span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">num_encoders</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize Vision Transformer (VIT).</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>patch_size_height</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Height of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>patch_size_width</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Width of each patch.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>img_height</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Height of input images.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>img_width</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Width of input images.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>in_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of input channels.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>num_encoders</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of encoder blocks.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of the model.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>d_ff</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of feed-forward layers.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>h</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of attention heads.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>num_classes</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of output classes.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dropout_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dropout rate for layers.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">patch_size_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">patch_size_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">img_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">img_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_encoders</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">h</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize Vision Transformer (VIT).</span>

<span class="sd">    Args:</span>
<span class="sd">        patch_size_height: Height of each patch.</span>
<span class="sd">        patch_size_width: Width of each patch.</span>
<span class="sd">        img_height: Height of input images.</span>
<span class="sd">        img_width: Width of input images.</span>
<span class="sd">        in_channels: Number of input channels.</span>
<span class="sd">        num_encoders: Number of encoder blocks.</span>
<span class="sd">        d_model: Dimension of the model.</span>
<span class="sd">        d_ff: Dimension of feed-forward layers.</span>
<span class="sd">        h: Number of attention heads.</span>
<span class="sd">        num_classes: Number of output classes.</span>
<span class="sd">        dropout_rate: Dropout rate for layers.</span>
<span class="sd">    """</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span> <span class="o">=</span> <span class="n">patch_size_height</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span> <span class="o">=</span> <span class="n">patch_size_width</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">img_height</span> <span class="o">=</span> <span class="n">img_height</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">img_width</span> <span class="o">=</span> <span class="n">img_width</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_encoders</span> <span class="o">=</span> <span class="n">num_encoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>

    <span class="c1"># Número de patches</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_height</span> <span class="o">//</span> <span class="n">patch_size_height</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">img_width</span> <span class="o">//</span> <span class="n">patch_size_width</span>
    <span class="p">)</span>

    <span class="c1"># CLS token permite tener una representación global de todos los inputs</span>
    <span class="c1"># de la imagen (de los diferentes embeddings de cada patch)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">patch_layer</span> <span class="o">=</span> <span class="n">Patches</span><span class="p">(</span>
        <span class="n">patch_size_height</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span><span class="p">,</span>
        <span class="n">patch_size_width</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span><span class="p">,</span>
        <span class="n">img_height</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">img_height</span><span class="p">,</span>
        <span class="n">img_width</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">img_width</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">(</span>
        <span class="n">patch_size_height</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_height</span><span class="p">,</span>
        <span class="n">patch_size_width</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_size_width</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Entiendo que la longitud de la secuencia coincide con el numero de patches</span>
    <span class="c1"># y un embedding más de la clase,</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
        <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">sequence_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Capas del Encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">EncoderBlock</span><span class="p">(</span>
                <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                <span class="n">d_ff</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span>
                <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span>
                <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_encoders</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h4 class="doc doc-heading" id="src.cv.layers.vit.VisionTransformer.forward"><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.9.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span></code>
</h4>
<div class="doc doc-contents">
<p>Process input tensor through VIT model.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_tensor</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Batch of input images.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Classification output tensor.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="mkdocstrings-source">
<summary>Source code in <code>src/cv/layers/vit.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Process input tensor through VIT model.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Batch of input images.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classification output tensor.</span>
<span class="sd">    """</span>

    <span class="c1"># Extraemos los patches</span>
    <span class="n">input_patches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_layer</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

    <span class="c1"># Convertimso a embeddings los patches</span>
    <span class="n">patch_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_patches</span><span class="p">)</span>

    <span class="c1"># Tenemos que añadir el token de la clase al inicio de la secuencia</span>
    <span class="c1"># (B, 1, d_model)</span>
    <span class="n">cls_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># (B, num_patches+1, d_model)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="n">patch_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Añadir positional encoding</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Encoders del transformer</span>
    <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">embeddings</span>
    <span class="k">for</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>

    <span class="c1"># Usar solo el CLS token para clasificación</span>
    <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
    <span class="n">cls_output</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Clasificación final</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_classifier</span><span class="p">(</span><span class="n">cls_output</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="Last update">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 11, 2025 17:42:06 UTC">August 11, 2025</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="Contributors">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"></path></svg>
</span>
<nav>
      danibcorr
    </nav>
</span>
</aside>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://www.linkedin.com/in/danibcorr/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.tabs.link", "content.code.annotate", "content.code.copy", "content.tooltips", "announce.dismiss", "navigation.tabs", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.preview", "navigation.instant.progress", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.indexes", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script>
<script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>