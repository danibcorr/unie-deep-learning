{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#papers-algorithms-with-code","title":"Papers &amp; Algorithms with Code","text":"<p>Implementations of algorithms, utilities, and code snippets I find interesting, along with PyTorch and Python versions of research papers.</p>"},{"location":"notebooks/Computer%20Vision/lenet.html","title":"LeNet","text":"<p>We begin by importing the necessary Python modules and libraries for building and training our neural network with the MNIST dataset.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nfrom typing import Any\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.manifold import TSNE\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n</pre> # Standard libraries from typing import Any  # 3pps import matplotlib.pyplot as plt import torch from sklearn.manifold import TSNE from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm In\u00a0[2]: Copied! <pre>def show_images(images, labels):\n    fig, axes = plt.subplots(1, len(images), figsize=(10, 2))\n    for img, label, ax in zip(images, labels, axes):\n        ax.imshow(img.squeeze(), cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n    plt.show()\n</pre> def show_images(images, labels):     fig, axes = plt.subplots(1, len(images), figsize=(10, 2))     for img, label, ax in zip(images, labels, axes):         ax.imshow(img.squeeze(), cmap='gray')         ax.set_title(f'Label: {label}')         ax.axis('off')     plt.show() <p>We start by loading the MNIST dataset, including both the training and test sets. While loading the data, we apply two important transformations. First, each image is converted into a PyTorch tensor, which allows the model to process the data efficiently. Second, we normalize the images using <code>transforms.Normalize((0.1307,), (0.3081,))</code>. These numbers represent the mean (<code>0.1307</code>) and standard deviation (<code>0.3081</code>) of the MNIST dataset, and normalization ensures that the data has a consistent scale. This step is important because it helps the model train more effectively and converge faster. By combining these transformations, we prepare the dataset in a way that is both suitable for the model and optimized for learning.</p> In\u00a0[3]: Copied! <pre>transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=True,\n    download=True,\n    transform=transform\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=False,\n    download=True,\n    transform=transform\n)\n</pre> transform = transforms.Compose([     transforms.ToTensor(),     transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST(     root=\"./data\",     train=True,     download=True,     transform=transform ) test_dataset = datasets.MNIST(     root=\"./data\",     train=False,     download=True,     transform=transform ) In\u00a0[4]: Copied! <pre>train_dataset\n</pre> train_dataset Out[4]: <pre>Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> In\u00a0[5]: Copied! <pre>test_dataset\n</pre> test_dataset Out[5]: <pre>Dataset MNIST\n    Number of datapoints: 10000\n    Root location: ./data\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> <p>We can use the <code>DataLoader</code> class to divide the dataset into batches and shuffle the data efficiently using PyTorch\u2019s built-in functionality. To get started, we first define some global variables or constants that will be used throughout the data loading and training process.</p> In\u00a0[6]: Copied! <pre>BATCH_SIZE: int = 32\n</pre> BATCH_SIZE: int = 32 In\u00a0[7]: Copied! <pre>train_dataloader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\n</pre> train_dataloader = DataLoader(     dataset=train_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) test_dataloader = DataLoader(     dataset=test_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) <p>We can visualize some examples of sample-label pairs from the dataset. If we take one batch from the <code>DataLoader</code>, we will get as many samples as the chosen batch size. Each MNIST sample is a grayscale image of size 28 \u00d7 28, meaning it has a single channel. By inspecting these batches, we can better understand the structure and format of the data before feeding it into a neural network.</p> In\u00a0[8]: Copied! <pre>data_iter = iter(train_dataloader)\ntrain_images, train_labels = next(data_iter)\ntrain_images.shape, train_labels.shape\n</pre> data_iter = iter(train_dataloader) train_images, train_labels = next(data_iter) train_images.shape, train_labels.shape Out[8]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>We will display the first 10 samples from the dataset. This allows us to quickly inspect the images and their corresponding labels to ensure that the data has been loaded and preprocessed correctly.</p> In\u00a0[9]: Copied! <pre>show_images(train_images[:10], train_labels[:10])\n</pre> show_images(train_images[:10], train_labels[:10]) <p>Next, we will create a convolutional neural network (CNN) model, inspired by Yann LeCun\u2019s LeNet architecture, and adapt it for the MNIST dataset. This model will use convolutional layers to automatically extract features from the images, followed by fully connected layers to perform classification.</p> In\u00a0[10]: Copied! <pre>class LeNet(nn.Module):\n\n    def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:\n\n        super().__init__(**kwargs)\n\n        self.input_tensor_shape = input_tensor_shape\n\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=16),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n            nn.Flatten(),\n            nn.Linear(32, 10),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.model(input_tensor)\n</pre> class LeNet(nn.Module):      def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:          super().__init__(**kwargs)          self.input_tensor_shape = input_tensor_shape          self.model = nn.Sequential(             nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=16),             nn.ReLU(),             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=32),             nn.ReLU(),             nn.AdaptiveAvgPool2d(output_size=(1,1)),             nn.Flatten(),             nn.Linear(32, 10),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.model(input_tensor) <p>We define the optimizer as AdamW and use cross-entropy as the loss function. AdamW is an adaptive optimizer that combines the benefits of Adam with correct weight decay, helping the model converge efficiently. Cross-entropy loss is well-suited for multi-class classification tasks like MNIST, as it measures the difference between the predicted probabilities and the true class labels.</p> In\u00a0[11]: Copied! <pre>model = LeNet(input_tensor_shape=(1,28,28))\nsummary(model, input_size=(BATCH_SIZE, 1,28,28))\n</pre> model = LeNet(input_tensor_shape=(1,28,28)) summary(model, input_size=(BATCH_SIZE, 1,28,28)) Out[11]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLeNet                                    [32, 10]                  --\n\u251c\u2500Sequential: 1-1                        [32, 10]                  --\n\u2502    \u2514\u2500Conv2d: 2-1                       [32, 16, 13, 13]          272\n\u2502    \u2514\u2500BatchNorm2d: 2-2                  [32, 16, 13, 13]          32\n\u2502    \u2514\u2500ReLU: 2-3                         [32, 16, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-4                       [32, 32, 5, 5]            8,224\n\u2502    \u2514\u2500BatchNorm2d: 2-5                  [32, 32, 5, 5]            64\n\u2502    \u2514\u2500ReLU: 2-6                         [32, 32, 5, 5]            --\n\u2502    \u2514\u2500AdaptiveAvgPool2d: 2-7            [32, 32, 1, 1]            --\n\u2502    \u2514\u2500Flatten: 2-8                      [32, 32]                  --\n\u2502    \u2514\u2500Linear: 2-9                       [32, 10]                  330\n==========================================================================================\nTotal params: 8,922\nTrainable params: 8,922\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 8.06\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 1.80\nParams size (MB): 0.04\nEstimated Total Size (MB): 1.93\n==========================================================================================</pre> In\u00a0[12]: Copied! <pre>optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4)\nloss_function = torch.nn.CrossEntropyLoss()\n</pre> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4) loss_function = torch.nn.CrossEntropyLoss() <p>Now, we need to create the training loop. This loop will iterate over the dataset for a number of epochs, feeding batches of data through the model, computing the loss, performing backpropagation, and updating the model\u2019s parameters. A well-structured training loop is essential for effectively training the network and monitoring its performance over time.</p> In\u00a0[13]: Copied! <pre>NUM_EPOCHS: int = 5\n</pre> NUM_EPOCHS: int = 5 In\u00a0[14]: Copied! <pre>train_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n    for batch_image, batch_label in train_loop:\n\n        optimizer.zero_grad()\n        outputs = model(batch_image)\n        loss = loss_function(outputs, batch_label)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += batch_label.size(0)\n        correct += (predicted == batch_label).sum().item()\n\n    train_losses.append(running_loss / len(train_dataloader))\n    train_accuracies.append(100 * correct / total)\n\n    model.eval()\n    test_loss, correct_test, total_test = 0.0, 0, 0\n\n    test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)\n\n    with torch.no_grad():\n        for images, labels in test_loop:\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n    test_losses.append(test_loss / len(test_dataloader))\n    test_accuracies.append(100 * correct_test / total_test)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"\n          f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")\n\nepochs = range(1, NUM_EPOCHS+1)\n\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Test Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\nplt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Training vs Test Accuracy\")\nplt.legend()\nplt.show()\n</pre> train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], []  for epoch in range(NUM_EPOCHS):     model.train()     running_loss, correct, total = 0.0, 0, 0      train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)      for batch_image, batch_label in train_loop:          optimizer.zero_grad()         outputs = model(batch_image)         loss = loss_function(outputs, batch_label)          loss.backward()         optimizer.step()          running_loss += loss.item()         _, predicted = torch.max(outputs, 1)         total += batch_label.size(0)         correct += (predicted == batch_label).sum().item()      train_losses.append(running_loss / len(train_dataloader))     train_accuracies.append(100 * correct / total)      model.eval()     test_loss, correct_test, total_test = 0.0, 0, 0      test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)      with torch.no_grad():         for images, labels in test_loop:             outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             _, predicted = torch.max(outputs, 1)             total_test += labels.size(0)             correct_test += (predicted == labels).sum().item()      test_losses.append(test_loss / len(test_dataloader))     test_accuracies.append(100 * correct_test / total_test)      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"           f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"           f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")  epochs = range(1, NUM_EPOCHS+1)  plt.plot(epochs, train_losses, label=\"Train Loss\") plt.plot(epochs, test_losses, label=\"Test Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Training vs Test Loss\") plt.legend() plt.show()  plt.plot(epochs, train_accuracies, label=\"Train Accuracy\") plt.plot(epochs, test_accuracies, label=\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy (%)\") plt.title(\"Training vs Test Accuracy\") plt.legend() plt.show() <pre>                                                                      \r</pre> <pre>Epoch [1/5] Train Loss: 0.9334, Train Acc: 76.77% | Test Loss: 0.4189, Test Acc: 90.49%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [2/5] Train Loss: 0.3321, Train Acc: 92.07% | Test Loss: 0.2243, Test Acc: 94.45%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [3/5] Train Loss: 0.2210, Train Acc: 94.38% | Test Loss: 0.1833, Test Acc: 95.19%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [4/5] Train Loss: 0.1745, Train Acc: 95.46% | Test Loss: 0.1389, Test Acc: 96.30%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [5/5] Train Loss: 0.1469, Train Acc: 96.10% | Test Loss: 0.1318, Test Acc: 96.39%\n</pre> <p>Now, we can visualize the data in a lower-dimensional space using t-SNE. This technique allows us to project high-dimensional representations\u2014such as the feature outputs from our model\u2014into two or three dimensions, making it easier to observe patterns, clusters, or separations between different classes. Visualizing the data in this way can provide valuable insights into how well the model is learning to distinguish between digits.</p> In\u00a0[15]: Copied! <pre>all_labels = []\nembeddings = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_image, batch_label in train_dataloader:\n        output = model(batch_image)\n        embeddings.append(output.cpu())\n        all_labels.append(batch_label) \n\nembeddings = torch.cat(embeddings, dim=0)\nall_labels = torch.cat(all_labels, dim=0)\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',\n                  init='random', perplexity=30).fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15)\nplt.colorbar(scatter, ticks=range(10), label=\"Classes\")\nplt.title(\"t-SNE Training Embeddings MNIST\")\nplt.show()\n</pre> all_labels = [] embeddings = []  model.eval() with torch.no_grad():     for batch_image, batch_label in train_dataloader:         output = model(batch_image)         embeddings.append(output.cpu())         all_labels.append(batch_label)   embeddings = torch.cat(embeddings, dim=0) all_labels = torch.cat(all_labels, dim=0)  X_embedded = TSNE(n_components=2, learning_rate='auto',                   init='random', perplexity=30).fit_transform(embeddings)  plt.figure(figsize=(10, 8)) scatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15) plt.colorbar(scatter, ticks=range(10), label=\"Classes\") plt.title(\"t-SNE Training Embeddings MNIST\") plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Computer%20Vision/lenet.html#lenet","title":"LeNet\u00b6","text":""},{"location":"notebooks/Computer%20Vision/lenet.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Computer%20Vision/lenet.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Computer%20Vision/lenet.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/activation_functions.html","title":"Activation Functions","text":"In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\nfrom typing import Callable\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> # Standard libraries import math from typing import Callable  # 3pps import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:\n    plt.title(f\"{name_function} function\")\n    plt.plot(steps, function(steps))\n    plt.grid()\n    plt.show()\n</pre> def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:     plt.title(f\"{name_function} function\")     plt.plot(steps, function(steps))     plt.grid()     plt.show() In\u00a0[3]: Copied! <pre>def sigmoid(input: np.ndarray) -&gt; np.ndarray:\n    return 1/(1+np.exp((-1) * input))\n\n\ndef tanh(input: np.ndarray) -&gt; np.ndarray:\n    return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))\n\n\ndef relu(input: np.ndarray) -&gt; np.ndarray:\n    return [max(0, elem) for elem in input]\n\n\ndef leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    return [max(0.1 * elem, elem) for elem in input]\n\n\ndef elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:\n    return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]\n\n\ndef swish(input: np.ndarray) -&gt; np.ndarray:\n    return input * sigmoid(input)\n\n\ndef gelu(input: np.ndarray) -&gt; np.ndarray:\n    return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3)))\n</pre> def sigmoid(input: np.ndarray) -&gt; np.ndarray:     return 1/(1+np.exp((-1) * input))   def tanh(input: np.ndarray) -&gt; np.ndarray:     return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))   def relu(input: np.ndarray) -&gt; np.ndarray:     return [max(0, elem) for elem in input]   def leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:     return [max(0.1 * elem, elem) for elem in input]   def elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:     return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]   def swish(input: np.ndarray) -&gt; np.ndarray:     return input * sigmoid(input)   def gelu(input: np.ndarray) -&gt; np.ndarray:     return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3))) In\u00a0[4]: Copied! <pre>steps = np.arange(-10, 10, 0.1)\n\nplot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid)\nplot_function(name_function=\"Tanh\", steps=steps, function=tanh)\nplot_function(name_function=\"ReLU\", steps=steps, function=relu)\nplot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu)\nplot_function(name_function=\"ELU\", steps=steps, function=elu)\nplot_function(name_function=\"Swish\", steps=steps, function=swish)\nplot_function(name_function=\"GELU\", steps=steps, function=gelu)\n</pre> steps = np.arange(-10, 10, 0.1)  plot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid) plot_function(name_function=\"Tanh\", steps=steps, function=tanh) plot_function(name_function=\"ReLU\", steps=steps, function=relu) plot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu) plot_function(name_function=\"ELU\", steps=steps, function=elu) plot_function(name_function=\"Swish\", steps=steps, function=swish) plot_function(name_function=\"GELU\", steps=steps, function=gelu) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Mathematics/activation_functions.html#activation-functions","title":"Activation Functions\u00b6","text":""},{"location":"notebooks/Mathematics/activation_functions.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/activation_functions.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Mathematics/activation_functions.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/cosine_similarity.html","title":"Cosine Similarity","text":"In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n</pre> # 3pps import numpy as np In\u00a0[2]: Copied! <pre>def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)\n\n\ndef cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz @ matriz.T\n</pre> def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:     return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)   def cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:     return matriz @ matriz.T In\u00a0[3]: Copied! <pre>X = np.array([\n    [1, 2, 3],  \n    [4, 5, 6],   \n    [1, 0, 0],  \n    [0, 1, 0]   \n], dtype=float)\n\nprint(\"Embeddings originales:\\n\", X)\n\nX_normalized = normalizar_matriz(matriz=X)\nprint(\"\\nEmbeddings normalizados:\\n\", X_normalized)\n\nsimilarity_matrix = cosine_similarity(matriz=X_normalized)\nprint(\"\\nMatriz de similitud:\\n\", similarity_matrix)\n</pre> X = np.array([     [1, 2, 3],       [4, 5, 6],        [1, 0, 0],       [0, 1, 0]    ], dtype=float)  print(\"Embeddings originales:\\n\", X)  X_normalized = normalizar_matriz(matriz=X) print(\"\\nEmbeddings normalizados:\\n\", X_normalized)  similarity_matrix = cosine_similarity(matriz=X_normalized) print(\"\\nMatriz de similitud:\\n\", similarity_matrix) <pre>Embeddings originales:\n [[1. 2. 3.]\n [4. 5. 6.]\n [1. 0. 0.]\n [0. 1. 0.]]\n\nEmbeddings normalizados:\n [[0.26726124 0.53452248 0.80178373]\n [0.45584231 0.56980288 0.68376346]\n [1.         0.         0.        ]\n [0.         1.         0.        ]]\n\nMatriz de similitud:\n [[1.         0.97463185 0.26726124 0.53452248]\n [0.97463185 1.         0.45584231 0.56980288]\n [0.26726124 0.45584231 1.         0.        ]\n [0.53452248 0.56980288 0.         1.        ]]\n</pre>"},{"location":"notebooks/Mathematics/cosine_similarity.html#cosine-similarity","title":"Cosine Similarity\u00b6","text":""},{"location":"notebooks/Mathematics/cosine_similarity.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/cosine_similarity.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Mathematics/cosine_similarity.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/gradient_descent.html","title":"Example 1","text":"In\u00a0[71]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Definici\u00f3n de la funci\u00f3n\n\n\ndef function(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n\n# C\u00e1lculo del gradiente (derivadas parciales)\n\n\ndef gradiente(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    \n    df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n    df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])\n\n    return np.stack([df_x1, df_x2], axis=1)\n\n# Algoritmo de descenso del gradiente\n\n\ndef descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):\n    dim = 2\n    X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]\n    trayectorias = [X.copy()]\n\n    for _ in range(num_iteraciones):\n        X = X - learning_rate * gradiente(input=X)\n        trayectorias.append(X.copy())\n        \n    return np.array(trayectorias)\n\n\n# Ejecuci\u00f3n del descenso del gradiente\ntrayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)\n\n# Visualizaci\u00f3n de trayectorias en el espacio 2D\nfor i in range(trayectoria.shape[1]):\n    plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Trayectorias del descenso del gradiente\")\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np  # Definici\u00f3n de la funci\u00f3n   def function(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"     return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])  # C\u00e1lculo del gradiente (derivadas parciales)   def gradiente(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"          df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])     df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])      return np.stack([df_x1, df_x2], axis=1)  # Algoritmo de descenso del gradiente   def descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):     dim = 2     X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]     trayectorias = [X.copy()]      for _ in range(num_iteraciones):         X = X - learning_rate * gradiente(input=X)         trayectorias.append(X.copy())              return np.array(trayectorias)   # Ejecuci\u00f3n del descenso del gradiente trayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)  # Visualizaci\u00f3n de trayectorias en el espacio 2D for i in range(trayectoria.shape[1]):     plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.title(\"Trayectorias del descenso del gradiente\") plt.show() In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\n</pre> # 3pps import matplotlib.pyplot as plt import torch In\u00a0[\u00a0]: Copied! <pre>tiempo = torch.arange(0, 20).float()\ntiempo\n</pre> tiempo = torch.arange(0, 20).float() tiempo <pre>tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])</pre> In\u00a0[\u00a0]: Copied! <pre>velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1\nplt.scatter(tiempo, velocidad)\n</pre> velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1 plt.scatter(tiempo, velocidad) <pre>&lt;matplotlib.collections.PathCollection at 0x12ad8a150&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>velocidad.shape, tiempo.shape\n</pre> velocidad.shape, tiempo.shape <pre>(torch.Size([20]), torch.Size([20]))</pre> In\u00a0[\u00a0]: Copied! <pre>def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:\n    a, b, c = parametros\n    return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c\n</pre> def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:     a, b, c = parametros     return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c In\u00a0[\u00a0]: Copied! <pre>def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:\n    return (real - predicted).square().mean()\n</pre> def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:     return (real - predicted).square().mean() In\u00a0[\u00a0]: Copied! <pre>parametros = torch.randn(3).requires_grad_()\nparametros\n</pre> parametros = torch.randn(3).requires_grad_() parametros <pre>tensor([-0.7357,  0.2811,  0.4520], requires_grad=True)</pre> In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones <pre>tensor([ 4.5200e-01, -2.6610e-03, -1.9288e+00, -5.3263e+00, -1.0195e+01,\n        -1.6536e+01, -2.4348e+01, -3.3631e+01, -4.4385e+01, -5.6612e+01,\n        -7.0309e+01, -8.5478e+01, -1.0212e+02, -1.2023e+02, -1.3981e+02,\n        -1.6087e+02, -1.8339e+02, -2.0739e+02, -2.3286e+02, -2.5980e+02],\n       grad_fn=&lt;AddBackward0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>def show_preds(tiempo, real, preds: torch.Tensor):\n    plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")\n    plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")\n    plt.legend()\n    plt.show()\n\n\nshow_preds(tiempo, velocidad, predicciones)\n</pre> def show_preds(tiempo, real, preds: torch.Tensor):     plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")     plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")     plt.legend()     plt.show()   show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>perdida = loss_function(predicciones, velocidad)\nperdida\n</pre> perdida = loss_function(predicciones, velocidad) perdida <pre>tensor(20906.3555, grad_fn=&lt;MeanBackward0&gt;)</pre> <p>Aplicamos backward y comprobamos los gradientes</p> In\u00a0[\u00a0]: Copied! <pre>perdida.backward()\nparametros.grad\n</pre> perdida.backward() parametros.grad <pre>tensor([-47704.9453,  -3050.2773,   -226.2895])</pre> <p>Podemos utilizar un ratio de aprendizaje, actualizar el gradiente a partir de ese ratio y volver a colocar 0 en los gradientes para realizar una nueva evaluaci\u00f3n</p> In\u00a0[\u00a0]: Copied! <pre>lr = 1e-5\nparametros.data = parametros.data - lr * parametros.grad.data\nparametros.grad = None\n</pre> lr = 1e-5 parametros.data = parametros.data - lr * parametros.grad.data parametros.grad = None In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones <pre>tensor([ 4.5426e-01,  5.0715e-01,  4.2707e-02, -9.3908e-01, -2.4382e+00,\n        -4.4547e+00, -6.9884e+00, -1.0040e+01, -1.3608e+01, -1.7694e+01,\n        -2.2297e+01, -2.7417e+01, -3.3055e+01, -3.9210e+01, -4.5883e+01,\n        -5.3073e+01, -6.0780e+01, -6.9004e+01, -7.7746e+01, -8.7005e+01],\n       grad_fn=&lt;AddBackward0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>show_preds(tiempo, velocidad, predicciones)\n</pre> show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):\n    predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)\n    perdida = loss_function(predicted=predicciones, real=datos_a_predecir)\n    perdida.backward()\n\n    # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles\n    with torch.no_grad():\n        parametros_aprendibles -= lr * parametros_aprendibles.grad\n    \n    # Otra forma de resetear los gradientes\n    parametros_aprendibles.grad.zero_()\n\n    show_preds(tiempo, datos_a_predecir, predicciones)\n    return predicciones, parametros_aprendibles, perdida\n</pre> def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):     predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)     perdida = loss_function(predicted=predicciones, real=datos_a_predecir)     perdida.backward()      # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles     with torch.no_grad():         parametros_aprendibles -= lr * parametros_aprendibles.grad          # Otra forma de resetear los gradientes     parametros_aprendibles.grad.zero_()      show_preds(tiempo, datos_a_predecir, predicciones)     return predicciones, parametros_aprendibles, perdida In\u00a0[\u00a0]: Copied! <pre># 3pps\nfrom tqdm import tqdm\n</pre> # 3pps from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>num_epochs = 20\nparametros_aprendibles = torch.randn(3, requires_grad=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    predicciones, parametros_aprendibles, perdida = apply_step_training(\n        tiempo=tiempo, \n        parametros_aprendibles=parametros_aprendibles, \n        datos_a_predecir=velocidad\n    )\n    print(f\"Epoch {epoch+1}, perdida: {perdida}\")\n</pre> num_epochs = 20 parametros_aprendibles = torch.randn(3, requires_grad=True)  for epoch in tqdm(range(num_epochs)):     predicciones, parametros_aprendibles, perdida = apply_step_training(         tiempo=tiempo,          parametros_aprendibles=parametros_aprendibles,          datos_a_predecir=velocidad     )     print(f\"Epoch {epoch+1}, perdida: {perdida}\") <pre>  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> <pre>  5%|\u258c         | 1/20 [00:00&lt;00:03,  5.12it/s]</pre> <pre>Epoch 1, perdida: 18470.908203125\n</pre> <pre> 10%|\u2588         | 2/20 [00:00&lt;00:03,  5.29it/s]</pre> <pre>Epoch 2, perdida: 4058.17822265625\n</pre> <pre> 15%|\u2588\u258c        | 3/20 [00:00&lt;00:03,  5.32it/s]</pre> <pre>Epoch 3, perdida: 1330.846923828125\n</pre> <pre> 20%|\u2588\u2588        | 4/20 [00:00&lt;00:02,  5.37it/s]</pre> <pre>Epoch 4, perdida: 814.7498168945312\n</pre> <pre> 25%|\u2588\u2588\u258c       | 5/20 [00:00&lt;00:03,  4.80it/s]</pre> <pre>Epoch 5, perdida: 717.0855102539062\n</pre> <pre> 30%|\u2588\u2588\u2588       | 6/20 [00:01&lt;00:02,  4.99it/s]</pre> <pre>Epoch 6, perdida: 698.6016845703125\n</pre> <pre> 35%|\u2588\u2588\u2588\u258c      | 7/20 [00:01&lt;00:02,  5.01it/s]</pre> <pre>Epoch 7, perdida: 695.1009521484375\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 8/20 [00:01&lt;00:02,  5.03it/s]</pre> <pre>Epoch 8, perdida: 694.4356689453125\n</pre> <pre> 45%|\u2588\u2588\u2588\u2588\u258c     | 9/20 [00:01&lt;00:02,  5.07it/s]</pre> <pre>Epoch 9, perdida: 694.3067626953125\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 10/20 [00:01&lt;00:01,  5.08it/s]</pre> <pre>Epoch 10, perdida: 694.2796020507812\n</pre> <pre> 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 11/20 [00:02&lt;00:01,  5.10it/s]</pre> <pre>Epoch 11, perdida: 694.271484375\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 12/20 [00:02&lt;00:01,  4.25it/s]</pre> <pre>Epoch 12, perdida: 694.26708984375\n</pre> <pre> 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 13/20 [00:02&lt;00:01,  4.50it/s]</pre> <pre>Epoch 13, perdida: 694.2633056640625\n</pre> <pre> 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 14/20 [00:02&lt;00:01,  4.25it/s]</pre> <pre>Epoch 14, perdida: 694.2596435546875\n</pre> <pre> 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 15/20 [00:03&lt;00:01,  4.55it/s]</pre> <pre>Epoch 15, perdida: 694.256103515625\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 16/20 [00:03&lt;00:00,  4.69it/s]</pre> <pre>Epoch 16, perdida: 694.2525634765625\n</pre> <pre> 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 17/20 [00:03&lt;00:00,  4.81it/s]</pre> <pre>Epoch 17, perdida: 694.2490234375\n</pre> <pre> 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 18/20 [00:03&lt;00:00,  5.01it/s]</pre> <pre>Epoch 18, perdida: 694.245361328125\n</pre> <pre> 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:03&lt;00:00,  4.94it/s]</pre> <pre>Epoch 19, perdida: 694.2418212890625\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04&lt;00:00,  4.88it/s]</pre> <pre>Epoch 20, perdida: 694.2382202148438\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:\n\n    # (tensor_entrada) -&gt; (B, N)\n    # peso -&gt; (B, N, 1)\n    # (N) \n    return tensor_entrada @ w + b\n\n\nclass CapaLineal:\n\n    def __init__(self, shape_entrada: int) -&gt; None:\n\n        self.w = torch.randn()\n</pre> def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:      # (tensor_entrada) -&gt; (B, N)     # peso -&gt; (B, N, 1)     # (N)      return tensor_entrada @ w + b   class CapaLineal:      def __init__(self, shape_entrada: int) -&gt; None:          self.w = torch.randn()"},{"location":"notebooks/Mathematics/gradient_descent.html#example-1","title":"Example 1\u00b6","text":""},{"location":"notebooks/Mathematics/gradient_descent.html#example-2","title":"Example 2\u00b6","text":""},{"location":"notebooks/Time%20Series/gramian_angular_field.html","title":"Gramian Angular Field","text":"<p>Gramian Angular Difference Field (GADF) permite codificar series temporales a im\u00e1genes, permiten realizar una interpretaci\u00f3n en una imagen en 2D de una serie temporal univariable.</p> <p>Podemos convertir series temporales, representadas en un eje de coordenadas cartesianas, donde tenemos un valor en el eje y que es el valor de variable en si, en el eje x tenemos el tiempo que transcurre y como varia esa variable en el tiempo.</p> <p></p> <p>Para convertir a coordenadas polares, representado tal que (r, \u03b8), donde:</p> <ul> <li>r (radio) representa la distancia desde el origen.</li> <li>\u03b8 (\u00e1ngulo) representa la direcci\u00f3n del punto respecto a un eje de referencia (como el eje x).</li> </ul> <p>Podemos convertir a coordenadas polares de 2 formas:</p> <ul> <li>La primera ser\u00eda considerar una se\u00f1al peri\u00f3dica, como un seno o coseno, que parta desde 0 hasta 2pi, este es el periodo de la se\u00f1al y luego tendr\u00edamos que el radio es el valor en si. Visualizar ciclos o estacionalidades (en el m\u00e9todo 1).</li> <li>La segunda es medir la diferencia de tiempo que existe entre un instante t y un instante t + 1, y luego se calcula la diferencia del valor de la variable como un delta, y se calcula el angulo entre la delta de tiempo y la delta de la variable. Por tanto ser\u00eda algo como: \u03b8 = arctan(\u0394y / \u0394t), r = sqrt((\u0394t)\u00b2 + (\u0394y)\u00b2) que mide la magnitud del cambio. Detectar patrones direccionales, como si los cambios tienen una orientaci\u00f3n predominante (en el m\u00e9todo 2).</li> </ul> <p></p> <p>En este caso, como la distribuci\u00f3n del tiempo es discreta, que representa los meses, podemos convertirlo a grados haciendo:</p> <ul> <li>2 * math.pi * (t / max(t)) En el caso de tener un formato basado en datetime, con HH:MM:SS, habr\u00eda que normalizar respecto al tiempo:</li> <li>2 * math.pi * (t / 24 horas * 3600 segundos), donde t es el tiempo desde las 00:00:00 en segundos</li> </ul> In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\n\n# Datos sint\u00e9ticos\n# Tiempo (por ejemplo, meses)\nmonths = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n# Valores sint\u00e9ticos (algo como una onda)\nvalues = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n\n# M\u00e9todo 1 ----------\ntheta_1 = [2 * math.pi * (month / len(months)) for month in months]\nprint(theta_1)\n\nrho_1 = values\nprint(rho_1)\n\ncoord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)]\nprint(coord_method1)\n\n# M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ----------\n# Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos\n# arrays tienen la misma cantidad de elementos\ntheta_2 = []\nrho_2 = []\nfor index, month in enumerate(months):\n    if index + 1 &lt; len(values):\n        delta_values = values[index + 1] - values[index]\n        delta_time = months[index + 1] - month\n\n        # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que\n        # saturar el valor entre 2pi y -2pi\n        if delta_time != 0:\n            theta_2.append(math.atan(delta_values / delta_time))\n        else:\n            theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)\n\n        rho_2.append(math.sqrt(delta_values**2 + delta_time**2))\nprint(\"\\n\")\nprint(theta_2)\nprint(rho_2)\ncoord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)]\nprint(coord_method2)\n</pre> # Standard libraries import math  # Datos sint\u00e9ticos # Tiempo (por ejemplo, meses) months = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Valores sint\u00e9ticos (algo como una onda) values = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]  # M\u00e9todo 1 ---------- theta_1 = [2 * math.pi * (month / len(months)) for month in months] print(theta_1)  rho_1 = values print(rho_1)  coord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)] print(coord_method1)  # M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ---------- # Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos # arrays tienen la misma cantidad de elementos theta_2 = [] rho_2 = [] for index, month in enumerate(months):     if index + 1 &lt; len(values):         delta_values = values[index + 1] - values[index]         delta_time = months[index + 1] - month          # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que         # saturar el valor entre 2pi y -2pi         if delta_time != 0:             theta_2.append(math.atan(delta_values / delta_time))         else:             theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)          rho_2.append(math.sqrt(delta_values**2 + delta_time**2)) print(\"\\n\") print(theta_2) print(rho_2) coord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)] print(coord_method2) <pre>[0.0, 0.5235987755982988, 1.0471975511965976, 1.5707963267948966, 2.0943951023931953, 2.6179938779914944, 3.141592653589793, 3.6651914291880923, 4.1887902047863905, 4.71238898038469, 5.235987755982989, 5.759586531581287]\n[5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n[(0.0, 5), (0.5235987755982988, 6), (1.0471975511965976, 8), (1.5707963267948966, 9), (2.0943951023931953, 10), (2.6179938779914944, 9), (3.141592653589793, 8), (3.6651914291880923, 6), (4.1887902047863905, 4), (4.71238898038469, 3), (5.235987755982989, 4), (5.759586531581287, 5)]\n\n\n[0.7853981633974483, 1.1071487177940906, 0.7853981633974483, 0.7853981633974483, -0.7853981633974483, -0.7853981633974483, -1.1071487177940906, -1.1071487177940906, -0.7853981633974483, 0.7853981633974483, 0.7853981633974483]\n[1.4142135623730951, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 2.23606797749979, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951]\n[(0.7853981633974483, 1.4142135623730951), (1.1071487177940906, 2.23606797749979), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-1.1071487177940906, 2.23606797749979), (-1.1071487177940906, 2.23606797749979), (-0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951)]\n</pre> In\u00a0[2]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\n\n# M\u00e9todo 1\nplt.subplot(1, 2, 1, projection=\"polar\")\nplt.plot(theta_1, rho_1, marker=\"o\")\nplt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")\n\n# M\u00e9todo 2\nplt.subplot(1, 2, 2, projection=\"polar\")\nplt.plot(theta_2, rho_2, marker=\"o\")\nplt.title(\"M\u00e9todo 2: Cambio como vector\")\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt  # M\u00e9todo 1 plt.subplot(1, 2, 1, projection=\"polar\") plt.plot(theta_1, rho_1, marker=\"o\") plt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")  # M\u00e9todo 2 plt.subplot(1, 2, 2, projection=\"polar\") plt.plot(theta_2, rho_2, marker=\"o\") plt.title(\"M\u00e9todo 2: Cambio como vector\")  plt.tight_layout() plt.show() <p>Continuando con Gramian, tenemos que la matriz Gramian es una matriz que consiste en realizar el producto vectorial entre cada pareja de vectores.</p> <p></p> <p>La matriz de Gram preserva la dependencia temporal, pues el tiempo incrementa del mismo modo que lo hace la posici\u00f3n de la matriz 2D de arriba a la izquierda y de arriba a la derecha, por lo que el tiempo se codifica en la geometr\u00eda de la matriz. Es decir, la matriz mantiene las relaciones angulares entre todos los puntos de la serie. Para ello se siguen los pasos siguientes:</p> <ul> <li><p>Paso 1: Normaliza la serie: Primero necesitas normalizar tu serie a un rango de $[-1, 1]$ (porque luego aplicaremos el arccos):</p> <p>$$   \\tilde{x}_i = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)} \\times 2 - 1   $$</p> </li> <li><p>Paso 2: Convierte los valores a \u00e1ngulos. Para cada valor de la serie normalizada:</p> <p>$$   \\phi_i = \\arccos(\\tilde{x}_i)   $$</p> <p>Esto convierte cada valor en un \u00e1ngulo entre $0$ y $\\pi$, que representa su posici\u00f3n relativa dentro del ciclo.</p> </li> <li><p>Paso 3: Construye la matriz GADF</p> <p>La idea es comparar cada par de puntos $(\\phi_i, \\phi_j)$ de la serie y calcular:</p> <p>$$   \\text{GADF}[i,j] = \\sin(\\phi_i - \\phi_j)   $$</p> <ul> <li>Esto mide la diferencia angular entre dos puntos.</li> <li>El resultado es una matriz cuadrada $N \\times N$ que puedes tratar como una imagen.</li> </ul> </li> </ul> <p>Tambi\u00e9n, existe el Gramian Angular Summation Field (GASF) usa la suma en vez de la diferencia:</p> <p>$$ \\text{GASF}[i,j] = \\cos(\\phi_i + \\phi_j) $$</p> In\u00a0[5]: Copied! <pre># Standard libraries\nimport math\nimport random\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef normalization(samples: list) -&gt; list:\n\n    min_val = min(samples)\n    max_val = max(samples)\n\n    return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]\n\n\n@njit\ndef degrees(samples: list) -&gt; list:\n\n    return [math.acos(sample) for sample in samples]\n\n\n@njit\ndef gadf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.sin(sample_i - sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n@njit\ndef gasf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.cos(sample_i + sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n# Datos temporales (por ejemplo, 16 puntos en el tiempo)\nx = random.sample(range(1, 4000), 2400)\nx_gadf_matrix = gadf_matrix(samples=x)\nx_gasf_matrix = gasf_matrix(samples=x)\n\nprint(x_gadf_matrix.shape)\nprint(x_gasf_matrix.shape)\n\nplt.imshow(x_gadf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n\nplt.imshow(x_gasf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n</pre> # Standard libraries import math import random  # 3pps import matplotlib.pyplot as plt import numpy as np from numba import njit   @njit def normalization(samples: list) -&gt; list:      min_val = min(samples)     max_val = max(samples)      return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]   @njit def degrees(samples: list) -&gt; list:      return [math.acos(sample) for sample in samples]   @njit def gadf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.sin(sample_i - sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   @njit def gasf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.cos(sample_i + sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   # Datos temporales (por ejemplo, 16 puntos en el tiempo) x = random.sample(range(1, 4000), 2400) x_gadf_matrix = gadf_matrix(samples=x) x_gasf_matrix = gasf_matrix(samples=x)  print(x_gadf_matrix.shape) print(x_gasf_matrix.shape)  plt.imshow(x_gadf_matrix, origin=\"upper\") plt.colorbar() plt.show()  plt.imshow(x_gasf_matrix, origin=\"upper\") plt.colorbar() plt.show() <pre>(2400, 2400)\n(2400, 2400)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Time%20Series/gramian_angular_field.html#gramian-angular-field","title":"Gramian Angular Field\u00b6","text":""},{"location":"notebooks/Time%20Series/gramian_angular_field.html#coordenadas-polares","title":"Coordenadas polares\u00b6","text":""},{"location":"repo/cv/layers.html","title":"Layers","text":""},{"location":"repo/cv/layers.html#src.cv.layers.aps","title":"<code>aps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214</p>"},{"location":"repo/cv/layers.html#src.cv.layers.aps.AdaptivePolyphaseSampling","title":"<code>AdaptivePolyphaseSampling(norm=2)</code>","text":"<p>Initializes the class with normalization option.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>int | float | Literal['fro', 'nuc', 'inf', '-inf'] | None</code> <p>Normalization type or value, defaults to 2.</p> <code>2</code> Source code in <code>src/cv/layers/aps.py</code> <pre><code>def __init__(\n    self,\n    norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes the class with normalization option.\n\n    Args:\n        norm: Normalization type or value, defaults to 2.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n    self.norm = norm\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.aps.AdaptivePolyphaseSampling.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input tensor to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Output tensor, optionally with index if return_index is True.</p> Source code in <code>src/cv/layers/aps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input tensor to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Output tensor, optionally with index if return_index is True.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Extraemos las dimensiones\n    b, p, _, _, _ = polyphase_combined.size()\n\n    # Combinamos los valores de los canales, altura y anchura del tensor\n    polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = torch.linalg.vector_norm(\n        input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)\n    )\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.lps","title":"<code>lps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001</p>"},{"location":"repo/cv/layers.html#src.cv.layers.lps.LearnablePolyphaseSampling","title":"<code>LearnablePolyphaseSampling(channel_size, hidden_size)</code>","text":"<p>Initializes the model with specified channel and hidden sizes.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of input channels for the Conv2D layer.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden units for the Conv2D layer.</p> required Source code in <code>src/cv/layers/lps.py</code> <pre><code>def __init__(self, channel_size: int, hidden_size: int) -&gt; None:\n    \"\"\"\n    Initializes the model with specified channel and hidden sizes.\n\n    Args:\n        channel_size: Number of input channels for the Conv2D layer.\n        hidden_size: Number of hidden units for the Conv2D layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n\n    # Definimos el modelo \u00fanico para cada componente\n    self.conv_model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=channel_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.Flatten(),\n        nn.AdaptiveAvgPool2d(1),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.lps.LearnablePolyphaseSampling.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Tensor of dominant component, optionally with index.</p> Source code in <code>src/cv/layers/lps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Tensor of dominant component, optionally with index.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Utilizamos el modelo basado en convoluciones por cada componente\n    _logits = []\n    for polyphase in range(polyphase_combined.size()[1]):\n        _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))\n    logits = torch.squeeze(torch.stack(_logits))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.se","title":"<code>se</code>","text":"<p>Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507</p>"},{"location":"repo/cv/layers.html#src.cv.layers.se.SqueezeExcitation","title":"<code>SqueezeExcitation(channel_size, ratio)</code>","text":"<p>Implements Squeeze-and-Excitation (SE) block.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of channels in the input tensor.</p> required <code>ratio</code> <code>int</code> <p>Reduction factor for the compression layer.</p> required Source code in <code>src/cv/layers/se.py</code> <pre><code>def __init__(self, channel_size: int, ratio: int) -&gt; None:\n    \"\"\"\n    Implements Squeeze-and-Excitation (SE) block.\n\n    Args:\n        channel_size: Number of channels in the input tensor.\n        ratio: Reduction factor for the compression layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Vamos a crear un modelo Sequential\n    self.se_block = nn.Sequential(\n        nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)\n        nn.Flatten(),  # (B, C)\n        nn.Linear(\n            in_features=channel_size, out_features=channel_size // ratio\n        ),  # (B, C//ratio)\n        nn.ReLU(),  # (B, C//ratio)\n        nn.Linear(\n            in_features=channel_size // ratio, out_features=channel_size\n        ),  # (B, C)\n        nn.Sigmoid(),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.se.SqueezeExcitation.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Applies attention mechanism to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor with shape (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with attention applied, same shape as input.</p> Source code in <code>src/cv/layers/se.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies attention mechanism to input tensor.\n\n    Args:\n        input_tensor: Input tensor with shape (B, C, H, W).\n\n    Returns:\n        Tensor with attention applied, same shape as input.\n    \"\"\"\n\n    # Primero podemos obtener el tama\u00f1o del tensor de entrada\n    b, c, _, _ = input_tensor.size()\n\n    # Obtenemos el tensor de aplicar SE\n    x = self.se_block(input_tensor)\n\n    # Modificamos el shape del tensor para ajustarlo al input\n    x = x.view(b, c, 1, 1)\n\n    # Aplicamos el producto como mecanismo de atenci\u00f3n\n    return x * input_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit","title":"<code>vit</code>","text":""},{"location":"repo/cv/layers.html#src.cv.layers.vit.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Initialize encoder block module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize encoder block module.\n\n    Args:\n        d_model: Number of features in input.\n        d_ff: Hidden layer feature dimensions.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Process input tensor through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after encoder block processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through encoder block.\n\n    Args:\n        input_tensor: Batch of input tensors.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after encoder block processing.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Initialize feed-forward neural network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Input and output feature dimensions.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize feed-forward neural network.\n\n    Args:\n        d_model: Input and output feature dimensions.\n        d_ff: Hidden layer feature dimensions.\n        dropout_rate: Dropout rate applied on layers.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.GELU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through feed-forward layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after feed-forward processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through feed-forward layers.\n\n    Args:\n        input_tensor: Batch of input tensors.\n\n    Returns:\n        Output tensor after feed-forward processing.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Initialize layer normalization module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-06</code> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initialize layer normalization module.\n\n    Args:\n        features: Number of features in input.\n        eps: Small value to avoid division by zero.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Apply layer normalization to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized embeddings.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply layer normalization to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Normalized embeddings.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Initialize multi-head attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on scores.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize multi-head attention module.\n\n    Args:\n        d_model: Number of features in input.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate applied on scores.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Compute attention scores and output.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Dropout layer, optional.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Compute attention scores and output.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n        dropout: Dropout layer, optional.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Process input tensors through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after attention processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensors through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after attention processing.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PatchEmbedding","title":"<code>PatchEmbedding(patch_size_height, patch_size_width, in_channels, d_model)</code>","text":"<p>Initialize patch embedding module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    in_channels: int,\n    d_model: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch embedding module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        in_channels: Number of input channels.\n        d_model: Dimension of the model.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.in_channels = in_channels\n    self.d_model = d_model\n\n    # Esta es una de las diferencias con usar transformers en el texto\n    # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n\n    # de los pixeles\n    self.embedding = nn.Linear(\n        in_features=self.in_channels\n        * self.patch_size_height\n        * self.patch_size_width,\n        out_features=self.d_model,\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PatchEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Apply linear projection to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of image patches as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after linear projection of patches.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply linear projection to input tensor.\n\n    Args:\n        input_tensor: Batch of image patches as a tensor.\n\n    Returns:\n        Tensor after linear projection of patches.\n    \"\"\"\n\n    return self.embedding(input_tensor)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.Patches","title":"<code>Patches(patch_size_height, patch_size_width, img_height, img_width)</code>","text":"<p>Initialize patch extraction module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of the input image.</p> required <code>img_width</code> <code>int</code> <p>Width of the input image.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If img_height not divisible by patch height.</p> <code>ValueError</code> <p>If img_width not divisible by patch width.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch extraction module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of the input image.\n        img_width: Width of the input image.\n\n    Raises:\n        ValueError: If img_height not divisible by patch height.\n        ValueError: If img_width not divisible by patch width.\n    \"\"\"\n\n    super().__init__()\n\n    if img_height % patch_size_height != 0:\n        raise ValueError(\n            \"img_height tiene que se divisible entre el patch_size_height\"\n        )\n\n    if img_width % patch_size_width != 0:\n        raise ValueError(\n            \"img_width tiene que se divisible entre el patch_size_width\"\n        )\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.unfold = nn.Unfold(\n        kernel_size=(self.patch_size_height, self.patch_size_width),\n        stride=(self.patch_size_height, self.patch_size_width),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.Patches.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Extract patches from input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of images as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with patches from input images.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Extract patches from input tensor.\n\n    Args:\n        input_tensor: Batch of images as a tensor.\n\n    Returns:\n        Tensor with patches from input images.\n    \"\"\"\n\n    # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n    patches = self.unfold(input_tensor)\n    # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n    return patches.transpose(2, 1)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Initialize positional encoding module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>sequence_length</code> <code>int</code> <p>Max length of input sequences.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on outputs.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize positional encoding module.\n\n    Args:\n        d_model: Dimension of the model.\n        sequence_length: Max length of input sequences.\n        dropout_rate: Dropout rate applied on outputs.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Add positional encoding to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Embeddings with added positional encoding.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Add positional encoding to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Initialize residual connection module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for sublayer output.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize residual connection module.\n\n    Args:\n        features: Number of features in input.\n        dropout_rate: Dropout rate for sublayer output.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Apply residual connection to sublayer output.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Original input tensor.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer module to apply.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Apply residual connection to sublayer output.\n\n    Args:\n        input_tensor: Original input tensor.\n        sublayer: Sublayer module to apply.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.VisionTransformer","title":"<code>VisionTransformer(patch_size_height, patch_size_width, img_height, img_width, in_channels, num_encoders, d_model, d_ff, h, num_classes, dropout_rate)</code>","text":"<p>Initialize Vision Transformer (VIT).</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of input images.</p> required <code>img_width</code> <code>int</code> <p>Width of input images.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>d_ff</code> <code>int</code> <p>Dimension of feed-forward layers.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n    in_channels: int,\n    num_encoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    num_classes: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initialize Vision Transformer (VIT).\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of input images.\n        img_width: Width of input images.\n        in_channels: Number of input channels.\n        num_encoders: Number of encoder blocks.\n        d_model: Dimension of the model.\n        d_ff: Dimension of feed-forward layers.\n        h: Number of attention heads.\n        num_classes: Number of output classes.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.img_height = img_height\n    self.img_width = img_width\n    self.in_channels = in_channels\n    self.num_encoders = num_encoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.num_classes = num_classes\n    self.dropout_rate = dropout_rate\n\n    # N\u00famero de patches\n    self.num_patches = (img_height // patch_size_height) * (\n        img_width // patch_size_width\n    )\n\n    # CLS token permite tener una representaci\u00f3n global de todos los inputs\n    # de la imagen (de los diferentes embeddings de cada patch)\n    self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n\n    self.patch_layer = Patches(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        img_height=self.img_height,\n        img_width=self.img_width,\n    )\n\n    self.embeddings = PatchEmbedding(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        in_channels=self.in_channels,\n        d_model=self.d_model,\n    )\n\n    # Entiendo que la longitud de la secuencia coincide con el numero de patches\n    # y un embedding m\u00e1s de la clase,\n    self.positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.num_patches + 1,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    self.layer_norm = LayerNormalization(features=self.d_model)\n\n    self.mlp_classifier = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_model),\n        nn.GELU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=self.d_model, out_features=num_classes),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.VisionTransformer.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through VIT model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Classification output tensor.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through VIT model.\n\n    Args:\n        input_tensor: Batch of input images.\n\n    Returns:\n        Classification output tensor.\n    \"\"\"\n\n    # Extraemos los patches\n    input_patches = self.patch_layer(input_tensor)\n\n    # Convertimso a embeddings los patches\n    patch_embeddings = self.embeddings(input_patches)\n\n    # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia\n    # (B, 1, d_model)\n    cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n    # (B, num_patches+1, d_model)\n    embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n\n    # A\u00f1adir positional encoding\n    embeddings = self.positional_encoding(embeddings)\n\n    # Encoders del transformer\n    encoder_output = embeddings\n    for encoder_layer in self.encoder_layers:\n        encoder_output = encoder_layer(encoder_output)\n\n    # Usar solo el CLS token para clasificaci\u00f3n\n    encoder_output = self.layer_norm(encoder_output)\n    cls_output = encoder_output[:, 0]\n\n    # Clasificaci\u00f3n final\n    return self.mlp_classifier(cls_output)\n</code></pre>"},{"location":"repo/cv/models.html","title":"Models","text":""},{"location":"repo/cv/models.html#src.cv.models.vq_vae","title":"<code>vq_vae</code>","text":""},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Decoder","title":"<code>Decoder(in_channels, num_residuals, out_channels=3, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes a decoder with residual blocks and transpose convolutional layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the decoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the decoder.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels, e.g., RGB.</p> <code>3</code> <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    out_channels: int = 3,  # Channel output (RGB)\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes a decoder with residual blocks and transpose\n    convolutional layers.\n\n    Args:\n        in_channels: Number of input channels to the decoder.\n        num_residuals: Number of residual blocks in the decoder.\n        out_channels: Number of output channels, e.g., RGB.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.out_channels = out_channels\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(\n                in_channels=self.in_channels, hidden_size=self.hidden_size\n            )\n            for _ in range(self.num_residuals)\n        ]\n    )\n\n    self.model = nn.Sequential(\n        nn.ConvTranspose2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=self.hidden_size,\n            out_channels=self.out_channels,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Decoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the decoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by residual blocks and transpose</p> <code>Tensor</code> <p>convolutional layers.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the decoder.\n\n    Args:\n        input_tensor: The input tensor to the decoder.\n\n    Returns:\n        A tensor processed by residual blocks and transpose\n        convolutional layers.\n    \"\"\"\n\n    decoder_output = input_tensor\n    for res_block in self.residual_blocks:\n        decoder_output = res_block(decoder_output)\n\n    return self.model(decoder_output)\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Encoder","title":"<code>Encoder(in_channels, num_residuals, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes an encoder with convolutional layers and residual blocks.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the encoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the encoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes an encoder with convolutional layers and residual\n    blocks.\n\n    Args:\n        in_channels: Number of input channels to the encoder.\n        num_residuals: Number of residual blocks in the encoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n    )\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n            for _ in range(self.num_residuals)\n        ]\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Encoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the encoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by convolutional layers and residual</p> <code>Tensor</code> <p>blocks.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the encoder.\n\n    Args:\n        input_tensor: The input tensor to the encoder.\n\n    Returns:\n        A tensor processed by convolutional layers and residual\n        blocks.\n    \"\"\"\n\n    encoder_output = self.model(input_tensor)\n    for res_block in self.residual_blocks:\n        encoder_output = res_block(encoder_output)\n    return encoder_output\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.ResidualBlock","title":"<code>ResidualBlock(in_channels, hidden_size=256)</code>","text":"<p>Initializes a residual block that applies two convolutional layers and ReLU activations.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the block.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in the hidden layer.</p> <code>256</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:\n    \"\"\"\n    Initializes a residual block that applies two convolutional\n    layers and ReLU activations.\n\n    Args:\n        in_channels: Number of input channels for the block.\n        hidden_size: Number of channels in the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.hidden_size = hidden_size\n\n    self.res_block = nn.Sequential(\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.hidden_size,\n            out_channels=self.in_channels,\n            kernel_size=1,\n            stride=1,\n            bias=False,\n        ),\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.ResidualBlock.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the residual block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the block.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the sum of the input tensor and the</p> <code>Tensor</code> <p>block's output.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the residual block.\n\n    Args:\n        input_tensor: The input tensor to the block.\n\n    Returns:\n        A tensor that is the sum of the input tensor and the\n        block's output.\n    \"\"\"\n\n    return input_tensor + self.res_block(input_tensor)\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VQVAE","title":"<code>VQVAE(in_channels, size_discrete_space, size_embeddings, num_residuals, hidden_size, kernel_size, stride, beta=0.25)</code>","text":"<p>Initializes a VQ-VAE model with encoder, decoder, and quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the model.</p> required <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in encoder/decoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> required <code>kernel_size</code> <code>int</code> <p>Size of convolutional kernels.</p> required <code>stride</code> <code>int</code> <p>Stride of convolutional kernels.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    size_discrete_space: int,\n    size_embeddings: int,\n    num_residuals: int,\n    hidden_size: int,\n    kernel_size: int,\n    stride: int,\n    beta: float = 0.25,\n) -&gt; None:\n    \"\"\"\n    Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n\n    Args:\n        in_channels: Number of input channels for the model.\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        num_residuals: Number of residual blocks in encoder/decoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of convolutional kernels.\n        stride: Stride of convolutional kernels.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.beta = beta\n\n    self.encoder = Encoder(\n        in_channels=self.in_channels,\n        num_residuals=self.num_residuals,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n    self.decoder = Decoder(\n        in_channels=self.hidden_size,\n        num_residuals=self.num_residuals,\n        out_channels=self.in_channels,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n\n    self.vector_quantizer = VectorQuantizer(\n        size_discrete_space=self.size_discrete_space,\n        size_embeddings=self.hidden_size,\n        beta=self.beta,\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VQVAE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through VQ-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, reconstructed tensor,</p> <code>Tensor</code> <p>and perplexity.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass through VQ-VAE model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        A tuple containing VQ loss, reconstructed tensor,\n        and perplexity.\n    \"\"\"\n\n    encoder_output = self.encoder(input_tensor)\n    vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n    decoder_output = self.decoder(quantized)\n    return vq_loss, decoder_output, perplexity\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VectorQuantizer","title":"<code>VectorQuantizer(size_discrete_space, size_embeddings, beta=0.25)</code>","text":"<p>Initializes a vector quantizer with a learnable codebook.</p> <p>Parameters:</p> Name Type Description Default <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n) -&gt; None:\n    \"\"\"\n    Initializes a vector quantizer with a learnable codebook.\n\n    Args:\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.beta = beta\n\n    # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings\n    # Ha de ser una matriz aprendible\n    self.codebook = nn.Embedding(\n        num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n    )\n    # Initialize weights uniformly\n    self.codebook.weight.data.uniform_(\n        -1 / self.size_discrete_space, 1 / self.size_discrete_space\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VectorQuantizer.forward","title":"<code>forward(encoder_output)</code>","text":"<p>Quantizes the encoder output using the codebook.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_output</code> <code>Tensor</code> <p>Tensor of encoder outputs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, quantized tensor, perplexity,</p> <code>Tensor</code> <p>and encodings.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(\n    self, encoder_output: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the encoder output using the codebook.\n\n    Args:\n        encoder_output: Tensor of encoder outputs.\n\n    Returns:\n        A tuple containing VQ loss, quantized tensor, perplexity,\n        and encodings.\n    \"\"\"\n\n    # Comentario de otras implementaciones: The channels are used as the space\n    # in which to quantize.\n    # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)\n    encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n    b, h, w, c = encoder_output.size()\n    encoder_output_flat = encoder_output.reshape(-1, c)\n\n    # Calculamos la distancia entre ambos vectores\n    distances = (\n        torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n        + torch.sum(self.codebook.weight**2, dim=1)\n        - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n    )\n\n    # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n    encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n    # Matriz de ceros de (indices, size_discrete_space)\n    encodings = torch.zeros(\n        encoding_indices.shape[0],\n        self.size_discrete_space,\n        device=encoder_output.device,\n    )\n    # Colocamos un 1 en los indices de los encodings con el\n    # valor m\u00ednimo de distancia creando un vector one-hot\n    encodings.scatter_(1, encoding_indices, 1)\n\n    # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n    # del codebook y le damos formato de nuevo al tensor\n    quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n\n    # VQ-VAE loss terms\n    # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2\n    # FIX: Corrected variable names and loss calculation\n    commitment_loss = F.mse_loss(\n        quantized.detach(), encoder_output\n    )  # ||sg[z_e] - e||^2\n    embedding_loss = F.mse_loss(\n        quantized, encoder_output.detach()\n    )  # ||z_e - sg[e]||^2\n    vq_loss = commitment_loss + self.beta * embedding_loss\n\n    # Straight-through estimator\n    quantized = encoder_output + (quantized - encoder_output).detach()\n\n    # Calculate perplexity\n    avg_probs = torch.mean(encodings, dim=0)\n    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n    # convert quantized from BHWC -&gt; BCHW\n    return (\n        vq_loss,\n        quantized.permute(0, 3, 1, 2).contiguous(),\n        perplexity,\n        encodings,\n    )\n</code></pre>"},{"location":"repo/generative/models.html","title":"Models","text":""},{"location":"repo/generative/models.html#src.generative.models.gan","title":"<code>gan</code>","text":""},{"location":"repo/generative/models.html#src.generative.models.gan.show_generated_samples","title":"<code>show_generated_samples(generator, noise, device, num_samples=16)</code>","text":"<p>Funci\u00f3n auxiliar para mostrar muestras generadas</p> Source code in <code>src/generative/models/gan.py</code> <pre><code>def show_generated_samples(\n    generator: nn.Module, noise, device: str, num_samples: int = 16\n) -&gt; None:\n    \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        samples = generator(noise[:num_samples]).cpu()\n        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(num_samples):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n            axes[row, col].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"repo/nlp/layers.html","title":"Layers","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.moe","title":"<code>moe</code>","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.ExpertModel","title":"<code>ExpertModel(input_dim, output_dim, hidden_dim)</code>","text":"<p>Modelo experto individual para MoE</p> <p>Initializes an expert model with a simple feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output data.</p> required <code>hidden_dim</code> <code>int</code> <p>Dimensionality of the hidden layer.</p> required Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:\n    \"\"\"\n    Initializes an expert model with a simple feed-forward network.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        output_dim: Dimensionality of the output data.\n        hidden_dim: Dimensionality of the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.ExpertModel.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the expert model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's output tensor.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the expert model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        The model's output tensor.\n    \"\"\"\n\n    return self.model(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.Gating","title":"<code>Gating(input_dim, num_experts, dropout_rate=0.2)</code>","text":"<p>Gating mechanism to select experts.</p> <p>Initializes a gating network for expert selection.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts to select from.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> <code>0.2</code> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(\n    self, input_dim: int, num_experts: int, dropout_rate: float = 0.2\n) -&gt; None:\n    \"\"\"\n    Initializes a gating network for expert selection.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        num_experts: Number of experts to select from.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.num_experts = num_experts\n    self.dropout_rate = dropout_rate\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=128),\n        nn.Dropout(self.dropout_rate),\n        nn.LeakyReLU(),\n        nn.Linear(in_features=128, out_features=256),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=256, out_features=128),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=128, out_features=num_experts),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.Gating.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the gating network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the network.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Softmax probabilities for expert selection.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the gating network.\n\n    Args:\n        input_tensor: Input tensor to the network.\n\n    Returns:\n        Softmax probabilities for expert selection.\n    \"\"\"\n\n    return F.softmax(self.model(input_tensor), dim=-1)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.MoE","title":"<code>MoE(trained_experts, input_dim, dropout_rate=0.2)</code>","text":"<p>Mixture of Experts</p> <p>Initializes a mixture of experts with gating.</p> <p>Parameters:</p> Name Type Description Default <code>trained_experts</code> <code>list[ExpertModel]</code> <p>List of trained expert models.</p> required <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout in the gating network.</p> <code>0.2</code> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(\n    self,\n    trained_experts: list[ExpertModel],\n    input_dim: int,\n    dropout_rate: float = 0.2,\n) -&gt; None:\n    \"\"\"\n    Initializes a mixture of experts with gating.\n\n    Args:\n        trained_experts: List of trained expert models.\n        input_dim: Dimensionality of the input data.\n        dropout_rate: Rate of dropout in the gating network.\n    \"\"\"\n\n    super().__init__()\n\n    self.experts = nn.ModuleList(trained_experts)\n    self.num_experts = len(trained_experts)\n    self.input_dim = input_dim\n    self.dropout_rate = dropout_rate\n\n    self.gating_layer = Gating(\n        input_dim=self.input_dim,\n        num_experts=self.num_experts,\n        dropout_rate=self.dropout_rate,\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.MoE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the mixture of experts.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Weighted sum of expert outputs.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the mixture of experts.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        Weighted sum of expert outputs.\n    \"\"\"\n\n    # Obtenemos los pesos del selector\n    expert_weights = self.gating_layer(input_tensor)\n\n    # Obtenemos la salida de todos los expertos\n    _expert_outputs: list[torch.Tensor] = []\n    for expert in self.experts:\n        _expert_outputs.append(expert(input_tensor))\n\n    # Stack de salidas [b, output_dim, num_experts]\n    expert_outputs = torch.stack(_expert_outputs, dim=-1)\n\n    # [b, num_experts] -&gt; [b, 1, num_experts]\n    expert_weights = expert_weights.unsqueeze(1)\n\n    # Suma ponderada de la selecci\u00f3n de expertos\n    # [b, output_dim, num_experts] * [b, 1, num_experts]\n    return torch.sum(expert_outputs * expert_weights, dim=-1)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer","title":"<code>transformer</code>","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.DecoderBlock","title":"<code>DecoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Decoder block with masked attention, cross-attention, and feed-forward layers.</p> <p>Initializes decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes decoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    self.masked_multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_3 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.DecoderBlock.forward","title":"<code>forward(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Forward pass through decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder block.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the decoder block.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,  # M\u00e1scara para el encoder (padding)\n    tgt_mask: torch.Tensor | None = None,  # M\u00e1scara causal para el decoder\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through decoder block.\n\n    Args:\n        decoder_input: Input tensor to the decoder block.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Tensor after processing by the decoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    decoder_input = self.residual_layer_1(\n        decoder_input,\n        lambda x: self.masked_multi_head_attention_layer(\n            k=x, q=x, v=x, mask=tgt_mask\n        ),\n    )\n\n    # Aqu\u00ed tenemos que hacer cross-attention, usamos como K, V los encoder\n    # y Q del decoder\n    decoder_input = self.residual_layer_2(\n        decoder_input,\n        lambda x: self.multi_head_attention_layer(\n            k=encoder_output, q=x, v=encoder_output, mask=src_mask\n        ),\n    )\n\n    decoder_output = self.residual_layer_3(\n        decoder_input, lambda x: self.feed_forward_layer(x)\n    )\n\n    return decoder_output\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Encoder block with attention and feed-forward layers.</p> <p>Initializes encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes encoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Forward pass through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the encoder block.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the encoder block.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through encoder block.\n\n    Args:\n        input_tensor: Input tensor to the encoder block.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after processing by the encoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Feed-forward neural network layer.</p> <p>Initializes feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes feed-forward network.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.ReLU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor processed by feed-forward network.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through feed-forward network.\n\n    Args:\n        input_tensor: Tensor of input embeddings.\n\n    Returns:\n        Tensor processed by feed-forward network.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.InputEmbedding","title":"<code>InputEmbedding(d_model, vocab_size)</code>","text":"<p>Embeds input tokens into vectors of dimension d_model.</p> <p>Initializes input embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes input embedding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    # Utilizamos la capa Embedding de PyTorch que funciona como\n    # una tabal lookup that stores embeddings of a fixed dictionary and size.\n    # Osea que es un diccionario que tiene por cada token, hasta un total de\n    # vocab_size, un vector de tama\u00f1o d_model. En el paper: we use learned\n    # embeddings to convert the input tokens and output tokens to vectors\n    # of dimension dmodel\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.InputEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor of token indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embedded input scaled by sqrt(d_model).</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the embedding layer.\n\n    Args:\n        input_tensor: Input tensor of token indices.\n\n    Returns:\n        Tensor of embedded input scaled by sqrt(d_model).\n    \"\"\"\n\n    # Paper: In the embedding layers, we multiply those weights by sqrt(d_model)\n    # Input_tensor (B, ...) -&gt; (B, ..., d_model)\n    return self.embedding(input_tensor) * math.sqrt(self.d_model)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Applies layer normalization to input embeddings.</p> <p>Initializes layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initializes layer normalization.\n\n    Args:\n        features: Number of features in the input.\n        eps: Small constant for numerical stability.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass for layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for layer normalization.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Applies multi-head attention mechanism.</p> <p>Initializes multi-head attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes multi-head attention layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Computes scaled dot-product attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Optional dropout layer.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Computes scaled dot-product attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n        dropout: Optional dropout layer.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Forward pass through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after attention and concatenation.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after attention and concatenation.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Adds positional encoding to input embeddings.</p> <p>Initializes positional encoding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>sequence_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes positional encoding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        sequence_length: Maximum sequence length.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # # Ahora rellenamos la matriz de posiciones\n    # # La posici\u00f3n va hasta el m\u00e1ximo de la longitud de la secuencia\n    # for pos in range(self.sequence_length):\n    # \tfor i in range(0, d_model, 2):\n    # \t\t# Para las posiciones pares usamos el seno\n    # \t\tpe_matrix[pos, i] = torch.sin(pos / (10000 ** ((2 * i) / d_model)))\n    # \t\t# Para las posiciones impares usamos el coseno\n    # \t\tpe_matrix[pos, i + 1] = torch.cos(\n    # \t\t\tpos / (10000 ** ((2 * (i + 1)) / d_model))\n    # \t\t)\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass to add positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embeddings with added positional encoding.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass to add positional encoding.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Tensor of embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ProjectionLayer","title":"<code>ProjectionLayer(d_model, vocab_size)</code>","text":"<p>Converts d_model dimensions back to vocab_size.</p> <p>Initializes projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes projection layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    super().__init__()\n\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    self.projection_layer = nn.Linear(in_features=d_model, out_features=vocab_size)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ProjectionLayer.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the projection layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with projected dimensions.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through projection layer.\n\n    Args:\n        input_tensor: Input tensor to the projection layer.\n\n    Returns:\n        Tensor with projected dimensions.\n    \"\"\"\n\n    return self.projection_layer(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Applies residual connection around a sublayer.</p> <p>Initializes residual connection layer.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes residual connection layer.\n\n    Args:\n        features: Number of features in the input.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Forward pass using residual connection.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the residual layer.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer to apply within the residual connection.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using residual connection.\n\n    Args:\n        input_tensor: Input tensor to the residual layer.\n        sublayer: Sublayer to apply within the residual connection.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer","title":"<code>Transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, num_encoders, num_decoders, d_model, d_ff, h, dropout_rate)</code>","text":"<p>Transformer model with encoder and decoder blocks.</p> <p>Initializes transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>src_vocab_size</code> <code>int</code> <p>Size of source vocabulary.</p> required <code>tgt_vocab_size</code> <code>int</code> <p>Size of target vocabulary.</p> required <code>src_seq_len</code> <code>int</code> <p>Maximum source sequence length.</p> required <code>tgt_seq_len</code> <code>int</code> <p>Maximum target sequence length.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>num_decoders</code> <code>int</code> <p>Number of decoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(\n    self,\n    src_vocab_size: int,\n    tgt_vocab_size: int,\n    src_seq_len: int,\n    tgt_seq_len: int,\n    num_encoders: int,\n    num_decoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initializes transformer model.\n\n    Args:\n        src_vocab_size: Size of source vocabulary.\n        tgt_vocab_size: Size of target vocabulary.\n        src_seq_len: Maximum source sequence length.\n        tgt_seq_len: Maximum target sequence length.\n        num_encoders: Number of encoder blocks.\n        num_decoders: Number of decoder blocks.\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Par\u00e1metros\n    self.src_vocab_size = src_vocab_size\n    self.tgt_vocab_size = tgt_vocab_size\n    self.src_seq_len = src_seq_len\n    self.tgt_seq_len = tgt_seq_len\n    self.num_encoders = num_encoders\n    self.num_decoders = num_decoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Embeddings y Positional Encoding\n    self.src_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.src_vocab_size\n    )\n    self.tgt_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n    self.src_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.src_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n    self.tgt_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.tgt_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    # Capas del Decoder\n    self.decoder_layers = nn.ModuleList(\n        [\n            DecoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_decoders)\n        ]\n    )\n\n    # Capa de proyecci\u00f3n final\n    self.projection_layer = ProjectionLayer(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.decode","title":"<code>decode(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Decodes target input tensor using decoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def decode(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Decodes target input tensor using decoder blocks.\n\n    Args:\n        decoder_input: Input tensor to the decoder.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Decoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.tgt_embedding(decoder_input)\n    x = self.tgt_positional_encoding(x)\n\n    # Pasar por todas las capas del decoder\n    for decoder_layer in self.decoder_layers:\n        x = decoder_layer(\n            decoder_input=x,\n            encoder_output=encoder_output,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n        )\n\n    return x\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.encode","title":"<code>encode(encoder_input, src_mask=None)</code>","text":"<p>Encodes source input tensor using encoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_input</code> <code>Tensor</code> <p>Input tensor to the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def encode(\n    self, encoder_input: torch.Tensor, src_mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Encodes source input tensor using encoder blocks.\n\n    Args:\n        encoder_input: Input tensor to the encoder.\n        src_mask: Optional source mask tensor.\n\n    Returns:\n        Encoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.src_embedding(encoder_input)\n    x = self.src_positional_encoding(x)\n\n    # Pasar por todas las capas del encoder\n    for encoder_layer in self.encoder_layers:\n        x = encoder_layer(input_tensor=x, mask=src_mask)\n\n    return x\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.forward","title":"<code>forward(src, tgt, src_mask=None, tgt_mask=None)</code>","text":"<p>Processes input and target sequences through the encoder and decoder, applying optional source and target masks.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input sequence tensor.</p> required <code>tgt</code> <code>Tensor</code> <p>Target sequence tensor.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional mask for the input sequence.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional mask for the target sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing the final output after projection.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    src: torch.Tensor,\n    tgt: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Processes input and target sequences through the encoder\n    and decoder, applying optional source and target masks.\n\n    Args:\n        src: Input sequence tensor.\n        tgt: Target sequence tensor.\n        src_mask: Optional mask for the input sequence.\n        tgt_mask: Optional mask for the target sequence.\n\n    Returns:\n        Tensor containing the final output after projection.\n    \"\"\"\n\n    # Encoder\n    encoder_output = self.encode(src, src_mask)\n\n    # Decoder\n    decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n\n    # Projection\n    return self.projection_layer(decoder_output)\n</code></pre>"},{"location":"repo/nlp/utils.html","title":"Utilities","text":""},{"location":"repo/nlp/utils.html#src.nlp.utils","title":"<code>utils</code>","text":""},{"location":"repo/nlp/utils.html#src.nlp.utils.create_causal_mask","title":"<code>create_causal_mask(size)</code>","text":"<p>Creates a causal mask to prevent the decoder from attending to future tokens during training.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Length of the sequence.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Causal mask of shape (size, size).</p> Source code in <code>src/nlp/utils.py</code> <pre><code>def create_causal_mask(size: int) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a causal mask to prevent the decoder from attending\n    to future tokens during training.\n\n    Args:\n        size: Length of the sequence.\n\n    Returns:\n        Causal mask of shape (size, size).\n    \"\"\"\n\n    return torch.tril(torch.ones(size, size))\n</code></pre>"},{"location":"repo/nlp/utils.html#src.nlp.utils.create_padding_mask","title":"<code>create_padding_mask(seq, pad_token=0)</code>","text":"<p>Creates a mask to ignore padding tokens in a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Tensor</code> <p>Sequence of tokens, shape (B, seq_len).</p> required <code>pad_token</code> <code>int</code> <p>Padding token value.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Padding mask of shape (B, 1, 1, seq_len).</p> Source code in <code>src/nlp/utils.py</code> <pre><code>def create_padding_mask(seq: torch.Tensor, pad_token: int = 0) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a mask to ignore padding tokens in a sequence.\n\n    Args:\n        seq: Sequence of tokens, shape (B, seq_len).\n        pad_token: Padding token value.\n\n    Returns:\n        Padding mask of shape (B, 1, 1, seq_len).\n    \"\"\"\n\n    return (seq != pad_token).unsqueeze(1).unsqueeze(1)\n</code></pre>"}]}