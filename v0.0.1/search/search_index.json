{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#deep-learning-course","title":"Deep Learning Course","text":"<p>Deep learning course repository.</p>"},{"location":"course/Computer%20Vision/step_01_image_processing.html","title":"Image Processing","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Computer%20Vision/step_01_image_processing.html#image-processing","title":"Image Processing\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html","title":"Normalization","text":"<p>La Normalizaci\u00f3n de Respuesta Local (LRN, por sus siglas en ingl\u00e9s) es una t\u00e9cnica introducida en las primeras arquitecturas de redes neuronales convolucionales (CNNs), destacando especialmente en AlexNet (2012). Su prop\u00f3sito principal es mejorar la capacidad de generalizaci\u00f3n del modelo y promover la competencia entre neuronas dentro de una misma capa convolucional.</p> <p>La LRN se inspira en los mecanismos biol\u00f3gicos de inhibici\u00f3n lateral observados en el sistema visual humano, particularmente en la retina. En este proceso biol\u00f3gico, la activaci\u00f3n de una c\u00e9lula nerviosa inhibe la respuesta de las neuronas vecinas, lo que incrementa el contraste y mejora la percepci\u00f3n de bordes y detalles. De manera an\u00e1loga, la LRN permite que una neurona con una activaci\u00f3n alta reduzca la magnitud de las activaciones de las neuronas cercanas, resaltando as\u00ed aquellas respuestas m\u00e1s relevantes y disminuyendo la redundancia entre filtros.</p> <p>El procedimiento de la LRN puede describirse del siguiente modo: para cada neurona activada, se considera un conjunto reducido de canales adyacentes (por ejemplo, los cinco canales circundantes). La activaci\u00f3n de la neurona se normaliza dividi\u00e9ndola por un factor dependiente de la energ\u00eda local, es decir, de la suma de los cuadrados de las activaciones dentro de esa vecindad. En consecuencia, las neuronas con activaciones significativamente superiores a las de sus vecinas mantienen su valor elevado, mientras que aquellas con activaciones m\u00e1s bajas son atenuadas. Esta din\u00e1mica fomenta la especializaci\u00f3n de los filtros y contribuye a una representaci\u00f3n m\u00e1s discriminativa de las caracter\u00edsticas.</p> <p>A pesar de su utilidad inicial, la LRN fue gradualmente reemplazada por m\u00e9todos de normalizaci\u00f3n m\u00e1s eficientes y estables, tales como Batch Normalization (BN), Layer Normalization (LN) e Instance Normalization (IN). Estas t\u00e9cnicas ofrecen mayor estabilidad num\u00e9rica, aceleran el entrenamiento y mejoran el rendimiento general de las redes profundas. En la pr\u00e1ctica moderna, el uso de LRN es escaso, dado que las nuevas estrategias de normalizaci\u00f3n resultan m\u00e1s simples, robustas y efectivas en una amplia variedad de arquitecturas y contextos de aprendizaje profundo.</p> <p>Paper: https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</p> In\u00a0[1]: Copied! <pre># 3pps\nimport torch\nfrom torch import nn\n</pre> # 3pps import torch from torch import nn In\u00a0[2]: Copied! <pre># 3pps\nimport torch\nimport torch.nn as nn\n\n\nclass LocalResponseNormalization(nn.Module):\n\n    def __init__(self, k: float = 2.0, n: int = 5, alpha: float = 1e-4, beta: float = 0.75) -&gt; None:\n        super().__init__()\n        self.k = k\n        self.n = n\n        self.alpha = alpha\n        self.beta = beta\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        batch, channels, height, width = input_tensor.shape\n        response_normalization = input_tensor.clone()  \n\n        for channel in range(channels):\n            for x in range(height):\n                for y in range(width):\n                    end_iterator = min(channels - 1, (channel + self.n)//2)\n                    start_iterator = max(0, (channel - self.n)//2)\n                    numerator = input_tensor[:, channel, x, y]\n                    denominator = (\n                        self.k\n                        + self.alpha * sum(\n                            (input_tensor[:, i, x, y] ** 2) for i in range(start_iterator, end_iterator + 1)\n                        )\n                    ) ** self.beta\n\n                    response_normalization[:, channel, x, y] = numerator / denominator\n\n        return response_normalization\n</pre> # 3pps import torch import torch.nn as nn   class LocalResponseNormalization(nn.Module):      def __init__(self, k: float = 2.0, n: int = 5, alpha: float = 1e-4, beta: float = 0.75) -&gt; None:         super().__init__()         self.k = k         self.n = n         self.alpha = alpha         self.beta = beta      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         batch, channels, height, width = input_tensor.shape         response_normalization = input_tensor.clone()            for channel in range(channels):             for x in range(height):                 for y in range(width):                     end_iterator = min(channels - 1, (channel + self.n)//2)                     start_iterator = max(0, (channel - self.n)//2)                     numerator = input_tensor[:, channel, x, y]                     denominator = (                         self.k                         + self.alpha * sum(                             (input_tensor[:, i, x, y] ** 2) for i in range(start_iterator, end_iterator + 1)                         )                     ) ** self.beta                      response_normalization[:, channel, x, y] = numerator / denominator          return response_normalization In\u00a0[3]: Copied! <pre>lrn = LocalResponseNormalization()\nx = torch.randn(1, 10, 32, 32)\ny = lrn(x)\n</pre> lrn = LocalResponseNormalization() x = torch.randn(1, 10, 32, 32) y = lrn(x) <p>La Global Response Normalization (GRN) es una t\u00e9cnica reciente en el \u00e1mbito de la visi\u00f3n por computadora, introducida en el trabajo \u201cConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\u201d.</p> <p>Esta t\u00e9cnica se incorpora como una capa de normalizaci\u00f3n global cuyo prop\u00f3sito principal es fomentar la competencia entre canales dentro de los mapas de caracter\u00edsticas de las redes convolucionales. Su implementaci\u00f3n busca mitigar el fen\u00f3meno conocido como feature collapse, frecuente en autoencoders enmascarados completamente convolucionales. El feature collapse ocurre cuando varios canales de una red neuronal presentan redundancia o p\u00e9rdida de diversidad. En tales casos, algunos canales pueden generar activaciones constantes o saturarse, reduciendo la variabilidad de las representaciones internas y, en consecuencia, la calidad de las caracter\u00edsticas aprendidas. GRN aborda este problema mediante un proceso de normalizaci\u00f3n y recalibraci\u00f3n que equilibra las contribuciones de los distintos canales.</p> <p>El mecanismo de GRN se estructura en tres etapas fundamentales. Considerando un tensor de activaciones X con dimensiones (N, C, H, W), correspondientes a tama\u00f1o de lote, n\u00famero de canales, altura y anchura, el proceso se desarrolla de la siguiente manera:</p> <ol> <li><p>Agregaci\u00f3n global de caracter\u00edsticas: Para cada canal i, se calcula una norma global (usualmente la norma $L_2$) a partir de todos los valores espaciales del mapa de caracter\u00edsticas. $$ G_i = \\sqrt{\\sum_{h,w} X_{i,h,w}^2} $$ Este c\u00e1lculo produce un vector $G(X) = [G\u2081, G\u2082, \u2026, G_C]$, que representa la magnitud global de activaci\u00f3n de cada canal.</p> </li> <li><p>Normalizaci\u00f3n intercanal: Posteriormente, los valores de norma se normalizan entre canales, dividi\u00e9ndose cada uno por la media global de las normas o por otra estad\u00edstica equivalente. $$ N_i = \\frac{G_i}{\\mathrm{mean}(G(X)) + \\epsilon} $$ Este paso genera un factor de ponderaci\u00f3n relativo que indica la intensidad de activaci\u00f3n de cada canal respecto al resto.</p> </li> <li><p>Recalibraci\u00f3n de caracter\u00edsticas y conexi\u00f3n residual: Cada canal del mapa de entrada se reescala multiplic\u00e1ndolo por su correspondiente factor normalizado (N\u1d62), aplicando adem\u00e1s un escalado (\u03b3) y un sesgo (\u03b2) aprendibles, junto con una conexi\u00f3n residual hacia la entrada original: $$ \\text{Output}_i = \\gamma \\cdot (X_i \\cdot N_i) + \\beta + X_i $$ Los par\u00e1metros \u03b3 y \u03b2 se ajustan durante el entrenamiento y son espec\u00edficos de cada canal.</p> </li> </ol> In\u00a0[4]: Copied! <pre>class GlobalResponseNormalization(nn.Module):\n\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n        \n        self.num_channels = num_channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))\n        self.beta = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \n        gx = torch.norm(input_tensor, p=2, dim=(2,3), keepdim=True)\n        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n\n        return self.gamma * (input_tensor * nx) + self.beta + input_tensor\n</pre> class GlobalResponseNormalization(nn.Module):      def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()                  self.num_channels = num_channels         self.eps = eps          self.gamma = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))         self.beta = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:                  gx = torch.norm(input_tensor, p=2, dim=(2,3), keepdim=True)         nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)          return self.gamma * (input_tensor * nx) + self.beta + input_tensor In\u00a0[5]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\ngrn = GlobalResponseNormalization(x.shape[1])\ny = grn(x)\nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) grn = GlobalResponseNormalization(x.shape[1]) y = grn(x) print(y.shape) <pre>torch.Size([8, 128, 32, 32])\n</pre> In\u00a0[6]: Copied! <pre>class BatchNormalization2D(nn.Module):\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n\n        self.num_channels = num_channels\n        self.eps = eps\n        \n        # For inference\n        self.register_buffer('running_mean', torch.zeros(1, num_channels, 1, 1))\n        self.register_buffer('running_std', torch.ones(1, num_channels, 1, 1))\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        # Input tensor -&gt; (B, C, H, W) -&gt; Batch norm is applied for each batch in H X W\n        mean = x.mean(dim=(0, 2, 3), keepdim=True)\n        std = x.std(dim=(0, 2, 3), keepdim=True)\n        \n        self.running_mean = mean.detach()\n        self.running_std = std.detach()\n        \n        return (x - mean) / (std + self.eps)\n</pre> class BatchNormalization2D(nn.Module):     def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()          self.num_channels = num_channels         self.eps = eps                  # For inference         self.register_buffer('running_mean', torch.zeros(1, num_channels, 1, 1))         self.register_buffer('running_std', torch.ones(1, num_channels, 1, 1))      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          # Input tensor -&gt; (B, C, H, W) -&gt; Batch norm is applied for each batch in H X W         mean = x.mean(dim=(0, 2, 3), keepdim=True)         std = x.std(dim=(0, 2, 3), keepdim=True)                  self.running_mean = mean.detach()         self.running_std = std.detach()                  return (x - mean) / (std + self.eps) In\u00a0[7]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\nbn = BatchNormalization2D(num_channels=x.shape[1]).train()\ny = bn(x)\nprint(y.shape)\n\nbn.eval()           \nwith torch.no_grad():  \n    y = bn(x)       \nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) bn = BatchNormalization2D(num_channels=x.shape[1]).train() y = bn(x) print(y.shape)  bn.eval()            with torch.no_grad():       y = bn(x)        print(y.shape) <pre>torch.Size([8, 128, 32, 32])\ntorch.Size([8, 128, 32, 32])\n</pre> In\u00a0[8]: Copied! <pre>class LayerNormalization2D(nn.Module):\n\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n\n        self.num_channels = num_channels\n        self.eps = eps\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        mean = input_tensor.mean(dim=(1,2,3), keepdim=True) \n        std = input_tensor.mean(dim=(1,2,3), keepdim=True)\n\n        return (x - mean) / (std + self.eps)\n</pre> class LayerNormalization2D(nn.Module):      def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()          self.num_channels = num_channels         self.eps = eps      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          mean = input_tensor.mean(dim=(1,2,3), keepdim=True)          std = input_tensor.mean(dim=(1,2,3), keepdim=True)          return (x - mean) / (std + self.eps) In\u00a0[9]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\nln = LayerNormalization2D(num_channels=x.shape[1]).train()\ny = ln(x)\nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) ln = LayerNormalization2D(num_channels=x.shape[1]).train() y = ln(x) print(y.shape) <pre>torch.Size([8, 128, 32, 32])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Computer%20Vision/step_02_normalization.html#normalization","title":"Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#data-normalization","title":"Data Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#deep-learning-layers-normalization","title":"Deep Learning Layers Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#local-reponse-normalization","title":"Local Reponse Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#global-response-normalization","title":"Global Response Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_02_normalization.html#layer-normalization","title":"Layer Normalization\u00b6","text":""},{"location":"course/Computer%20Vision/step_03_convolutional_layer.html","title":"Convolutional Layers","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Computer%20Vision/step_03_convolutional_layer.html#convolutional-layers","title":"Convolutional Layers\u00b6","text":""},{"location":"course/Computer%20Vision/step_04_shift_invariance.html","title":"Shift Invariance in Convolutions","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEste clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214\n\"\"\"\n\n# Standard libraries\nfrom typing import Literal\n\n# 3pps\nimport torch\nfrom torch import nn\n\n\nclass AdaptivePolyphaseSampling(nn.Module):\n    def __init__(\n        self,\n        norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the class with normalization option.\n\n        Args:\n            norm: Normalization type or value, defaults to 2.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self._stride = 2\n        self.norm = norm\n\n    def forward(\n        self, input_tensor: torch.Tensor, return_index: bool = False\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Processes input tensor to extract dominant polyphase component.\n\n        Args:\n            input_tensor: Tensor with shape (B, C, H, W).\n            return_index: If True, returns index of dominant component.\n\n        Returns:\n            Output tensor, optionally with index if return_index is True.\n        \"\"\"\n\n        # Tenemos a la entrada un tensor de (B, C, H, W)\n        # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n        # de paso elevado al cuadrado, porque nos vemos tanto en la\n        # altura como en la anchura , en total 4\n        poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n        poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n        poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n        poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n        # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n        polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n        # Extraemos las dimensiones\n        b, p, _, _, _ = polyphase_combined.size()\n\n        # Combinamos los valores de los canales, altura y anchura del tensor\n        polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))\n\n        # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n        polyphase_norms = torch.linalg.vector_norm(\n            input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)\n        )\n\n        # Seleccionamos el componente polif\u00e1sico de mayor orden\n        polyphase_max_norm = torch.argmax(polyphase_norms)\n\n        # Obtenemos el componente polif\u00e1sico de mayor orden\n        output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n        # En el paper existe la opci\u00f3n de devolver el \u00edndice\n        if return_index:\n            return output_tensor, polyphase_max_norm\n\n        # En caso contrario solo devolvemos el tensor\n        return output_tensor\n\n\nif __name__ == \"__main__\":\n    model = AdaptivePolyphaseSampling()\n\n    x = torch.randn(1, 3, 4, 4)\n    output_model = model(x)\n\n    print(output_model)\n</pre> \"\"\" Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214 \"\"\"  # Standard libraries from typing import Literal  # 3pps import torch from torch import nn   class AdaptivePolyphaseSampling(nn.Module):     def __init__(         self,         norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,     ) -&gt; None:         \"\"\"         Initializes the class with normalization option.          Args:             norm: Normalization type or value, defaults to 2.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self._stride = 2         self.norm = norm      def forward(         self, input_tensor: torch.Tensor, return_index: bool = False     ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:         \"\"\"         Processes input tensor to extract dominant polyphase component.          Args:             input_tensor: Tensor with shape (B, C, H, W).             return_index: If True, returns index of dominant component.          Returns:             Output tensor, optionally with index if return_index is True.         \"\"\"          # Tenemos a la entrada un tensor de (B, C, H, W)         # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o         # de paso elevado al cuadrado, porque nos vemos tanto en la         # altura como en la anchura , en total 4         poly_a = input_tensor[:, :, :: self._stride, :: self._stride]         poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]         poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]         poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]          # Combinamos las componentes en un solo tensor (B, P, C, H, W)         polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)          # Extraemos las dimensiones         b, p, _, _, _ = polyphase_combined.size()          # Combinamos los valores de los canales, altura y anchura del tensor         polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))          # Aplicamos la norma a la \u00faltima dimensi\u00f3n         polyphase_norms = torch.linalg.vector_norm(             input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)         )          # Seleccionamos el componente polif\u00e1sico de mayor orden         polyphase_max_norm = torch.argmax(polyphase_norms)          # Obtenemos el componente polif\u00e1sico de mayor orden         output_tensor = polyphase_combined[:, polyphase_max_norm, ...]          # En el paper existe la opci\u00f3n de devolver el \u00edndice         if return_index:             return output_tensor, polyphase_max_norm          # En caso contrario solo devolvemos el tensor         return output_tensor   if __name__ == \"__main__\":     model = AdaptivePolyphaseSampling()      x = torch.randn(1, 3, 4, 4)     output_model = model(x)      print(output_model)  In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEste clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001\n\"\"\"\n\n# 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass LearnablePolyphaseSampling(nn.Module):\n    def __init__(self, channel_size: int, hidden_size: int) -&gt; None:\n        \"\"\"\n        Initializes the model with specified channel and hidden sizes.\n\n        Args:\n            channel_size: Number of input channels for the Conv2D layer.\n            hidden_size: Number of hidden units for the Conv2D layer.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self._stride = 2\n\n        # Definimos el modelo \u00fanico para cada componente\n        self.conv_model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channel_size,\n                out_channels=hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=hidden_size,\n                out_channels=hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.Flatten(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor, return_index: bool = False\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Processes input to extract dominant polyphase component.\n\n        Args:\n            input_tensor: Tensor with shape (B, C, H, W).\n            return_index: If True, returns index of dominant component.\n\n        Returns:\n            Tensor of dominant component, optionally with index.\n        \"\"\"\n\n        # Tenemos a la entrada un tensor de (B, C, H, W)\n        # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n        # de paso elevado al cuadrado, porque nos vemos tanto en la\n        # altura como en la anchura , en total 4\n        poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n        poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n        poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n        poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n        # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n        polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n        # Utilizamos el modelo basado en convoluciones por cada componente\n        _logits = []\n        for polyphase in range(polyphase_combined.size()[1]):\n            _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))\n        logits = torch.squeeze(torch.stack(_logits))\n\n        # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n        polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)\n\n        # Seleccionamos el componente polif\u00e1sico de mayor orden\n        polyphase_max_norm = torch.argmax(polyphase_norms)\n\n        # Obtenemos el componente polif\u00e1sico de mayor orden\n        output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n        # En el paper existe la opci\u00f3n de devolver el \u00edndice\n        if return_index:\n            return output_tensor, polyphase_max_norm\n\n        # En caso contrario solo devolvemos el tensor\n        return output_tensor\n\n\nif __name__ == \"__main__\":\n    model = LearnablePolyphaseSampling(channel_size=3, hidden_size=64)\n\n    x = torch.randn(1, 3, 4, 4)\n    output_model = model(x)\n\n    print(output_model)\n</pre> \"\"\" Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001 \"\"\"  # 3pps import torch from torch import nn from torch.nn import functional as F   class LearnablePolyphaseSampling(nn.Module):     def __init__(self, channel_size: int, hidden_size: int) -&gt; None:         \"\"\"         Initializes the model with specified channel and hidden sizes.          Args:             channel_size: Number of input channels for the Conv2D layer.             hidden_size: Number of hidden units for the Conv2D layer.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self._stride = 2          # Definimos el modelo \u00fanico para cada componente         self.conv_model = nn.Sequential(             nn.Conv2d(                 in_channels=channel_size,                 out_channels=hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,             ),             nn.ReLU(),             nn.Conv2d(                 in_channels=hidden_size,                 out_channels=hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,             ),             nn.Flatten(),             nn.AdaptiveAvgPool2d(1),         )      def forward(         self, input_tensor: torch.Tensor, return_index: bool = False     ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:         \"\"\"         Processes input to extract dominant polyphase component.          Args:             input_tensor: Tensor with shape (B, C, H, W).             return_index: If True, returns index of dominant component.          Returns:             Tensor of dominant component, optionally with index.         \"\"\"          # Tenemos a la entrada un tensor de (B, C, H, W)         # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o         # de paso elevado al cuadrado, porque nos vemos tanto en la         # altura como en la anchura , en total 4         poly_a = input_tensor[:, :, :: self._stride, :: self._stride]         poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]         poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]         poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]          # Combinamos las componentes en un solo tensor (B, P, C, H, W)         polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)          # Utilizamos el modelo basado en convoluciones por cada componente         _logits = []         for polyphase in range(polyphase_combined.size()[1]):             _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))         logits = torch.squeeze(torch.stack(_logits))          # Aplicamos la norma a la \u00faltima dimensi\u00f3n         polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)          # Seleccionamos el componente polif\u00e1sico de mayor orden         polyphase_max_norm = torch.argmax(polyphase_norms)          # Obtenemos el componente polif\u00e1sico de mayor orden         output_tensor = polyphase_combined[:, polyphase_max_norm, ...]          # En el paper existe la opci\u00f3n de devolver el \u00edndice         if return_index:             return output_tensor, polyphase_max_norm          # En caso contrario solo devolvemos el tensor         return output_tensor   if __name__ == \"__main__\":     model = LearnablePolyphaseSampling(channel_size=3, hidden_size=64)      x = torch.randn(1, 3, 4, 4)     output_model = model(x)      print(output_model)"},{"location":"course/Computer%20Vision/step_04_shift_invariance.html#shift-invariance-in-convolutions","title":"Shift Invariance in Convolutions\u00b6","text":""},{"location":"course/Computer%20Vision/step_04_shift_invariance.html#aps","title":"APS\u00b6","text":""},{"location":"course/Computer%20Vision/step_04_shift_invariance.html#lps","title":"LPS\u00b6","text":""},{"location":"course/Computer%20Vision/step_05_scale_invariance.html","title":"Scale Invariance in Convolutions","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n\nclass RieszLayer(Layer):\n    def __init__(self, output_channels, **kwargs):\n        super().__init__(**kwargs)\n        self.output_channels = output_channels\n\n    def build(self, input_shape):\n        _, _, _, input_channels = input_shape\n        self.riesz_weights = self.add_weight(\n            shape=(5 * input_channels, self.output_channels),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"riesz_weights\",\n        )\n        super().build(input_shape)\n\n    @tf.function\n    def riesz_transform(self, input_image, H, W):\n        # Recompute frequency grids based on current input dimensions\n        n1 = tf.cast(\n            tf.signal.fftshift(tf.linspace(-H // 2, H // 2 - 1, H)), tf.float32\n        )\n        n2 = tf.cast(\n            tf.signal.fftshift(tf.linspace(-W // 2, W // 2 - 1, W)), tf.float32\n        )\n\n        n1 = tf.reshape(n1, (-1, 1))  # Column vector\n        n2 = tf.reshape(n2, (1, -1))  # Row vector\n        norm = tf.sqrt(n1**2 + n2**2 + 1e-8)\n\n        real_part_R1 = n1 / norm\n        imag_part_R1 = -tf.sqrt(1 - real_part_R1**2)\n        real_part_R2 = n2 / norm\n        imag_part_R2 = -tf.sqrt(1 - real_part_R2**2)\n\n        # Fourier transform of the input\n        I_hat = tf.signal.fft2d(tf.cast(input_image, tf.complex64))\n\n        # First-order Riesz transforms\n        I1 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R1, imag_part_R1))\n        )\n        I2 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R2, imag_part_R2))\n        )\n\n        # Second-order Riesz transforms\n        I_20 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R1**2, imag_part_R1**2))\n        )\n        I_02 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R2**2, imag_part_R2**2))\n        )\n        I_11 = tf.math.real(\n            tf.signal.ifft2d(\n                I_hat\n                * tf.complex(real_part_R1 * real_part_R2, imag_part_R1 * imag_part_R2)\n            )\n        )\n\n        return tf.stack([I1, I2, I_20, I_11, I_02], axis=-1)\n\n    def call(self, inputs):\n        batch_size, H, W, input_channels = (\n            tf.shape(inputs)[0],\n            tf.shape(inputs)[1],\n            tf.shape(inputs)[2],\n            tf.shape(inputs)[3],\n        )\n\n        # Reshape for channel-wise processing\n        inputs_reshaped = tf.reshape(inputs, (-1, H, W))  # Combine batch and channels\n\n        # Apply Riesz transform to each input slice using vectorization\n        riesz_transformed = tf.map_fn(\n            lambda x: self.riesz_transform(x, H, W),\n            inputs_reshaped,\n            fn_output_signature=tf.float32,\n        )\n\n        # Reshape back to original batch format with Riesz feature dimension\n        riesz_features = tf.reshape(\n            riesz_transformed, (batch_size, H, W, 5 * input_channels)\n        )\n\n        # Linear combination using weights\n        riesz_features_flat = tf.reshape(\n            riesz_features, (batch_size * H * W, 5 * input_channels)\n        )\n        combined_features_flat = tf.matmul(riesz_features_flat, self.riesz_weights)\n        combined_features = tf.reshape(\n            combined_features_flat, (batch_size, H, W, self.output_channels)\n        )\n\n        return combined_features\n\n\n# Verify functionality on a sample input with variable sizes\nsample_input_small = np.random.rand(1, 14, 14, 1).astype(np.float32)\nsample_input_large = np.random.rand(1, 56, 56, 1).astype(np.float32)\n\nprint(\"Small input shape:\", sample_input_small.shape)\ntransformed_output_small = RieszLayer(output_channels=5)(sample_input_small)\nprint(\"Transformed output shape (small input):\", transformed_output_small.shape)\n\nprint(\"Large input shape:\", sample_input_large.shape)\ntransformed_output_large = RieszLayer(output_channels=5)(sample_input_large)\nprint(\"Transformed output shape (large input):\", transformed_output_large.shape)\n\n\n# Helper function to resize the dataset\ndef resize_dataset(images, target_size):\n    resized_images = np.array(\n        [cv2.resize(img, target_size, interpolation=cv2.INTER_AREA) for img in images]\n    )\n    return np.expand_dims(resized_images, axis=-1)\n\n\n# Load and preprocess MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n\n# Add channel dimension (grayscale)\nx_train = np.expand_dims(x_train, axis=-1)\nx_test = np.expand_dims(x_test, axis=-1)\n\n# Resize images for evaluation at different scales\nx_test_small = resize_dataset(x_test[..., 0], (14, 14))\nx_test_large = resize_dataset(x_test[..., 0], (56, 56))\n\n\n# Define the neural network model using the functional API\ndef create_riesz_cnn(input_shape=(None, None, 1)):\n    input_layer = tf.keras.Input(shape=input_shape)\n\n    x = RieszLayer(16, name=\"riesz_1\")(input_layer)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(32, name=\"riesz_2\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(40, name=\"riesz_3\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(48, name=\"riesz_4\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    output = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    return tf.keras.Model(inputs=input_layer, outputs=output)\n\n\n# Create the Riesz-based CNN model\nmodel = create_riesz_cnn(input_shape=(None, None, 1))\n\n# Compile the model with AdamW optimizer\nmodel.compile(\n    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()  # Corrected typo from 'suummary' to 'summary'\n\n# Train the model with the learning rate scheduler\nhistory = model.fit(\n    x_train,\n    y_train,\n    shuffle=True,\n    validation_split=0.2,\n    batch_size=64,\n    epochs=10,\n)\n\n# Evaluate the model on different scales\noriginal_acc = model.evaluate(x_test, y_test, verbose=0)\nsmall_acc = model.evaluate(x_test_small, y_test, verbose=0)\nlarge_acc = model.evaluate(x_test_large, y_test, verbose=0)\n\n# Print results\nprint(f\"Accuracy on original scale (28x28): {original_acc[1]:.4f}\")\nprint(f\"Accuracy on smaller scale (14x14): {small_acc[1]:.4f}\")\nprint(f\"Accuracy on larger scale (56x56): {large_acc[1]:.4f}\")\n</pre> # 3pps import cv2 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Layer   class RieszLayer(Layer):     def __init__(self, output_channels, **kwargs):         super().__init__(**kwargs)         self.output_channels = output_channels      def build(self, input_shape):         _, _, _, input_channels = input_shape         self.riesz_weights = self.add_weight(             shape=(5 * input_channels, self.output_channels),             initializer=\"glorot_uniform\",             trainable=True,             name=\"riesz_weights\",         )         super().build(input_shape)      @tf.function     def riesz_transform(self, input_image, H, W):         # Recompute frequency grids based on current input dimensions         n1 = tf.cast(             tf.signal.fftshift(tf.linspace(-H // 2, H // 2 - 1, H)), tf.float32         )         n2 = tf.cast(             tf.signal.fftshift(tf.linspace(-W // 2, W // 2 - 1, W)), tf.float32         )          n1 = tf.reshape(n1, (-1, 1))  # Column vector         n2 = tf.reshape(n2, (1, -1))  # Row vector         norm = tf.sqrt(n1**2 + n2**2 + 1e-8)          real_part_R1 = n1 / norm         imag_part_R1 = -tf.sqrt(1 - real_part_R1**2)         real_part_R2 = n2 / norm         imag_part_R2 = -tf.sqrt(1 - real_part_R2**2)          # Fourier transform of the input         I_hat = tf.signal.fft2d(tf.cast(input_image, tf.complex64))          # First-order Riesz transforms         I1 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R1, imag_part_R1))         )         I2 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R2, imag_part_R2))         )          # Second-order Riesz transforms         I_20 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R1**2, imag_part_R1**2))         )         I_02 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R2**2, imag_part_R2**2))         )         I_11 = tf.math.real(             tf.signal.ifft2d(                 I_hat                 * tf.complex(real_part_R1 * real_part_R2, imag_part_R1 * imag_part_R2)             )         )          return tf.stack([I1, I2, I_20, I_11, I_02], axis=-1)      def call(self, inputs):         batch_size, H, W, input_channels = (             tf.shape(inputs)[0],             tf.shape(inputs)[1],             tf.shape(inputs)[2],             tf.shape(inputs)[3],         )          # Reshape for channel-wise processing         inputs_reshaped = tf.reshape(inputs, (-1, H, W))  # Combine batch and channels          # Apply Riesz transform to each input slice using vectorization         riesz_transformed = tf.map_fn(             lambda x: self.riesz_transform(x, H, W),             inputs_reshaped,             fn_output_signature=tf.float32,         )          # Reshape back to original batch format with Riesz feature dimension         riesz_features = tf.reshape(             riesz_transformed, (batch_size, H, W, 5 * input_channels)         )          # Linear combination using weights         riesz_features_flat = tf.reshape(             riesz_features, (batch_size * H * W, 5 * input_channels)         )         combined_features_flat = tf.matmul(riesz_features_flat, self.riesz_weights)         combined_features = tf.reshape(             combined_features_flat, (batch_size, H, W, self.output_channels)         )          return combined_features   # Verify functionality on a sample input with variable sizes sample_input_small = np.random.rand(1, 14, 14, 1).astype(np.float32) sample_input_large = np.random.rand(1, 56, 56, 1).astype(np.float32)  print(\"Small input shape:\", sample_input_small.shape) transformed_output_small = RieszLayer(output_channels=5)(sample_input_small) print(\"Transformed output shape (small input):\", transformed_output_small.shape)  print(\"Large input shape:\", sample_input_large.shape) transformed_output_large = RieszLayer(output_channels=5)(sample_input_large) print(\"Transformed output shape (large input):\", transformed_output_large.shape)   # Helper function to resize the dataset def resize_dataset(images, target_size):     resized_images = np.array(         [cv2.resize(img, target_size, interpolation=cv2.INTER_AREA) for img in images]     )     return np.expand_dims(resized_images, axis=-1)   # Load and preprocess MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]  # Add channel dimension (grayscale) x_train = np.expand_dims(x_train, axis=-1) x_test = np.expand_dims(x_test, axis=-1)  # Resize images for evaluation at different scales x_test_small = resize_dataset(x_test[..., 0], (14, 14)) x_test_large = resize_dataset(x_test[..., 0], (56, 56))   # Define the neural network model using the functional API def create_riesz_cnn(input_shape=(None, None, 1)):     input_layer = tf.keras.Input(shape=input_shape)      x = RieszLayer(16, name=\"riesz_1\")(input_layer)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(32, name=\"riesz_2\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(40, name=\"riesz_3\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(48, name=\"riesz_4\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = tf.keras.layers.GlobalAveragePooling2D()(x)     x = tf.keras.layers.Dense(128, activation=\"relu\")(x)     output = tf.keras.layers.Dense(10, activation=\"softmax\")(x)      return tf.keras.Model(inputs=input_layer, outputs=output)   # Create the Riesz-based CNN model model = create_riesz_cnn(input_shape=(None, None, 1))  # Compile the model with AdamW optimizer model.compile(     optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3),     loss=\"sparse_categorical_crossentropy\",     metrics=[\"accuracy\"], ) model.summary()  # Corrected typo from 'suummary' to 'summary'  # Train the model with the learning rate scheduler history = model.fit(     x_train,     y_train,     shuffle=True,     validation_split=0.2,     batch_size=64,     epochs=10, )  # Evaluate the model on different scales original_acc = model.evaluate(x_test, y_test, verbose=0) small_acc = model.evaluate(x_test_small, y_test, verbose=0) large_acc = model.evaluate(x_test_large, y_test, verbose=0)  # Print results print(f\"Accuracy on original scale (28x28): {original_acc[1]:.4f}\") print(f\"Accuracy on smaller scale (14x14): {small_acc[1]:.4f}\") print(f\"Accuracy on larger scale (56x56): {large_acc[1]:.4f}\")"},{"location":"course/Computer%20Vision/step_05_scale_invariance.html#scale-invariance-in-convolutions","title":"Scale Invariance in Convolutions\u00b6","text":""},{"location":"course/Computer%20Vision/step_05_scale_invariance.html#riesz","title":"Riesz\u00b6","text":""},{"location":"course/Computer%20Vision/step_06_lenet.html","title":"LeNet","text":"<p>La arquitectura LeNet-5, desarrollada por Yann LeCun y su equipo en durante 1988 a 1998, se considera el antecedente directo y fundacional de las redes neuronales convolucionales modernas. Su dise\u00f1o responde a la necesidad de resolver de manera eficiente el problema del reconocimiento autom\u00e1tico de caracteres manuscritos, en particular los d\u00edgitos del 0 al 9. Esta arquitectura fue implementada con \u00e9xito en sistemas reales para el procesamiento autom\u00e1tico de cheques bancarios en Estados Unidos, lo que constituy\u00f3 una de las primeras aplicaciones industriales del aprendizaje profundo.</p> <p>We begin by importing the necessary Python modules and libraries for building and training our neural network with the MNIST dataset.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nfrom typing import Any\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.manifold import TSNE\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n</pre> # Standard libraries from typing import Any  # 3pps import matplotlib.pyplot as plt import torch from sklearn.manifold import TSNE from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm In\u00a0[2]: Copied! <pre>def show_images(images, labels):\n    fig, axes = plt.subplots(1, len(images), figsize=(10, 2))\n    for img, label, ax in zip(images, labels, axes):\n        ax.imshow(img.squeeze(), cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n    plt.show()\n</pre> def show_images(images, labels):     fig, axes = plt.subplots(1, len(images), figsize=(10, 2))     for img, label, ax in zip(images, labels, axes):         ax.imshow(img.squeeze(), cmap='gray')         ax.set_title(f'Label: {label}')         ax.axis('off')     plt.show() <p>We start by loading the MNIST dataset, including both the training and test sets. While loading the data, we apply two important transformations. First, each image is converted into a PyTorch tensor, which allows the model to process the data efficiently. Second, we normalize the images using <code>transforms.Normalize((0.1307,), (0.3081,))</code>. These numbers represent the mean (<code>0.1307</code>) and standard deviation (<code>0.3081</code>) of the MNIST dataset, and normalization ensures that the data has a consistent scale. This step is important because it helps the model train more effectively and converge faster. By combining these transformations, we prepare the dataset in a way that is both suitable for the model and optimized for learning.</p> In\u00a0[3]: Copied! <pre>transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=True,\n    download=True,\n    transform=transform\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=False,\n    download=True,\n    transform=transform\n)\n</pre> transform = transforms.Compose([     transforms.ToTensor(),     transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST(     root=\"./data\",     train=True,     download=True,     transform=transform ) test_dataset = datasets.MNIST(     root=\"./data\",     train=False,     download=True,     transform=transform ) In\u00a0[4]: Copied! <pre>train_dataset\n</pre> train_dataset Out[4]: <pre>Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> In\u00a0[5]: Copied! <pre>test_dataset\n</pre> test_dataset Out[5]: <pre>Dataset MNIST\n    Number of datapoints: 10000\n    Root location: ./data\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> <p>We can use the <code>DataLoader</code> class to divide the dataset into batches and shuffle the data efficiently using PyTorch\u2019s built-in functionality. To get started, we first define some global variables or constants that will be used throughout the data loading and training process.</p> In\u00a0[6]: Copied! <pre>BATCH_SIZE: int = 32\n</pre> BATCH_SIZE: int = 32 In\u00a0[7]: Copied! <pre>train_dataloader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\n</pre> train_dataloader = DataLoader(     dataset=train_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) test_dataloader = DataLoader(     dataset=test_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) <p>We can visualize some examples of sample-label pairs from the dataset. If we take one batch from the <code>DataLoader</code>, we will get as many samples as the chosen batch size. Each MNIST sample is a grayscale image of size 28 \u00d7 28, meaning it has a single channel. By inspecting these batches, we can better understand the structure and format of the data before feeding it into a neural network.</p> In\u00a0[8]: Copied! <pre>data_iter = iter(train_dataloader)\ntrain_images, train_labels = next(data_iter)\ntrain_images.shape, train_labels.shape\n</pre> data_iter = iter(train_dataloader) train_images, train_labels = next(data_iter) train_images.shape, train_labels.shape Out[8]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>We will display the first 10 samples from the dataset. This allows us to quickly inspect the images and their corresponding labels to ensure that the data has been loaded and preprocessed correctly.</p> In\u00a0[9]: Copied! <pre>show_images(train_images[:10], train_labels[:10])\n</pre> show_images(train_images[:10], train_labels[:10]) <p>Next, we will create a convolutional neural network (CNN) model, inspired by Yann LeCun\u2019s LeNet architecture, and adapt it for the MNIST dataset. This model will use convolutional layers to automatically extract features from the images, followed by fully connected layers to perform classification.</p> In\u00a0[10]: Copied! <pre>class LeNet(nn.Module):\n\n    def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:\n\n        super().__init__(**kwargs)\n\n        self.input_tensor_shape = input_tensor_shape\n\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=16),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n            nn.Flatten(),\n            nn.Linear(32, 10),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.model(input_tensor)\n</pre> class LeNet(nn.Module):      def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:          super().__init__(**kwargs)          self.input_tensor_shape = input_tensor_shape          self.model = nn.Sequential(             nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=16),             nn.ReLU(),             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=32),             nn.ReLU(),             nn.AdaptiveAvgPool2d(output_size=(1,1)),             nn.Flatten(),             nn.Linear(32, 10),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.model(input_tensor) <p>We define the optimizer as AdamW and use cross-entropy as the loss function. AdamW is an adaptive optimizer that combines the benefits of Adam with correct weight decay, helping the model converge efficiently. Cross-entropy loss is well-suited for multi-class classification tasks like MNIST, as it measures the difference between the predicted probabilities and the true class labels.</p> In\u00a0[11]: Copied! <pre>model = LeNet(input_tensor_shape=(1,28,28))\nsummary(model, input_size=(BATCH_SIZE, 1,28,28))\n</pre> model = LeNet(input_tensor_shape=(1,28,28)) summary(model, input_size=(BATCH_SIZE, 1,28,28)) Out[11]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLeNet                                    [32, 10]                  --\n\u251c\u2500Sequential: 1-1                        [32, 10]                  --\n\u2502    \u2514\u2500Conv2d: 2-1                       [32, 16, 13, 13]          272\n\u2502    \u2514\u2500BatchNorm2d: 2-2                  [32, 16, 13, 13]          32\n\u2502    \u2514\u2500ReLU: 2-3                         [32, 16, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-4                       [32, 32, 5, 5]            8,224\n\u2502    \u2514\u2500BatchNorm2d: 2-5                  [32, 32, 5, 5]            64\n\u2502    \u2514\u2500ReLU: 2-6                         [32, 32, 5, 5]            --\n\u2502    \u2514\u2500AdaptiveAvgPool2d: 2-7            [32, 32, 1, 1]            --\n\u2502    \u2514\u2500Flatten: 2-8                      [32, 32]                  --\n\u2502    \u2514\u2500Linear: 2-9                       [32, 10]                  330\n==========================================================================================\nTotal params: 8,922\nTrainable params: 8,922\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 8.06\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 1.80\nParams size (MB): 0.04\nEstimated Total Size (MB): 1.93\n==========================================================================================</pre> In\u00a0[12]: Copied! <pre>optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4)\nloss_function = torch.nn.CrossEntropyLoss()\n</pre> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4) loss_function = torch.nn.CrossEntropyLoss() <p>Now, we need to create the training loop. This loop will iterate over the dataset for a number of epochs, feeding batches of data through the model, computing the loss, performing backpropagation, and updating the model\u2019s parameters. A well-structured training loop is essential for effectively training the network and monitoring its performance over time.</p> In\u00a0[13]: Copied! <pre>NUM_EPOCHS: int = 5\n</pre> NUM_EPOCHS: int = 5 In\u00a0[14]: Copied! <pre>train_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n    for batch_image, batch_label in train_loop:\n\n        optimizer.zero_grad()\n        outputs = model(batch_image)\n        loss = loss_function(outputs, batch_label)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += batch_label.size(0)\n        correct += (predicted == batch_label).sum().item()\n\n    train_losses.append(running_loss / len(train_dataloader))\n    train_accuracies.append(100 * correct / total)\n\n    model.eval()\n    test_loss, correct_test, total_test = 0.0, 0, 0\n\n    test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)\n\n    with torch.no_grad():\n        for images, labels in test_loop:\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n    test_losses.append(test_loss / len(test_dataloader))\n    test_accuracies.append(100 * correct_test / total_test)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"\n          f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")\n\nepochs = range(1, NUM_EPOCHS+1)\n\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Test Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\nplt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Training vs Test Accuracy\")\nplt.legend()\nplt.show()\n</pre> train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], []  for epoch in range(NUM_EPOCHS):     model.train()     running_loss, correct, total = 0.0, 0, 0      train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)      for batch_image, batch_label in train_loop:          optimizer.zero_grad()         outputs = model(batch_image)         loss = loss_function(outputs, batch_label)          loss.backward()         optimizer.step()          running_loss += loss.item()         _, predicted = torch.max(outputs, 1)         total += batch_label.size(0)         correct += (predicted == batch_label).sum().item()      train_losses.append(running_loss / len(train_dataloader))     train_accuracies.append(100 * correct / total)      model.eval()     test_loss, correct_test, total_test = 0.0, 0, 0      test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)      with torch.no_grad():         for images, labels in test_loop:             outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             _, predicted = torch.max(outputs, 1)             total_test += labels.size(0)             correct_test += (predicted == labels).sum().item()      test_losses.append(test_loss / len(test_dataloader))     test_accuracies.append(100 * correct_test / total_test)      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"           f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"           f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")  epochs = range(1, NUM_EPOCHS+1)  plt.plot(epochs, train_losses, label=\"Train Loss\") plt.plot(epochs, test_losses, label=\"Test Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Training vs Test Loss\") plt.legend() plt.show()  plt.plot(epochs, train_accuracies, label=\"Train Accuracy\") plt.plot(epochs, test_accuracies, label=\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy (%)\") plt.title(\"Training vs Test Accuracy\") plt.legend() plt.show() <pre>                                                                      \r</pre> <pre>Epoch [1/5] Train Loss: 0.9334, Train Acc: 76.77% | Test Loss: 0.4189, Test Acc: 90.49%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [2/5] Train Loss: 0.3321, Train Acc: 92.07% | Test Loss: 0.2243, Test Acc: 94.45%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [3/5] Train Loss: 0.2210, Train Acc: 94.38% | Test Loss: 0.1833, Test Acc: 95.19%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [4/5] Train Loss: 0.1745, Train Acc: 95.46% | Test Loss: 0.1389, Test Acc: 96.30%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [5/5] Train Loss: 0.1469, Train Acc: 96.10% | Test Loss: 0.1318, Test Acc: 96.39%\n</pre> <p>Now, we can visualize the data in a lower-dimensional space using t-SNE. This technique allows us to project high-dimensional representations\u2014such as the feature outputs from our model\u2014into two or three dimensions, making it easier to observe patterns, clusters, or separations between different classes. Visualizing the data in this way can provide valuable insights into how well the model is learning to distinguish between digits.</p> In\u00a0[15]: Copied! <pre>all_labels = []\nembeddings = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_image, batch_label in train_dataloader:\n        output = model(batch_image)\n        embeddings.append(output.cpu())\n        all_labels.append(batch_label) \n\nembeddings = torch.cat(embeddings, dim=0)\nall_labels = torch.cat(all_labels, dim=0)\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',\n                  init='random', perplexity=30).fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15)\nplt.colorbar(scatter, ticks=range(10), label=\"Classes\")\nplt.title(\"t-SNE Training Embeddings MNIST\")\nplt.show()\n</pre> all_labels = [] embeddings = []  model.eval() with torch.no_grad():     for batch_image, batch_label in train_dataloader:         output = model(batch_image)         embeddings.append(output.cpu())         all_labels.append(batch_label)   embeddings = torch.cat(embeddings, dim=0) all_labels = torch.cat(all_labels, dim=0)  X_embedded = TSNE(n_components=2, learning_rate='auto',                   init='random', perplexity=30).fit_transform(embeddings)  plt.figure(figsize=(10, 8)) scatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15) plt.colorbar(scatter, ticks=range(10), label=\"Classes\") plt.title(\"t-SNE Training Embeddings MNIST\") plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Computer%20Vision/step_06_lenet.html#lenet","title":"LeNet\u00b6","text":""},{"location":"course/Computer%20Vision/step_06_lenet.html#teoria","title":"Teor\u00eda\u00b6","text":""},{"location":"course/Computer%20Vision/step_06_lenet.html#fundamentos-conceptuales","title":"Fundamentos Conceptuales\u00b6","text":"<p>Antes de la introducci\u00f3n de LeNet-5, el reconocimiento de im\u00e1genes se abordaba principalmente mediante redes neuronales densas o perceptrones multicapa. Este enfoque presentaba una limitaci\u00f3n estructural fundamental: las redes densas no incorporan de forma expl\u00edcita la informaci\u00f3n espacial de los datos. Como consecuencia, peque\u00f1as traslaciones o deformaciones en la imagen de entrada pod\u00edan provocar fallos significativos en el reconocimiento, ya que la red trataba cada p\u00edxel como una caracter\u00edstica independiente.</p> <p>LeNet-5 introduce un cambio conceptual decisivo al incorporar mecanismos que explotan la naturaleza espacial de las im\u00e1genes. En particular, establece tres principios que hoy constituyen la base del aprendizaje profundo aplicado a visi\u00f3n artificial. En primer lugar, el uso de convoluciones permite extraer caracter\u00edsticas locales relevantes, como bordes y esquinas, preservando la estructura bidimensional de la imagen. En segundo lugar, el submuestreo o pooling reduce progresivamente la resoluci\u00f3n espacial, lo que aporta robustez frente a peque\u00f1as variaciones en la posici\u00f3n de los rasgos. Finalmente, la compartici\u00f3n de pesos reduce de forma dr\u00e1stica el n\u00famero de par\u00e1metros entrenables, mejorando la eficiencia computacional y la capacidad de generalizaci\u00f3n del modelo.</p>"},{"location":"course/Computer%20Vision/step_06_lenet.html#descripcion-detallada-de-la-arquitectura","title":"Descripci\u00f3n Detallada de la Arquitectura\u00b6","text":"<p>LeNet-5 est\u00e1 compuesta por siete capas entrenables, sin considerar la capa de entrada. El modelo recibe im\u00e1genes en escala de grises de 32 \u00d7 32 p\u00edxeles, un tama\u00f1o ligeramente superior al de los d\u00edgitos originales del conjunto MNIST, con el fin de permitir un margen espacial durante las operaciones convolucionales.</p> <p>La primera capa, denominada C1, es una capa convolucional que aplica seis filtros de 5 \u00d7 5 p\u00edxeles sobre la imagen de entrada. Como resultado, se obtienen seis mapas de caracter\u00edsticas de 28 \u00d7 28 p\u00edxeles. El objetivo principal de esta capa es detectar patrones elementales, tales como l\u00edneas horizontales, verticales y oblicuas, que constituyen la base de representaciones m\u00e1s complejas.</p> <p>A continuaci\u00f3n, la capa S2 realiza una operaci\u00f3n de submuestreo mediante average pooling. En esta etapa, cada bloque de 2 \u00d7 2 p\u00edxeles se reemplaza por su valor promedio, generando seis mapas de caracter\u00edsticas de 14 \u00d7 14 p\u00edxeles. Este proceso reduce la sensibilidad del modelo a peque\u00f1as variaciones espaciales y contribuye a disminuir la dimensionalidad de los datos.</p> <p>La segunda capa convolucional, C3, aplica diecis\u00e9is filtros de 5 \u00d7 5 p\u00edxeles y produce diecis\u00e9is mapas de caracter\u00edsticas de 10 \u00d7 10 p\u00edxeles. Una caracter\u00edstica distintiva de esta capa es que no todos los mapas de entrada se conectan con todos los filtros. Esta conectividad parcial se dise\u00f1\u00f3 para romper la simetr\u00eda entre neuronas y fomentar el aprendizaje de representaciones m\u00e1s diversas, una decisi\u00f3n relevante en el contexto de las limitaciones computacionales de la \u00e9poca.</p> <p>Posteriormente, la capa S4 realiza un nuevo submuestreo similar al de S2, reduciendo cada mapa de caracter\u00edsticas a un tama\u00f1o de 5 \u00d7 5 p\u00edxeles. En este punto, la red dispone de diecis\u00e9is mapas compactos que contienen informaci\u00f3n de alto nivel sobre la imagen de entrada.</p> <p>La capa C5 act\u00faa como una transici\u00f3n entre las capas convolucionales y las capas densas. Aplica ciento veinte filtros de 5 \u00d7 5 p\u00edxeles sobre la entrada de 5 \u00d7 5, lo que produce un vector de 120 unidades. Debido a que el tama\u00f1o del filtro coincide con el tama\u00f1o del mapa de entrada, esta capa se comporta de facto como una capa totalmente conectada.</p> <p>A continuaci\u00f3n, la capa F6 es una capa completamente conectada con 84 unidades. En el dise\u00f1o original de LeNet-5 se empleaban funciones de activaci\u00f3n como la sigmoide o la tangente hiperb\u00f3lica, ya que la funci\u00f3n ReLU a\u00fan no se hab\u00eda popularizado en el \u00e1mbito del aprendizaje profundo.</p> <p>Finalmente, la capa de salida consta de 10 unidades, correspondientes a las diez clases posibles de d\u00edgitos. En su formulaci\u00f3n original, LeNet-5 utilizaba funciones de base radial para la clasificaci\u00f3n. En implementaciones modernas, esta capa suele reemplazarse por una funci\u00f3n Softmax, que permite interpretar las salidas como probabilidades normalizadas.</p>"},{"location":"course/Computer%20Vision/step_06_lenet.html#caracteristicas-tecnicas-principales","title":"Caracter\u00edsticas T\u00e9cnicas Principales\u00b6","text":"<p>Desde un punto de vista cuantitativo, LeNet-5 cuenta con aproximadamente 60 000 par\u00e1metros entrenables, una cifra muy reducida en comparaci\u00f3n con las arquitecturas contempor\u00e1neas. El modelo opera sobre im\u00e1genes en escala de grises de 32 \u00d7 32 p\u00edxeles, emplea funciones de activaci\u00f3n suaves como tanh o sigmoide y utiliza average pooling en lugar de max pooling, reflejando las decisiones de dise\u00f1o predominantes a finales de la d\u00e9cada de 1990. El conjunto de datos de referencia para su evaluaci\u00f3n fue MNIST, que se convirti\u00f3 posteriormente en un est\u00e1ndar para el an\u00e1lisis comparativo de modelos de clasificaci\u00f3n de im\u00e1genes.</p>"},{"location":"course/Computer%20Vision/step_06_lenet.html#implementacion","title":"Implementaci\u00f3n\u00b6","text":""},{"location":"course/Computer%20Vision/step_07_vgg.html","title":"VGG","text":"<p>La arquitectura VGG, desarrollada por el Visual Geometry Group de la Universidad de Oxford y presentada en 2014, representa un punto de inflexi\u00f3n en la evoluci\u00f3n del aprendizaje profundo aplicado a la visi\u00f3n artificial. Mientras que LeNet-5 sent\u00f3 las bases conceptuales de las redes neuronales convolucionales, VGG demostr\u00f3 de manera sistem\u00e1tica que el aumento de la profundidad de la red, combinado con una estructura extremadamente simple y homog\u00e9nea, conduce a mejoras sustanciales en el rendimiento. Las variantes m\u00e1s influyentes de esta familia son VGG-16 y VGG-19, denominadas as\u00ed por el n\u00famero de capas con par\u00e1metros entrenables que las componen.</p>"},{"location":"course/Computer%20Vision/step_07_vgg.html#vgg","title":"VGG\u00b6","text":""},{"location":"course/Computer%20Vision/step_07_vgg.html#teoria","title":"Teor\u00eda\u00b6","text":""},{"location":"course/Computer%20Vision/step_07_vgg.html#principios-de-diseno","title":"Principios de Dise\u00f1o\u00b6","text":"<p>El rasgo distintivo de VGG radica en su filosof\u00eda de dise\u00f1o deliberadamente conservadora. A diferencia de arquitecturas anteriores que empleaban filtros convolucionales de distintos tama\u00f1os, como 5 \u00d7 5 o 7 \u00d7 7, VGG adopta una estandarizaci\u00f3n estricta de todos sus componentes. En esta arquitectura se utilizan exclusivamente filtros convolucionales de 3 \u00d7 3, aplicados de forma repetida, y se incrementa progresivamente el n\u00famero de filtros a medida que disminuye la resoluci\u00f3n espacial de los mapas de caracter\u00edsticas, generalmente duplicando dicho n\u00famero tras cada operaci\u00f3n de pooling.</p> <p>La elecci\u00f3n sistem\u00e1tica de filtros 3 \u00d7 3 responde a una justificaci\u00f3n te\u00f3rica y pr\u00e1ctica. Aunque Yann LeCun hab\u00eda utilizado filtros 5 \u00d7 5 en LeNet-5, los dise\u00f1adores de VGG demostraron que dos capas convolucionales consecutivas de 3 \u00d7 3 poseen un campo receptivo equivalente al de una sola capa de 5 \u00d7 5. Esta sustituci\u00f3n aporta dos ventajas fundamentales. En primer lugar, reduce el n\u00famero total de par\u00e1metros y, por tanto, el coste computacional del modelo. En segundo lugar, introduce un mayor n\u00famero de no linealidades, ya que cada capa adicional incorpora una funci\u00f3n de activaci\u00f3n, lo que permite a la red aprender representaciones m\u00e1s complejas y expresivas.</p>"},{"location":"course/Computer%20Vision/step_07_vgg.html#organizacion-arquitectonica-de-vgg-16","title":"Organizaci\u00f3n Arquitect\u00f3nica de VGG-16\u00b6","text":"<p>La variante VGG-16 recibe como entrada im\u00e1genes en color (RGB) de 224 \u00d7 224 p\u00edxeles y se organiza en una secuencia jer\u00e1rquica de cinco bloques convolucionales, seguidos de un conjunto de capas totalmente conectadas destinadas a la clasificaci\u00f3n final.</p> <p>Los dos primeros bloques convolucionales est\u00e1n formados por dos capas de convoluci\u00f3n cada uno. El primer bloque emplea 64 filtros, mientras que el segundo utiliza 128 filtros, manteniendo siempre el tama\u00f1o de 3 \u00d7 3. Al final de cada bloque se aplica una operaci\u00f3n de max pooling de 2 \u00d7 2, cuya funci\u00f3n es reducir a la mitad la resoluci\u00f3n espacial y concentrar la informaci\u00f3n m\u00e1s relevante.</p> <p>Los bloques tercero, cuarto y quinto incrementan la profundidad de la red mediante tres capas convolucionales consecutivas en cada bloque. El n\u00famero de filtros asciende a 256 en el tercer bloque y a 512 en los dos bloques finales. En estas etapas profundas, la red aprende caracter\u00edsticas altamente abstractas, tales como partes de objetos, texturas complejas y configuraciones visuales de alto nivel, que resultan cruciales para la discriminaci\u00f3n entre clases.</p> <p>Tras los bloques convolucionales, los mapas de caracter\u00edsticas resultantes se transforman en un vector unidimensional que alimenta las capas clasificadoras. Este segmento final consta de dos capas densas de 4096 neuronas cada una, seguidas de una capa de salida de 1000 neuronas, correspondiente a las mil categor\u00edas del conjunto de datos ImageNet. La activaci\u00f3n Softmax permite interpretar la salida como una distribuci\u00f3n de probabilidad sobre las clases posibles. En toda la arquitectura se emplea la funci\u00f3n de activaci\u00f3n ReLU, que contribuye a acelerar el entrenamiento y mitigar el problema del desvanecimiento del gradiente.</p>"},{"location":"course/Computer%20Vision/step_07_vgg.html#comparacion-conceptual-entre-lenet-5-y-vgg-16","title":"Comparaci\u00f3n Conceptual entre LeNet-5 y VGG-16\u00b6","text":"<p>Desde una perspectiva hist\u00f3rica y t\u00e9cnica, la comparaci\u00f3n entre LeNet-5 y VGG-16 ilustra con claridad la evoluci\u00f3n del aprendizaje profundo. LeNet-5 opera sobre im\u00e1genes peque\u00f1as de 32 \u00d7 32 p\u00edxeles en escala de grises, cuenta con solo 7 capas entrenables y aproximadamente 60 000 par\u00e1metros, y emplea funciones de activaci\u00f3n suaves como la tangente hiperb\u00f3lica o la sigmoide, junto con average pooling. En contraste, VGG-16 procesa im\u00e1genes de alta resoluci\u00f3n de 224 \u00d7 224 p\u00edxeles en color, incorpora 16 capas con pesos, utiliza filtros 3 \u00d7 3 de forma uniforme, adopta ReLU como funci\u00f3n de activaci\u00f3n y emplea max pooling. Como consecuencia directa de su profundidad y tama\u00f1o, VGG-16 alcanza alrededor de 138 millones de par\u00e1metros, lo que refleja tanto su potencia representacional como su elevado coste computacional.</p>"},{"location":"course/Computer%20Vision/step_07_vgg.html#impacto-ventajas-y-limitaciones","title":"Impacto, Ventajas y Limitaciones\u00b6","text":"<p>VGG-16 obtuvo el segundo puesto en el reto ImageNet de 2014, pero su influencia trascendi\u00f3 ampliamente la competici\u00f3n. La comunidad cient\u00edfica y t\u00e9cnica adopt\u00f3 esta arquitectura como un referente debido a su dise\u00f1o claro, regular y f\u00e1cil de interpretar, lo que la convirti\u00f3 en una herramienta fundamental para el estudio y la ense\u00f1anza de redes convolucionales profundas.</p> <p>Una de sus principales fortalezas reside en su idoneidad para el aprendizaje por transferencia. Las capas iniciales de VGG aprenden representaciones gen\u00e9ricas y robustas que pueden reutilizarse eficazmente en una amplia variedad de tareas de visi\u00f3n artificial. No obstante, esta ventaja se ve contrarrestada por una limitaci\u00f3n significativa: el elevado n\u00famero de par\u00e1metros hace que el modelo sea pesado en t\u00e9rminos de memoria, superando los 500 MB, y costoso de entrenar y desplegar. Estas restricciones impulsaron el desarrollo de arquitecturas posteriores, como Inception y ResNet, orientadas a mantener o mejorar el rendimiento reduciendo el coste computacional y facilitando el entrenamiento de redes a\u00fan m\u00e1s profundas.</p>"},{"location":"course/Computer%20Vision/step_07_vgg.html#implementacion","title":"Implementaci\u00f3n\u00b6","text":""},{"location":"course/Computer%20Vision/step_08_resnet.html","title":"ResNet","text":"<p>Si LeNet-5 constituye el punto de partida hist\u00f3rico de las redes neuronales convolucionales y VGG representa la consolidaci\u00f3n de la profundidad como factor clave del rendimiento, ResNet (Residual Network), presentada por Kaiming He y su equipo de Microsoft Research en 2015, marca una aut\u00e9ntica ruptura conceptual. Esta arquitectura permiti\u00f3 superar de forma efectiva la barrera pr\u00e1ctica que imped\u00eda entrenar redes extremadamente profundas y demostr\u00f3 que modelos con m\u00e1s de un centenar de capas no solo eran entrenables, sino tambi\u00e9n altamente precisos.</p> <p>ResNet alcanz\u00f3 una repercusi\u00f3n inmediata al ganar el desaf\u00edo ImageNet 2015 con una versi\u00f3n de 152 capas, una profundidad que hasta ese momento se consideraba inabordable desde el punto de vista del entrenamiento estable.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#resnet","title":"ResNet\u00b6","text":""},{"location":"course/Computer%20Vision/step_08_resnet.html#teoria","title":"Teor\u00eda\u00b6","text":""},{"location":"course/Computer%20Vision/step_08_resnet.html#la-degradacion-del-gradiente","title":"La Degradaci\u00f3n del Gradiente\u00b6","text":"<p>Antes de la introducci\u00f3n de ResNet, predominaba la intuici\u00f3n de que aumentar el n\u00famero de capas conduc\u00eda de forma casi autom\u00e1tica a una mayor precisi\u00f3n. Sin embargo, los experimentos emp\u00edricos mostraron que, a partir de un umbral relativamente bajo \u2014en torno a veinte o treinta capas\u2014, el rendimiento comenzaba a deteriorarse de manera significativa.</p> <p>Este fen\u00f3meno no se deb\u00eda al overfitting, sino a un problema estructural m\u00e1s profundo conocido como degradaci\u00f3n. En redes muy profundas, la se\u00f1al de error que se propaga hacia atr\u00e1s durante el proceso de entrenamiento tiende a debilitarse progresivamente. Como resultado, las capas m\u00e1s cercanas a la entrada reciben gradientes muy peque\u00f1os o inestables, lo que dificulta o impide el aprendizaje efectivo. En este contexto, a\u00f1adir m\u00e1s capas no solo dejaba de ser beneficioso, sino que empeoraba la capacidad de la red para representar la funci\u00f3n deseada.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#conexiones-residuales","title":"Conexiones Residuales\u00b6","text":"<p>La innovaci\u00f3n central de ResNet consiste en la introducci\u00f3n de las conexiones de atajo, tambi\u00e9n denominadas skip connections, que dan lugar al denominado bloque residual. En lugar de obligar a cada conjunto de capas a aprender directamente una transformaci\u00f3n completa, la arquitectura permite que dichas capas aprendan \u00fanicamente la diferencia, o residuo, entre la entrada y la salida.</p> <p>Desde un punto de vista funcional, si una capa recibe una entrada (x), una red convencional aprende una transformaci\u00f3n (F(x)) y produce como salida (F(x)). En ResNet, la salida se define como (F(x) + x). Esta simple suma introduce una ruta alternativa por la que la informaci\u00f3n puede fluir sin modificaciones, creando una especie de \u201cautopista\u201d que atraviesa la red.</p> <p>Este mecanismo tiene consecuencias profundas para el entrenamiento. Si una capa o bloque no resulta \u00fatil, el modelo puede aproximar el residuo a cero, permitiendo que la se\u00f1al original se propague sin alteraciones. Adem\u00e1s, durante la retropropagaci\u00f3n, el gradiente puede atravesar estas conexiones directas sin atenuarse, lo que estabiliza el aprendizaje incluso en redes extremadamente profundas.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#variantes-arquitectonicas-y-diseno-interno","title":"Variantes Arquitect\u00f3nicas y Dise\u00f1o Interno\u00b6","text":"<p>La familia ResNet incluye m\u00faltiples variantes que se diferencian principalmente por su profundidad, entre las que destacan ResNet-18, ResNet-34, ResNet-50, ResNet-101 y ResNet-152. A partir de cincuenta capas, la arquitectura adopta un dise\u00f1o optimizado conocido como bloque \u201cbottleneck\u201d o cuello de botella, cuyo objetivo es reducir el coste computacional sin sacrificar capacidad representacional.</p> <p>Este bloque se compone de tres convoluciones consecutivas. La primera es una convoluci\u00f3n 1 \u00d7 1 que reduce la dimensionalidad del espacio de caracter\u00edsticas, la segunda es una convoluci\u00f3n 3 \u00d7 3 que realiza el procesamiento principal, y la tercera es otra convoluci\u00f3n 1 \u00d7 1 que restaura la dimensionalidad original. Esta estrategia permite construir redes muy profundas manteniendo un n\u00famero de par\u00e1metros relativamente contenido.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#resnet-en-perspectiva-evolutiva","title":"ResNet en Perspectiva Evolutiva\u00b6","text":"<p>La comparaci\u00f3n entre LeNet-5, VGG-16 y ResNet-50 pone de manifiesto la evoluci\u00f3n conceptual del aprendizaje profundo. LeNet-5, presentado en 1998, cuenta con siete capas y unos sesenta mil par\u00e1metros, y su principal innovaci\u00f3n fue la introducci\u00f3n de la convoluci\u00f3n como mecanismo para explotar la estructura espacial de las im\u00e1genes. VGG-16, en 2014, elev\u00f3 la profundidad a diecis\u00e9is capas mediante una arquitectura uniforme basada en filtros 3 \u00d7 3, pero a costa de un n\u00famero muy elevado de par\u00e1metros, cercano a los ciento treinta y ocho millones. ResNet-50, introducida en 2015, alcanza cincuenta capas con tan solo unos veinticinco millones de par\u00e1metros gracias al uso sistem\u00e1tico de conexiones residuales, logrando una combinaci\u00f3n notable de profundidad, estabilidad y eficiencia.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#impacto-y-vigencia-actual","title":"Impacto y Vigencia Actual\u00b6","text":"<p>En la actualidad, ResNet se considera una arquitectura de referencia tanto en el \u00e1mbito acad\u00e9mico como en aplicaciones industriales. Su equilibrio entre profundidad, estabilidad y eficiencia la ha convertido en la base de numerosos sistemas de reconocimiento facial, conducci\u00f3n aut\u00f3noma, diagn\u00f3stico m\u00e9dico asistido por imagen y an\u00e1lisis visual a gran escala.</p> <p>Entre sus principales ventajas destaca el hecho de ser significativamente m\u00e1s ligera que VGG, ya que, por ejemplo, ResNet-50 utiliza aproximadamente cinco veces menos par\u00e1metros que VGG-16. Adem\u00e1s, su dise\u00f1o residual permite entrenar redes de cientos, e incluso miles, de capas sin que el rendimiento se degrade, algo impensable antes de su introducci\u00f3n. Por estas razones, ResNet no solo resolvi\u00f3 un problema t\u00e9cnico concreto, sino que redefini\u00f3 la forma de concebir la profundidad en las redes neuronales convolucionales.</p>"},{"location":"course/Computer%20Vision/step_08_resnet.html#implementacion","title":"Implementaci\u00f3n\u00b6","text":""},{"location":"course/Computer%20Vision/step_09_transfer_learning.html","title":"Transfer Learning","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Computer%20Vision/step_09_transfer_learning.html#transfer-learning","title":"Transfer Learning\u00b6","text":""},{"location":"course/Computer%20Vision/step_10_knowledge_distillation.html","title":"Knowledge Distillation","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Computer%20Vision/step_10_knowledge_distillation.html#knowledge-distillation","title":"Knowledge Distillation\u00b6","text":""},{"location":"course/Computer%20Vision/step_11_autoencoders.html","title":"Auto Encoders","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:\n        \"\"\"\n        Initializes a residual block that applies two convolutional\n        layers and ReLU activations.\n\n        Args:\n            in_channels: Number of input channels for the block.\n            hidden_size: Number of channels in the hidden layer.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.hidden_size = hidden_size\n\n        self.res_block = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False,\n            ),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=self.hidden_size,\n                out_channels=self.in_channels,\n                kernel_size=1,\n                stride=1,\n                bias=False,\n            ),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the residual block.\n\n        Args:\n            input_tensor: The input tensor to the block.\n\n        Returns:\n            A tensor that is the sum of the input tensor and the\n            block's output.\n        \"\"\"\n\n        return input_tensor + self.res_block(input_tensor)\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_residuals: int,\n        hidden_size: int = 256,\n        kernel_size: int = 4,\n        stride: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes an encoder with convolutional layers and residual\n        blocks.\n\n        Args:\n            in_channels: Number of input channels to the encoder.\n            num_residuals: Number of residual blocks in the encoder.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of the convolutional kernels.\n            stride: Stride of the convolutional kernels.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.num_residuals = num_residuals\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        self.model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=hidden_size,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=1,\n            ),\n            nn.Conv2d(\n                in_channels=hidden_size,\n                out_channels=hidden_size,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=1,\n            ),\n        )\n\n        self.residual_blocks = nn.ModuleList(\n            [\n                ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n                for _ in range(self.num_residuals)\n            ]\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the encoder.\n\n        Args:\n            input_tensor: The input tensor to the encoder.\n\n        Returns:\n            A tensor processed by convolutional layers and residual\n            blocks.\n        \"\"\"\n\n        encoder_output = self.model(input_tensor)\n        for res_block in self.residual_blocks:\n            encoder_output = res_block(encoder_output)\n        return encoder_output\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_residuals: int,\n        out_channels: int = 3,  # Channel output (RGB)\n        hidden_size: int = 256,\n        kernel_size: int = 4,\n        stride: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a decoder with residual blocks and transpose\n        convolutional layers.\n\n        Args:\n            in_channels: Number of input channels to the decoder.\n            num_residuals: Number of residual blocks in the decoder.\n            out_channels: Number of output channels, e.g., RGB.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of the convolutional kernels.\n            stride: Stride of the convolutional kernels.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.num_residuals = num_residuals\n        self.out_channels = out_channels\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        self.residual_blocks = nn.ModuleList(\n            [\n                ResidualBlock(\n                    in_channels=self.in_channels, hidden_size=self.hidden_size\n                )\n                for _ in range(self.num_residuals)\n            ]\n        )\n\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size,\n                kernel_size=self.kernel_size,\n                stride=self.stride,\n                padding=1,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.out_channels,\n                kernel_size=self.kernel_size,\n                stride=self.stride,\n                padding=1,\n            ),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the decoder.\n\n        Args:\n            input_tensor: The input tensor to the decoder.\n\n        Returns:\n            A tensor processed by residual blocks and transpose\n            convolutional layers.\n        \"\"\"\n\n        decoder_output = input_tensor\n        for res_block in self.residual_blocks:\n            decoder_output = res_block(decoder_output)\n\n        return self.model(decoder_output)\n\n\nclass VectorQuantizer(nn.Module):\n    def __init__(\n        self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n    ) -&gt; None:\n        \"\"\"\n        Initializes a vector quantizer with a learnable codebook.\n\n        Args:\n            size_discrete_space: Number of discrete embeddings.\n            size_embeddings: Size of each embedding vector.\n            beta: Weighting factor for the commitment loss.\n        \"\"\"\n\n        super().__init__()\n\n        self.size_discrete_space = size_discrete_space\n        self.size_embeddings = size_embeddings\n        self.beta = beta\n\n        # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings\n        # Ha de ser una matriz aprendible\n        self.codebook = nn.Embedding(\n            num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n        )\n        # Initialize weights uniformly\n        self.codebook.weight.data.uniform_(\n            -1 / self.size_discrete_space, 1 / self.size_discrete_space\n        )\n\n    def forward(\n        self, encoder_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Quantizes the encoder output using the codebook.\n\n        Args:\n            encoder_output: Tensor of encoder outputs.\n\n        Returns:\n            A tuple containing VQ loss, quantized tensor, perplexity,\n            and encodings.\n        \"\"\"\n\n        # Comentario de otras implementaciones: The channels are used as the space\n        # in which to quantize.\n        # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)\n        encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n        b, h, w, c = encoder_output.size()\n        encoder_output_flat = encoder_output.reshape(-1, c)\n\n        # Calculamos la distancia entre ambos vectores\n        distances = (\n            torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n            + torch.sum(self.codebook.weight**2, dim=1)\n            - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n        )\n\n        # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n        # Matriz de ceros de (indices, size_discrete_space)\n        encodings = torch.zeros(\n            encoding_indices.shape[0],\n            self.size_discrete_space,\n            device=encoder_output.device,\n        )\n        # Colocamos un 1 en los indices de los encodings con el\n        # valor m\u00ednimo de distancia creando un vector one-hot\n        encodings.scatter_(1, encoding_indices, 1)\n\n        # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n        # del codebook y le damos formato de nuevo al tensor\n        quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n\n        # VQ-VAE loss terms\n        # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2\n        # FIX: Corrected variable names and loss calculation\n        commitment_loss = F.mse_loss(\n            quantized.detach(), encoder_output\n        )  # ||sg[z_e] - e||^2\n        embedding_loss = F.mse_loss(\n            quantized, encoder_output.detach()\n        )  # ||z_e - sg[e]||^2\n        vq_loss = commitment_loss + self.beta * embedding_loss\n\n        # Straight-through estimator\n        quantized = encoder_output + (quantized - encoder_output).detach()\n\n        # Calculate perplexity\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        # convert quantized from BHWC -&gt; BCHW\n        return (\n            vq_loss,\n            quantized.permute(0, 3, 1, 2).contiguous(),\n            perplexity,\n            encodings,\n        )\n\n\nclass VQVAE(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        size_discrete_space: int,\n        size_embeddings: int,\n        num_residuals: int,\n        hidden_size: int,\n        kernel_size: int,\n        stride: int,\n        beta: float = 0.25,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n\n        Args:\n            in_channels: Number of input channels for the model.\n            size_discrete_space: Number of discrete embeddings.\n            size_embeddings: Size of each embedding vector.\n            num_residuals: Number of residual blocks in encoder/decoder.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of convolutional kernels.\n            stride: Stride of convolutional kernels.\n            beta: Weighting factor for the commitment loss.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.size_discrete_space = size_discrete_space\n        self.size_embeddings = size_embeddings\n        self.num_residuals = num_residuals\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.beta = beta\n\n        self.encoder = Encoder(\n            in_channels=self.in_channels,\n            num_residuals=self.num_residuals,\n            hidden_size=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n        )\n        self.decoder = Decoder(\n            in_channels=self.hidden_size,\n            num_residuals=self.num_residuals,\n            out_channels=self.in_channels,\n            hidden_size=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n        )\n\n        self.vector_quantizer = VectorQuantizer(\n            size_discrete_space=self.size_discrete_space,\n            size_embeddings=self.hidden_size,\n            beta=self.beta,\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass through VQ-VAE model.\n\n        Args:\n            input_tensor: Input tensor to the model.\n\n        Returns:\n            A tuple containing VQ loss, reconstructed tensor,\n            and perplexity.\n        \"\"\"\n\n        encoder_output = self.encoder(input_tensor)\n        vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n        decoder_output = self.decoder(quantized)\n        return vq_loss, decoder_output, perplexity\n\n\nif __name__ == \"__main__\":\n    model = VQVAE(\n        in_channels=3,\n        size_discrete_space=512,\n        size_embeddings=64,\n        num_residuals=2,\n        hidden_size=128,\n        kernel_size=4,\n        stride=2,\n        beta=0.25,\n    )\n\n    x = torch.randn(4, 3, 64, 64)\n    vq_loss, reconstruction, perplexity = model(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Reconstruction shape: {reconstruction.shape}\")\n    print(f\"VQ Loss: {vq_loss.item():.4f}\")\n    print(f\"Perplexity: {perplexity.item():.4f}\")\n</pre> # 3pps import torch from torch import nn from torch.nn import functional as F   class ResidualBlock(nn.Module):     def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:         \"\"\"         Initializes a residual block that applies two convolutional         layers and ReLU activations.          Args:             in_channels: Number of input channels for the block.             hidden_size: Number of channels in the hidden layer.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.hidden_size = hidden_size          self.res_block = nn.Sequential(             nn.ReLU(),             nn.Conv2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,                 bias=False,             ),             nn.ReLU(),             nn.Conv2d(                 in_channels=self.hidden_size,                 out_channels=self.in_channels,                 kernel_size=1,                 stride=1,                 bias=False,             ),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the residual block.          Args:             input_tensor: The input tensor to the block.          Returns:             A tensor that is the sum of the input tensor and the             block's output.         \"\"\"          return input_tensor + self.res_block(input_tensor)   class Encoder(nn.Module):     def __init__(         self,         in_channels: int,         num_residuals: int,         hidden_size: int = 256,         kernel_size: int = 4,         stride: int = 2,     ) -&gt; None:         \"\"\"         Initializes an encoder with convolutional layers and residual         blocks.          Args:             in_channels: Number of input channels to the encoder.             num_residuals: Number of residual blocks in the encoder.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of the convolutional kernels.             stride: Stride of the convolutional kernels.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.num_residuals = num_residuals         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride          self.model = nn.Sequential(             nn.Conv2d(                 in_channels=in_channels,                 out_channels=hidden_size,                 kernel_size=kernel_size,                 stride=stride,                 padding=1,             ),             nn.Conv2d(                 in_channels=hidden_size,                 out_channels=hidden_size,                 kernel_size=kernel_size,                 stride=stride,                 padding=1,             ),         )          self.residual_blocks = nn.ModuleList(             [                 ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)                 for _ in range(self.num_residuals)             ]         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the encoder.          Args:             input_tensor: The input tensor to the encoder.          Returns:             A tensor processed by convolutional layers and residual             blocks.         \"\"\"          encoder_output = self.model(input_tensor)         for res_block in self.residual_blocks:             encoder_output = res_block(encoder_output)         return encoder_output   class Decoder(nn.Module):     def __init__(         self,         in_channels: int,         num_residuals: int,         out_channels: int = 3,  # Channel output (RGB)         hidden_size: int = 256,         kernel_size: int = 4,         stride: int = 2,     ) -&gt; None:         \"\"\"         Initializes a decoder with residual blocks and transpose         convolutional layers.          Args:             in_channels: Number of input channels to the decoder.             num_residuals: Number of residual blocks in the decoder.             out_channels: Number of output channels, e.g., RGB.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of the convolutional kernels.             stride: Stride of the convolutional kernels.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.num_residuals = num_residuals         self.out_channels = out_channels         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride          self.residual_blocks = nn.ModuleList(             [                 ResidualBlock(                     in_channels=self.in_channels, hidden_size=self.hidden_size                 )                 for _ in range(self.num_residuals)             ]         )          self.model = nn.Sequential(             nn.ConvTranspose2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size,                 kernel_size=self.kernel_size,                 stride=self.stride,                 padding=1,             ),             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.out_channels,                 kernel_size=self.kernel_size,                 stride=self.stride,                 padding=1,             ),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the decoder.          Args:             input_tensor: The input tensor to the decoder.          Returns:             A tensor processed by residual blocks and transpose             convolutional layers.         \"\"\"          decoder_output = input_tensor         for res_block in self.residual_blocks:             decoder_output = res_block(decoder_output)          return self.model(decoder_output)   class VectorQuantizer(nn.Module):     def __init__(         self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25     ) -&gt; None:         \"\"\"         Initializes a vector quantizer with a learnable codebook.          Args:             size_discrete_space: Number of discrete embeddings.             size_embeddings: Size of each embedding vector.             beta: Weighting factor for the commitment loss.         \"\"\"          super().__init__()          self.size_discrete_space = size_discrete_space         self.size_embeddings = size_embeddings         self.beta = beta          # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings         # Ha de ser una matriz aprendible         self.codebook = nn.Embedding(             num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings         )         # Initialize weights uniformly         self.codebook.weight.data.uniform_(             -1 / self.size_discrete_space, 1 / self.size_discrete_space         )      def forward(         self, encoder_output: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"         Quantizes the encoder output using the codebook.          Args:             encoder_output: Tensor of encoder outputs.          Returns:             A tuple containing VQ loss, quantized tensor, perplexity,             and encodings.         \"\"\"          # Comentario de otras implementaciones: The channels are used as the space         # in which to quantize.         # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)         encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()         b, h, w, c = encoder_output.size()         encoder_output_flat = encoder_output.reshape(-1, c)          # Calculamos la distancia entre ambos vectores         distances = (             torch.sum(encoder_output_flat**2, dim=1, keepdim=True)             + torch.sum(self.codebook.weight**2, dim=1)             - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())         )          # Realizamos el encoding y extendemos una dimension (B*H*W, 1)         encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)          # Matriz de ceros de (indices, size_discrete_space)         encodings = torch.zeros(             encoding_indices.shape[0],             self.size_discrete_space,             device=encoder_output.device,         )         # Colocamos un 1 en los indices de los encodings con el         # valor m\u00ednimo de distancia creando un vector one-hot         encodings.scatter_(1, encoding_indices, 1)          # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)         # del codebook y le damos formato de nuevo al tensor         quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)          # VQ-VAE loss terms         # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2         # FIX: Corrected variable names and loss calculation         commitment_loss = F.mse_loss(             quantized.detach(), encoder_output         )  # ||sg[z_e] - e||^2         embedding_loss = F.mse_loss(             quantized, encoder_output.detach()         )  # ||z_e - sg[e]||^2         vq_loss = commitment_loss + self.beta * embedding_loss          # Straight-through estimator         quantized = encoder_output + (quantized - encoder_output).detach()          # Calculate perplexity         avg_probs = torch.mean(encodings, dim=0)         perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))          # convert quantized from BHWC -&gt; BCHW         return (             vq_loss,             quantized.permute(0, 3, 1, 2).contiguous(),             perplexity,             encodings,         )   class VQVAE(nn.Module):     def __init__(         self,         in_channels: int,         size_discrete_space: int,         size_embeddings: int,         num_residuals: int,         hidden_size: int,         kernel_size: int,         stride: int,         beta: float = 0.25,     ) -&gt; None:         \"\"\"         Initializes a VQ-VAE model with encoder, decoder, and quantizer.          Args:             in_channels: Number of input channels for the model.             size_discrete_space: Number of discrete embeddings.             size_embeddings: Size of each embedding vector.             num_residuals: Number of residual blocks in encoder/decoder.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of convolutional kernels.             stride: Stride of convolutional kernels.             beta: Weighting factor for the commitment loss.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.size_discrete_space = size_discrete_space         self.size_embeddings = size_embeddings         self.num_residuals = num_residuals         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride         self.beta = beta          self.encoder = Encoder(             in_channels=self.in_channels,             num_residuals=self.num_residuals,             hidden_size=self.hidden_size,             kernel_size=self.kernel_size,             stride=self.stride,         )         self.decoder = Decoder(             in_channels=self.hidden_size,             num_residuals=self.num_residuals,             out_channels=self.in_channels,             hidden_size=self.hidden_size,             kernel_size=self.kernel_size,             stride=self.stride,         )          self.vector_quantizer = VectorQuantizer(             size_discrete_space=self.size_discrete_space,             size_embeddings=self.hidden_size,             beta=self.beta,         )      def forward(         self, input_tensor: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"         Forward pass through VQ-VAE model.          Args:             input_tensor: Input tensor to the model.          Returns:             A tuple containing VQ loss, reconstructed tensor,             and perplexity.         \"\"\"          encoder_output = self.encoder(input_tensor)         vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)         decoder_output = self.decoder(quantized)         return vq_loss, decoder_output, perplexity   if __name__ == \"__main__\":     model = VQVAE(         in_channels=3,         size_discrete_space=512,         size_embeddings=64,         num_residuals=2,         hidden_size=128,         kernel_size=4,         stride=2,         beta=0.25,     )      x = torch.randn(4, 3, 64, 64)     vq_loss, reconstruction, perplexity = model(x)      print(f\"Input shape: {x.shape}\")     print(f\"Reconstruction shape: {reconstruction.shape}\")     print(f\"VQ Loss: {vq_loss.item():.4f}\")     print(f\"Perplexity: {perplexity.item():.4f}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Computer%20Vision/step_11_autoencoders.html#auto-encoders","title":"Auto Encoders\u00b6","text":""},{"location":"course/Computer%20Vision/step_11_autoencoders.html#auto-encoders-basados-en-cnns","title":"Auto Encoders basados en CNNs\u00b6","text":""},{"location":"course/Computer%20Vision/step_11_autoencoders.html#variational-auto-encoders-vae","title":"Variational Auto Encoders (VAE)\u00b6","text":""},{"location":"course/Computer%20Vision/step_11_autoencoders.html#vector-quantization-variational-auto-encoders-vq-vae","title":"Vector Quantization Variational Auto Encoders (VQ-VAE)\u00b6","text":""},{"location":"course/Computer%20Vision/step_12_attention.html","title":"Attention Mechanism in Convolutions","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEste clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507\n\"\"\"\n\n# 3pps\nimport torch\nfrom torch import nn\n\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, channel_size: int, ratio: int) -&gt; None:\n        \"\"\"\n        Implements Squeeze-and-Excitation (SE) block.\n\n        Args:\n            channel_size: Number of channels in the input tensor.\n            ratio: Reduction factor for the compression layer.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Vamos a crear un modelo Sequential\n        self.se_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)\n            nn.Flatten(),  # (B, C)\n            nn.Linear(\n                in_features=channel_size, out_features=channel_size // ratio\n            ),  # (B, C//ratio)\n            nn.ReLU(),  # (B, C//ratio)\n            nn.Linear(\n                in_features=channel_size // ratio, out_features=channel_size\n            ),  # (B, C)\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Applies attention mechanism to input tensor.\n\n        Args:\n            input_tensor: Input tensor with shape (B, C, H, W).\n\n        Returns:\n            Tensor with attention applied, same shape as input.\n        \"\"\"\n\n        # Primero podemos obtener el tama\u00f1o del tensor de entrada\n        b, c, _, _ = input_tensor.size()\n\n        # Obtenemos el tensor de aplicar SE\n        x = self.se_block(input_tensor)\n\n        # Modificamos el shape del tensor para ajustarlo al input\n        x = x.view(b, c, 1, 1)\n\n        # Aplicamos el producto como mecanismo de atenci\u00f3n\n        return x * input_tensor\n\n\nif __name__ == \"__main__\":\n    model = SqueezeExcitation(channel_size=3, ratio=16)\n\n    x = torch.randn(1, 3, 4, 4)\n    output_model = model(x)\n\n    print(output_model)\n</pre> \"\"\" Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507 \"\"\"  # 3pps import torch from torch import nn   class SqueezeExcitation(nn.Module):     def __init__(self, channel_size: int, ratio: int) -&gt; None:         \"\"\"         Implements Squeeze-and-Excitation (SE) block.          Args:             channel_size: Number of channels in the input tensor.             ratio: Reduction factor for the compression layer.         \"\"\"          # Constructor de la clase         super().__init__()          # Vamos a crear un modelo Sequential         self.se_block = nn.Sequential(             nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)             nn.Flatten(),  # (B, C)             nn.Linear(                 in_features=channel_size, out_features=channel_size // ratio             ),  # (B, C//ratio)             nn.ReLU(),  # (B, C//ratio)             nn.Linear(                 in_features=channel_size // ratio, out_features=channel_size             ),  # (B, C)             nn.Sigmoid(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Applies attention mechanism to input tensor.          Args:             input_tensor: Input tensor with shape (B, C, H, W).          Returns:             Tensor with attention applied, same shape as input.         \"\"\"          # Primero podemos obtener el tama\u00f1o del tensor de entrada         b, c, _, _ = input_tensor.size()          # Obtenemos el tensor de aplicar SE         x = self.se_block(input_tensor)          # Modificamos el shape del tensor para ajustarlo al input         x = x.view(b, c, 1, 1)          # Aplicamos el producto como mecanismo de atenci\u00f3n         return x * input_tensor   if __name__ == \"__main__\":     model = SqueezeExcitation(channel_size=3, ratio=16)      x = torch.randn(1, 3, 4, 4)     output_model = model(x)      print(output_model)"},{"location":"course/Computer%20Vision/step_12_attention.html#attention-mechanism-in-convolutions","title":"Attention Mechanism in Convolutions\u00b6","text":""},{"location":"course/Computer%20Vision/step_12_attention.html#se","title":"SE\u00b6","text":""},{"location":"course/Computer%20Vision/step_13_vision_transformers.html","title":"Transformers in Computer Vision","text":"In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Patches(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        img_height: int,\n        img_width: int,\n    ) -&gt; None:\n        \"\"\"\n        Initialize patch extraction module.\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            img_height: Height of the input image.\n            img_width: Width of the input image.\n\n        Raises:\n            ValueError: If img_height not divisible by patch height.\n            ValueError: If img_width not divisible by patch width.\n        \"\"\"\n\n        super().__init__()\n\n        if img_height % patch_size_height != 0:\n            raise ValueError(\n                \"img_height tiene que se divisible entre el patch_size_height\"\n            )\n\n        if img_width % patch_size_width != 0:\n            raise ValueError(\n                \"img_width tiene que se divisible entre el patch_size_width\"\n            )\n\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.unfold = nn.Unfold(\n            kernel_size=(self.patch_size_height, self.patch_size_width),\n            stride=(self.patch_size_height, self.patch_size_width),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Extract patches from input tensor.\n\n        Args:\n            input_tensor: Batch of images as a tensor.\n\n        Returns:\n            Tensor with patches from input images.\n        \"\"\"\n\n        # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n        patches = self.unfold(input_tensor)\n        # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n        return patches.transpose(2, 1)\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        in_channels: int,\n        d_model: int,\n    ) -&gt; None:\n        \"\"\"\n        Initialize patch embedding module.\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            in_channels: Number of input channels.\n            d_model: Dimension of the model.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.in_channels = in_channels\n        self.d_model = d_model\n\n        # Esta es una de las diferencias con usar transformers en el texto\n        # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n\n        # de los pixeles\n        self.embedding = nn.Linear(\n            in_features=self.in_channels\n            * self.patch_size_height\n            * self.patch_size_width,\n            out_features=self.d_model,\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Apply linear projection to input tensor.\n\n        Args:\n            input_tensor: Batch of image patches as a tensor.\n\n        Returns:\n            Tensor after linear projection of patches.\n        \"\"\"\n\n        return self.embedding(input_tensor)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize positional encoding module.\n\n        Args:\n            d_model: Dimension of the model.\n            sequence_length: Max length of input sequences.\n            dropout_rate: Dropout rate applied on outputs.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.d_model = d_model\n\n        # Cuando le damos una secuencia de tokens, tenemos que saber\n        # la longitud m\u00e1xima de la secuencia\n        self.sequence_length = sequence_length\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Creamos una matriz del positional embedding\n        # (sequence_length, d_model)\n        pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n        # Crear vector de posiciones\n        position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n        # Crear vector de divisores\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n\n        # Aplicar sin y cos\n        pe_matrix[:, 0::2] = torch.sin(position * div_term)\n        pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n        # Tenemos que convertirlo a (1, sequence_length, d_model) para\n        # procesarlo por lotes\n        pe_matrix = pe_matrix.unsqueeze(0)\n\n        # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n        self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Add positional encoding to input embeddings.\n\n        Args:\n            input_embedding: Batch of input embeddings.\n\n        Returns:\n            Embeddings with added positional encoding.\n        \"\"\"\n\n        # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n        # Seleccionamos\n        x = input_embedding + (\n            self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n        ).requires_grad_(False)\n        return self.dropout(x)\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n        \"\"\"\n        Initialize layer normalization module.\n\n        Args:\n            features: Number of features in input.\n            eps: Small value to avoid division by zero.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.features = features\n        self.eps = eps\n\n        # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n        self.alpha = nn.Parameter(torch.ones(self.features))\n        # Utilizamos un factor del sesgo para sumar\n        self.bias = nn.Parameter(torch.zeros(self.features))\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Apply layer normalization to input embeddings.\n\n        Args:\n            input_embedding: Batch of input embeddings.\n\n        Returns:\n            Normalized embeddings.\n        \"\"\"\n\n        # (B, sequence_length, d_model)\n        mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n        var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n        return (\n            self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n            + self.bias\n        )\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize feed-forward neural network.\n\n        Args:\n            d_model: Input and output feature dimensions.\n            d_ff: Hidden layer feature dimensions.\n            dropout_rate: Dropout rate applied on layers.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        # Creamos el modelo secuencial\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through feed-forward layers.\n\n        Args:\n            input_tensor: Batch of input tensors.\n\n        Returns:\n            Output tensor after feed-forward processing.\n        \"\"\"\n\n        # (B, sequence_length, d_model)\n        return self.ffn(input_tensor)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize multi-head attention module.\n\n        Args:\n            d_model: Number of features in input.\n            h: Number of attention heads.\n            dropout_rate: Dropout rate applied on scores.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n        # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n        if d_model % h != 0:\n            raise ValueError(\"d_model ha de ser divisible entre h\")\n\n        self.d_model = d_model\n        self.h = h\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Valore establecidos en el paper\n        self.d_k = self.d_model // self.h\n        self.d_v = self.d_model // self.h\n\n        # Par\u00e1metros\n        self.W_K = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_Q = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_V = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_OUTPUT_CONCAT = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n\n    @staticmethod\n    def attention(\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n        dropout: nn.Dropout | None = None,\n    ):\n        \"\"\"\n        Compute attention scores and output.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Mask tensor, optional.\n            dropout: Dropout layer, optional.\n\n        Returns:\n            Tuple of attention output and scores.\n        \"\"\"\n\n        # Primero realizamos el producto matricial con la transpuesta\n        # q = (Batch, h, seq_len, d_k)\n        # k.T = (Batch, h, d_k, seq_len)\n        # matmul_q_k = (Batch, h, seq_len, seq_len)\n        matmul_q_k = q @ k.transpose(-2, -1)\n\n        # Luego realizamos el escalado\n        d_k = k.shape[-1]\n        matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n        # El enmascarado es para el decoder, relleno de infinitos\n        if mask is not None:\n            matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n        # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n        attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n        # Aplicamos dropout\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n\n        # Multiplicamos por el valor\n        # attention_scores = (Batch, h, seq_len, seq_len)\n        # v = (Batch, h, seq_len, d_k)\n        # Output = (Batch, h, seq_len, d_k)\n        return (attention_scores @ v), attention_scores\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensors through multi-head attention.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Mask tensor, optional.\n\n        Returns:\n            Output tensor after attention processing.\n        \"\"\"\n\n        # k -&gt; (Batch, seq_len, d_model) igual para el resto\n        key_prima = self.W_K(k)\n        query_prima = self.W_Q(q)\n        value_prima = self.W_V(v)\n\n        # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n        # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n        # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n        key_prima = key_prima.view(\n            key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        query_prima = query_prima.view(\n            query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        value_prima = value_prima.view(\n            value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n\n        # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n        # attention = (Batch, h, seq_len, d_k)\n        # attention_scores = (Batch, h, seq_len, seq_len)\n        attention, attention_scores = MultiHeadAttention.attention(\n            k=key_prima,\n            q=query_prima,\n            v=value_prima,\n            mask=mask,\n            dropout=self.dropout,\n        )\n\n        # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n        # Queremos (Batch, seq_len, d_model)\n        # self.d_k = self.d_model // self.h; d_model = d_k * h\n        attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n        b, seq_len, h, d_k = attention.size()\n        # Al parecer, contiguous permite evitar errores de memoria\n        attention_concat = attention.contiguous().view(\n            b, seq_len, h * d_k\n        )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n        return self.W_OUTPUT_CONCAT(attention_concat)\n\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, features: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize residual connection module.\n\n        Args:\n            features: Number of features in input.\n            dropout_rate: Dropout rate for sublayer output.\n        \"\"\"\n\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNormalization(features=features)\n\n    def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n        \"\"\"\n        Apply residual connection to sublayer output.\n\n        Args:\n            input_tensor: Original input tensor.\n            sublayer: Sublayer module to apply.\n\n        Returns:\n            Tensor with residual connection applied.\n        \"\"\"\n\n        return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize encoder block module.\n\n        Args:\n            d_model: Number of features in input.\n            d_ff: Hidden layer feature dimensions.\n            h: Number of attention heads.\n            dropout_rate: Dropout rate for layers.\n        \"\"\"\n\n        super().__init__()\n\n        # Parametros\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout_rate = dropout_rate\n\n        # Definicion de las capas\n        self.multi_head_attention_layer = MultiHeadAttention(\n            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_1 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n        self.feed_forward_layer = FeedForward(\n            d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_2 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through encoder block.\n\n        Args:\n            input_tensor: Batch of input tensors.\n            mask: Mask tensor, optional.\n\n        Returns:\n            Output tensor after encoder block processing.\n        \"\"\"\n\n        # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n        input_tensor = self.residual_layer_1(\n            input_tensor,\n            lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n        )\n\n        # Segunda conexi\u00f3n residual con feed-forward\n        input_tensor = self.residual_layer_2(\n            input_tensor, lambda x: self.feed_forward_layer(x)\n        )\n\n        return input_tensor\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        img_height: int,\n        img_width: int,\n        in_channels: int,\n        num_encoders: int,\n        d_model: int,\n        d_ff: int,\n        h: int,\n        num_classes: int,\n        dropout_rate: float,\n    ) -&gt; None:\n        \"\"\"\n        Initialize Vision Transformer (VIT).\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            img_height: Height of input images.\n            img_width: Width of input images.\n            in_channels: Number of input channels.\n            num_encoders: Number of encoder blocks.\n            d_model: Dimension of the model.\n            d_ff: Dimension of feed-forward layers.\n            h: Number of attention heads.\n            num_classes: Number of output classes.\n            dropout_rate: Dropout rate for layers.\n        \"\"\"\n\n        super().__init__()\n\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.img_height = img_height\n        self.img_width = img_width\n        self.in_channels = in_channels\n        self.num_encoders = num_encoders\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n\n        # N\u00famero de patches\n        self.num_patches = (img_height // patch_size_height) * (\n            img_width // patch_size_width\n        )\n\n        # CLS token permite tener una representaci\u00f3n global de todos los inputs\n        # de la imagen (de los diferentes embeddings de cada patch)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n\n        self.patch_layer = Patches(\n            patch_size_height=self.patch_size_height,\n            patch_size_width=self.patch_size_width,\n            img_height=self.img_height,\n            img_width=self.img_width,\n        )\n\n        self.embeddings = PatchEmbedding(\n            patch_size_height=self.patch_size_height,\n            patch_size_width=self.patch_size_width,\n            in_channels=self.in_channels,\n            d_model=self.d_model,\n        )\n\n        # Entiendo que la longitud de la secuencia coincide con el numero de patches\n        # y un embedding m\u00e1s de la clase,\n        self.positional_encoding = PositionalEncoding(\n            d_model=self.d_model,\n            sequence_length=self.num_patches + 1,\n            dropout_rate=self.dropout_rate,\n        )\n\n        # Capas del Encoder\n        self.encoder_layers = nn.ModuleList(\n            [\n                EncoderBlock(\n                    d_model=self.d_model,\n                    d_ff=self.d_ff,\n                    h=self.h,\n                    dropout_rate=self.dropout_rate,\n                )\n                for _ in range(self.num_encoders)\n            ]\n        )\n\n        self.layer_norm = LayerNormalization(features=self.d_model)\n\n        self.mlp_classifier = nn.Sequential(\n            nn.Linear(in_features=self.d_model, out_features=self.d_model),\n            nn.GELU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(in_features=self.d_model, out_features=num_classes),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through VIT model.\n\n        Args:\n            input_tensor: Batch of input images.\n\n        Returns:\n            Classification output tensor.\n        \"\"\"\n\n        # Extraemos los patches\n        input_patches = self.patch_layer(input_tensor)\n\n        # Convertimso a embeddings los patches\n        patch_embeddings = self.embeddings(input_patches)\n\n        # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia\n        # (B, 1, d_model)\n        cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n        # (B, num_patches+1, d_model)\n        embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n\n        # A\u00f1adir positional encoding\n        embeddings = self.positional_encoding(embeddings)\n\n        # Encoders del transformer\n        encoder_output = embeddings\n        for encoder_layer in self.encoder_layers:\n            encoder_output = encoder_layer(encoder_output)\n\n        # Usar solo el CLS token para clasificaci\u00f3n\n        encoder_output = self.layer_norm(encoder_output)\n        cls_output = encoder_output[:, 0]\n\n        # Clasificaci\u00f3n final\n        return self.mlp_classifier(cls_output)\n\n\nif __name__ == \"__main__\":\n    model = VisionTransformer(\n        patch_size_height=16,\n        patch_size_width=16,\n        img_height=224,\n        img_width=224,\n        in_channels=3,\n        num_encoders=12,\n        d_model=768,\n        d_ff=3072,\n        h=12,\n        num_classes=1000,\n        dropout_rate=0.1,\n    )\n\n    x = torch.randn(2, 3, 224, 224)\n    output = model(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n</pre> # Standard libraries import math  # 3pps import torch from torch import nn from torch.nn import functional as F   class Patches(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         img_height: int,         img_width: int,     ) -&gt; None:         \"\"\"         Initialize patch extraction module.          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             img_height: Height of the input image.             img_width: Width of the input image.          Raises:             ValueError: If img_height not divisible by patch height.             ValueError: If img_width not divisible by patch width.         \"\"\"          super().__init__()          if img_height % patch_size_height != 0:             raise ValueError(                 \"img_height tiene que se divisible entre el patch_size_height\"             )          if img_width % patch_size_width != 0:             raise ValueError(                 \"img_width tiene que se divisible entre el patch_size_width\"             )          self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.unfold = nn.Unfold(             kernel_size=(self.patch_size_height, self.patch_size_width),             stride=(self.patch_size_height, self.patch_size_width),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Extract patches from input tensor.          Args:             input_tensor: Batch of images as a tensor.          Returns:             Tensor with patches from input images.         \"\"\"          # unfold devuelve (b, c * patch_height * patch_width, num_patches)         patches = self.unfold(input_tensor)         # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)         return patches.transpose(2, 1)   class PatchEmbedding(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         in_channels: int,         d_model: int,     ) -&gt; None:         \"\"\"         Initialize patch embedding module.          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             in_channels: Number of input channels.             d_model: Dimension of the model.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.in_channels = in_channels         self.d_model = d_model          # Esta es una de las diferencias con usar transformers en el texto         # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n         # de los pixeles         self.embedding = nn.Linear(             in_features=self.in_channels             * self.patch_size_height             * self.patch_size_width,             out_features=self.d_model,         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Apply linear projection to input tensor.          Args:             input_tensor: Batch of image patches as a tensor.          Returns:             Tensor after linear projection of patches.         \"\"\"          return self.embedding(input_tensor)   class PositionalEncoding(nn.Module):     def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize positional encoding module.          Args:             d_model: Dimension of the model.             sequence_length: Max length of input sequences.             dropout_rate: Dropout rate applied on outputs.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.d_model = d_model          # Cuando le damos una secuencia de tokens, tenemos que saber         # la longitud m\u00e1xima de la secuencia         self.sequence_length = sequence_length         self.dropout = nn.Dropout(dropout_rate)          # Creamos una matriz del positional embedding         # (sequence_length, d_model)         pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))          # Crear vector de posiciones         position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)          # Crear vector de divisores         div_term = torch.exp(             torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)         )          # Aplicar sin y cos         pe_matrix[:, 0::2] = torch.sin(position * div_term)         pe_matrix[:, 1::2] = torch.cos(position * div_term)          # Tenemos que convertirlo a (1, sequence_length, d_model) para         # procesarlo por lotes         pe_matrix = pe_matrix.unsqueeze(0)          # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo         self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Add positional encoding to input embeddings.          Args:             input_embedding: Batch of input embeddings.          Returns:             Embeddings with added positional encoding.         \"\"\"          # (B, ..., d_model) -&gt; (B, sequence_length, d_model)         # Seleccionamos         x = input_embedding + (             self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore         ).requires_grad_(False)         return self.dropout(x)   class LayerNormalization(nn.Module):     def __init__(self, features: int, eps: float = 1e-6) -&gt; None:         \"\"\"         Initialize layer normalization module.          Args:             features: Number of features in input.             eps: Small value to avoid division by zero.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.features = features         self.eps = eps          # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n         self.alpha = nn.Parameter(torch.ones(self.features))         # Utilizamos un factor del sesgo para sumar         self.bias = nn.Parameter(torch.zeros(self.features))      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Apply layer normalization to input embeddings.          Args:             input_embedding: Batch of input embeddings.          Returns:             Normalized embeddings.         \"\"\"          # (B, sequence_length, d_model)         mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)         var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)         return (             self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))             + self.bias         )   class FeedForward(nn.Module):     def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize feed-forward neural network.          Args:             d_model: Input and output feature dimensions.             d_ff: Hidden layer feature dimensions.             dropout_rate: Dropout rate applied on layers.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.d_model = d_model         self.d_ff = d_ff          # Creamos el modelo secuencial         self.ffn = nn.Sequential(             nn.Linear(in_features=self.d_model, out_features=self.d_ff),             nn.GELU(),             nn.Dropout(dropout_rate),             nn.Linear(in_features=self.d_ff, out_features=self.d_model),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Process input tensor through feed-forward layers.          Args:             input_tensor: Batch of input tensors.          Returns:             Output tensor after feed-forward processing.         \"\"\"          # (B, sequence_length, d_model)         return self.ffn(input_tensor)   class MultiHeadAttention(nn.Module):     def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize multi-head attention module.          Args:             d_model: Number of features in input.             h: Number of attention heads.             dropout_rate: Dropout rate applied on scores.         \"\"\"          # Constructor de la clase         super().__init__()          # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas         # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0         if d_model % h != 0:             raise ValueError(\"d_model ha de ser divisible entre h\")          self.d_model = d_model         self.h = h         self.dropout = nn.Dropout(dropout_rate)          # Valore establecidos en el paper         self.d_k = self.d_model // self.h         self.d_v = self.d_model // self.h          # Par\u00e1metros         self.W_K = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_Q = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_V = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_OUTPUT_CONCAT = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )      @staticmethod     def attention(         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,         dropout: nn.Dropout | None = None,     ):         \"\"\"         Compute attention scores and output.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Mask tensor, optional.             dropout: Dropout layer, optional.          Returns:             Tuple of attention output and scores.         \"\"\"          # Primero realizamos el producto matricial con la transpuesta         # q = (Batch, h, seq_len, d_k)         # k.T = (Batch, h, d_k, seq_len)         # matmul_q_k = (Batch, h, seq_len, seq_len)         matmul_q_k = q @ k.transpose(-2, -1)          # Luego realizamos el escalado         d_k = k.shape[-1]         matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)          # El enmascarado es para el decoder, relleno de infinitos         if mask is not None:             matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)          # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n         attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)          # Aplicamos dropout         if dropout is not None:             attention_scores = dropout(attention_scores)          # Multiplicamos por el valor         # attention_scores = (Batch, h, seq_len, seq_len)         # v = (Batch, h, seq_len, d_k)         # Output = (Batch, h, seq_len, d_k)         return (attention_scores @ v), attention_scores      def forward(         self,         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Process input tensors through multi-head attention.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Mask tensor, optional.          Returns:             Output tensor after attention processing.         \"\"\"          # k -&gt; (Batch, seq_len, d_model) igual para el resto         key_prima = self.W_K(k)         query_prima = self.W_Q(q)         value_prima = self.W_V(v)          # Cambiamos las dimensiones y hacemos el split de los embedding para cada head         # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)         # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)         key_prima = key_prima.view(             key_prima.shape[0], key_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         query_prima = query_prima.view(             query_prima.shape[0], query_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         value_prima = value_prima.view(             value_prima.shape[0], value_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)          # Obtenemos la matriz de atencion y la puntuaci\u00f3n         # attention = (Batch, h, seq_len, d_k)         # attention_scores = (Batch, h, seq_len, seq_len)         attention, attention_scores = MultiHeadAttention.attention(             k=key_prima,             q=query_prima,             v=value_prima,             mask=mask,             dropout=self.dropout,         )          # Tenemos que concatenar la informaci\u00f3n de todas las cabezas         # Queremos (Batch, seq_len, d_model)         # self.d_k = self.d_model // self.h; d_model = d_k * h         attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)         b, seq_len, h, d_k = attention.size()         # Al parecer, contiguous permite evitar errores de memoria         attention_concat = attention.contiguous().view(             b, seq_len, h * d_k         )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)          return self.W_OUTPUT_CONCAT(attention_concat)   class ResidualConnection(nn.Module):     def __init__(self, features: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize residual connection module.          Args:             features: Number of features in input.             dropout_rate: Dropout rate for sublayer output.         \"\"\"          super().__init__()          self.dropout = nn.Dropout(dropout_rate)         self.layer_norm = LayerNormalization(features=features)      def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:         \"\"\"         Apply residual connection to sublayer output.          Args:             input_tensor: Original input tensor.             sublayer: Sublayer module to apply.          Returns:             Tensor with residual connection applied.         \"\"\"          return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))   class EncoderBlock(nn.Module):     def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize encoder block module.          Args:             d_model: Number of features in input.             d_ff: Hidden layer feature dimensions.             h: Number of attention heads.             dropout_rate: Dropout rate for layers.         \"\"\"          super().__init__()          # Parametros         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.dropout_rate = dropout_rate          # Definicion de las capas         self.multi_head_attention_layer = MultiHeadAttention(             d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate         )         self.residual_layer_1 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )         self.feed_forward_layer = FeedForward(             d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate         )         self.residual_layer_2 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )      def forward(         self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None     ) -&gt; torch.Tensor:         \"\"\"         Process input tensor through encoder block.          Args:             input_tensor: Batch of input tensors.             mask: Mask tensor, optional.          Returns:             Output tensor after encoder block processing.         \"\"\"          # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada         input_tensor = self.residual_layer_1(             input_tensor,             lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),         )          # Segunda conexi\u00f3n residual con feed-forward         input_tensor = self.residual_layer_2(             input_tensor, lambda x: self.feed_forward_layer(x)         )          return input_tensor   class VisionTransformer(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         img_height: int,         img_width: int,         in_channels: int,         num_encoders: int,         d_model: int,         d_ff: int,         h: int,         num_classes: int,         dropout_rate: float,     ) -&gt; None:         \"\"\"         Initialize Vision Transformer (VIT).          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             img_height: Height of input images.             img_width: Width of input images.             in_channels: Number of input channels.             num_encoders: Number of encoder blocks.             d_model: Dimension of the model.             d_ff: Dimension of feed-forward layers.             h: Number of attention heads.             num_classes: Number of output classes.             dropout_rate: Dropout rate for layers.         \"\"\"          super().__init__()          self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.img_height = img_height         self.img_width = img_width         self.in_channels = in_channels         self.num_encoders = num_encoders         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.num_classes = num_classes         self.dropout_rate = dropout_rate          # N\u00famero de patches         self.num_patches = (img_height // patch_size_height) * (             img_width // patch_size_width         )          # CLS token permite tener una representaci\u00f3n global de todos los inputs         # de la imagen (de los diferentes embeddings de cada patch)         self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))          self.patch_layer = Patches(             patch_size_height=self.patch_size_height,             patch_size_width=self.patch_size_width,             img_height=self.img_height,             img_width=self.img_width,         )          self.embeddings = PatchEmbedding(             patch_size_height=self.patch_size_height,             patch_size_width=self.patch_size_width,             in_channels=self.in_channels,             d_model=self.d_model,         )          # Entiendo que la longitud de la secuencia coincide con el numero de patches         # y un embedding m\u00e1s de la clase,         self.positional_encoding = PositionalEncoding(             d_model=self.d_model,             sequence_length=self.num_patches + 1,             dropout_rate=self.dropout_rate,         )          # Capas del Encoder         self.encoder_layers = nn.ModuleList(             [                 EncoderBlock(                     d_model=self.d_model,                     d_ff=self.d_ff,                     h=self.h,                     dropout_rate=self.dropout_rate,                 )                 for _ in range(self.num_encoders)             ]         )          self.layer_norm = LayerNormalization(features=self.d_model)          self.mlp_classifier = nn.Sequential(             nn.Linear(in_features=self.d_model, out_features=self.d_model),             nn.GELU(),             nn.Dropout(self.dropout_rate),             nn.Linear(in_features=self.d_model, out_features=num_classes),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Process input tensor through VIT model.          Args:             input_tensor: Batch of input images.          Returns:             Classification output tensor.         \"\"\"          # Extraemos los patches         input_patches = self.patch_layer(input_tensor)          # Convertimso a embeddings los patches         patch_embeddings = self.embeddings(input_patches)          # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia         # (B, 1, d_model)         cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)         # (B, num_patches+1, d_model)         embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)          # A\u00f1adir positional encoding         embeddings = self.positional_encoding(embeddings)          # Encoders del transformer         encoder_output = embeddings         for encoder_layer in self.encoder_layers:             encoder_output = encoder_layer(encoder_output)          # Usar solo el CLS token para clasificaci\u00f3n         encoder_output = self.layer_norm(encoder_output)         cls_output = encoder_output[:, 0]          # Clasificaci\u00f3n final         return self.mlp_classifier(cls_output)   if __name__ == \"__main__\":     model = VisionTransformer(         patch_size_height=16,         patch_size_width=16,         img_height=224,         img_width=224,         in_channels=3,         num_encoders=12,         d_model=768,         d_ff=3072,         h=12,         num_classes=1000,         dropout_rate=0.1,     )      x = torch.randn(2, 3, 224, 224)     output = model(x)      print(f\"Input shape: {x.shape}\")     print(f\"Output shape: {output.shape}\")"},{"location":"course/Computer%20Vision/step_13_vision_transformers.html#transformers-in-computer-vision","title":"Transformers in Computer Vision\u00b6","text":""},{"location":"course/Computer%20Vision/step_13_vision_transformers.html#vit","title":"ViT\u00b6","text":""},{"location":"course/Computer%20Vision/step_14_interpretability.html","title":"Interpretability","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Computer%20Vision/step_14_interpretability.html#interpretability","title":"Interpretability\u00b6","text":""},{"location":"course/Generative/step_01_gans.html","title":"Generative Adversarial Networks","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n\ndef show_generated_samples(\n    generator: nn.Module, noise, device: str, num_samples: int = 16\n) -&gt; None:\n    \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        samples = generator(noise[:num_samples]).cpu()\n        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(num_samples):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n            axes[row, col].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n\n\nclass Discriminator(nn.Module):\n    def __init__(\n        self, in_channels: int, hidden_size: int = 64, dropout_rate: float = 0.2\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.in_channels = in_channels\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n\n        # Creamos el modelo que lo ajustaremos para MNIST\n        self.model = nn.Sequential(\n            # Input MNIST = (B, C, H, W) = (B, 1, 28, 28) -&gt; (B, H, 14, 14)\n            nn.Conv2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size // 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout2d(p=self.dropout_rate),\n            # (B, H, 14, 14) -&gt; (B, H, 7, 7)\n            nn.Conv2d(\n                in_channels=self.hidden_size // 2,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            nn.Dropout2d(p=self.dropout_rate),\n            # (B, H, 7, 7) -&gt; (B, 1, 7, 7)\n            nn.Conv2d(\n                in_channels=self.hidden_size,\n                out_channels=1,\n                kernel_size=1,\n                stride=1,\n                bias=False,\n            ),\n            # (B, 1, 7, 7) -&gt; (B, 1, 1, 1)\n            nn.AdaptiveAvgPool2d((1, 1)),\n            # (B, 1, 1, 1) -&gt; (B, 1),\n            nn.Flatten(),\n            nn.Dropout(p=self.dropout_rate),\n            # El discriminador ha de devolver un escalar entre 0 y 1 (falso/real)\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(input_tensor)\n\n\nclass Generator(nn.Module):\n    def __init__(\n        self, z_dim: int, data_shape: tuple[int, int, int], hidden_size: int\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.z_dim = z_dim\n        self.data_shape = data_shape\n        self.hidden_size = hidden_size\n\n        # Del ruido que es un tensor plano, vamos a crear una matriz inicial\n        # de (B, H, 7, 7) que es el tama\u00f1o antes de aplanar en el Discriminador\n        self.projection = nn.Sequential(\n            # (B, z_dim) -&gt; (B, H * 7 * 7)\n            nn.Linear(\n                in_features=self.z_dim,\n                out_features=self.hidden_size * 7 * 7,\n                bias=False,\n            ),\n        )\n\n        self.model = nn.Sequential(\n            # (B, H, 7, 7) -&gt; (B, H, 14, 14)\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            # (B, H, 14, 14) -&gt; (B, H, 28, 28)\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            # (B, H, 28, 28) -&gt; (B, 1, 28, 28)\n            nn.Conv2d(\n                in_channels=self.hidden_size, out_channels=1, kernel_size=1, stride=1\n            ),\n            # Normalizamos los valores entre -1 y 1\n            nn.Tanh(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        # (B, z_dim) -&gt; (B, H * 7 * 7)\n        projection = self.projection(input_tensor)\n        # (B, H * 7 * 7) -&gt; (B, H, 7, 7)\n        projection = projection.view(projection.size(0), self.hidden_size, 7, 7)\n\n        return self.model(projection)\n\n\nclass GAN(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_size_discriminator: int,\n        dropout_rate: float,\n        z_dim: int,\n        data_shape: tuple[int, int, int],\n        hidden_size_generator: int,\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros\n        self.in_channels = in_channels\n        self.hidden_size_discriminator = hidden_size_discriminator\n        self.dropout_rate = dropout_rate\n        self.z_dim = z_dim\n        self.data_shape = data_shape\n        self.hidden_size_generator = hidden_size_generator\n\n        # Definimos los modelos\n        self.discriminator = Discriminator(\n            in_channels=self.in_channels,\n            hidden_size=self.hidden_size_discriminator,\n            dropout_rate=self.dropout_rate,\n        )\n        self.generator = Generator(\n            z_dim=self.z_dim,\n            data_shape=self.data_shape,\n            hidden_size=self.hidden_size_generator,\n        )\n\n    def forward(self, real_data: torch.Tensor, batch_size: int | None = None) -&gt; dict:\n        if batch_size is None:\n            batch_size = real_data.size(0)\n\n        # Generar ruido aleatorio\n        noise = torch.randn(batch_size, self.z_dim, device=real_data.device)\n\n        # Generar im\u00e1genes falsas\n        fake_data = self.generator(noise)\n\n        # Pasar datos reales por el discriminador\n        real_predictions = self.discriminator(real_data)\n\n        # Pasar datos falsos por el discriminador\n        fake_predictions = self.discriminator(fake_data)\n\n        return {\n            \"real_predictions\": real_predictions,\n            \"fake_predictions\": fake_predictions,\n            \"fake_data\": fake_data,\n            \"noise\": noise,\n        }\n\n    def generate_samples_inference(self, num_samples: int, device: str) -&gt; torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            # (Num_samples, z_dim)\n            noise = torch.randn(num_samples, self.z_dim, device=device)\n            # (Num_samples, z_dim) -&gt; MNIST: (Num_samples, 1, 28, 28)\n            samples = self.generator(input_tensor=noise)\n\n        return samples\n\n    def discriminator_loss(\n        self, real_predictions: torch.Tensor, fake_predictions: torch.Tensor\n    ) -&gt; torch.Tensor:\n        criterion = nn.BCELoss()\n        # Matriz de 1s\n        real_labels = torch.ones_like(real_predictions)\n        # Matriz de 0s\n        fake_labels = torch.zeros_like(fake_predictions)\n\n        real_loss = criterion(real_predictions, real_labels)\n        fake_loss = criterion(fake_predictions, fake_labels)\n\n        return (real_loss + fake_loss) / 2\n\n    def generator_loss(self, fake_predictions: torch.Tensor) -&gt; torch.Tensor:\n        criterion = nn.BCELoss()\n        fake_real_labels = torch.ones_like(fake_predictions)\n        return criterion(fake_predictions, fake_real_labels)\n\n\nif __name__ == \"__main__\":\n    # Seleccionamos el dispositivo actual\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Usando dispositivo: {device}\")\n\n    # Las GANs son muy sensibles a los hiperpar\u00e1metros\n    lr = 2e-4\n    # Dimensi\u00f3n de los datos de entrada (C, H, W)\n    data_dimension = (1, 28, 28)\n    # Esta es la dimension del ruido\n    z_dim = 100\n    batch_size = 128\n    num_epochs = 50\n\n    # Definimos el modelo\n    model = GAN(\n        in_channels=data_dimension[0],\n        hidden_size_discriminator=64,\n        dropout_rate=0.2,\n        z_dim=z_dim,\n        data_shape=data_dimension,\n        hidden_size_generator=256,\n    ).to(device)\n\n    # Transformaciones que vamos a aplicar a MNIST\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                # Normalizar a [-1, 1] para coincidir con Tanh\n                (0.5,),\n                (0.5,),\n            ),\n        ]\n    )\n\n    # Descargamos MNIST\n    dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Vamos a utilizar un optimizador para cada modelo\n    opt_disc = optim.AdamW(params=model.discriminator.parameters(), lr=lr)\n    opt_gen = optim.AdamW(params=model.generator.parameters(), lr=lr)\n\n    # Ruido fijo para generar muestras durante el entrenamiento\n    fixed_noise = torch.randn(64, z_dim, device=device)\n\n    # Listas para guardar las p\u00e9rdidas\n    disc_losses = []\n    gen_losses = []\n\n    print(\"Iniciando entrenamiento de la GAN...\")\n\n    for epoch in range(num_epochs):\n        epoch_disc_loss: int | float = 0\n        epoch_gen_loss: int | float = 0\n\n        pbar = tqdm(loader, desc=f\"\u00c9poca {epoch + 1}/{num_epochs}\")\n\n        for _, (real, _) in enumerate(pbar):\n            real = real.to(device)\n            batch_size_current = real.shape[0]\n\n            # ENTRENAR DISCRIMINADOR\n            opt_disc.zero_grad()\n\n            # Generar datos falsos\n            noise = torch.randn(batch_size_current, z_dim, device=device)\n            fake_data = model.generator(\n                noise\n            ).detach()  # Detach para no actualizar generador\n\n            # Predicciones del discriminador\n            real_preds = model.discriminator(real)\n            fake_preds = model.discriminator(fake_data)\n\n            # P\u00e9rdida del discriminador\n            lossD = model.discriminator_loss(real_preds, fake_preds)\n            lossD.backward()\n            opt_disc.step()\n\n            # ENTRENAR GENERADOR\n            opt_gen.zero_grad()\n\n            # Generar nuevos datos falsos (sin detach)\n            noise = torch.randn(batch_size_current, z_dim, device=device)\n            fake_data = model.generator(noise)\n            fake_preds_for_gen = model.discriminator(fake_data)\n\n            # P\u00e9rdida del generador\n            lossG = model.generator_loss(fake_preds_for_gen)\n            lossG.backward()\n            opt_gen.step()\n\n            # Acumular p\u00e9rdidas\n            epoch_disc_loss += lossD.item()\n            epoch_gen_loss += lossG.item()\n\n            # Actualizar barra de progreso\n            pbar.set_postfix(\n                {\"D_loss\": f\"{lossD.item():.4f}\", \"G_loss\": f\"{lossG.item():.4f}\"}\n            )\n\n        # P\u00e9rdidas promedio de la \u00e9poca\n        avg_disc_loss = epoch_disc_loss / len(loader)\n        avg_gen_loss = epoch_gen_loss / len(loader)\n        disc_losses.append(avg_disc_loss)\n        gen_losses.append(avg_gen_loss)\n\n        print(\n            f\"\u00c9poca {epoch + 1}, \"\n            f\"P\u00e9rdida D: {avg_disc_loss:.4f}, \"\n            f\"P\u00e9rdida G: {avg_gen_loss:.4f}\"\n        )\n\n        # Mostrar muestras cada 5 \u00e9pocas\n        if (epoch + 1) % 5 == 0:\n            print(f\"\\nMostrando muestras generadas en \u00e9poca {epoch + 1}...\")\n            show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n\n    print(\"\\n\u00a1Entrenamiento completado!\")\n\n    # Mostrar gr\u00e1fica de p\u00e9rdidas\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(disc_losses, label=\"Discriminador\", color=\"red\")\n    plt.plot(gen_losses, label=\"Generador\", color=\"blue\")\n    plt.xlabel(\"\u00c9poca\")\n    plt.ylabel(\"P\u00e9rdida\")\n    plt.title(\"Evoluci\u00f3n de las P\u00e9rdidas\")\n    plt.legend()\n    plt.grid(True)\n\n    # Mostrar muestras generadas finales\n    plt.subplot(1, 2, 2)\n    model.generator.eval()\n    with torch.no_grad():\n        final_samples = model.generator(fixed_noise[:16]).cpu()\n        final_samples = (final_samples + 1) / 2  # Desnormalizar\n        grid = torchvision.utils.make_grid(final_samples, nrow=4, padding=2)\n        plt.imshow(grid.permute(1, 2, 0), cmap=\"gray\")\n        plt.title(\"Muestras Finales Generadas\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nMostrando muestras generadas...\")\n    show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n</pre> # 3pps import matplotlib.pyplot as plt import torch import torchvision from torch import nn, optim from torch.utils.data import DataLoader from torchvision import datasets, transforms from tqdm import tqdm   def show_generated_samples(     generator: nn.Module, noise, device: str, num_samples: int = 16 ) -&gt; None:     \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"     generator.eval()     with torch.no_grad():         samples = generator(noise[:num_samples]).cpu()         samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]          fig, axes = plt.subplots(4, 4, figsize=(8, 8))         for i in range(num_samples):             row, col = i // 4, i % 4             axes[row, col].imshow(samples[i, 0], cmap=\"gray\")             axes[row, col].axis(\"off\")         plt.tight_layout()         plt.show()   class Discriminator(nn.Module):     def __init__(         self, in_channels: int, hidden_size: int = 64, dropout_rate: float = 0.2     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.in_channels = in_channels         self.hidden_size = hidden_size         self.dropout_rate = dropout_rate          # Creamos el modelo que lo ajustaremos para MNIST         self.model = nn.Sequential(             # Input MNIST = (B, C, H, W) = (B, 1, 28, 28) -&gt; (B, H, 14, 14)             nn.Conv2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size // 2,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size // 2),             nn.GELU(),             nn.Dropout2d(p=self.dropout_rate),             # (B, H, 14, 14) -&gt; (B, H, 7, 7)             nn.Conv2d(                 in_channels=self.hidden_size // 2,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             nn.Dropout2d(p=self.dropout_rate),             # (B, H, 7, 7) -&gt; (B, 1, 7, 7)             nn.Conv2d(                 in_channels=self.hidden_size,                 out_channels=1,                 kernel_size=1,                 stride=1,                 bias=False,             ),             # (B, 1, 7, 7) -&gt; (B, 1, 1, 1)             nn.AdaptiveAvgPool2d((1, 1)),             # (B, 1, 1, 1) -&gt; (B, 1),             nn.Flatten(),             nn.Dropout(p=self.dropout_rate),             # El discriminador ha de devolver un escalar entre 0 y 1 (falso/real)             nn.Sigmoid(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         return self.model(input_tensor)   class Generator(nn.Module):     def __init__(         self, z_dim: int, data_shape: tuple[int, int, int], hidden_size: int     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.z_dim = z_dim         self.data_shape = data_shape         self.hidden_size = hidden_size          # Del ruido que es un tensor plano, vamos a crear una matriz inicial         # de (B, H, 7, 7) que es el tama\u00f1o antes de aplanar en el Discriminador         self.projection = nn.Sequential(             # (B, z_dim) -&gt; (B, H * 7 * 7)             nn.Linear(                 in_features=self.z_dim,                 out_features=self.hidden_size * 7 * 7,                 bias=False,             ),         )          self.model = nn.Sequential(             # (B, H, 7, 7) -&gt; (B, H, 14, 14)             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             # (B, H, 14, 14) -&gt; (B, H, 28, 28)             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             # (B, H, 28, 28) -&gt; (B, 1, 28, 28)             nn.Conv2d(                 in_channels=self.hidden_size, out_channels=1, kernel_size=1, stride=1             ),             # Normalizamos los valores entre -1 y 1             nn.Tanh(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         # (B, z_dim) -&gt; (B, H * 7 * 7)         projection = self.projection(input_tensor)         # (B, H * 7 * 7) -&gt; (B, H, 7, 7)         projection = projection.view(projection.size(0), self.hidden_size, 7, 7)          return self.model(projection)   class GAN(nn.Module):     def __init__(         self,         in_channels: int,         hidden_size_discriminator: int,         dropout_rate: float,         z_dim: int,         data_shape: tuple[int, int, int],         hidden_size_generator: int,     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros         self.in_channels = in_channels         self.hidden_size_discriminator = hidden_size_discriminator         self.dropout_rate = dropout_rate         self.z_dim = z_dim         self.data_shape = data_shape         self.hidden_size_generator = hidden_size_generator          # Definimos los modelos         self.discriminator = Discriminator(             in_channels=self.in_channels,             hidden_size=self.hidden_size_discriminator,             dropout_rate=self.dropout_rate,         )         self.generator = Generator(             z_dim=self.z_dim,             data_shape=self.data_shape,             hidden_size=self.hidden_size_generator,         )      def forward(self, real_data: torch.Tensor, batch_size: int | None = None) -&gt; dict:         if batch_size is None:             batch_size = real_data.size(0)          # Generar ruido aleatorio         noise = torch.randn(batch_size, self.z_dim, device=real_data.device)          # Generar im\u00e1genes falsas         fake_data = self.generator(noise)          # Pasar datos reales por el discriminador         real_predictions = self.discriminator(real_data)          # Pasar datos falsos por el discriminador         fake_predictions = self.discriminator(fake_data)          return {             \"real_predictions\": real_predictions,             \"fake_predictions\": fake_predictions,             \"fake_data\": fake_data,             \"noise\": noise,         }      def generate_samples_inference(self, num_samples: int, device: str) -&gt; torch.Tensor:         self.eval()         with torch.no_grad():             # (Num_samples, z_dim)             noise = torch.randn(num_samples, self.z_dim, device=device)             # (Num_samples, z_dim) -&gt; MNIST: (Num_samples, 1, 28, 28)             samples = self.generator(input_tensor=noise)          return samples      def discriminator_loss(         self, real_predictions: torch.Tensor, fake_predictions: torch.Tensor     ) -&gt; torch.Tensor:         criterion = nn.BCELoss()         # Matriz de 1s         real_labels = torch.ones_like(real_predictions)         # Matriz de 0s         fake_labels = torch.zeros_like(fake_predictions)          real_loss = criterion(real_predictions, real_labels)         fake_loss = criterion(fake_predictions, fake_labels)          return (real_loss + fake_loss) / 2      def generator_loss(self, fake_predictions: torch.Tensor) -&gt; torch.Tensor:         criterion = nn.BCELoss()         fake_real_labels = torch.ones_like(fake_predictions)         return criterion(fake_predictions, fake_real_labels)   if __name__ == \"__main__\":     # Seleccionamos el dispositivo actual     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     print(f\"Usando dispositivo: {device}\")      # Las GANs son muy sensibles a los hiperpar\u00e1metros     lr = 2e-4     # Dimensi\u00f3n de los datos de entrada (C, H, W)     data_dimension = (1, 28, 28)     # Esta es la dimension del ruido     z_dim = 100     batch_size = 128     num_epochs = 50      # Definimos el modelo     model = GAN(         in_channels=data_dimension[0],         hidden_size_discriminator=64,         dropout_rate=0.2,         z_dim=z_dim,         data_shape=data_dimension,         hidden_size_generator=256,     ).to(device)      # Transformaciones que vamos a aplicar a MNIST     transform = transforms.Compose(         [             transforms.ToTensor(),             transforms.Normalize(                 # Normalizar a [-1, 1] para coincidir con Tanh                 (0.5,),                 (0.5,),             ),         ]     )      # Descargamos MNIST     dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)     loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)      # Vamos a utilizar un optimizador para cada modelo     opt_disc = optim.AdamW(params=model.discriminator.parameters(), lr=lr)     opt_gen = optim.AdamW(params=model.generator.parameters(), lr=lr)      # Ruido fijo para generar muestras durante el entrenamiento     fixed_noise = torch.randn(64, z_dim, device=device)      # Listas para guardar las p\u00e9rdidas     disc_losses = []     gen_losses = []      print(\"Iniciando entrenamiento de la GAN...\")      for epoch in range(num_epochs):         epoch_disc_loss: int | float = 0         epoch_gen_loss: int | float = 0          pbar = tqdm(loader, desc=f\"\u00c9poca {epoch + 1}/{num_epochs}\")          for _, (real, _) in enumerate(pbar):             real = real.to(device)             batch_size_current = real.shape[0]              # ENTRENAR DISCRIMINADOR             opt_disc.zero_grad()              # Generar datos falsos             noise = torch.randn(batch_size_current, z_dim, device=device)             fake_data = model.generator(                 noise             ).detach()  # Detach para no actualizar generador              # Predicciones del discriminador             real_preds = model.discriminator(real)             fake_preds = model.discriminator(fake_data)              # P\u00e9rdida del discriminador             lossD = model.discriminator_loss(real_preds, fake_preds)             lossD.backward()             opt_disc.step()              # ENTRENAR GENERADOR             opt_gen.zero_grad()              # Generar nuevos datos falsos (sin detach)             noise = torch.randn(batch_size_current, z_dim, device=device)             fake_data = model.generator(noise)             fake_preds_for_gen = model.discriminator(fake_data)              # P\u00e9rdida del generador             lossG = model.generator_loss(fake_preds_for_gen)             lossG.backward()             opt_gen.step()              # Acumular p\u00e9rdidas             epoch_disc_loss += lossD.item()             epoch_gen_loss += lossG.item()              # Actualizar barra de progreso             pbar.set_postfix(                 {\"D_loss\": f\"{lossD.item():.4f}\", \"G_loss\": f\"{lossG.item():.4f}\"}             )          # P\u00e9rdidas promedio de la \u00e9poca         avg_disc_loss = epoch_disc_loss / len(loader)         avg_gen_loss = epoch_gen_loss / len(loader)         disc_losses.append(avg_disc_loss)         gen_losses.append(avg_gen_loss)          print(             f\"\u00c9poca {epoch + 1}, \"             f\"P\u00e9rdida D: {avg_disc_loss:.4f}, \"             f\"P\u00e9rdida G: {avg_gen_loss:.4f}\"         )          # Mostrar muestras cada 5 \u00e9pocas         if (epoch + 1) % 5 == 0:             print(f\"\\nMostrando muestras generadas en \u00e9poca {epoch + 1}...\")             show_generated_samples(model.generator, fixed_noise, device, num_samples=16)      print(\"\\n\u00a1Entrenamiento completado!\")      # Mostrar gr\u00e1fica de p\u00e9rdidas     plt.figure(figsize=(12, 5))      plt.subplot(1, 2, 1)     plt.plot(disc_losses, label=\"Discriminador\", color=\"red\")     plt.plot(gen_losses, label=\"Generador\", color=\"blue\")     plt.xlabel(\"\u00c9poca\")     plt.ylabel(\"P\u00e9rdida\")     plt.title(\"Evoluci\u00f3n de las P\u00e9rdidas\")     plt.legend()     plt.grid(True)      # Mostrar muestras generadas finales     plt.subplot(1, 2, 2)     model.generator.eval()     with torch.no_grad():         final_samples = model.generator(fixed_noise[:16]).cpu()         final_samples = (final_samples + 1) / 2  # Desnormalizar         grid = torchvision.utils.make_grid(final_samples, nrow=4, padding=2)         plt.imshow(grid.permute(1, 2, 0), cmap=\"gray\")         plt.title(\"Muestras Finales Generadas\")         plt.axis(\"off\")      plt.tight_layout()     plt.show()      print(\"\\nMostrando muestras generadas...\")     show_generated_samples(model.generator, fixed_noise, device, num_samples=16)"},{"location":"course/Generative/step_01_gans.html#generative-adversarial-networks","title":"Generative Adversarial Networks\u00b6","text":""},{"location":"course/Generative/step_02_diffusion_models.html","title":"Difussion Models","text":""},{"location":"course/Generative/step_02_diffusion_models.html#difussion-models","title":"Difussion Models\u00b6","text":""},{"location":"course/Graphs/step_01_gnns.html","title":"Graph Neural Networks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Graphs/step_01_gnns.html#graph-neural-networks","title":"Graph Neural Networks\u00b6","text":""},{"location":"course/Graphs/step_02_pytorch_geometric.html","title":"PyTorch Geometric","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Graphs/step_02_pytorch_geometric.html#pytorch-geometric","title":"PyTorch Geometric\u00b6","text":""},{"location":"course/Mathematics/step_01_data_structures.html","title":"Data Structures","text":"<p>Comprobar disponibilidad de la GPU</p> In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport torch\n\nprint(f\"Version de Pytorch: {torch.__version__}\")\n\n# Comprobar si la GPU est\u00e1 disponible\nif torch.cuda.is_available():\n    print(\"GPU disponible\")\n    # Obtener el nombre de la GPU\n    print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")\n    # Obtener el n\u00famero de GPUs\n    print(f\"N\u00famero de GPUs: {torch.cuda.device_count()}\")\nelse:\n    print(\"No se detect\u00f3 GPU, se usar\u00e1 la CPU\")\n</pre> # 3pps import torch  print(f\"Version de Pytorch: {torch.__version__}\")  # Comprobar si la GPU est\u00e1 disponible if torch.cuda.is_available():     print(\"GPU disponible\")     # Obtener el nombre de la GPU     print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")     # Obtener el n\u00famero de GPUs     print(f\"N\u00famero de GPUs: {torch.cuda.device_count()}\") else:     print(\"No se detect\u00f3 GPU, se usar\u00e1 la CPU\") <p>Crear un tensor</p> In\u00a0[\u00a0]: Copied! <pre># Tensor escalar\nescalar = torch.tensor(7)\nescalar\n</pre> # Tensor escalar escalar = torch.tensor(7) escalar In\u00a0[\u00a0]: Copied! <pre># Un valor escalar no tiene dimensiones, no tiene rango\nescalar.ndim\n</pre> # Un valor escalar no tiene dimensiones, no tiene rango escalar.ndim In\u00a0[\u00a0]: Copied! <pre># Obtener el valores del escalar\nescalar.item()\n</pre> # Obtener el valores del escalar escalar.item() In\u00a0[\u00a0]: Copied! <pre># Creacion de un vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Creacion de un vector vector = torch.tensor([7, 7]) vector In\u00a0[\u00a0]: Copied! <pre>vector.ndim\n</pre> vector.ndim In\u00a0[\u00a0]: Copied! <pre>vector.shape\n</pre> vector.shape In\u00a0[\u00a0]: Copied! <pre># Creacion de una matriz \nmatriz = torch.tensor([[1,2,3], [4,5,6]])\nmatriz\n</pre> # Creacion de una matriz  matriz = torch.tensor([[1,2,3], [4,5,6]]) matriz In\u00a0[\u00a0]: Copied! <pre>matriz.ndim\n</pre> matriz.ndim In\u00a0[\u00a0]: Copied! <pre>matriz.shape\n</pre> matriz.shape In\u00a0[\u00a0]: Copied! <pre># Creacion de un tensor\ntensor = torch.tensor([[\n    [[1,2,3],\n    [4,5,6]],\n    [[7,8,9],\n    [6,4,3]]\n]])\ntensor\n</pre> # Creacion de un tensor tensor = torch.tensor([[     [[1,2,3],     [4,5,6]],     [[7,8,9],     [6,4,3]] ]]) tensor In\u00a0[\u00a0]: Copied! <pre>tensor.ndim\n</pre> tensor.ndim In\u00a0[\u00a0]: Copied! <pre>tensor.shape\n</pre> tensor.shape In\u00a0[\u00a0]: Copied! <pre># Tensores aleatorios\ntensor_aleatorio = torch.rand((2,3,4))\ntensor_aleatorio\n</pre> # Tensores aleatorios tensor_aleatorio = torch.rand((2,3,4)) tensor_aleatorio In\u00a0[\u00a0]: Copied! <pre>tensor_aleatorio.ndim, tensor_aleatorio.shape\n</pre> tensor_aleatorio.ndim, tensor_aleatorio.shape In\u00a0[\u00a0]: Copied! <pre># Tensores con ceros y unos\ntensores_ceros = torch.zeros((3,4))\ntensores_ceros\n</pre> # Tensores con ceros y unos tensores_ceros = torch.zeros((3,4)) tensores_ceros In\u00a0[\u00a0]: Copied! <pre>tensores_unos = torch.ones((3,4))\ntensores_unos\n</pre> tensores_unos = torch.ones((3,4)) tensores_unos In\u00a0[\u00a0]: Copied! <pre># Crear un tensor con un rango de valores\nrango = torch.arange(start=0, end=100, step=2)\nrango\n</pre> # Crear un tensor con un rango de valores rango = torch.arange(start=0, end=100, step=2) rango In\u00a0[\u00a0]: Copied! <pre># Crear tensores con la misma dimension de otro tensor\nprint(f\"Shape de 'rango': {rango.shape}\")\ncopia_rango = torch.zeros_like(input=rango)\nprint(f\"Shape de 'copia_rango': {copia_rango.shape}\")\ncopia_rango\n</pre> # Crear tensores con la misma dimension de otro tensor print(f\"Shape de 'rango': {rango.shape}\") copia_rango = torch.zeros_like(input=rango) print(f\"Shape de 'copia_rango': {copia_rango.shape}\") copia_rango <p>En el caso de realizar operaciones entre tensores que no tengan el mismo tipo de dato (data type), o tengan las dimensiones adecuadas para operar, o no se encuentren en el mismo espacio de memoria de la CPU o GPU, pueden generar conflictos y errores</p> In\u00a0[\u00a0]: Copied! <pre>tensor = torch.rand(size=(2,2,3))\ntensor\n</pre> tensor = torch.rand(size=(2,2,3)) tensor In\u00a0[\u00a0]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") In\u00a0[\u00a0]: Copied! <pre>tensor = torch.rand(size=(2,2,3), dtype=torch.float16)\ntensor\n</pre> tensor = torch.rand(size=(2,2,3), dtype=torch.float16) tensor In\u00a0[\u00a0]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") In\u00a0[\u00a0]: Copied! <pre>tensor = torch.rand(size=(2,3))\ntensor\n</pre> tensor = torch.rand(size=(2,3)) tensor In\u00a0[\u00a0]: Copied! <pre>tensor.max(dim=0)\n</pre> tensor.max(dim=0) In\u00a0[\u00a0]: Copied! <pre>tensor.max(dim=1)\n</pre> tensor.max(dim=1) <p>Tenemos que las columnas est\u00e1n representadas por el eje/dim 0, y las filas por el eje/dim 1</p> In\u00a0[\u00a0]: Copied! <pre>tensor.mean(dim=0), tensor.mean(dim=1)\n</pre> tensor.mean(dim=0), tensor.mean(dim=1) In\u00a0[\u00a0]: Copied! <pre>torch.mean(tensor, dim=0), torch.mean(tensor, dim=1)\n</pre> torch.mean(tensor, dim=0), torch.mean(tensor, dim=1) In\u00a0[\u00a0]: Copied! <pre>tensor\n</pre> tensor In\u00a0[\u00a0]: Copied! <pre>torch.argmax(tensor, dim=0), tensor.argmax(dim=0)\n</pre> torch.argmax(tensor, dim=0), tensor.argmax(dim=0) In\u00a0[\u00a0]: Copied! <pre>tensor.argmax(dim=1)\n</pre> tensor.argmax(dim=1) In\u00a0[\u00a0]: Copied! <pre>tensor\n</pre> tensor In\u00a0[\u00a0]: Copied! <pre>tensor[:, tensor.argmax(dim=1)[0]]\n</pre> tensor[:, tensor.argmax(dim=1)[0]] In\u00a0[\u00a0]: Copied! <pre>matriz = torch.rand((4,4))\nmatriz\n</pre> matriz = torch.rand((4,4)) matriz In\u00a0[\u00a0]: Copied! <pre>submatrix_1 = matriz[0::2, 0::2]\nsubmatrix_2 = matriz[0::2, 1::2]\nsubmatrix_3 = matriz[1::2, 0::2]\nsubmatrix_4 = matriz[1::2, 1::2]\nsubmatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4])\nsubmatrices\n</pre> submatrix_1 = matriz[0::2, 0::2] submatrix_2 = matriz[0::2, 1::2] submatrix_3 = matriz[1::2, 0::2] submatrix_4 = matriz[1::2, 1::2] submatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4]) submatrices In\u00a0[\u00a0]: Copied! <pre>submatrices.shape\n</pre> submatrices.shape In\u00a0[\u00a0]: Copied! <pre>submatrix_1, submatrix_2, submatrix_3, submatrix_4\n</pre> submatrix_1, submatrix_2, submatrix_3, submatrix_4 In\u00a0[\u00a0]: Copied! <pre>print(submatrices.shape)\nsubmatrices = submatrices.unsqueeze(dim=0)\nsubmatrices.shape\n</pre> print(submatrices.shape) submatrices = submatrices.unsqueeze(dim=0) submatrices.shape In\u00a0[\u00a0]: Copied! <pre>norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1))\nnorm\n</pre> norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1)) norm In\u00a0[\u00a0]: Copied! <pre>(0.5262 ** 2 + 0.0480 ** 2 + 0.0578 ** 2 + 0.1393 ** 2) ** (1/2)\n</pre> (0.5262 ** 2 + 0.0480 ** 2 + 0.0578 ** 2 + 0.1393 ** 2) ** (1/2) In\u00a0[\u00a0]: Copied! <pre>(0.2742 ** 2 + 0.6927 ** 2 + 0.9618 ** 2 + 0.0417 ** 2) ** (1/2)\n</pre> (0.2742 ** 2 + 0.6927 ** 2 + 0.9618 ** 2 + 0.0417 ** 2) ** (1/2) In\u00a0[\u00a0]: Copied! <pre>submatrices[:, torch.argmax(norm), :, :]\n</pre> submatrices[:, torch.argmax(norm), :, :] In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n</pre> torch.manual_seed(42) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Mathematics/step_01_data_structures.html#data-structures","title":"Data Structures\u00b6","text":""},{"location":"course/Mathematics/step_01_data_structures.html#introduccion-a-los-tensores","title":"Introducci\u00f3n a los tensores\u00b6","text":""},{"location":"course/Mathematics/step_02_basic_maths.html","title":"Basic Maths","text":"In\u00a0[\u00a0]: Copied! <pre># Libraries to import\n\nimport torch\n\nimport sympy as sp\n\n# Create input tensor with gradient tracking\n\nx = torch.tensor([2.0, 3.0], requires_grad=True)\n\n\n\n# Define a differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2\n\ny = x[0]**2 + 3*x[0]*x[1] + x[1]**2\n\n\n\n# Compute gradients\n\ny.backward()\n\n\n\n# Gradients each input\n\ngrad_x1 = x.grad[0]  # \u2202f/\u2202x1\n\ngrad_x2 = x.grad[1]  # \u2202f/\u2202x2\n\n\n\n# Print results\n\nprint(\"PyTorch gradients:\")\n\nprint(\"Gradient \u2202f/\u2202x1:\", grad_x1)\n\nprint(\"Gradient \u2202f/\u2202x2:\", grad_x2)\n\n\n\n# Define symbolic variables\n\nx1, x2 = sp.symbols('x1 x2')\n\n\n\n# Define the same function symbolically\n\nf = x1**2 + 3*x1*x2 + x2**2\n\n\n\n# Compute symbolic derivatives\n\ndf_dx1 = sp.diff(f, x1)\n\ndf_dx2 = sp.diff(f, x2)\n\n\n\n# Show the derivative formulas\n\nprint(\"SymPy derivative formulas:\")\n\nprint(\"\u2202f/\u2202x1 =\", df_dx1)\n\nprint(\"\u2202f/\u2202x2 =\", df_dx2)\n\n\n\n# Evaluate derivatives at a specific point (x1=2, x2=3)\n\ngrad_x1_sym = df_dx1.evalf(subs={x1:2, x2:3})\n\ngrad_x2_sym = df_dx2.evalf(subs={x1:2, x2:3})\n\n\n\n# Print numerical results\n\nprint(\"SymPy symbolic gradients evaluated at (x1=2, x2=3):\")\n\nprint(\"Gradient x1:\", grad_x1_sym)\n\nprint(\"Gradient x2:\", grad_x2_sym)\n</pre> # Libraries to import  import torch  import sympy as sp  # Create input tensor with gradient tracking  x = torch.tensor([2.0, 3.0], requires_grad=True)    # Define a differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2  y = x[0]**2 + 3*x[0]*x[1] + x[1]**2    # Compute gradients  y.backward()    # Gradients each input  grad_x1 = x.grad[0]  # \u2202f/\u2202x1  grad_x2 = x.grad[1]  # \u2202f/\u2202x2    # Print results  print(\"PyTorch gradients:\")  print(\"Gradient \u2202f/\u2202x1:\", grad_x1)  print(\"Gradient \u2202f/\u2202x2:\", grad_x2)    # Define symbolic variables  x1, x2 = sp.symbols('x1 x2')    # Define the same function symbolically  f = x1**2 + 3*x1*x2 + x2**2    # Compute symbolic derivatives  df_dx1 = sp.diff(f, x1)  df_dx2 = sp.diff(f, x2)    # Show the derivative formulas  print(\"SymPy derivative formulas:\")  print(\"\u2202f/\u2202x1 =\", df_dx1)  print(\"\u2202f/\u2202x2 =\", df_dx2)    # Evaluate derivatives at a specific point (x1=2, x2=3)  grad_x1_sym = df_dx1.evalf(subs={x1:2, x2:3})  grad_x2_sym = df_dx2.evalf(subs={x1:2, x2:3})    # Print numerical results  print(\"SymPy symbolic gradients evaluated at (x1=2, x2=3):\")  print(\"Gradient x1:\", grad_x1_sym)  print(\"Gradient x2:\", grad_x2_sym) <pre>\n  Cell In[6], line 3\n    import torch\u200b\n                ^\nSyntaxError: invalid non-printable character U+200B\n</pre> In\u00a0[11]: Copied! <pre># 3pps\nimport torch\n\n# Example 1: Quadratic Function\n# y = x\u00b2, dy/dx = 2x\nx = torch.tensor(3.0, requires_grad=True)\ny = x**2\ny.backward()\nprint(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n\n# Example 2: Multiple Variables\n# z = 2a + 3b, dz/da = 2, dz/db = 3\na = torch.tensor(4.0, requires_grad=True)\nb = torch.tensor(5.0, requires_grad=True)\nz = 2*a + 3*b\nz.backward()\nprint(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")\n\n# Example 3: Chain Rule\n# y = (2x + 1)\u00b2, dy/dx = 4(2x + 1)\nx = torch.tensor(3.0, requires_grad=True)\ny = (2*x + 1)**2\ny.backward()\nprint(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n</pre> # 3pps import torch  # Example 1: Quadratic Function # y = x\u00b2, dy/dx = 2x x = torch.tensor(3.0, requires_grad=True) y = x**2 y.backward() print(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")  # Example 2: Multiple Variables # z = 2a + 3b, dz/da = 2, dz/db = 3 a = torch.tensor(4.0, requires_grad=True) b = torch.tensor(5.0, requires_grad=True) z = 2*a + 3*b z.backward() print(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")  # Example 3: Chain Rule # y = (2x + 1)\u00b2, dy/dx = 4(2x + 1) x = torch.tensor(3.0, requires_grad=True) y = (2*x + 1)**2 y.backward() print(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\") <pre>y = x\u00b2 | x=3.0, dy/dx=6.0\nz = 2a + 3b | dz/da=2.0, dz/db=3.0\ny = (2x+1)\u00b2 | x=3.0, dy/dx=28.0\n</pre> In\u00a0[\u00a0]: Copied! <pre># Example 4: Linear Regression\n# y = w\u00b7x + b, dy/dx = w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nw = torch.tensor([0.5, -1.0])\nb = 2.0\ny = w[0]*x[0] + w[1]*x[1] + b\ny.backward()\nprint(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")\n\n# Example 5: Logistic Regression\n# y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nz = w[0]*x[0] + w[1]*x[1] + b\ny = torch.sigmoid(z)\ny.backward()\nprint(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\")\n</pre> # Example 4: Linear Regression # y = w\u00b7x + b, dy/dx = w x = torch.tensor([2.0, 3.0], requires_grad=True) w = torch.tensor([0.5, -1.0]) b = 2.0 y = w[0]*x[0] + w[1]*x[1] + b y.backward() print(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")  # Example 5: Logistic Regression # y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w x = torch.tensor([2.0, 3.0], requires_grad=True) z = w[0]*x[0] + w[1]*x[1] + b y = torch.sigmoid(z) y.backward() print(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\") <pre>Linear | dy/dx1=0.5, dy/dx2=-1.0\nLogistic | dy/dx1=0.1250, dy/dx2=-0.2500\ntensor(0.5000, grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> In\u00a0[16]: Copied! <pre># 3pps\n# Libraries to import\nimport torch\nimport torch.nn.functional as F\n\n# Input features\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Weights and bias for 3 classes\nW = torch.tensor([[0.2, -0.5, 0.3],   # weights for x[0]\n                  [0.4, 0.1, -0.2],   # weights for x[1]\n                  [0.1, 0.3, 0.2]], requires_grad=False)\nb = torch.tensor([0.0, 0.0, 0.0])\n\n# Linear scores for each class: s = W^T x + b\nlogits = torch.matmul(x, W) + b  # shape [3]\n\n# Apply Softmax to get probabilities\nprobs = F.softmax(logits, dim=0)\n\n# Pick the predicted class probability\npred_class_idx = probs.argmax()\ntop_prob = probs[pred_class_idx]\n\n# Compute gradients w.r.t input\ntop_prob.backward()\n\n# Print results\nprint(\"Multiclass Classification | Probabilities:\", probs.detach().numpy())\nprint(\"Predicted class index:\", pred_class_idx.item())\nprint(\"Gradients inputs:\", x.grad.detach().numpy())\n</pre> # 3pps # Libraries to import import torch import torch.nn.functional as F  # Input features x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # Weights and bias for 3 classes W = torch.tensor([[0.2, -0.5, 0.3],   # weights for x[0]                   [0.4, 0.1, -0.2],   # weights for x[1]                   [0.1, 0.3, 0.2]], requires_grad=False) b = torch.tensor([0.0, 0.0, 0.0])  # Linear scores for each class: s = W^T x + b logits = torch.matmul(x, W) + b  # shape [3]  # Apply Softmax to get probabilities probs = F.softmax(logits, dim=0)  # Pick the predicted class probability pred_class_idx = probs.argmax() top_prob = probs[pred_class_idx]  # Compute gradients w.r.t input top_prob.backward()  # Print results print(\"Multiclass Classification | Probabilities:\", probs.detach().numpy()) print(\"Predicted class index:\", pred_class_idx.item()) print(\"Gradients inputs:\", x.grad.detach().numpy()) <pre>Multiclass Classification | Probabilities: [0.51389724 0.25519383 0.23090893]\nPredicted class index: 0\nGradients inputs: [ 0.07993404  0.1105411  -0.03809503]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Mathematics/step_02_basic_maths.html#basic-maths","title":"Basic Maths\u00b6","text":""},{"location":"course/Mathematics/step_03_mathematical_concepts.html","title":"Mathematical Concepts","text":"In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n</pre> # 3pps import numpy as np In\u00a0[2]: Copied! <pre>def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)\n\n\ndef cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz @ matriz.T\n</pre> def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:     return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)   def cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:     return matriz @ matriz.T In\u00a0[3]: Copied! <pre>X = np.array([\n    [1, 2, 3],  \n    [4, 5, 6],   \n    [1, 0, 0],  \n    [0, 1, 0]   \n], dtype=float)\n\nprint(\"Embeddings originales:\\n\", X)\n\nX_normalized = normalizar_matriz(matriz=X)\nprint(\"\\nEmbeddings normalizados:\\n\", X_normalized)\n\nsimilarity_matrix = cosine_similarity(matriz=X_normalized)\nprint(\"\\nMatriz de similitud:\\n\", similarity_matrix)\n</pre> X = np.array([     [1, 2, 3],       [4, 5, 6],        [1, 0, 0],       [0, 1, 0]    ], dtype=float)  print(\"Embeddings originales:\\n\", X)  X_normalized = normalizar_matriz(matriz=X) print(\"\\nEmbeddings normalizados:\\n\", X_normalized)  similarity_matrix = cosine_similarity(matriz=X_normalized) print(\"\\nMatriz de similitud:\\n\", similarity_matrix) <pre>Embeddings originales:\n [[1. 2. 3.]\n [4. 5. 6.]\n [1. 0. 0.]\n [0. 1. 0.]]\n\nEmbeddings normalizados:\n [[0.26726124 0.53452248 0.80178373]\n [0.45584231 0.56980288 0.68376346]\n [1.         0.         0.        ]\n [0.         1.         0.        ]]\n\nMatriz de similitud:\n [[1.         0.97463185 0.26726124 0.53452248]\n [0.97463185 1.         0.45584231 0.56980288]\n [0.26726124 0.45584231 1.         0.        ]\n [0.53452248 0.56980288 0.         1.        ]]\n</pre>"},{"location":"course/Mathematics/step_03_mathematical_concepts.html#mathematical-concepts","title":"Mathematical Concepts\u00b6","text":""},{"location":"course/Mathematics/step_03_mathematical_concepts.html#cosine-similarity","title":"Cosine Similarity\u00b6","text":""},{"location":"course/Mathematics/step_04_artificial_neuron.html","title":"Artificial Neuron","text":"In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\nfrom typing import Callable\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> # Standard libraries import math from typing import Callable  # 3pps import matplotlib.pyplot as plt import numpy as np In\u00a0[\u00a0]: Copied! <pre>def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:\n    plt.title(f\"{name_function} function\")\n    plt.plot(steps, function(steps))\n    plt.grid()\n    plt.show()\n</pre> def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:     plt.title(f\"{name_function} function\")     plt.plot(steps, function(steps))     plt.grid()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def sigmoid(input: np.ndarray) -&gt; np.ndarray:\n    return 1/(1+np.exp((-1) * input))\n\n\ndef tanh(input: np.ndarray) -&gt; np.ndarray:\n    return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))\n\n\ndef relu(input: np.ndarray) -&gt; np.ndarray:\n    return [max(0, elem) for elem in input]\n\n\ndef leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    return [max(0.1 * elem, elem) for elem in input]\n\n\ndef elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:\n    return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]\n\n\ndef swish(input: np.ndarray) -&gt; np.ndarray:\n    return input * sigmoid(input)\n\n\ndef gelu(input: np.ndarray) -&gt; np.ndarray:\n    return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3)))\n</pre> def sigmoid(input: np.ndarray) -&gt; np.ndarray:     return 1/(1+np.exp((-1) * input))   def tanh(input: np.ndarray) -&gt; np.ndarray:     return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))   def relu(input: np.ndarray) -&gt; np.ndarray:     return [max(0, elem) for elem in input]   def leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:     return [max(0.1 * elem, elem) for elem in input]   def elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:     return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]   def swish(input: np.ndarray) -&gt; np.ndarray:     return input * sigmoid(input)   def gelu(input: np.ndarray) -&gt; np.ndarray:     return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3))) In\u00a0[\u00a0]: Copied! <pre>steps = np.arange(-10, 10, 0.1)\n\nplot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid)\nplot_function(name_function=\"Tanh\", steps=steps, function=tanh)\nplot_function(name_function=\"ReLU\", steps=steps, function=relu)\nplot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu)\nplot_function(name_function=\"ELU\", steps=steps, function=elu)\nplot_function(name_function=\"Swish\", steps=steps, function=swish)\nplot_function(name_function=\"GELU\", steps=steps, function=gelu)\n</pre> steps = np.arange(-10, 10, 0.1)  plot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid) plot_function(name_function=\"Tanh\", steps=steps, function=tanh) plot_function(name_function=\"ReLU\", steps=steps, function=relu) plot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu) plot_function(name_function=\"ELU\", steps=steps, function=elu) plot_function(name_function=\"Swish\", steps=steps, function=swish) plot_function(name_function=\"GELU\", steps=steps, function=gelu) In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n</pre> # Standard libraries import math  # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split from torch import nn In\u00a0[\u00a0]: Copied! <pre>class BinaryClassifier(nn.Module):\n\n    def __init__(self, num_classes: int) -&gt; None:\n\n        super().__init__()\n\n        self.num_classes = num_classes\n\n        self.model = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.GELU(),           \n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.model(input_tensor)\n</pre> class BinaryClassifier(nn.Module):      def __init__(self, num_classes: int) -&gt; None:          super().__init__()          self.num_classes = num_classes          self.model = nn.Sequential(             nn.Linear(2, 16),             nn.GELU(),                        nn.Linear(16, 1),             nn.Sigmoid()         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.model(input_tensor) In\u00a0[\u00a0]: Copied! <pre>n_samples = 1000\nX, y = make_circles(\n    n_samples, noise=0.03, random_state=42\n)\nX.shape, y.shape\n</pre> n_samples = 1000 X, y = make_circles(     n_samples, noise=0.03, random_state=42 ) X.shape, y.shape In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X[:, 0], X[:, 1], c=y)\n</pre> plt.scatter(X[:, 0], X[:, 1], c=y) In\u00a0[\u00a0]: Copied! <pre>model = BinaryClassifier(num_classes=2)\nloss_function = nn.BCELoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2)\n</pre> model = BinaryClassifier(num_classes=2) loss_function = nn.BCELoss() optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2) In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32)) In\u00a0[\u00a0]: Copied! <pre>print(y_train.min(), y_train.max(), y_train.dtype)\nprint(y_test.min(), y_test.max(), y_test.dtype)\n</pre> print(y_train.min(), y_train.max(), y_train.dtype) print(y_test.min(), y_test.max(), y_test.dtype) In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.show()\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\n</pre> plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train) plt.show() plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show() In\u00a0[\u00a0]: Copied! <pre>num_epochs = 20\nbatch_size = 32\nnum_batches = math.ceil(len(X_train) / batch_size)\nnum_batches_test = math.ceil(len(X_test) / batch_size)\n\nplot_loss_train = []\nplot_loss_test = []\nplot_acc_train = []\nplot_acc_test = []\n\nfor epoch in range(num_epochs): \n    loss_epoch_train = []\n    loss_epoch_test = []\n    accuracy_train = []\n    accuracy_test = []\n    \n    model.train()\n    for i in range(num_batches):\n        X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n        y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n        optimizer.zero_grad()\n        predictions = model(X_batch)\n        loss = loss_function(predictions, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        loss_epoch_train.append(loss.item())\n        pred_labels = (predictions &gt;= 0.5).float()\n        acc = (pred_labels == y_batch).float().mean().item() * 100\n        accuracy_train.append(acc)\n\n    model.eval()\n    with torch.inference_mode():\n        for i in range(num_batches_test):\n            X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]\n            y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n            predictions_inference = model(X_test_batch)\n            loss_test = loss_function(predictions_inference, y_test_batch)\n            loss_epoch_test.append(loss_test.item())\n\n            pred_labels_test = (predictions_inference &gt;= 0.5).float()\n            acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100\n            accuracy_test.append(acc_test)\n\n    # Promedios de la \u00e9poca\n    train_loss_mean = np.mean(loss_epoch_train)\n    test_loss_mean = np.mean(loss_epoch_test)\n    train_acc_mean = np.mean(accuracy_train)\n    test_acc_mean = np.mean(accuracy_test)\n\n    print(\n        f\"Epoch: {epoch+1}, \"\n        f\"Train Loss: {train_loss_mean:.4f}, \"\n        f\"Test Loss: {test_loss_mean:.4f}, \"\n        f\"Train Acc: {train_acc_mean:.2f}%, \"\n        f\"Test Acc: {test_acc_mean:.2f}%\"\n    )\n\n    # Guardar para graficar\n    plot_loss_train.append(train_loss_mean)\n    plot_loss_test.append(test_loss_mean)\n    plot_acc_train.append(train_acc_mean)\n    plot_acc_test.append(test_acc_mean)\n</pre> num_epochs = 20 batch_size = 32 num_batches = math.ceil(len(X_train) / batch_size) num_batches_test = math.ceil(len(X_test) / batch_size)  plot_loss_train = [] plot_loss_test = [] plot_acc_train = [] plot_acc_test = []  for epoch in range(num_epochs):      loss_epoch_train = []     loss_epoch_test = []     accuracy_train = []     accuracy_test = []          model.train()     for i in range(num_batches):         X_batch = X_train[i * batch_size : (i + 1) * batch_size]         y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)          optimizer.zero_grad()         predictions = model(X_batch)         loss = loss_function(predictions, y_batch)         loss.backward()         optimizer.step()          loss_epoch_train.append(loss.item())         pred_labels = (predictions &gt;= 0.5).float()         acc = (pred_labels == y_batch).float().mean().item() * 100         accuracy_train.append(acc)      model.eval()     with torch.inference_mode():         for i in range(num_batches_test):             X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]             y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)              predictions_inference = model(X_test_batch)             loss_test = loss_function(predictions_inference, y_test_batch)             loss_epoch_test.append(loss_test.item())              pred_labels_test = (predictions_inference &gt;= 0.5).float()             acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100             accuracy_test.append(acc_test)      # Promedios de la \u00e9poca     train_loss_mean = np.mean(loss_epoch_train)     test_loss_mean = np.mean(loss_epoch_test)     train_acc_mean = np.mean(accuracy_train)     test_acc_mean = np.mean(accuracy_test)      print(         f\"Epoch: {epoch+1}, \"         f\"Train Loss: {train_loss_mean:.4f}, \"         f\"Test Loss: {test_loss_mean:.4f}, \"         f\"Train Acc: {train_acc_mean:.2f}%, \"         f\"Test Acc: {test_acc_mean:.2f}%\"     )      # Guardar para graficar     plot_loss_train.append(train_loss_mean)     plot_loss_test.append(test_loss_mean)     plot_acc_train.append(train_acc_mean)     plot_acc_test.append(test_acc_mean) In\u00a0[\u00a0]: Copied! <pre>plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\")\nplt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\")\nplt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\")\nplt.legend()\nplt.show()\n</pre> plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\") plt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\") plt.legend() plt.show()  plt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\") plt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\nwith torch.inference_mode():\n    predictions=model(X_test)\npredictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\nplt.show()\n</pre> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show() with torch.inference_mode():     predictions=model(X_test) predictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0) plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions) plt.show()"},{"location":"course/Mathematics/step_04_artificial_neuron.html#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"course/Mathematics/step_04_artificial_neuron.html#activation-functions","title":"Activation Functions\u00b6","text":""},{"location":"course/Mathematics/step_04_artificial_neuron.html#classification-toy-example","title":"Classification Toy Example\u00b6","text":""},{"location":"course/Mathematics/step_05_gradient_descent.html","title":"Gradient Descent","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Definici\u00f3n de la funci\u00f3n\n\n\ndef function(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n\n# C\u00e1lculo del gradiente (derivadas parciales)\n\n\ndef gradiente(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    \n    df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n    df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])\n\n    return np.stack([df_x1, df_x2], axis=1)\n\n# Algoritmo de descenso del gradiente\n\n\ndef descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):\n    dim = 2\n    X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]\n    trayectorias = [X.copy()]\n\n    for _ in range(num_iteraciones):\n        X = X - learning_rate * gradiente(input=X)\n        trayectorias.append(X.copy())\n        \n    return np.array(trayectorias)\n\n\n# Ejecuci\u00f3n del descenso del gradiente\ntrayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)\n\n# Visualizaci\u00f3n de trayectorias en el espacio 2D\nfor i in range(trayectoria.shape[1]):\n    plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Trayectorias del descenso del gradiente\")\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np  # Definici\u00f3n de la funci\u00f3n   def function(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"     return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])  # C\u00e1lculo del gradiente (derivadas parciales)   def gradiente(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"          df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])     df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])      return np.stack([df_x1, df_x2], axis=1)  # Algoritmo de descenso del gradiente   def descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):     dim = 2     X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]     trayectorias = [X.copy()]      for _ in range(num_iteraciones):         X = X - learning_rate * gradiente(input=X)         trayectorias.append(X.copy())              return np.array(trayectorias)   # Ejecuci\u00f3n del descenso del gradiente trayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)  # Visualizaci\u00f3n de trayectorias en el espacio 2D for i in range(trayectoria.shape[1]):     plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.title(\"Trayectorias del descenso del gradiente\") plt.show() In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\n</pre> # 3pps import matplotlib.pyplot as plt import torch In\u00a0[\u00a0]: Copied! <pre>tiempo = torch.arange(0, 20).float()\ntiempo\n</pre> tiempo = torch.arange(0, 20).float() tiempo In\u00a0[\u00a0]: Copied! <pre>velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1\nplt.scatter(tiempo, velocidad)\n</pre> velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1 plt.scatter(tiempo, velocidad) In\u00a0[\u00a0]: Copied! <pre>velocidad.shape, tiempo.shape\n</pre> velocidad.shape, tiempo.shape In\u00a0[\u00a0]: Copied! <pre>def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:\n    a, b, c = parametros\n    return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c\n</pre> def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:     a, b, c = parametros     return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c In\u00a0[\u00a0]: Copied! <pre>def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:\n    return (real - predicted).square().mean()\n</pre> def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:     return (real - predicted).square().mean() In\u00a0[\u00a0]: Copied! <pre>parametros = torch.randn(3).requires_grad_()\nparametros\n</pre> parametros = torch.randn(3).requires_grad_() parametros In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones In\u00a0[\u00a0]: Copied! <pre>def show_preds(tiempo, real, preds: torch.Tensor):\n    plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")\n    plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")\n    plt.legend()\n    plt.show()\n\n\nshow_preds(tiempo, velocidad, predicciones)\n</pre> def show_preds(tiempo, real, preds: torch.Tensor):     plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")     plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")     plt.legend()     plt.show()   show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>perdida = loss_function(predicciones, velocidad)\nperdida\n</pre> perdida = loss_function(predicciones, velocidad) perdida <p>Aplicamos backward y comprobamos los gradientes</p> In\u00a0[\u00a0]: Copied! <pre>perdida.backward()\nparametros.grad\n</pre> perdida.backward() parametros.grad <p>Podemos utilizar un ratio de aprendizaje, actualizar el gradiente a partir de ese ratio y volver a colocar 0 en los gradientes para realizar una nueva evaluaci\u00f3n</p> In\u00a0[\u00a0]: Copied! <pre>lr = 1e-5\nparametros.data = parametros.data - lr * parametros.grad.data\nparametros.grad = None\n</pre> lr = 1e-5 parametros.data = parametros.data - lr * parametros.grad.data parametros.grad = None In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones In\u00a0[\u00a0]: Copied! <pre>show_preds(tiempo, velocidad, predicciones)\n</pre> show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):\n    predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)\n    perdida = loss_function(predicted=predicciones, real=datos_a_predecir)\n    perdida.backward()\n\n    # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles\n    with torch.no_grad():\n        parametros_aprendibles -= lr * parametros_aprendibles.grad\n    \n    # Otra forma de resetear los gradientes\n    parametros_aprendibles.grad.zero_()\n\n    show_preds(tiempo, datos_a_predecir, predicciones)\n    return predicciones, parametros_aprendibles, perdida\n</pre> def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):     predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)     perdida = loss_function(predicted=predicciones, real=datos_a_predecir)     perdida.backward()      # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles     with torch.no_grad():         parametros_aprendibles -= lr * parametros_aprendibles.grad          # Otra forma de resetear los gradientes     parametros_aprendibles.grad.zero_()      show_preds(tiempo, datos_a_predecir, predicciones)     return predicciones, parametros_aprendibles, perdida In\u00a0[\u00a0]: Copied! <pre># 3pps\nfrom tqdm import tqdm\n</pre> # 3pps from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>num_epochs = 20\nparametros_aprendibles = torch.randn(3, requires_grad=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    predicciones, parametros_aprendibles, perdida = apply_step_training(\n        tiempo=tiempo, \n        parametros_aprendibles=parametros_aprendibles, \n        datos_a_predecir=velocidad\n    )\n    print(f\"Epoch {epoch+1}, perdida: {perdida}\")\n</pre> num_epochs = 20 parametros_aprendibles = torch.randn(3, requires_grad=True)  for epoch in tqdm(range(num_epochs)):     predicciones, parametros_aprendibles, perdida = apply_step_training(         tiempo=tiempo,          parametros_aprendibles=parametros_aprendibles,          datos_a_predecir=velocidad     )     print(f\"Epoch {epoch+1}, perdida: {perdida}\") In\u00a0[\u00a0]: Copied! <pre>def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:\n\n    # (tensor_entrada) -&gt; (B, N)\n    # peso -&gt; (B, N, 1)\n    # (N) \n    return tensor_entrada @ w + b\n\n\nclass CapaLineal:\n\n    def __init__(self, shape_entrada: int) -&gt; None:\n\n        self.w = torch.randn()\n</pre> def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:      # (tensor_entrada) -&gt; (B, N)     # peso -&gt; (B, N, 1)     # (N)      return tensor_entrada @ w + b   class CapaLineal:      def __init__(self, shape_entrada: int) -&gt; None:          self.w = torch.randn() In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.model_selection import train_test_split from torch import nn In\u00a0[\u00a0]: Copied! <pre>class Linear(nn.Module):\n\n    def __init__(self, ) -&gt; None:\n\n        super().__init__()\n\n        self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)\n        self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.weight * input_tensor + self.bias\n</pre> class Linear(nn.Module):      def __init__(self, ) -&gt; None:          super().__init__()          self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)         self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.weight * input_tensor + self.bias In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre>start = 0\nend = 1\nsteps = 0.02\nX = np.arange(start, end, steps)\nX\n</pre> start = 0 end = 1 steps = 0.02 X = np.arange(start, end, steps) X In\u00a0[\u00a0]: Copied! <pre>bias = 0.3\nweight = 0.7\ny = weight * X + bias\n</pre> bias = 0.3 weight = 0.7 y = weight * X + bias In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\nX_train.dtype\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32)) X_train.dtype In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\")\nplt.show()\nplt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\")\nplt.show()\n</pre> plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\") plt.show() plt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\") plt.show() In\u00a0[\u00a0]: Copied! <pre>linear_model = Linear()\nlist(linear_model.parameters())\nlinear_model.state_dict()\n</pre> linear_model = Linear() list(linear_model.parameters()) linear_model.state_dict() In\u00a0[\u00a0]: Copied! <pre>linear_model.eval()\nwith torch.no_grad():\n    predictions = linear_model(X_test)\npredictions\n</pre> linear_model.eval() with torch.no_grad():     predictions = linear_model(X_test) predictions <p>De la documentacion: InferenceMode is analogous to no_grad and should be used when you are certain your operations will not interact with autograd (e.g., during data loading or model evaluation). Compared to no_grad, it removes additional overhead by disabling view tracking and version counter bumps. It is also more restrictive, in that tensors created in this mode cannot be used in computations recorded by autograd. Vamos que no tiene en cuenta el trackeo de los gradientes y lo hace m\u00e1s seguro para evitar la actualizaci\u00f3n de par\u00e1metros del modelo. A parte hace m\u00e1s r\u00e1pida la ejecuci\u00f3n de c\u00f3digo en inferencia</p> In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_2 = linear_model(X_test)\npredictions_2\n</pre> with torch.inference_mode():     predictions_2 = linear_model(X_test) predictions_2 In\u00a0[\u00a0]: Copied! <pre>X_test.shape, predictions.shape\n</pre> X_test.shape, predictions.shape In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>loss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(linear_model.parameters())\n</pre> loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(linear_model.parameters()) In\u00a0[\u00a0]: Copied! <pre>num_epochs: int = 50\n\nfor epoch in range(num_epochs):\n    epoch_losses_train = []\n    epoch_losses_test = []\n\n    for x, y in zip(X_train, y_train):\n        optimizer.zero_grad()\n\n        output_model = linear_model(x)\n        loss = loss_fn(output_model, y)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_losses_train.append(loss.item())\n\n    with torch.inference_mode():\n        for x, y in zip(X_test, y_test):\n            output_model = linear_model(x)\n            loss = loss_fn(output_model, y)\n            epoch_losses_test.append(loss.item())\n\n    print(f\"Epoch: {epoch+1}, \"\n          f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"\n          f\"Test Loss: {np.mean(epoch_losses_test):.4f}\")\n</pre> num_epochs: int = 50  for epoch in range(num_epochs):     epoch_losses_train = []     epoch_losses_test = []      for x, y in zip(X_train, y_train):         optimizer.zero_grad()          output_model = linear_model(x)         loss = loss_fn(output_model, y)          loss.backward()         optimizer.step()          epoch_losses_train.append(loss.item())      with torch.inference_mode():         for x, y in zip(X_test, y_test):             output_model = linear_model(x)             loss = loss_fn(output_model, y)             epoch_losses_test.append(loss.item())      print(f\"Epoch: {epoch+1}, \"           f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"           f\"Test Loss: {np.mean(epoch_losses_test):.4f}\") In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_trained = linear_model(X_test)\nplt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_trained = linear_model(X_test) plt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>torch.save(linear_model, \"linear_model.pth\")\nlinear_model_loaded = torch.load(\"linear_model.pth\")\nlinear_model_loaded.state_dict()\n</pre> torch.save(linear_model, \"linear_model.pth\") linear_model_loaded = torch.load(\"linear_model.pth\") linear_model_loaded.state_dict() In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_loaded = linear_model_loaded(X_test)\nplt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_loaded = linear_model_loaded(X_test) plt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show()"},{"location":"course/Mathematics/step_05_gradient_descent.html#gradient-descent","title":"Gradient Descent\u00b6","text":""},{"location":"course/Mathematics/step_05_gradient_descent.html#example-1","title":"Example 1\u00b6","text":""},{"location":"course/Mathematics/step_05_gradient_descent.html#example-2","title":"Example 2\u00b6","text":""},{"location":"course/Mathematics/step_05_gradient_descent.html#another-example","title":"Another example\u00b6","text":""},{"location":"course/Mathematics/step_06_deep_neural_networks.html","title":"Deep Neural Networks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Mathematics/step_06_deep_neural_networks.html#deep-neural-networks","title":"Deep Neural Networks\u00b6","text":""},{"location":"course/Mathematics/step_07_regularization_techniques.html","title":"Regularization Techniques","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Mathematics/step_07_regularization_techniques.html#regularization-techniques","title":"Regularization Techniques\u00b6","text":""},{"location":"course/Mathematics/step_08_optimizers.html","title":"Optimizers","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Mathematics/step_08_optimizers.html#optimizers","title":"Optimizers\u00b6","text":""},{"location":"course/Mathematics/step_09_metrics.html","title":"Metrics","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Mathematics/step_09_metrics.html#metrics","title":"Metrics\u00b6","text":""},{"location":"course/Sequential%20Models/step_01_tokenization.html","title":"Tokenization","text":""},{"location":"course/Sequential%20Models/step_01_tokenization.html#tokenization","title":"Tokenization\u00b6","text":""},{"location":"course/Sequential%20Models/step_02_rnns.html","title":"Recurrent Neural Networks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_02_rnns.html#recurrent-neural-networks","title":"Recurrent Neural Networks\u00b6","text":""},{"location":"course/Sequential%20Models/step_03_lstm.html","title":"Long Short Term Memory","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_03_lstm.html#long-short-term-memory","title":"Long Short Term Memory\u00b6","text":""},{"location":"course/Sequential%20Models/step_04_autoencoders.html","title":"Auto Encoders","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_04_autoencoders.html#auto-encoders","title":"Auto Encoders\u00b6","text":""},{"location":"course/Sequential%20Models/step_05_transformers.html","title":"Transformers","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_05_transformers.html#transformers","title":"Transformers\u00b6","text":""},{"location":"course/Sequential%20Models/step_06_moe.html","title":"Mixture of Experts","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_06_moe.html#mixture-of-experts","title":"Mixture of Experts\u00b6","text":""},{"location":"course/Sequential%20Models/step_07_open_models.html","title":"Open Models","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/Sequential%20Models/step_07_open_models.html#open-models","title":"Open Models\u00b6","text":""},{"location":"course/Time%20Series/step_01_other_techniques.html","title":"Time Series Techniques","text":"<p>Gramian Angular Difference Field (GADF) permite codificar series temporales a im\u00e1genes, permiten realizar una interpretaci\u00f3n en una imagen en 2D de una serie temporal univariable.</p> <p>Coordenadas polares</p> <p>Podemos convertir series temporales, representadas en un eje de coordenadas cartesianas, donde tenemos un valor en el eje y que es el valor de variable en si, en el eje x tenemos el tiempo que transcurre y como varia esa variable en el tiempo.</p> <p></p> <p>Para convertir a coordenadas polares, representado tal que (r, \u03b8), donde:</p> <ul> <li>r (radio) representa la distancia desde el origen.</li> <li>\u03b8 (\u00e1ngulo) representa la direcci\u00f3n del punto respecto a un eje de referencia (como el eje x).</li> </ul> <p>Podemos convertir a coordenadas polares de 2 formas:</p> <ul> <li>La primera ser\u00eda considerar una se\u00f1al peri\u00f3dica, como un seno o coseno, que parta desde 0 hasta 2pi, este es el periodo de la se\u00f1al y luego tendr\u00edamos que el radio es el valor en si. Visualizar ciclos o estacionalidades (en el m\u00e9todo 1).</li> <li>La segunda es medir la diferencia de tiempo que existe entre un instante t y un instante t + 1, y luego se calcula la diferencia del valor de la variable como un delta, y se calcula el angulo entre la delta de tiempo y la delta de la variable. Por tanto ser\u00eda algo como: \u03b8 = arctan(\u0394y / \u0394t), r = sqrt((\u0394t)\u00b2 + (\u0394y)\u00b2) que mide la magnitud del cambio. Detectar patrones direccionales, como si los cambios tienen una orientaci\u00f3n predominante (en el m\u00e9todo 2).</li> </ul> <p></p> <p>En este caso, como la distribuci\u00f3n del tiempo es discreta, que representa los meses, podemos convertirlo a grados haciendo:</p> <ul> <li>2 * math.pi * (t / max(t)) En el caso de tener un formato basado en datetime, con HH:MM:SS, habr\u00eda que normalizar respecto al tiempo:</li> <li>2 * math.pi * (t / 24 horas * 3600 segundos), donde t es el tiempo desde las 00:00:00 en segundos</li> </ul> In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\n\n# Datos sint\u00e9ticos\n# Tiempo (por ejemplo, meses)\nmonths = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n# Valores sint\u00e9ticos (algo como una onda)\nvalues = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n\n# M\u00e9todo 1 ----------\ntheta_1 = [2 * math.pi * (month / len(months)) for month in months]\nprint(theta_1)\n\nrho_1 = values\nprint(rho_1)\n\ncoord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)]\nprint(coord_method1)\n\n# M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ----------\n# Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos\n# arrays tienen la misma cantidad de elementos\ntheta_2 = []\nrho_2 = []\nfor index, month in enumerate(months):\n    if index + 1 &lt; len(values):\n        delta_values = values[index + 1] - values[index]\n        delta_time = months[index + 1] - month\n\n        # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que\n        # saturar el valor entre 2pi y -2pi\n        if delta_time != 0:\n            theta_2.append(math.atan(delta_values / delta_time))\n        else:\n            theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)\n\n        rho_2.append(math.sqrt(delta_values**2 + delta_time**2))\nprint(\"\\n\")\nprint(theta_2)\nprint(rho_2)\ncoord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)]\nprint(coord_method2)\n</pre> # Standard libraries import math  # Datos sint\u00e9ticos # Tiempo (por ejemplo, meses) months = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Valores sint\u00e9ticos (algo como una onda) values = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]  # M\u00e9todo 1 ---------- theta_1 = [2 * math.pi * (month / len(months)) for month in months] print(theta_1)  rho_1 = values print(rho_1)  coord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)] print(coord_method1)  # M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ---------- # Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos # arrays tienen la misma cantidad de elementos theta_2 = [] rho_2 = [] for index, month in enumerate(months):     if index + 1 &lt; len(values):         delta_values = values[index + 1] - values[index]         delta_time = months[index + 1] - month          # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que         # saturar el valor entre 2pi y -2pi         if delta_time != 0:             theta_2.append(math.atan(delta_values / delta_time))         else:             theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)          rho_2.append(math.sqrt(delta_values**2 + delta_time**2)) print(\"\\n\") print(theta_2) print(rho_2) coord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)] print(coord_method2) <pre>[0.0, 0.5235987755982988, 1.0471975511965976, 1.5707963267948966, 2.0943951023931953, 2.6179938779914944, 3.141592653589793, 3.6651914291880923, 4.1887902047863905, 4.71238898038469, 5.235987755982989, 5.759586531581287]\n[5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n[(0.0, 5), (0.5235987755982988, 6), (1.0471975511965976, 8), (1.5707963267948966, 9), (2.0943951023931953, 10), (2.6179938779914944, 9), (3.141592653589793, 8), (3.6651914291880923, 6), (4.1887902047863905, 4), (4.71238898038469, 3), (5.235987755982989, 4), (5.759586531581287, 5)]\n\n\n[0.7853981633974483, 1.1071487177940906, 0.7853981633974483, 0.7853981633974483, -0.7853981633974483, -0.7853981633974483, -1.1071487177940906, -1.1071487177940906, -0.7853981633974483, 0.7853981633974483, 0.7853981633974483]\n[1.4142135623730951, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 2.23606797749979, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951]\n[(0.7853981633974483, 1.4142135623730951), (1.1071487177940906, 2.23606797749979), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-1.1071487177940906, 2.23606797749979), (-1.1071487177940906, 2.23606797749979), (-0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951)]\n</pre> In\u00a0[2]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\n\n# M\u00e9todo 1\nplt.subplot(1, 2, 1, projection=\"polar\")\nplt.plot(theta_1, rho_1, marker=\"o\")\nplt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")\n\n# M\u00e9todo 2\nplt.subplot(1, 2, 2, projection=\"polar\")\nplt.plot(theta_2, rho_2, marker=\"o\")\nplt.title(\"M\u00e9todo 2: Cambio como vector\")\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt  # M\u00e9todo 1 plt.subplot(1, 2, 1, projection=\"polar\") plt.plot(theta_1, rho_1, marker=\"o\") plt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")  # M\u00e9todo 2 plt.subplot(1, 2, 2, projection=\"polar\") plt.plot(theta_2, rho_2, marker=\"o\") plt.title(\"M\u00e9todo 2: Cambio como vector\")  plt.tight_layout() plt.show() <p>Continuando con Gramian, tenemos que la matriz Gramian es una matriz que consiste en realizar el producto vectorial entre cada pareja de vectores.</p> <p></p> <p>La matriz de Gram preserva la dependencia temporal, pues el tiempo incrementa del mismo modo que lo hace la posici\u00f3n de la matriz 2D de arriba a la izquierda y de arriba a la derecha, por lo que el tiempo se codifica en la geometr\u00eda de la matriz. Es decir, la matriz mantiene las relaciones angulares entre todos los puntos de la serie. Para ello se siguen los pasos siguientes:</p> <ul> <li><p>Paso 1: Normaliza la serie: Primero necesitas normalizar tu serie a un rango de $[-1, 1]$ (porque luego aplicaremos el arccos):</p> <p>$$   \\tilde{x}_i = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)} \\times 2 - 1   $$</p> </li> <li><p>Paso 2: Convierte los valores a \u00e1ngulos. Para cada valor de la serie normalizada:</p> <p>$$   \\phi_i = \\arccos(\\tilde{x}_i)   $$</p> <p>Esto convierte cada valor en un \u00e1ngulo entre $0$ y $\\pi$, que representa su posici\u00f3n relativa dentro del ciclo.</p> </li> <li><p>Paso 3: Construye la matriz GADF</p> <p>La idea es comparar cada par de puntos $(\\phi_i, \\phi_j)$ de la serie y calcular:</p> <p>$$   \\text{GADF}[i,j] = \\sin(\\phi_i - \\phi_j)   $$</p> <ul> <li>Esto mide la diferencia angular entre dos puntos.</li> <li>El resultado es una matriz cuadrada $N \\times N$ que puedes tratar como una imagen.</li> </ul> </li> </ul> <p>Tambi\u00e9n, existe el Gramian Angular Summation Field (GASF) usa la suma en vez de la diferencia:</p> <p>$$ \\text{GASF}[i,j] = \\cos(\\phi_i + \\phi_j) $$</p> In\u00a0[5]: Copied! <pre># Standard libraries\nimport math\nimport random\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef normalization(samples: list) -&gt; list:\n\n    min_val = min(samples)\n    max_val = max(samples)\n\n    return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]\n\n\n@njit\ndef degrees(samples: list) -&gt; list:\n\n    return [math.acos(sample) for sample in samples]\n\n\n@njit\ndef gadf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.sin(sample_i - sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n@njit\ndef gasf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.cos(sample_i + sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n# Datos temporales (por ejemplo, 16 puntos en el tiempo)\nx = random.sample(range(1, 4000), 2400)\nx_gadf_matrix = gadf_matrix(samples=x)\nx_gasf_matrix = gasf_matrix(samples=x)\n\nprint(x_gadf_matrix.shape)\nprint(x_gasf_matrix.shape)\n\nplt.imshow(x_gadf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n\nplt.imshow(x_gasf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n</pre> # Standard libraries import math import random  # 3pps import matplotlib.pyplot as plt import numpy as np from numba import njit   @njit def normalization(samples: list) -&gt; list:      min_val = min(samples)     max_val = max(samples)      return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]   @njit def degrees(samples: list) -&gt; list:      return [math.acos(sample) for sample in samples]   @njit def gadf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.sin(sample_i - sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   @njit def gasf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.cos(sample_i + sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   # Datos temporales (por ejemplo, 16 puntos en el tiempo) x = random.sample(range(1, 4000), 2400) x_gadf_matrix = gadf_matrix(samples=x) x_gasf_matrix = gasf_matrix(samples=x)  print(x_gadf_matrix.shape) print(x_gasf_matrix.shape)  plt.imshow(x_gadf_matrix, origin=\"upper\") plt.colorbar() plt.show()  plt.imshow(x_gasf_matrix, origin=\"upper\") plt.colorbar() plt.show() <pre>(2400, 2400)\n(2400, 2400)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/Time%20Series/step_01_other_techniques.html#time-series-techniques","title":"Time Series Techniques\u00b6","text":""},{"location":"course/Time%20Series/step_01_other_techniques.html#gramian-angular-field","title":"Gramian Angular Field\u00b6","text":""}]}