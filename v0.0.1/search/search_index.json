{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Papers &amp; Algorithms with Code","text":"<p>Implementations of algorithms, utilities, and code snippets I find interesting, along with PyTorch and Python versions of research papers.</p>"},{"location":"layers/cv.html","title":"Computer Vision","text":""},{"location":"layers/cv.html#src.layers.cv.aps","title":"<code>aps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214</p>"},{"location":"layers/cv.html#src.layers.cv.aps.APS","title":"<code>APS(norm=2)</code>","text":"<p>Initializes the class with normalization option.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>int | float | Literal['fro', 'nuc', 'inf', '-inf'] | None</code> <p>Normalization type or value, defaults to 2.</p> <code>2</code> Source code in <code>src/layers/cv/aps.py</code> <pre><code>def __init__(\n    self,\n    norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes the class with normalization option.\n\n    Args:\n        norm: Normalization type or value, defaults to 2.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n    self.norm = norm\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.aps.APS.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input tensor to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Output tensor, optionally with index if return_index is True.</p> Source code in <code>src/layers/cv/aps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input tensor to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Output tensor, optionally with index if return_index is True.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Extraemos las dimensiones\n    b, p, _, _, _ = polyphase_combined.size()\n\n    # Combinamos los valores de los canales, altura y anchura del tensor\n    polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = torch.linalg.vector_norm(\n        input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)\n    )\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.lps","title":"<code>lps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001</p>"},{"location":"layers/cv.html#src.layers.cv.lps.LPS","title":"<code>LPS(channel_size, hidden_size)</code>","text":"<p>Initializes the model with specified channel and hidden sizes.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of input channels for the Conv2D layer.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden units for the Conv2D layer.</p> required Source code in <code>src/layers/cv/lps.py</code> <pre><code>def __init__(self, channel_size: int, hidden_size: int) -&gt; None:\n    \"\"\"\n    Initializes the model with specified channel and hidden sizes.\n\n    Args:\n        channel_size: Number of input channels for the Conv2D layer.\n        hidden_size: Number of hidden units for the Conv2D layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n\n    # Definimos el modelo \u00fanico para cada componente\n    self.conv_model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=channel_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.Flatten(),\n        nn.AdaptiveAvgPool2d(1),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.lps.LPS.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Tensor of dominant component, optionally with index.</p> Source code in <code>src/layers/cv/lps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Tensor of dominant component, optionally with index.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Utilizamos el modelo basado en convoluciones por cada componente\n    _logits = []\n    for polyphase in range(polyphase_combined.size()[1]):\n        _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))\n    logits = torch.squeeze(torch.stack(_logits))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.se","title":"<code>se</code>","text":"<p>Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507</p>"},{"location":"layers/cv.html#src.layers.cv.se.SqueezeExcitation","title":"<code>SqueezeExcitation(channel_size, ratio)</code>","text":"<p>Implements Squeeze-and-Excitation (SE) block.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of channels in the input tensor.</p> required <code>ratio</code> <code>int</code> <p>Reduction factor for the compression layer.</p> required Source code in <code>src/layers/cv/se.py</code> <pre><code>def __init__(self, channel_size: int, ratio: int) -&gt; None:\n    \"\"\"\n    Implements Squeeze-and-Excitation (SE) block.\n\n    Args:\n        channel_size: Number of channels in the input tensor.\n        ratio: Reduction factor for the compression layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Vamos a crear un modelo Sequential\n    self.se_block = nn.Sequential(\n        nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)\n        nn.Flatten(),  # (B, C)\n        nn.Linear(\n            in_features=channel_size, out_features=channel_size // ratio\n        ),  # (B, C//ratio)\n        nn.ReLU(),  # (B, C//ratio)\n        nn.Linear(\n            in_features=channel_size // ratio, out_features=channel_size\n        ),  # (B, C)\n        nn.Sigmoid(),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.se.SqueezeExcitation.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Applies attention mechanism to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor with shape (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with attention applied, same shape as input.</p> Source code in <code>src/layers/cv/se.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies attention mechanism to input tensor.\n\n    Args:\n        input_tensor: Input tensor with shape (B, C, H, W).\n\n    Returns:\n        Tensor with attention applied, same shape as input.\n    \"\"\"\n\n    # Primero podemos obtener el tama\u00f1o del tensor de entrada\n    b, c, _, _ = input_tensor.size()\n\n    # Obtenemos el tensor de aplicar SE\n    x = self.se_block(input_tensor)\n\n    # Modificamos el shape del tensor para ajustarlo al input\n    x = x.view(b, c, 1, 1)\n\n    # Aplicamos el producto como mecanismo de atenci\u00f3n\n    return x * input_tensor\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit","title":"<code>vit</code>","text":""},{"location":"layers/cv.html#src.layers.cv.vit.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Initialize encoder block module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize encoder block module.\n\n    Args:\n        d_model: Number of features in input.\n        d_ff: Hidden layer feature dimensions.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Process input tensor through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after encoder block processing.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through encoder block.\n\n    Args:\n        input_tensor: Batch of input tensors.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after encoder block processing.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Initialize feed-forward neural network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Input and output feature dimensions.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on layers.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize feed-forward neural network.\n\n    Args:\n        d_model: Input and output feature dimensions.\n        d_ff: Hidden layer feature dimensions.\n        dropout_rate: Dropout rate applied on layers.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.GELU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through feed-forward layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after feed-forward processing.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through feed-forward layers.\n\n    Args:\n        input_tensor: Batch of input tensors.\n\n    Returns:\n        Output tensor after feed-forward processing.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Initialize layer normalization module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-06</code> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initialize layer normalization module.\n\n    Args:\n        features: Number of features in input.\n        eps: Small value to avoid division by zero.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Apply layer normalization to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized embeddings.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply layer normalization to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Normalized embeddings.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Initialize multi-head attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on scores.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize multi-head attention module.\n\n    Args:\n        d_model: Number of features in input.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate applied on scores.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Compute attention scores and output.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Dropout layer, optional.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Compute attention scores and output.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n        dropout: Dropout layer, optional.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Process input tensors through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after attention processing.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensors through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after attention processing.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.PatchEmbedding","title":"<code>PatchEmbedding(patch_size_height, patch_size_width, in_channels, d_model)</code>","text":"<p>Initialize patch embedding module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    in_channels: int,\n    d_model: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch embedding module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        in_channels: Number of input channels.\n        d_model: Dimension of the model.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.in_channels = in_channels\n    self.d_model = d_model\n\n    # Esta es una de las diferencias con usar transformers en el texto\n    # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n\n    # de los pixeles\n    self.embedding = nn.Linear(\n        in_features=self.in_channels\n        * self.patch_size_height\n        * self.patch_size_width,\n        out_features=self.d_model,\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.PatchEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Apply linear projection to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of image patches as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after linear projection of patches.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply linear projection to input tensor.\n\n    Args:\n        input_tensor: Batch of image patches as a tensor.\n\n    Returns:\n        Tensor after linear projection of patches.\n    \"\"\"\n\n    return self.embedding(input_tensor)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.Patches","title":"<code>Patches(patch_size_height, patch_size_width, img_height, img_width)</code>","text":"<p>Initialize patch extraction module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of the input image.</p> required <code>img_width</code> <code>int</code> <p>Width of the input image.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If img_height not divisible by patch height.</p> <code>ValueError</code> <p>If img_width not divisible by patch width.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch extraction module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of the input image.\n        img_width: Width of the input image.\n\n    Raises:\n        ValueError: If img_height not divisible by patch height.\n        ValueError: If img_width not divisible by patch width.\n    \"\"\"\n\n    super().__init__()\n\n    if img_height % patch_size_height != 0:\n        raise ValueError(\n            \"img_height tiene que se divisible entre el patch_size_height\"\n        )\n\n    if img_width % patch_size_width != 0:\n        raise ValueError(\n            \"img_width tiene que se divisible entre el patch_size_width\"\n        )\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.unfold = nn.Unfold(\n        kernel_size=(self.patch_size_height, self.patch_size_width),\n        stride=(self.patch_size_height, self.patch_size_width),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.Patches.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Extract patches from input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of images as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with patches from input images.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Extract patches from input tensor.\n\n    Args:\n        input_tensor: Batch of images as a tensor.\n\n    Returns:\n        Tensor with patches from input images.\n    \"\"\"\n\n    # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n    patches = self.unfold(input_tensor)\n    # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n    return patches.transpose(2, 1)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Initialize positional encoding module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>sequence_length</code> <code>int</code> <p>Max length of input sequences.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on outputs.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize positional encoding module.\n\n    Args:\n        d_model: Dimension of the model.\n        sequence_length: Max length of input sequences.\n        dropout_rate: Dropout rate applied on outputs.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Add positional encoding to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Embeddings with added positional encoding.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Add positional encoding to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Initialize residual connection module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for sublayer output.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize residual connection module.\n\n    Args:\n        features: Number of features in input.\n        dropout_rate: Dropout rate for sublayer output.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Apply residual connection to sublayer output.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Original input tensor.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer module to apply.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Apply residual connection to sublayer output.\n\n    Args:\n        input_tensor: Original input tensor.\n        sublayer: Sublayer module to apply.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.VIT","title":"<code>VIT(patch_size_height, patch_size_width, img_height, img_width, in_channels, num_encoders, d_model, d_ff, h, num_classes, dropout_rate)</code>","text":"<p>Initialize Vision Transformer (VIT).</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of input images.</p> required <code>img_width</code> <code>int</code> <p>Width of input images.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>d_ff</code> <code>int</code> <p>Dimension of feed-forward layers.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/layers/cv/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n    in_channels: int,\n    num_encoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    num_classes: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initialize Vision Transformer (VIT).\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of input images.\n        img_width: Width of input images.\n        in_channels: Number of input channels.\n        num_encoders: Number of encoder blocks.\n        d_model: Dimension of the model.\n        d_ff: Dimension of feed-forward layers.\n        h: Number of attention heads.\n        num_classes: Number of output classes.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.img_height = img_height\n    self.img_width = img_width\n    self.in_channels = in_channels\n    self.num_encoders = num_encoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.num_classes = num_classes\n    self.dropout_rate = dropout_rate\n\n    # N\u00famero de patches\n    self.num_patches = (img_height // patch_size_height) * (\n        img_width // patch_size_width\n    )\n\n    # CLS token permite tener una representaci\u00f3n global de todos los inputs\n    # de la imagen (de los diferentes embeddings de cada patch)\n    self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n\n    self.patch_layer = Patches(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        img_height=self.img_height,\n        img_width=self.img_width,\n    )\n\n    self.embeddings = PatchEmbedding(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        in_channels=self.in_channels,\n        d_model=self.d_model,\n    )\n\n    # Entiendo que la longitud de la secuencia coincide con el numero de patches\n    # y un embedding m\u00e1s de la clase,\n    self.positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.num_patches + 1,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    self.layer_norm = LayerNormalization(features=self.d_model)\n\n    self.mlp_classifier = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_model),\n        nn.GELU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=self.d_model, out_features=num_classes),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vit.VIT.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through VIT model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Classification output tensor.</p> Source code in <code>src/layers/cv/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through VIT model.\n\n    Args:\n        input_tensor: Batch of input images.\n\n    Returns:\n        Classification output tensor.\n    \"\"\"\n\n    # Extraemos los patches\n    input_patches = self.patch_layer(input_tensor)\n\n    # Convertimso a embeddings los patches\n    patch_embeddings = self.embeddings(input_patches)\n\n    # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia\n    # (B, 1, d_model)\n    cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n    # (B, num_patches+1, d_model)\n    embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n\n    # A\u00f1adir positional encoding\n    embeddings = self.positional_encoding(embeddings)\n\n    # Encoders del transformer\n    encoder_output = embeddings\n    for encoder_layer in self.encoder_layers:\n        encoder_output = encoder_layer(encoder_output)\n\n    # Usar solo el CLS token para clasificaci\u00f3n\n    encoder_output = self.layer_norm(encoder_output)\n    cls_output = encoder_output[:, 0]\n\n    # Clasificaci\u00f3n final\n    return self.mlp_classifier(cls_output)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae","title":"<code>vq_vae</code>","text":""},{"location":"layers/cv.html#src.layers.cv.vq_vae.Decoder","title":"<code>Decoder(in_channels, num_residuals, out_channels=3, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes a decoder with residual blocks and transpose convolutional layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the decoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the decoder.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels, e.g., RGB.</p> <code>3</code> <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    out_channels: int = 3,  # Channel output (RGB)\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes a decoder with residual blocks and transpose\n    convolutional layers.\n\n    Args:\n        in_channels: Number of input channels to the decoder.\n        num_residuals: Number of residual blocks in the decoder.\n        out_channels: Number of output channels, e.g., RGB.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.out_channels = out_channels\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(\n                in_channels=self.in_channels, hidden_size=self.hidden_size\n            )\n            for _ in range(self.num_residuals)\n        ]\n    )\n\n    self.model = nn.Sequential(\n        nn.ConvTranspose2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=self.hidden_size,\n            out_channels=self.out_channels,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.Decoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the decoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by residual blocks and transpose</p> <code>Tensor</code> <p>convolutional layers.</p> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the decoder.\n\n    Args:\n        input_tensor: The input tensor to the decoder.\n\n    Returns:\n        A tensor processed by residual blocks and transpose\n        convolutional layers.\n    \"\"\"\n\n    decoder_output = input_tensor\n    for res_block in self.residual_blocks:\n        decoder_output = res_block(decoder_output)\n\n    return self.model(decoder_output)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.Encoder","title":"<code>Encoder(in_channels, num_residuals, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes an encoder with convolutional layers and residual blocks.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the encoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the encoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes an encoder with convolutional layers and residual\n    blocks.\n\n    Args:\n        in_channels: Number of input channels to the encoder.\n        num_residuals: Number of residual blocks in the encoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n    )\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n            for _ in range(self.num_residuals)\n        ]\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.Encoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the encoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by convolutional layers and residual</p> <code>Tensor</code> <p>blocks.</p> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the encoder.\n\n    Args:\n        input_tensor: The input tensor to the encoder.\n\n    Returns:\n        A tensor processed by convolutional layers and residual\n        blocks.\n    \"\"\"\n\n    encoder_output = self.model(input_tensor)\n    for res_block in self.residual_blocks:\n        encoder_output = res_block(encoder_output)\n    return encoder_output\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.ResidualBlock","title":"<code>ResidualBlock(in_channels, hidden_size=256)</code>","text":"<p>Initializes a residual block that applies two convolutional layers and ReLU activations.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the block.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in the hidden layer.</p> <code>256</code> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:\n    \"\"\"\n    Initializes a residual block that applies two convolutional\n    layers and ReLU activations.\n\n    Args:\n        in_channels: Number of input channels for the block.\n        hidden_size: Number of channels in the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.hidden_size = hidden_size\n\n    self.res_block = nn.Sequential(\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.hidden_size,\n            out_channels=self.in_channels,\n            kernel_size=1,\n            stride=1,\n            bias=False,\n        ),\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.ResidualBlock.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the residual block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the block.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the sum of the input tensor and the</p> <code>Tensor</code> <p>block's output.</p> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the residual block.\n\n    Args:\n        input_tensor: The input tensor to the block.\n\n    Returns:\n        A tensor that is the sum of the input tensor and the\n        block's output.\n    \"\"\"\n\n    return input_tensor + self.res_block(input_tensor)\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.VQVAE","title":"<code>VQVAE(in_channels, size_discrete_space, size_embeddings, num_residuals, hidden_size, kernel_size, stride, beta=0.25)</code>","text":"<p>Initializes a VQ-VAE model with encoder, decoder, and quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the model.</p> required <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in encoder/decoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> required <code>kernel_size</code> <code>int</code> <p>Size of convolutional kernels.</p> required <code>stride</code> <code>int</code> <p>Stride of convolutional kernels.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    size_discrete_space: int,\n    size_embeddings: int,\n    num_residuals: int,\n    hidden_size: int,\n    kernel_size: int,\n    stride: int,\n    beta: float = 0.25,\n) -&gt; None:\n    \"\"\"\n    Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n\n    Args:\n        in_channels: Number of input channels for the model.\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        num_residuals: Number of residual blocks in encoder/decoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of convolutional kernels.\n        stride: Stride of convolutional kernels.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.beta = beta\n\n    self.encoder = Encoder(\n        in_channels=self.in_channels,\n        num_residuals=self.num_residuals,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n    self.decoder = Decoder(\n        in_channels=self.hidden_size,\n        num_residuals=self.num_residuals,\n        out_channels=self.in_channels,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n\n    self.vector_quantizer = VectorQuantizer(\n        size_discrete_space=self.size_discrete_space,\n        size_embeddings=self.hidden_size,\n        beta=self.beta,\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.VQVAE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through VQ-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, reconstructed tensor,</p> <code>Tensor</code> <p>and perplexity.</p> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass through VQ-VAE model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        A tuple containing VQ loss, reconstructed tensor,\n        and perplexity.\n    \"\"\"\n\n    encoder_output = self.encoder(input_tensor)\n    vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n    decoder_output = self.decoder(quantized)\n    return vq_loss, decoder_output, perplexity\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.VectorQuantizer","title":"<code>VectorQuantizer(size_discrete_space, size_embeddings, beta=0.25)</code>","text":"<p>Initializes a vector quantizer with a learnable codebook.</p> <p>Parameters:</p> Name Type Description Default <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def __init__(\n    self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n) -&gt; None:\n    \"\"\"\n    Initializes a vector quantizer with a learnable codebook.\n\n    Args:\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.beta = beta\n\n    # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings\n    # Ha de ser una matriz aprendible\n    self.codebook = nn.Embedding(\n        num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n    )\n    # Initialize weights uniformly\n    self.codebook.weight.data.uniform_(\n        -1 / self.size_discrete_space, 1 / self.size_discrete_space\n    )\n</code></pre>"},{"location":"layers/cv.html#src.layers.cv.vq_vae.VectorQuantizer.forward","title":"<code>forward(encoder_output)</code>","text":"<p>Quantizes the encoder output using the codebook.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_output</code> <code>Tensor</code> <p>Tensor of encoder outputs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, quantized tensor, perplexity,</p> <code>Tensor</code> <p>and encodings.</p> Source code in <code>src/layers/cv/vq_vae.py</code> <pre><code>def forward(\n    self, encoder_output: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the encoder output using the codebook.\n\n    Args:\n        encoder_output: Tensor of encoder outputs.\n\n    Returns:\n        A tuple containing VQ loss, quantized tensor, perplexity,\n        and encodings.\n    \"\"\"\n\n    # Comentario de otras implementaciones: The channels are used as the space\n    # in which to quantize.\n    # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)\n    encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n    b, h, w, c = encoder_output.size()\n    encoder_output_flat = encoder_output.reshape(-1, c)\n\n    # Calculamos la distancia entre ambos vectores\n    distances = (\n        torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n        + torch.sum(self.codebook.weight**2, dim=1)\n        - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n    )\n\n    # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n    encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n    # Matriz de ceros de (indices, size_discrete_space)\n    encodings = torch.zeros(\n        encoding_indices.shape[0],\n        self.size_discrete_space,\n        device=encoder_output.device,\n    )\n    # Colocamos un 1 en los indices de los encodings con el\n    # valor m\u00ednimo de distancia creando un vector one-hot\n    encodings.scatter_(1, encoding_indices, 1)\n\n    # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n    # del codebook y le damos formato de nuevo al tensor\n    quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n\n    # VQ-VAE loss terms\n    # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2\n    # FIX: Corrected variable names and loss calculation\n    commitment_loss = F.mse_loss(\n        quantized.detach(), encoder_output\n    )  # ||sg[z_e] - e||^2\n    embedding_loss = F.mse_loss(\n        quantized, encoder_output.detach()\n    )  # ||z_e - sg[e]||^2\n    vq_loss = commitment_loss + self.beta * embedding_loss\n\n    # Straight-through estimator\n    quantized = encoder_output + (quantized - encoder_output).detach()\n\n    # Calculate perplexity\n    avg_probs = torch.mean(encodings, dim=0)\n    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n    # convert quantized from BHWC -&gt; BCHW\n    return (\n        vq_loss,\n        quantized.permute(0, 3, 1, 2).contiguous(),\n        perplexity,\n        encodings,\n    )\n</code></pre>"},{"location":"layers/generative.html","title":"Generative","text":""},{"location":"layers/generative.html#src.layers.generative.gan","title":"<code>gan</code>","text":""},{"location":"layers/generative.html#src.layers.generative.gan.show_generated_samples","title":"<code>show_generated_samples(generator, noise, device, num_samples=16)</code>","text":"<p>Funci\u00f3n auxiliar para mostrar muestras generadas</p> Source code in <code>src/layers/generative/gan.py</code> <pre><code>def show_generated_samples(\n    generator: nn.Module, noise, device: str, num_samples: int = 16\n) -&gt; None:\n    \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        samples = generator(noise[:num_samples]).cpu()\n        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(num_samples):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n            axes[row, col].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"layers/nlp.html","title":"Natural Language Processing","text":""},{"location":"layers/nlp.html#src.layers.nlp.moe","title":"<code>moe</code>","text":""},{"location":"layers/nlp.html#src.layers.nlp.moe.ExpertModel","title":"<code>ExpertModel(input_dim, output_dim, hidden_dim)</code>","text":"<p>Modelo experto individual para MoE</p> <p>Initializes an expert model with a simple feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output data.</p> required <code>hidden_dim</code> <code>int</code> <p>Dimensionality of the hidden layer.</p> required Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:\n    \"\"\"\n    Initializes an expert model with a simple feed-forward network.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        output_dim: Dimensionality of the output data.\n        hidden_dim: Dimensionality of the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim),\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.moe.ExpertModel.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the expert model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's output tensor.</p> Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the expert model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        The model's output tensor.\n    \"\"\"\n\n    return self.model(input_tensor)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.moe.Gating","title":"<code>Gating(input_dim, num_experts, dropout_rate=0.2)</code>","text":"<p>Gating mechanism to select experts.</p> <p>Initializes a gating network for expert selection.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts to select from.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> <code>0.2</code> Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def __init__(\n    self, input_dim: int, num_experts: int, dropout_rate: float = 0.2\n) -&gt; None:\n    \"\"\"\n    Initializes a gating network for expert selection.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        num_experts: Number of experts to select from.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.num_experts = num_experts\n    self.dropout_rate = dropout_rate\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=128),\n        nn.Dropout(self.dropout_rate),\n        nn.LeakyReLU(),\n        nn.Linear(in_features=128, out_features=256),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=256, out_features=128),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=128, out_features=num_experts),\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.moe.Gating.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the gating network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the network.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Softmax probabilities for expert selection.</p> Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the gating network.\n\n    Args:\n        input_tensor: Input tensor to the network.\n\n    Returns:\n        Softmax probabilities for expert selection.\n    \"\"\"\n\n    return F.softmax(self.model(input_tensor), dim=-1)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.moe.MoE","title":"<code>MoE(trained_experts, input_dim, dropout_rate=0.2)</code>","text":"<p>Mixture of Experts</p> <p>Initializes a mixture of experts with gating.</p> <p>Parameters:</p> Name Type Description Default <code>trained_experts</code> <code>list[ExpertModel]</code> <p>List of trained expert models.</p> required <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout in the gating network.</p> <code>0.2</code> Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def __init__(\n    self,\n    trained_experts: list[ExpertModel],\n    input_dim: int,\n    dropout_rate: float = 0.2,\n) -&gt; None:\n    \"\"\"\n    Initializes a mixture of experts with gating.\n\n    Args:\n        trained_experts: List of trained expert models.\n        input_dim: Dimensionality of the input data.\n        dropout_rate: Rate of dropout in the gating network.\n    \"\"\"\n\n    super().__init__()\n\n    self.experts = nn.ModuleList(trained_experts)\n    self.num_experts = len(trained_experts)\n    self.input_dim = input_dim\n    self.dropout_rate = dropout_rate\n\n    self.gating_layer = Gating(\n        input_dim=self.input_dim,\n        num_experts=self.num_experts,\n        dropout_rate=self.dropout_rate,\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.moe.MoE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the mixture of experts.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Weighted sum of expert outputs.</p> Source code in <code>src/layers/nlp/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the mixture of experts.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        Weighted sum of expert outputs.\n    \"\"\"\n\n    # Obtenemos los pesos del selector\n    expert_weights = self.gating_layer(input_tensor)\n\n    # Obtenemos la salida de todos los expertos\n    _expert_outputs: list[torch.Tensor] = []\n    for expert in self.experts:\n        _expert_outputs.append(expert(input_tensor))\n\n    # Stack de salidas [b, output_dim, num_experts]\n    expert_outputs = torch.stack(_expert_outputs, dim=-1)\n\n    # [b, num_experts] -&gt; [b, 1, num_experts]\n    expert_weights = expert_weights.unsqueeze(1)\n\n    # Suma ponderada de la selecci\u00f3n de expertos\n    # [b, output_dim, num_experts] * [b, 1, num_experts]\n    return torch.sum(expert_outputs * expert_weights, dim=-1)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer","title":"<code>transformer</code>","text":""},{"location":"layers/nlp.html#src.layers.nlp.transformer.DecoderBlock","title":"<code>DecoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Decoder block with masked attention, cross-attention, and feed-forward layers.</p> <p>Initializes decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes decoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    self.masked_multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_3 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.DecoderBlock.forward","title":"<code>forward(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Forward pass through decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder block.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the decoder block.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,  # M\u00e1scara para el encoder (padding)\n    tgt_mask: torch.Tensor | None = None,  # M\u00e1scara causal para el decoder\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through decoder block.\n\n    Args:\n        decoder_input: Input tensor to the decoder block.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Tensor after processing by the decoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    decoder_input = self.residual_layer_1(\n        decoder_input,\n        lambda x: self.masked_multi_head_attention_layer(\n            k=x, q=x, v=x, mask=tgt_mask\n        ),\n    )\n\n    # Aqu\u00ed tenemos que hacer cross-attention, usamos como K, V los encoder\n    # y Q del decoder\n    decoder_input = self.residual_layer_2(\n        decoder_input,\n        lambda x: self.multi_head_attention_layer(\n            k=encoder_output, q=x, v=encoder_output, mask=src_mask\n        ),\n    )\n\n    decoder_output = self.residual_layer_3(\n        decoder_input, lambda x: self.feed_forward_layer(x)\n    )\n\n    return decoder_output\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Encoder block with attention and feed-forward layers.</p> <p>Initializes encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes encoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Forward pass through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the encoder block.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the encoder block.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through encoder block.\n\n    Args:\n        input_tensor: Input tensor to the encoder block.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after processing by the encoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Feed-forward neural network layer.</p> <p>Initializes feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes feed-forward network.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.ReLU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor processed by feed-forward network.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through feed-forward network.\n\n    Args:\n        input_tensor: Tensor of input embeddings.\n\n    Returns:\n        Tensor processed by feed-forward network.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.InputEmbedding","title":"<code>InputEmbedding(d_model, vocab_size)</code>","text":"<p>Embeds input tokens into vectors of dimension d_model.</p> <p>Initializes input embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes input embedding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    # Utilizamos la capa Embedding de PyTorch que funciona como\n    # una tabal lookup that stores embeddings of a fixed dictionary and size.\n    # Osea que es un diccionario que tiene por cada token, hasta un total de\n    # vocab_size, un vector de tama\u00f1o d_model. En el paper: we use learned\n    # embeddings to convert the input tokens and output tokens to vectors\n    # of dimension dmodel\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.InputEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor of token indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embedded input scaled by sqrt(d_model).</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the embedding layer.\n\n    Args:\n        input_tensor: Input tensor of token indices.\n\n    Returns:\n        Tensor of embedded input scaled by sqrt(d_model).\n    \"\"\"\n\n    # Paper: In the embedding layers, we multiply those weights by sqrt(d_model)\n    # Input_tensor (B, ...) -&gt; (B, ..., d_model)\n    return self.embedding(input_tensor) * math.sqrt(self.d_model)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Applies layer normalization to input embeddings.</p> <p>Initializes layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initializes layer normalization.\n\n    Args:\n        features: Number of features in the input.\n        eps: Small constant for numerical stability.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass for layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for layer normalization.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Applies multi-head attention mechanism.</p> <p>Initializes multi-head attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes multi-head attention layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Computes scaled dot-product attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Optional dropout layer.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Computes scaled dot-product attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n        dropout: Optional dropout layer.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Forward pass through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after attention and concatenation.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after attention and concatenation.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Adds positional encoding to input embeddings.</p> <p>Initializes positional encoding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>sequence_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes positional encoding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        sequence_length: Maximum sequence length.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # # Ahora rellenamos la matriz de posiciones\n    # # La posici\u00f3n va hasta el m\u00e1ximo de la longitud de la secuencia\n    # for pos in range(self.sequence_length):\n    # \tfor i in range(0, d_model, 2):\n    # \t\t# Para las posiciones pares usamos el seno\n    # \t\tpe_matrix[pos, i] = torch.sin(pos / (10000 ** ((2 * i) / d_model)))\n    # \t\t# Para las posiciones impares usamos el coseno\n    # \t\tpe_matrix[pos, i + 1] = torch.cos(\n    # \t\t\tpos / (10000 ** ((2 * (i + 1)) / d_model))\n    # \t\t)\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass to add positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embeddings with added positional encoding.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass to add positional encoding.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Tensor of embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.ProjectionLayer","title":"<code>ProjectionLayer(d_model, vocab_size)</code>","text":"<p>Converts d_model dimensions back to vocab_size.</p> <p>Initializes projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes projection layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    super().__init__()\n\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    self.projection_layer = nn.Linear(in_features=d_model, out_features=vocab_size)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.ProjectionLayer.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the projection layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with projected dimensions.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through projection layer.\n\n    Args:\n        input_tensor: Input tensor to the projection layer.\n\n    Returns:\n        Tensor with projected dimensions.\n    \"\"\"\n\n    return self.projection_layer(input_tensor)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Applies residual connection around a sublayer.</p> <p>Initializes residual connection layer.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes residual connection layer.\n\n    Args:\n        features: Number of features in the input.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Forward pass using residual connection.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the residual layer.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer to apply within the residual connection.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using residual connection.\n\n    Args:\n        input_tensor: Input tensor to the residual layer.\n        sublayer: Sublayer to apply within the residual connection.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.Transformer","title":"<code>Transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, num_encoders, num_decoders, d_model, d_ff, h, dropout_rate)</code>","text":"<p>Transformer model with encoder and decoder blocks.</p> <p>Initializes transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>src_vocab_size</code> <code>int</code> <p>Size of source vocabulary.</p> required <code>tgt_vocab_size</code> <code>int</code> <p>Size of target vocabulary.</p> required <code>src_seq_len</code> <code>int</code> <p>Maximum source sequence length.</p> required <code>tgt_seq_len</code> <code>int</code> <p>Maximum target sequence length.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>num_decoders</code> <code>int</code> <p>Number of decoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def __init__(\n    self,\n    src_vocab_size: int,\n    tgt_vocab_size: int,\n    src_seq_len: int,\n    tgt_seq_len: int,\n    num_encoders: int,\n    num_decoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initializes transformer model.\n\n    Args:\n        src_vocab_size: Size of source vocabulary.\n        tgt_vocab_size: Size of target vocabulary.\n        src_seq_len: Maximum source sequence length.\n        tgt_seq_len: Maximum target sequence length.\n        num_encoders: Number of encoder blocks.\n        num_decoders: Number of decoder blocks.\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Par\u00e1metros\n    self.src_vocab_size = src_vocab_size\n    self.tgt_vocab_size = tgt_vocab_size\n    self.src_seq_len = src_seq_len\n    self.tgt_seq_len = tgt_seq_len\n    self.num_encoders = num_encoders\n    self.num_decoders = num_decoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Embeddings y Positional Encoding\n    self.src_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.src_vocab_size\n    )\n    self.tgt_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n    self.src_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.src_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n    self.tgt_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.tgt_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    # Capas del Decoder\n    self.decoder_layers = nn.ModuleList(\n        [\n            DecoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_decoders)\n        ]\n    )\n\n    # Capa de proyecci\u00f3n final\n    self.projection_layer = ProjectionLayer(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.Transformer.decode","title":"<code>decode(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Decodes target input tensor using decoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def decode(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Decodes target input tensor using decoder blocks.\n\n    Args:\n        decoder_input: Input tensor to the decoder.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Decoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.tgt_embedding(decoder_input)\n    x = self.tgt_positional_encoding(x)\n\n    # Pasar por todas las capas del decoder\n    for decoder_layer in self.decoder_layers:\n        x = decoder_layer(\n            decoder_input=x,\n            encoder_output=encoder_output,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n        )\n\n    return x\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.Transformer.encode","title":"<code>encode(encoder_input, src_mask=None)</code>","text":"<p>Encodes source input tensor using encoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_input</code> <code>Tensor</code> <p>Input tensor to the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded tensor.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def encode(\n    self, encoder_input: torch.Tensor, src_mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Encodes source input tensor using encoder blocks.\n\n    Args:\n        encoder_input: Input tensor to the encoder.\n        src_mask: Optional source mask tensor.\n\n    Returns:\n        Encoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.src_embedding(encoder_input)\n    x = self.src_positional_encoding(x)\n\n    # Pasar por todas las capas del encoder\n    for encoder_layer in self.encoder_layers:\n        x = encoder_layer(input_tensor=x, mask=src_mask)\n\n    return x\n</code></pre>"},{"location":"layers/nlp.html#src.layers.nlp.transformer.Transformer.forward","title":"<code>forward(src, tgt, src_mask=None, tgt_mask=None)</code>","text":"<p>Processes input and target sequences through the encoder and decoder, applying optional source and target masks.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input sequence tensor.</p> required <code>tgt</code> <code>Tensor</code> <p>Target sequence tensor.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional mask for the input sequence.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional mask for the target sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing the final output after projection.</p> Source code in <code>src/layers/nlp/transformer.py</code> <pre><code>def forward(\n    self,\n    src: torch.Tensor,\n    tgt: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Processes input and target sequences through the encoder\n    and decoder, applying optional source and target masks.\n\n    Args:\n        src: Input sequence tensor.\n        tgt: Target sequence tensor.\n        src_mask: Optional mask for the input sequence.\n        tgt_mask: Optional mask for the target sequence.\n\n    Returns:\n        Tensor containing the final output after projection.\n    \"\"\"\n\n    # Encoder\n    encoder_output = self.encode(src, src_mask)\n\n    # Decoder\n    decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n\n    # Projection\n    return self.projection_layer(decoder_output)\n</code></pre>"}]}