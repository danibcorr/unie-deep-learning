{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#deep-learning-course","title":"Deep Learning Course","text":"<p>This repository contains a comprehensive deep learning course with hands-on examples implemented in PyTorch. It provides a progressive introduction to core concepts, mathematical foundations, and practical applications of deep learning, while remaining accessible to non-specialist readers.</p>"},{"location":"index.html#course-overview","title":"Course Overview","text":"<p>The main topics covered in the course are:</p> <ul> <li> <p>Topic 1, Initial Concepts: Introduction to neural networks, differences between   traditional machine learning and deep learning, and main components of a deep learning   workflow: Datasets, models, loss functions, optimization algorithms, and evaluation   metrics.</p> </li> <li> <p>Topic 2, Mathematics: Mathematical foundations for deep learning, including linear   algebra, calculus, and basic probability and statistics, with emphasis on their   application to neural network training and tensor computation in PyTorch.</p> </li> <li> <p>Topic 3, Applications: End-to-end examples of deep learning applied to tasks such   as classification, regression, and recommendation, including model definition, choice   of loss functions and optimizers, and implementation of training and evaluation loops   in PyTorch.</p> </li> <li> <p>Topic 4, Computer Vision: Deep learning methods for image processing and computer   vision, including convolutional neural networks, image classification, object   detection, segmentation, and transfer learning using pre-trained models.</p> </li> <li> <p>Topic 5, Sequential Models: Models for sequential data, such as text and   time-ordered signals, covering recurrent architectures (RNNs, LSTMs, GRUs) and, where   appropriate, attention mechanisms and transformer-based models.</p> </li> <li> <p>Topic 6, Graph Models: Introduction to graph neural networks and related   architectures for node classification, link prediction, and graph classification, using   graph-structured data.</p> </li> </ul>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#prerequisites","title":"Prerequisites","text":"<p>The following tools are required to install and run the course materials:</p> <ul> <li>uv package manager: Tool used for fast and   reproducible dependency and environment management.</li> </ul>"},{"location":"index.html#installation","title":"Installation","text":"<p>To obtain the materials and configure the environment, execute the following commands in a terminal:</p> <pre><code># Clone the repository\ngit clone https://github.com/danibcorr/unie-deep-learning.git\ncd unie-deep-learning\n\n# Install dependencies and set up the environment\nmake setup\n</code></pre> <p>The <code>make setup</code> command automatically installs the required Python dependencies and prepares a suitable execution environment for the notebooks and scripts in the repository.</p>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>The full course documentation is available at: https://danibcorr.github.io/unie-deep-learning/</p> <p>To serve the documentation locally from the project root, run:</p> <pre><code>make doc\n</code></pre> <p>This command generates or updates the documentation and starts a local web server. The URL to access the documentation is displayed in the terminal output.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is distributed under the MIT License. The complete license text is available in the LICENSE file included in this repository.</p>"},{"location":"index.html#author","title":"Author","text":"<p>This course has been developed by Daniel Bazo Correa. Additional information and professional contact can be found through LinkedIn at @danibcorr or GitHub at @danibcorr.</p>"},{"location":"course/topic_01_initial_concepts/index.html","title":"Introduction","text":"<p>This initial block is conceived as a conceptual starting point for all the content developed throughout the course. Its purpose is to provide a solid foundation on the technical and structural elements that will be used recurrently in subsequent lessons, before delving into the mathematical foundations of deep learning and the detailed analysis of models and algorithms.</p> <p>First, the functioning of Python environments is addressed, focusing on the most common methods for their creation and on the modern tools available for dependency management. It analyzes how to configure reproducible and maintainable environments over time, as well as the importance of efficient package management to avoid dependency conflicts. This approach is especially relevant when thinking about the transition from an experimental project to a production environment, where stability, traceability, and professionalization of the workflow take on a central role.</p> <p>Next, the basic data structures of Python are reviewed, not as isolated concepts, but as transversal elements that appear constantly in the development of Machine Learning and Deep Learning applications. Although these structures may have been studied previously in other courses or with other instructors, they are revisited here with the aim of consolidating their understanding and analyzing their specific use within the Python language. It is emphasized that, while many of these structures are common to different programming languages, Python becomes the main focus as it is the predominant language in machine learning and deep learning libraries.</p> <p>The treatment of these data structures is accompanied by simple usage examples that illustrate their basic functioning. However, it is stressed that this block has an eminently formative and progressive character, in which learning is reinforced through direct experimentation. It is the student's responsibility to execute the examples, modify them, and explore variations, as this active process is key to consolidating the theoretical concepts presented.</p> <p>Once this foundation is established, the concept of tensor is introduced, understood as the fundamental data structure on which deep learning libraries operate. Tensors represent the way data is organized and processed internally, especially when working with hardware accelerators such as GPUs. This transition from basic data structures to tensors allows understanding how initial concepts evolve toward more abstract and efficient models of representation and computation.</p> <p>Finally, the block is completed with an introduction to the use of Python libraries, their installation, and their integration within properly managed environments. It explains how to avoid dependency conflicts and how to apply good design practices to create sustainable code. In this context, the creation of a custom library is also proposed, applying the concepts learned about data structures and object-oriented programming. This approach allows understanding how machine learning libraries are designed internally, based on class hierarchies, inheritance, and functionality reuse, and provides a deeper view of how the Python ecosystem works at a professional level.</p>"},{"location":"course/topic_01_initial_concepts/section_01_environment.html","title":"Setup Environment","text":"<p>There are several options for package management in Python. Regardless of which one you choose, my recommendation is to always opt for the simplest and most minimalist alternative. The fewer the number of dependencies and the lighter the project, the easier it will be to take it to production (for example, in a Docker image), share it with your team, or maintain it over time.</p> <p>Currently, the options I most recommend are Poetry and uv. Both tools streamline the creation and management of environments, allow you to maintain project configurations in an organized way through a pyproject.toml file and, above all, promote project reproducibility, which is essential both in development and deployment.</p>"},{"location":"course/topic_01_initial_concepts/section_01_environment.html#development-environment-configuration","title":"Development Environment Configuration","text":"<p>Setting up the development environment is the first step to ensure organized, reproducible, and technically sound work. In this context, a Python virtual environment is created using uv, ensuring that the Python version used matches exactly what is specified in the project's <code>pyproject.toml</code> file. This file acts as the single source of truth for project configuration, including the interpreter version and required dependencies.</p> <p>The virtual environment is created using the <code>uv venv</code> command, which initializes an isolated environment without requiring additional configurations. Once created, the environment is activated so that all subsequent operations, such as package installation or code execution, are restricted to that context. This isolation helps avoid conflicts between different projects and ensures that dependencies do not interfere with those installed globally on the system.</p> <p>The use of uv is especially relevant in this context. It is one of the most recent and efficient tools for managing environments and dependencies in Python, developed by Astral, the same team responsible for Ruff. Its main strength lies in its performance, as it is implemented in Rust, which allows it to install and resolve dependencies in milliseconds. Additionally, it adopts a configuration model inspired by Cargo, the package manager from the Rust ecosystem, using the <code>pyproject.toml</code> file as the central axis of the project.</p>"},{"location":"course/topic_01_initial_concepts/section_01_environment.html#conceptual-comparison-between-venv-and-uv","title":"Conceptual Comparison between venv and uv","text":"<p>In the Python ecosystem, there are multiple tools for creating and managing virtual environments, which can be confusing at first glance. To understand this diversity, it is useful to conceptually compare the traditional approach based on venv with the modern proposal introduced by uv, analyzing their differences from a practical and technical perspective.</p> <p>When creating an environment with venv, you use a tool included in Python's standard library whose sole purpose is to generate an isolated virtual environment. This environment contains a copy of the interpreter and basic libraries, but does not offer integrated mechanisms for advanced dependency management. In contrast, uv allows you to create another virtual environment in a similar way, but does so as part of a comprehensive solution that unifies environment creation, dependency resolution, and installation, all supported by the <code>pyproject.toml</code> file.</p> <p>The comparison between both tools can be summarized in the following conceptual table:</p> Criterion venv uv Speed Adequate, but dependent on external tools like pip to install dependencies Very high, with dependency resolution and installation in milliseconds Ease of use Simple for creating environments, but requires combining several tools Fluid and coherent, with a single workflow Generated files Virtual environment directory without project metadata Virtual environment and centralized management via <code>pyproject.toml</code> Dependency management Not integrated, depends on pip and files like <code>requirements.txt</code> Integrated and reproducible from <code>pyproject.toml</code> <p>This contrast highlights that the existence of multiple tools responds to the evolution of Python development needs. While venv solves the basic problem of isolation, uv simultaneously addresses speed, reproducibility, and workflow simplification.</p> <p>Dependency reproducibility is a key aspect in modern projects. To guarantee it, a <code>pyproject.toml</code> file is created in which project dependencies are explicitly defined and, when appropriate, constraints on the Python version. This file becomes the formal specification of the environment needed to run the project correctly.</p> <p>Once dependencies are defined, the existing virtual environment is completely removed. Next, the environment is recreated from scratch using uv, which reads the contents of <code>pyproject.toml</code>, resolves dependencies, and automatically installs them in the new environment. This process demonstrates practically that the project can be faithfully reproduced on another machine or system, as long as the same configuration file is available.</p> <p>In this way, the use of uv not only simplifies the technical management of the environment, but also reinforces fundamental principles of software development such as portability, reproducibility, and reliability of the execution environment.</p>"},{"location":"course/topic_01_initial_concepts/section_01_environment.html#environment-configuration-in-google-colab","title":"Environment Configuration in Google Colab","text":"<p>Google Colab provides a cloud-based execution environment that allows you to experiment and develop Python code without the need for local installations, which is especially useful for computationally intensive tasks and machine learning. The process begins by opening a Notebook in Google Colab, which offers a fully functional Python environment by default.</p> <p>Once the notebook is created, GPU usage is activated from the runtime environment configuration. This action enables access to hardware accelerators provided by Google's infrastructure, allowing computationally demanding operations to run significantly more efficiently than on a conventional CPU.</p> <p></p> <p>To verify that the GPU is correctly enabled and available, the <code>!nvidia-smi</code> command is executed. This command queries the status of the graphics card assigned to the environment and displays relevant information such as the GPU model, available memory, and current resource usage. Identifying the specific GPU model is essential, as performance can vary notably between different architectures.</p> <p>With the GPU properly configured, a comparison of execution times between CPU and GPU is performed using a large-scale matrix operation, for example, multiplication of dense matrices. This practical test allows you to directly observe how the same computational task presents substantial differences in execution time depending on the hardware used. On the CPU, the calculation is performed sequentially or with limited parallelism, while on the GPU, massive parallelism is leveraged to process thousands of operations simultaneously.</p> <p>The objective of this exercise is to understand the real impact of hardware on intensive computational tasks. Beyond theory, the empirical comparison between CPU and GPU demonstrates why graphics accelerators have become a key component in areas such as deep learning, numerical simulation, and large-scale data analysis. This understanding is essential for making informed decisions about the most appropriate infrastructure based on the type of problem to be solved.</p>"},{"location":"course/topic_01_initial_concepts/section_01_environment.html#installing-dependencies","title":"Installing Dependencies","text":"<p>Installing dependencies is a central process in any Python project, as it determines which external libraries will be available and under which versions the code will run. The main public package repository for Python is PyPI (Python Package Index), accessible at https://pypi.org/, where tens of thousands of libraries are published and maintained.</p> <p>PyPI acts as a global catalog containing both the source code and metadata associated with each package, including available versions, required dependencies, compatibility with different Python versions, and basic documentation. This repository allows dependency management tools to automatically locate, download, and install the libraries needed for a given project.</p> <p>In a modern workflow, dependency installation is not conceived as an isolated action, but as part of a declarative project configuration. Instead of installing packages manually and ad hoc, dependencies are explicitly defined in a configuration file, usually within the <code>pyproject.toml</code> file. This file indicates which packages are required and under what conditions, and management tools, such as <code>uv</code>, are responsible for resolving the appropriate versions by querying PyPI.</p> <p>This approach provides several fundamental advantages. First, it guarantees consistency between different development environments, since all are built from the same specification. Second, it improves reproducibility, allowing the complete environment to be recreated on another machine without ambiguity. Finally, it reduces issues arising from version incompatibilities by centralizing and automating dependency resolution.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html","title":"Data Structures","text":"<p>In Python, data structures constitute an essential element for storing, organizing, and manipulating information efficiently. In the context of Deep Learning and scientific computing, a solid understanding of these structures is key, as they serve as the conceptual foundation upon which more complex abstractions are built, such as tensors, datasets, and batches. Below are the most common data structures in Python, explained progressively and with a practical approach.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#lists","title":"Lists","text":"<p>Lists are data structures that allow storing ordered and mutable sequences of elements. Unlike other languages, lists in Python can contain elements of different types and their size is dynamic, meaning they can grow or shrink during program execution. Indexing starts at zero, and negative indices allow accessing elements starting from the end of the list.</p> <p>A list is defined using brackets and separating elements with commas:</p> <pre><code>friends_list = [\"Jorge\", \"Fran\", \"Ricardo\"]\n</code></pre> <p>It is also possible to initialize an empty list, which is useful when you want to build it progressively:</p> <pre><code>lista = []\n</code></pre> <p>Access to elements is done through indices, and Python offers flexible syntax for selecting individual elements or complete subsets:</p> <pre><code>friends_list = [\"Jorge\", \"Fran\", \"Ricardo\"]\n\nprint(f\"The first friend is {friends_list[0]}\")\nprint(f\"My hometown friend is {friends_list[-1]}\")\nprint(friends_list[0:2])\nprint(friends_list)\n</code></pre> <p>This indexing and slicing capability makes lists especially suitable for representing data sequences, such as sets of examples or intermediate calculation results.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#main-list-methods","title":"Main List Methods","text":"<p>Lists have a wide set of methods that allow modifying their content and structure. Among the most relevant are the following:</p> Function Definition <code>list[index] = x</code> Changes the element at the specified index to <code>x</code>. <code>list.extend(x)</code> Adds the elements of <code>x</code> to the end of the list. <code>list.append(x)</code> Adds a single element <code>x</code> to the end of the list. <code>list.insert(index, x)</code> Inserts <code>x</code> at the indicated position. <code>list.remove(x)</code> Removes the first occurrence of <code>x</code>. <code>list.clear()</code> Completely empties the list. <code>list.pop()</code> Removes and returns the last element or the one indicated by index. <code>list.index(x)</code> Returns the index of the first occurrence of <code>x</code>. <code>list.count(x)</code> Counts how many times <code>x</code> appears. <code>list.sort()</code> Sorts the list in ascending order. <code>list.reverse()</code> Reverses the order of elements. <code>list2 = list1.copy()</code> Creates a shallow copy of the list. <code>max(list)</code> Returns the maximum value. <code>min(list)</code> Returns the minimum value. <code>del list[x]</code> Removes the element at index <code>x</code>."},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#for-loops-and-list-comprehension","title":"<code>for</code> Loops and List Comprehension","text":"<p>Iteration over lists using <code>for</code> loops is a common operation in Python. Additionally, the language offers list comprehension, a concise and expressive syntax for generating new lists from existing sequences.</p> <pre><code>my_list = [letter for letter in \"Hello\"]\nprint(my_list)\n\nmy_list = [number ** 2 for number in range(0, 20, 2)]\nprint(my_list)\n\ncelsius = [0, 10, 20, 34.5]\nfahrenheit = [((9/5) * temp + 32) for temp in celsius]\nprint(fahrenheit)\n\nmy_list = [number ** 2 for number in range(0, 15, 2) if number % 2 == 0]\nprint(my_list)\n</code></pre> <p>This mechanism is especially useful in data science and machine learning, where it is common to apply repetitive transformations over collections of values.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#nested-lists-and-matrix-representation","title":"Nested Lists and Matrix Representation","text":"<p>Lists can contain other lists, which allows representing multidimensional structures like matrices or data tables:</p> <pre><code>number_grid = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n    [0]\n]\n\nprint(number_grid[2][2])\n</code></pre> <p>Although in real projects matrices are usually managed with libraries like NumPy or PyTorch, this model helps understand the underlying logic of multidimensional data.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#tuples","title":"Tuples","text":"<p>Tuples are ordered and immutable sequences. Once created, their elements cannot be modified, which makes them a suitable option when you want to guarantee data integrity. Additionally, they are usually more memory-efficient and slightly faster than lists.</p> <pre><code>coordinates = (4, 5)\n\nprint(f\"Complete coordinate {coordinates}\")\nprint(f\"First coordinate {coordinates[0]} and second coordinate {coordinates[1]}\")\n</code></pre> <p>Tuples are frequently used to group related values, for example, coordinates, parameters, or multiple results from a function.</p>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#tuple-methods","title":"Tuple Methods","text":"<p>Although they are immutable, tuples offer some basic methods:</p> Function Description <code>tuple.count(x)</code> Returns the number of times <code>x</code> appears. <code>tuple.index(x)</code> Returns the index of the first occurrence of <code>x</code>."},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#sets","title":"Sets","text":"<p>Sets are unordered collections of unique elements. They do not allow duplicates, which makes them especially useful for removing repetitions or for performing set theory operations like unions and intersections.</p> <pre><code>my_set = set()\nmy_set.add(1)\nmy_set.add(1)\n\nmy_new_set = {'a', 'b', 'c'}\n</code></pre>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#set-methods","title":"Set Methods","text":"<p>Sets provide efficient operations for comparing collections:</p> Function Definition <code>s.add(x)</code> Adds an element to the set. <code>s.clear()</code> Removes all elements. <code>sc = s.copy()</code> Creates a copy of the set. <code>s1.difference(s2)</code> Returns the elements of <code>s1</code> that are not in <code>s2</code>. <code>s1.difference_update(s2)</code> Removes from <code>s1</code> the elements present in <code>s2</code>. <code>s.discard(elem)</code> Removes <code>elem</code> without error if it doesn't exist. <code>s1.intersection(s2)</code> Returns the common elements. <code>s1.issubset(s2)</code> Checks if <code>s1</code> is a subset of <code>s2</code>. <code>s1.union(s2)</code> Returns the union of both sets."},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#dictionaries","title":"Dictionaries","text":"<p>Dictionaries store information in key-value pairs, where each key is unique. They are mutable and allow very efficient data access, making them one of the most used structures in Python.</p> <pre><code>month_conversion = {\n    \"Jan\": \"January\",\n    \"Feb\": \"February\",\n    \"Mar\": \"March\"\n}\n\nprint(month_conversion[\"Jan\"])\nprint(month_conversion.get(\"Jan\"))\n\nkey = \"Daniel\"\nprint(month_conversion.get(key, f\"The key {key} is not in the dictionary\"))\n</code></pre>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#main-dictionary-methods","title":"Main Dictionary Methods","text":"Function Definition <code>dictionary.items()</code> Returns the key-value pairs. <code>dictionary.keys()</code> Returns the keys. <code>dictionary.values()</code> Returns the values."},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#practical-cases-and-compound-structures","title":"Practical Cases and Compound Structures","text":"<p>Dictionaries can be nested or combined with lists to represent complex structures:</p> <pre><code>dictionary = {\"k3\": {'insideKey': 100}}\nprint(dictionary[\"k3\"]['insideKey'])\n</code></pre> <p>Iteration over dictionaries allows traversing keys, values, or both simultaneously:</p> <pre><code>d = {'k1': 1, 'k2': 2}\n\nfor key in d.keys():\n    print(key)\n\nfor value in d.values():\n    print(value)\n\nfor item in d.items():\n    print(item)\n</code></pre> <p>A more realistic example combines lists and dictionaries to model entities with variable attributes:</p> <pre><code>clients = [\n    {\"name\": \"Daniel\", \"animals\": [\"Pakito\", \"Pakon\", \"Pakonazo\"]},\n    {\"name\": \"Clemencia\", \"animals\": [\"Rodolfo\"]},\n    {\"name\": \"Carolina\"}\n]\n\nfor client in clients:\n    print(f\"{client['name']} has: {client.get('animals', 'No animals')}\")\n</code></pre>"},{"location":"course/topic_01_initial_concepts/section_02_data_structures.html#tensors","title":"Tensors","text":"<p>In the field of Deep Learning, tensors constitute the fundamental data structure upon which machine learning models and algorithms are built. Conceptually, a tensor can be understood as a generalization of the structures seen previously: a scalar is a zero-order tensor, a vector is a one-dimensional tensor, a matrix is a two-dimensional tensor and, more generally, a higher-order tensor represents data in multiple dimensions. This abstraction allows describing complex information in a uniform way, such as images, temporal sequences, or data batches.</p> <p>From a practical point of view, tensors are not usually implemented using nested Python lists, as these are not optimized for intensive numerical computing. Instead, specialized libraries such as NumPy, PyTorch, or TensorFlow are used, which provide efficient implementations based on contiguous memory, vectorized operations and, in many cases, GPU acceleration.</p> <p>A tensor is mainly characterized by its shape, which describes the number of dimensions and the size of each one, and by its data type (dtype), which indicates how values are stored in memory. These properties are essential to guarantee compatibility between operations and to optimize computational performance.</p> <p>In PyTorch, for example, tensor creation is done explicitly from existing data or through initialization functions:</p> <pre><code>import torch\n\n# Scalar tensor\na = torch.tensor(3.14)\n\n# One-dimensional tensor (vector)\nv = torch.tensor([1, 2, 3])\n\n# Two-dimensional tensor (matrix)\nm = torch.tensor([[1, 2], [3, 4]])\n</code></pre> <p>Each of these tensors has a specific shape that can be queried directly, which facilitates dimension verification before applying mathematical operations. This explicit control of dimensions is crucial in Deep Learning, where shape mismatches are often a common source of errors.</p> <p>Operations on tensors are defined in a vectorized manner, which allows expressing complex calculations concisely and efficiently. For example, addition or multiplication between tensors is performed element-wise whenever their shapes are compatible, following well-defined broadcasting rules. This computational model avoids explicit loops and leverages internal system optimizations.</p> <p>Another fundamental aspect of tensors in Deep Learning libraries is their integration with automatic differentiation mechanisms. In PyTorch, tensors can be marked for gradient tracking, which allows automatically calculating derivatives during the model training process. This capability makes tensors not just data containers, but active elements within the computational graph.</p> <p>Finally, tensors facilitate code execution on both CPU and GPU without modifying the program logic. By moving a tensor to a specific device, associated operations are automatically executed on that hardware, which reinforces the abstraction and allows focusing on model design rather than low-level details. Overall, tensors represent the natural evolution of basic data structures toward an optimized and expressive computational model, indispensable in the modern development of Deep Learning systems.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html","title":"Libraries and Software Design","text":"<p>The development of libraries in Python is based on design principles that seek to maximize clarity, reusability, and long-term code maintenance. In this context, modularization and object-oriented programming (OOP) constitute fundamental pillars, especially in Deep Learning libraries such as TensorFlow or PyTorch, whose design reflects engineering decisions designed to scale to complex and collaborative projects. The ultimate goal is to adopt the mindset of a professional Machine Learning engineer, capable of building reusable, extensible code that can be easily integrated into other projects.</p> <p>Modularization in Python consists of dividing code into well-defined logical units, avoiding monolithic files and favoring separation of responsibilities. This approach leads to cleaner and more maintainable code, in which each module fulfills a specific function within the system. When this modularization is combined with object-oriented design, the result is a coherent, structured library aligned with industry practices.</p> <p>A key step in this process is converting the project into an installable package, so that it can be distributed and installed in other projects using modern tools like uv. This packaging capability reinforces code reusability and allows validation that the design is truly independent of the environment in which it is developed.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#object-oriented-programming","title":"Object-Oriented Programming","text":"<p>Object-Oriented Programming is a paradigm that organizes software around objects, rather than focusing solely on functions and control flows. Each object encapsulates data, represented by attributes, and behavior, defined through methods that operate on that data. This model facilitates modularity, reusability, and code scalability\u2014essential qualities in complex systems like Deep Learning libraries.</p> <p>In Python, OOP offers a remarkable balance between expressiveness and simplicity. Through the use of classes, inheritance, and polymorphism, it is possible to build flexible structures that adapt to new needs without introducing drastic changes to existing code. This approach favors maintainability and reduces the likelihood of errors when extending or modifying functionalities.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#classes-and-objects","title":"Classes and Objects","text":"<p>A class acts as a mold or template from which objects are created, which are concrete instances of that class. Each object has attributes that describe its state and methods that define its behavior. In Python, classes are defined using the <code>class</code> keyword, and the special method <code>__init__</code> is used as a constructor to initialize the object's attributes at the time of creation.</p> <pre><code>class ClassName:\n\n    def __init__(self, parameter1, parameter2):\n        self.parameter1 = parameter1\n        self.parameter2 = parameter2\n\n    def some_method(self):\n        print(\"This is a method inside the class\")\n</code></pre> <p>In this example, <code>self</code> refers to the concrete instance of the object and allows access to its attributes and methods. The explicit use of <code>self</code> is an essential feature of Python's object model.</p> <p>A practical case illustrates this concept more concretely:</p> <pre><code>class Car:\n\n    def __init__(self, brand, model, upgraded, car_access):\n        self.brand = brand\n        self.model = model\n        self.upgraded = upgraded\n        self.car_access = car_access\n\nmy_car = Car(\"Toyota\", \"Corolla\", True, [\"Juan\", \"Maria\"])\nprint(f\"My car is a {my_car.brand} {my_car.model}\")\n</code></pre> <p>Here the <code>Car</code> class is defined with several attributes that describe the object's state. The <code>my_car</code> instance represents a specific car, with its own values for each attribute, which exemplifies how a class can generate multiple independent objects.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#methods-and-attributes","title":"Methods and Attributes","text":"<p>Attributes represent the characteristics of an object, while methods describe the actions it can perform. In Python, instance attributes, which are specific to each object, are distinguished from class attributes, which are shared by all instances of the same class.</p> <pre><code>class Dog:\n\n    species = \"mammal\"\n\n    def __init__(self, breed, name, age):\n        self.breed = breed\n        self.name = name\n        self.age = age\n\n    def sound(self):\n        return \"Woof!\"\n\n    def information(self):\n        print(\n            f\"Name: {self.name}, \"\n            f\"Breed: {self.breed}, \"\n            f\"Age: {self.age}, \"\n            f\"Species: {self.species}\"\n        )\n\nif __name__ == \"__main__\":\n    my_dog = Dog(\"Labrador\", \"Fido\", 3)\n    my_dog.information()\n</code></pre> <p>In this example, <code>species</code> is a class attribute common to all dogs, while <code>breed</code>, <code>name</code>, and <code>age</code> are instance attributes. This distinction is especially useful for modeling shared concepts versus individual characteristics.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#inheritance-and-polymorphism","title":"Inheritance and Polymorphism","text":"<p>Inheritance allows creating new classes from existing ones, promoting code reusability and progressive specialization. A subclass inherits the attributes and methods of its base class but can extend or redefine them as needed.</p> <pre><code>class Animal:\n\n    def __init__(self, name):\n        self.name = name\n\n    def who_am_i(self):\n        print(\"I am an animal\")\n\n    def eat(self):\n        print(\"I am eating\")\n\nclass Dog(Animal):\n\n    def who_am_i(self):\n        print(f\"I am a dog named {self.name}\")\n\nmy_dog = Dog(\"Fido\")\nmy_dog.who_am_i()\nmy_dog.eat()\n</code></pre> <p>In this case, <code>Dog</code> inherits from <code>Animal</code>, reusing the <code>eat</code> method and redefining the <code>who_am_i</code> method. This mechanism reduces code duplication and clarifies hierarchical relationships between concepts.</p> <p>Polymorphism complements inheritance by allowing different objects to respond to the same method in different ways. This translates into common interfaces with specific behaviors depending on the concrete class of the object.</p> <pre><code>class Dog:\n\n    def __init__(self, name):\n        self.name = name\n\n    def sound(self):\n        print(f\"The dog {self.name} barks\")\n\nclass Cat:\n\n    def __init__(self, name):\n        self.name = name\n\n    def sound(self):\n        print(f\"The cat {self.name} meows\")\n\nmy_dog = Dog(\"Fido\")\nmy_cat = Cat(\"Meow\")\n\nmy_dog.sound()\nmy_cat.sound()\n</code></pre> <p>Both classes implement the <code>sound</code> method, but each does so differently, allowing objects to be treated uniformly without losing specificity.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#abstract-classes","title":"Abstract Classes","text":"<p>Abstract classes define a common interface that subclasses must implement, without necessarily providing a complete implementation. Although Python does not enforce strict abstraction by default, it is possible to simulate it through methods that raise exceptions when not implemented.</p> <pre><code>class Animal:\n\n    def __init__(self, name):\n        self.name = name\n\n    def sound(self):\n        raise NotImplementedError(\"Subclass must implement this method\")\n\nclass Dog(Animal):\n\n    def sound(self):\n        return f\"{self.name} makes woof!\"\n\nmy_dog = Dog(\"Fido\")\nprint(my_dog.sound())\n</code></pre> <p>This pattern forces subclasses to define certain behaviors, ensuring consistency in the class hierarchy design.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#modules-and-packages","title":"Modules and Packages","text":"<p>Organizing code into modules and packages is an essential step for building reusable libraries. A module is simply a <code>.py</code> file that contains definitions of functions, classes, or variables. A package is a directory that groups several related modules and includes an <code>__init__.py</code> file, necessary for Python to recognize it as such.</p>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#importing-modules-and-using-libraries-in-python","title":"Importing Modules and Using Libraries in Python","text":"<p>Python allows importing custom or external modules to reuse already implemented functionality. Traditionally, the installation of external libraries has been done using pip, which downloads packages from PyPI, Python's official repository. For example, the installation and use of the <code>colorama</code> library to print colored text is done as follows:</p> <pre><code>pip install colorama\n</code></pre> <pre><code>from colorama import init, Fore\n\ninit()\nprint(Fore.RED + \"Test text\")\n</code></pre>"},{"location":"course/topic_01_initial_concepts/section_03_python_libraries_oop.html#package-structure-and-code-reusability","title":"Package Structure and Code Reusability","text":"<p>A clear modular structure facilitates code readability and its reuse in different projects. An example of organizing a project with packages and subpackages is as follows:</p> <pre><code># main.py\nfrom package78 import some_main_script as p\nfrom package78.Subpackages import mysubscript as s\n\np.main_report()\ns.sub_report()\n</code></pre> <pre><code># package78/some_main_script.py\ndef main_report():\n    print(\"Hello, I am a function inside my main script.\")\n</code></pre> <pre><code># package78/Subpackages/mysubscript.py\ndef sub_report():\n    print(\"Hello, I am a function inside my subscript.\")\n</code></pre> <p>This structure reflects how Python locates and organizes code, and constitutes the basis for converting a project into an installable package. By packaging the project and distributing it using modern tools like uv, a professional workflow is consolidated that prioritizes reproducibility, reusability, and software design quality.</p>"},{"location":"course/topic_02_mathematics/index.html","title":"Introduction","text":"<p>In this second topic of the course, we delve deeper into the practice of deep learning, moving from a mainly theoretical perspective to one oriented toward implementation. The objective is for students to acquire the ability to build, understand, and experiment with machine learning models using specialized libraries, as well as to establish the mathematical foundations that support them.</p> <p>Throughout the topic, different sections and content are introduced, some of which may not be covered in the main course presentation. It is recommended to review all available sections and materials, including those marked as optional or additional. When there are doubts about the status or update of certain content, it is advisable to consult the instructors, who can provide guidance on its validity and relevance. Exploring these complementary materials allows discovering new methods and approaches, and constitutes a good opportunity to expand knowledge beyond the mandatory minimums.</p> <p>In this block, the fundamental concepts of machine learning closely related to mathematics and their implementation in code are worked on with special intensity. We start from the data structures previously seen theoretically and move to their practical application through Jupyter notebooks, using deep learning libraries, with special emphasis on PyTorch. It shows how to define and manipulate tensors and matrices, as well as how to perform basic and advanced operations between these tensors, understanding their mathematical and computational interpretation.</p> <p>Additionally, essential mathematical concepts for machine learning are introduced and consolidated. Among them, gradients and automatic differentiation are studied, a central feature of PyTorch and other deep learning libraries, which allows calculating derivatives efficiently and accurately during model training. Classic models such as linear regression and logistic regression are also addressed, analyzing their mathematical formulation, their interpretation, and the role they play as the basis and foundation of many modern machine learning algorithms.</p> <p>The topic also includes the study of other mathematical tools useful in practice, such as cosine similarity, the Hadamard product, and other related matrix operations. These operations are especially relevant in various machine learning and deep learning algorithms and architectures, for example in recommendation models, natural language processing, or deep neural networks, where the relationships and combinations between vectors and matrices constitute the core of computation.</p> <p>From these foundations, the gradient descent method and its variants are presented, explaining how it is used to adjust a model's parameters to the available data. Subsequently, it shows how to combine the basic elements involved in learning a single neuron or perceptron to build complete layers and, from these, multi-layer neural networks (multilayer perceptrons). This step is key to understanding the transition from simple linear models to what is now known as deep learning.</p> <p>Finally, regularization techniques aimed at avoiding overfitting are introduced, that is, the excessive adaptation of model parameters to training data. Different types of optimizers designed to accelerate and stabilize the convergence process during training are studied, as well as the most common metrics for evaluating model performance. In this way, the topic offers an integrated vision that connects mathematical foundations, data structures, training strategies, and evaluation, providing a solid framework for the development and understanding of deep learning systems.</p>"},{"location":"course/topic_02_mathematics/section_01_data_structures.html","title":"Data Structures and Tensors in PyTorch","text":"<p>This section introduces the fundamental data structures of PyTorch, with particular emphasis on the tensor, which constitutes the core abstraction of the framework. The objective is to provide a clear and rigorous understanding of what a tensor is, how it is constructed, which types are available, and why it plays a central role throughout the entire deep learning workflow, from model design and experimentation to training and inference.</p> <p>In PyTorch, tensors represent the primary means of storing and manipulating data. Conceptually, a tensor can be understood as a generalization of familiar mathematical objects such as scalars, vectors, and matrices to an arbitrary number of dimensions. Each tensor contains numerical values arranged according to a specific shape, which defines both its dimensionality and the size along each dimension. Depending on computational requirements, tensors may reside either in main memory on the CPU or in device memory on a GPU, enabling efficient execution of large-scale numerical operations.</p> <p>From a practical perspective, PyTorch tensors are closely analogous to NumPy\u2019s <code>ndarray</code>, sharing similar semantics and many common operations. However, tensors are specifically designed to support high-performance computation and seamless acceleration through specialized hardware, particularly graphics processing units. This capability makes tensors especially suitable for the intensive linear algebra operations that underpin modern deep learning models.</p> <p>Within this context, the section explains how tensors can be created from predefined Python lists or matrix-like structures, and how their data types can be explicitly specified. This includes choosing between integer and floating-point representations, as well as selecting different levels of numerical precision. Such choices have direct implications for memory consumption, computational efficiency, and numerical stability.</p> <p>In addition, the fundamental properties of tensors are examined in detail. These include the number of dimensions, accessible through the <code>ndim</code> attribute, the tensor\u2019s shape, which describes its dimensional structure, and the device on which the tensor is stored, indicated by the <code>device</code> attribute. Understanding how to inspect and modify these properties is essential, as many tensor operations require compatible shapes, data types, and devices in order to execute correctly and efficiently. Collectively, these concepts form the foundation for effective and reliable use of PyTorch in deep learning applications.</p> <p>In the context of machine learning, the use of GPUs allows significantly accelerating tensor processing and the execution of large-scale neural models. PyTorch provides utilities to verify if the system has a compatible GPU and to obtain information about available devices. The following code snippet illustrates how to perform this check:</p> In\u00a0[1]: Copied! <pre># 3pps\nimport torch\n\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\nif torch.cuda.is_available():\n    print(\"GPU available\")\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\nelse:\n    print(\"No GPU detected, CPU will be used\")\n</pre> # 3pps import torch   print(f\"PyTorch version: {torch.__version__}\")  if torch.cuda.is_available():     print(\"GPU available\")     print(f\"GPU name: {torch.cuda.get_device_name(0)}\")     print(f\"Number of GPUs: {torch.cuda.device_count()}\") else:     print(\"No GPU detected, CPU will be used\") <pre>PyTorch version: 2.9.1+cu128\nNo GPU detected, CPU will be used\n</pre> <p>This type of check is especially useful at the beginning of a Jupyter notebook or training script, as it allows dynamically adapting the code to available hardware, moving tensors and models to the appropriate device through operations like <code>tensor.to(\"cuda\")</code> or <code>model.to(\"cuda\")</code>.</p> <p>To illustrate the computational advantages of using GPUs, consider the following example that compares the execution time of matrix multiplication operations on both devices:</p> In\u00a0[2]: Copied! <pre># Standard libraries\nimport time\n\n\n# Define matrix size\nsize = (5000, 5000)\n\n# Create tensors on CPU and GPU\ncpu_tensor = torch.rand(size)\nif torch.cuda.is_available():\n    gpu_tensor = torch.rand(size, device=\"cuda\")\n\n    # Measure CPU time\n    start = time.time()\n    cpu_result = cpu_tensor @ cpu_tensor\n    cpu_time = time.time() - start\n    print(f\"CPU time: {cpu_time:.4f} seconds\")\n\n    # Measure GPU time\n    start = time.time()\n    gpu_result = gpu_tensor @ gpu_tensor\n    torch.cuda.synchronize()  # Wait for GPU operations to complete\n    gpu_time = time.time() - start\n    print(f\"GPU time: {gpu_time:.4f} seconds\")\n    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n</pre> # Standard libraries import time   # Define matrix size size = (5000, 5000)  # Create tensors on CPU and GPU cpu_tensor = torch.rand(size) if torch.cuda.is_available():     gpu_tensor = torch.rand(size, device=\"cuda\")      # Measure CPU time     start = time.time()     cpu_result = cpu_tensor @ cpu_tensor     cpu_time = time.time() - start     print(f\"CPU time: {cpu_time:.4f} seconds\")      # Measure GPU time     start = time.time()     gpu_result = gpu_tensor @ gpu_tensor     torch.cuda.synchronize()  # Wait for GPU operations to complete     gpu_time = time.time() - start     print(f\"GPU time: {gpu_time:.4f} seconds\")     print(f\"Speedup: {cpu_time/gpu_time:.2f}x\") <p>The <code>torch.cuda.synchronize()</code> call is essential to ensure accurate timing measurements, as GPU operations are asynchronous by default.</p> <p></p> <p>A tensor is the fundamental data structure in PyTorch. From an intuitive point of view, a scalar is a 0-dimensional tensor, a vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, and from there on, we speak of higher-order tensors (3D, 4D, etc.). This generalization allows representing very diverse data, such as images, tokenized text sequences, or multivariate time series, in a unified way.</p> <p>Tensors allow storing data efficiently, both in CPU and GPU, and support a wide variety of mathematical operations: additions, products, reductions, linear algebra operations, and many others. Most deep learning algorithms are implemented as compositions of operations on tensors.</p> <p>PyTorch supports various data types optimized for different use cases. The following table summarizes the most common types:</p> dtype Description Typical Use Case Memory per Element <code>torch.float32</code> (or <code>torch.float</code>) 32-bit floating point Default type, general purpose training 4 bytes <code>torch.float64</code> (or <code>torch.double</code>) 64-bit floating point High precision scientific computing 8 bytes <code>torch.float16</code> (or <code>torch.half</code>) 16-bit floating point Mixed precision training, inference acceleration 2 bytes <code>torch.bfloat16</code> Brain floating point (16-bit) Modern TPU/GPU training, better range than float16 2 bytes <code>torch.int64</code> (or <code>torch.long</code>) 64-bit integer Indices, labels, sizes 8 bytes <code>torch.int32</code> (or <code>torch.int</code>) 32-bit integer Integer computations 4 bytes <code>torch.int16</code> (or <code>torch.short</code>) 16-bit integer Memory-constrained integer storage 2 bytes <code>torch.int8</code> 8-bit integer Quantized models, extreme memory savings 1 byte <code>torch.uint8</code> 8-bit unsigned integer Image data (0-255 range) 1 byte <code>torch.bool</code> Boolean Masks, logical conditions 1 byte <p>The choice of data type significantly impacts memory consumption, computational speed, and numerical stability. For instance, using <code>float16</code> can reduce memory usage by 50% compared to <code>float32</code>, enabling training of larger models, but may require careful handling of numerical underflow/overflow.</p> <p>PyTorch facilitates the creation of tensors of different dimensions. The following code snippet illustrates how to construct a scalar, a vector, a matrix, and a higher-order tensor:</p> In\u00a0[3]: Copied! <pre># Scalar tensor\nscalar = torch.tensor(7)\nscalar\n</pre> # Scalar tensor scalar = torch.tensor(7) scalar Out[3]: <pre>tensor(7)</pre> <p>A scalar has no additional dimensions, so its number of dimensions is 0:</p> In\u00a0[4]: Copied! <pre>scalar.ndim\n</pre> scalar.ndim Out[4]: <pre>0</pre> <p>To obtain the Python numerical value associated with the scalar, the <code>.item()</code> method is used:</p> In\u00a0[5]: Copied! <pre>scalar.item()\n</pre> scalar.item() Out[5]: <pre>7</pre> <p>From there, vectors (1-dimensional tensors) can be defined by providing a list of values:</p> In\u00a0[6]: Copied! <pre># Creating a vector\nvector = torch.tensor([7, 7])\nprint(vector)\nprint(vector.ndim)\nprint(vector.shape)\n</pre> # Creating a vector vector = torch.tensor([7, 7]) print(vector) print(vector.ndim) print(vector.shape) <pre>tensor([7, 7])\n1\ntorch.Size([2])\n</pre> <p>In this case, <code>vector.ndim</code> returns <code>1</code>, and <code>vector.shape</code> indicates the vector's length. Similarly, a matrix is represented as a list of lists, generating a 2-dimensional tensor:</p> In\u00a0[7]: Copied! <pre># Creating a matrix\nmatrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(matrix)\nprint(matrix.ndim)\nprint(matrix.shape)\n</pre> # Creating a matrix matrix = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(matrix) print(matrix.ndim) print(matrix.shape) <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])\n2\ntorch.Size([2, 3])\n</pre> <p>The above matrix has two rows and three columns, so its shape is <code>(2, 3)</code>.</p> <p>Higher-order tensors are constructed by nesting additional lists. For example, the following tensor has three dimensions, organized hierarchically:</p> In\u00a0[8]: Copied! <pre># Creating a three-dimensional tensor\ntensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [6, 4, 3]]])\nprint(tensor)\nprint(tensor.ndim)\nprint(tensor.shape)\n</pre> # Creating a three-dimensional tensor tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [6, 4, 3]]]) print(tensor) print(tensor.ndim) print(tensor.shape) <pre>tensor([[[1, 2, 3],\n         [4, 5, 6]],\n\n        [[7, 8, 9],\n         [6, 4, 3]]])\n3\ntorch.Size([2, 2, 3])\n</pre> <p>The <code>shape</code> attribute describes the size of each dimension. Understanding this structure is key to designing and interpreting neural network architectures, as the inputs and outputs of each layer are represented as tensors with specific shapes.</p> <p>In practice, it is very common to create test tensors or initialize model parameters using random values. PyTorch allows creating tensors with values generated randomly from different distributions. For example:</p> In\u00a0[9]: Copied! <pre># Random tensors\nrandom_tensor = torch.rand((2, 3, 4))\nprint(random_tensor)\nrandom_tensor.ndim, random_tensor.shape\n</pre> # Random tensors random_tensor = torch.rand((2, 3, 4)) print(random_tensor) random_tensor.ndim, random_tensor.shape <pre>tensor([[[0.4981, 0.3473, 0.0921, 0.2960],\n         [0.7056, 0.7720, 0.8547, 0.3498],\n         [0.2466, 0.4664, 0.2039, 0.7090]],\n\n        [[0.9969, 0.2773, 0.9765, 0.7975],\n         [0.3633, 0.1168, 0.2942, 0.6372],\n         [0.4122, 0.8123, 0.4176, 0.6414]]])\n</pre> Out[9]: <pre>(3, torch.Size([2, 3, 4]))</pre> <p>Here a tensor with shape <code>(2, 3, 4)</code> is created whose elements are uniformly distributed in the interval <code>[0, 1)</code>. This type of tensor is useful for:</p> <ol> <li>Verifying that input and output dimensions of a model are coherent between layers.</li> <li>Checking that internal operations are performed without errors before using real data.</li> <li>Exploring model behavior in unit tests or quick experiments.</li> </ol> <p>In addition to random tensors, it is common to use tensors initialized with zeros or ones, for example, to define masks, templates, or initial values:</p> In\u00a0[10]: Copied! <pre>zero_tensors = torch.zeros((3, 4))\nzero_tensors\n</pre> zero_tensors = torch.zeros((3, 4)) zero_tensors Out[10]: <pre>tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])</pre> In\u00a0[11]: Copied! <pre>ones_tensors = torch.ones((3, 4))\nones_tensors\n</pre> ones_tensors = torch.ones((3, 4)) ones_tensors Out[11]: <pre>tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])</pre> <p>You can also create tensors containing sequences of equally spaced values using <code>torch.arange</code>:</p> In\u00a0[12]: Copied! <pre>range_tensor = torch.arange(start=0, end=100, step=2)\nprint(range_tensor)\nprint(f\"Shape of 'range_tensor': {range_tensor.shape}\")\n</pre> range_tensor = torch.arange(start=0, end=100, step=2) print(range_tensor) print(f\"Shape of 'range_tensor': {range_tensor.shape}\") <pre>tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n        72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98])\nShape of 'range_tensor': torch.Size([50])\n</pre> <p>This tensor contains values from 0 to 98 with a step of 2. From it, other tensors that inherit its shape can be constructed:</p> In\u00a0[13]: Copied! <pre># Create a tensor with the same dimension as another tensor\nrange_copy = torch.zeros_like(input=range_tensor)\nprint(f\"Shape of 'range_copy': {range_copy.shape}\")\nrange_copy\n</pre> # Create a tensor with the same dimension as another tensor range_copy = torch.zeros_like(input=range_tensor) print(f\"Shape of 'range_copy': {range_copy.shape}\") range_copy <pre>Shape of 'range_copy': torch.Size([50])\n</pre> Out[13]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])</pre> <p>The use of functions like <code>zeros_like</code> or <code>ones_like</code> facilitates creating tensors compatible in shape and data type with existing ones.</p> <p>PyTorch tensors and NumPy arrays are closely related, and conversion between them is straightforward and efficient. This interoperability is crucial when integrating PyTorch with other scientific computing libraries.</p> In\u00a0[14]: Copied! <pre># 3pps\nimport numpy as np\n\n\n# NumPy array to PyTorch tensor\nnumpy_array = np.array([[1, 2, 3], [4, 5, 6]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"Original NumPy array:\\n{numpy_array}\")\nprint(f\"Converted tensor:\\n{tensor_from_numpy}\")\nprint(f\"Tensor dtype: {tensor_from_numpy.dtype}\")\n\n# PyTorch tensor to NumPy array\ntensor = torch.tensor([[7, 8, 9], [10, 11, 12]])\nnumpy_from_tensor = tensor.numpy()\nprint(f\"\\nOriginal tensor:\\n{tensor}\")\nprint(f\"Converted NumPy array:\\n{numpy_from_tensor}\")\n</pre> # 3pps import numpy as np   # NumPy array to PyTorch tensor numpy_array = np.array([[1, 2, 3], [4, 5, 6]]) tensor_from_numpy = torch.from_numpy(numpy_array) print(f\"Original NumPy array:\\n{numpy_array}\") print(f\"Converted tensor:\\n{tensor_from_numpy}\") print(f\"Tensor dtype: {tensor_from_numpy.dtype}\")  # PyTorch tensor to NumPy array tensor = torch.tensor([[7, 8, 9], [10, 11, 12]]) numpy_from_tensor = tensor.numpy() print(f\"\\nOriginal tensor:\\n{tensor}\") print(f\"Converted NumPy array:\\n{numpy_from_tensor}\") <pre>Original NumPy array:\n[[1 2 3]\n [4 5 6]]\nConverted tensor:\ntensor([[1, 2, 3],\n        [4, 5, 6]])\nTensor dtype: torch.int64\n\nOriginal tensor:\ntensor([[ 7,  8,  9],\n        [10, 11, 12]])\nConverted NumPy array:\n[[ 7  8  9]\n [10 11 12]]\n</pre> <p>Important considerations:</p> <ol> <li>Memory sharing: By default, <code>torch.from_numpy()</code> creates a tensor that shares memory with the original NumPy array. Modifications to one will affect the other.</li> </ol> In\u00a0[15]: Copied! <pre># Demonstrate memory sharing\nnp_array = np.array([1, 2, 3])\ntensor = torch.from_numpy(np_array)\nnp_array[0] = 999\nprint(f\"Modified NumPy array: {np_array}\")\nprint(f\"Tensor (shares memory): {tensor}\")  # Also shows 999\n</pre> # Demonstrate memory sharing np_array = np.array([1, 2, 3]) tensor = torch.from_numpy(np_array) np_array[0] = 999 print(f\"Modified NumPy array: {np_array}\") print(f\"Tensor (shares memory): {tensor}\")  # Also shows 999 <pre>Modified NumPy array: [999   2   3]\nTensor (shares memory): tensor([999,   2,   3])\n</pre> <ol> <li>GPU tensors: Tensors on GPU must be moved to CPU before converting to NumPy:</li> </ol> In\u00a0[16]: Copied! <pre>if torch.cuda.is_available():\n    gpu_tensor = torch.tensor([1, 2, 3], device=\"cuda\")\n    # This would raise an error: gpu_tensor.numpy()\n    numpy_from_gpu = gpu_tensor.cpu().numpy()  # Correct approach\n</pre> if torch.cuda.is_available():     gpu_tensor = torch.tensor([1, 2, 3], device=\"cuda\")     # This would raise an error: gpu_tensor.numpy()     numpy_from_gpu = gpu_tensor.cpu().numpy()  # Correct approach <ol> <li>Gradient tracking: Tensors with gradients enabled must have gradients detached:</li> </ol> In\u00a0[17]: Copied! <pre>tensor_with_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n# This would raise an error: tensor_with_grad.numpy()\nnumpy_array = tensor_with_grad.detach().numpy()  # Correct approach\n</pre> tensor_with_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # This would raise an error: tensor_with_grad.numpy() numpy_array = tensor_with_grad.detach().numpy()  # Correct approach <p>Each tensor in PyTorch is characterized, among other aspects, by its data type (<code>dtype</code>), its shape (<code>shape</code>), and the computing device on which it resides (<code>device</code>). These attributes influence operation compatibility and computational performance. When operations are performed between tensors whose data types do not match, whose dimensions are not compatible for the defined operation, or that are on different devices (for example, one on CPU and another on GPU), conflicts and errors can occur during execution.</p> <p>The following snippet shows how to inspect these properties:</p> In\u00a0[18]: Copied! <pre>tensor = torch.rand(size=(2, 2, 3))\ntensor\n</pre> tensor = torch.rand(size=(2, 2, 3)) tensor Out[18]: <pre>tensor([[[0.3591, 0.1326, 0.0896],\n         [0.8296, 0.8643, 0.7267]],\n\n        [[0.4558, 0.9901, 0.4453],\n         [0.8218, 0.3611, 0.6967]]])</pre> In\u00a0[19]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") <pre>Data type: torch.float32\nShape: torch.Size([2, 2, 3])\nDevice: cpu\n</pre> <p>By default, floating-point tensors are created with type <code>torch.float32</code>. However, it is possible to specify a different data type, such as <code>float16</code> or <code>float64</code>:</p> In\u00a0[20]: Copied! <pre>tensor = torch.rand(size=(2, 2, 3), dtype=torch.float16)\ntensor\n</pre> tensor = torch.rand(size=(2, 2, 3), dtype=torch.float16) tensor Out[20]: <pre>tensor([[[0.5640, 0.3555, 0.1885],\n         [0.5562, 0.1538, 0.0625]],\n\n        [[0.0186, 0.6177, 0.5962],\n         [0.2549, 0.4341, 0.1724]]], dtype=torch.float16)</pre> In\u00a0[21]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") <pre>Data type: torch.float16\nShape: torch.Size([2, 2, 3])\nDevice: cpu\n</pre> <p>The choice of numerical precision involves a trade-off between computational cost, memory consumption, training stability, and result accuracy. For example, <code>float16</code> allows significantly accelerating training and inference on appropriate hardware, but may increase the risk of numerical stability problems in certain models.</p> <p>In general, it is important that tensors involved in the same operation share a compatible data type and are on the same device. Otherwise, it is necessary to explicitly convert the type (<code>tensor.to(torch.float32)</code>, <code>tensor.int()</code>, etc.) or move the tensor to the corresponding device (<code>tensor.to(\"cuda\")</code>, <code>tensor.to(\"cpu\")</code>).</p> <p>When working with tensors, certain errors frequently arise. Understanding these common pitfalls helps prevent debugging headaches:</p> In\u00a0[22]: Copied! <pre># Error: dimension mismatch\ntry:\n    a = torch.rand(2, 3)\n    b = torch.rand(3, 5)\n    c = a + b  # Raises error: shapes [2, 3] and [3, 5] are incompatible\nexcept RuntimeError as e:\n    print(f\"Error: {e}\")\n\n# Correct: use matrix multiplication for different shapes\nc = torch.matmul(a, b)  # Results in shape [2, 5]\nprint(f\"Correct result shape: {c.shape}\")\n</pre> # Error: dimension mismatch try:     a = torch.rand(2, 3)     b = torch.rand(3, 5)     c = a + b  # Raises error: shapes [2, 3] and [3, 5] are incompatible except RuntimeError as e:     print(f\"Error: {e}\")  # Correct: use matrix multiplication for different shapes c = torch.matmul(a, b)  # Results in shape [2, 5] print(f\"Correct result shape: {c.shape}\") <pre>Error: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1\nCorrect result shape: torch.Size([2, 5])\n</pre> In\u00a0[23]: Copied! <pre># Error: tensors on different devices\nif torch.cuda.is_available():\n    try:\n        cpu_tensor = torch.rand(2, 3)\n        gpu_tensor = torch.rand(2, 3, device=\"cuda\")\n        result = cpu_tensor + gpu_tensor  # Raises error\n    except RuntimeError as e:\n        print(f\"Error: {e}\")\n\n    # Correct: ensure both tensors are on the same device\n    result = cpu_tensor.to(\"cuda\") + gpu_tensor\n    # Or: result = cpu_tensor + gpu_tensor.to('cpu')\n</pre> # Error: tensors on different devices if torch.cuda.is_available():     try:         cpu_tensor = torch.rand(2, 3)         gpu_tensor = torch.rand(2, 3, device=\"cuda\")         result = cpu_tensor + gpu_tensor  # Raises error     except RuntimeError as e:         print(f\"Error: {e}\")      # Correct: ensure both tensors are on the same device     result = cpu_tensor.to(\"cuda\") + gpu_tensor     # Or: result = cpu_tensor + gpu_tensor.to('cpu') In\u00a0[24]: Copied! <pre># Error: incompatible data types\ntry:\n    int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n    float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n    # Some operations may fail or produce unexpected results\n    result = int_tensor / float_tensor  # Works but with implicit conversion\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Best practice: explicit type conversion\nresult = int_tensor.float() / float_tensor\nprint(f\"Result dtype: {result.dtype}\")\n</pre> # Error: incompatible data types try:     int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)     float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)     # Some operations may fail or produce unexpected results     result = int_tensor / float_tensor  # Works but with implicit conversion except Exception as e:     print(f\"Error: {e}\")  # Best practice: explicit type conversion result = int_tensor.float() / float_tensor print(f\"Result dtype: {result.dtype}\") <pre>Result dtype: torch.float32\n</pre> In\u00a0[25]: Copied! <pre># Error: modifying tensors with gradients in-place\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x**2\n\ntry:\n    x[0] = 10.0  # In-place modification raises error\nexcept RuntimeError as e:\n    print(f\"Error: {e}\")\n\n# Correct: use operations that don't modify tensors in-place\nx_new = torch.tensor([10.0, 2.0, 3.0], requires_grad=True)\ny = x_new**2\n</pre> # Error: modifying tensors with gradients in-place x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = x**2  try:     x[0] = 10.0  # In-place modification raises error except RuntimeError as e:     print(f\"Error: {e}\")  # Correct: use operations that don't modify tensors in-place x_new = torch.tensor([10.0, 2.0, 3.0], requires_grad=True) y = x_new**2 <pre>Error: a view of a leaf Variable that requires grad is being used in an in-place operation.\n</pre> In\u00a0[26]: Copied! <pre># Potential memory leak: accumulating gradients in a loop\nlosses = []\nfor i in range(100):\n    x = torch.randn(1000, 1000, requires_grad=True)\n    y = x.sum()\n    # losses.append(y)  # BAD: keeps entire computational graph\n    losses.append(y.item())  # GOOD: only stores the scalar value\n</pre> # Potential memory leak: accumulating gradients in a loop losses = [] for i in range(100):     x = torch.randn(1000, 1000, requires_grad=True)     y = x.sum()     # losses.append(y)  # BAD: keeps entire computational graph     losses.append(y.item())  # GOOD: only stores the scalar value <p>Once tensors are created, PyTorch allows applying various reduction and aggregation operations on them. For example, you can calculate maximums, means, or maximum indices along specific dimensions. Consider the following two-dimensional tensor:</p> In\u00a0[27]: Copied! <pre>tensor = torch.rand(size=(2, 3))\ntensor\n</pre> tensor = torch.rand(size=(2, 3)) tensor Out[27]: <pre>tensor([[0.4426, 0.4323, 0.7752],\n        [0.0720, 0.0204, 0.6300]])</pre> <p>The <code>max</code> method allows obtaining the maximum value along a dimension and its index:</p> In\u00a0[28]: Copied! <pre># Maximum by columns (dimension 0)\ntensor.max(dim=0)\n</pre> # Maximum by columns (dimension 0) tensor.max(dim=0) Out[28]: <pre>torch.return_types.max(\nvalues=tensor([0.4426, 0.4323, 0.7752]),\nindices=tensor([0, 0, 0]))</pre> In\u00a0[29]: Copied! <pre># Maximum by rows (dimension 1)\ntensor.max(dim=1)\n</pre> # Maximum by rows (dimension 1) tensor.max(dim=1) Out[29]: <pre>torch.return_types.max(\nvalues=tensor([0.7752, 0.6300]),\nindices=tensor([2, 2]))</pre> <p>In this context, the convention is adopted that columns correspond to axis or dimension <code>0</code>, while rows are associated with axis or dimension <code>1</code>. Similarly, the mean can be calculated:</p> In\u00a0[30]: Copied! <pre>tensor.mean(dim=0), tensor.mean(dim=1)\n</pre> tensor.mean(dim=0), tensor.mean(dim=1) Out[30]: <pre>(tensor([0.2573, 0.2264, 0.7026]), tensor([0.5500, 0.2408]))</pre> In\u00a0[31]: Copied! <pre>torch.mean(tensor, dim=0), torch.mean(tensor, dim=1)\n</pre> torch.mean(tensor, dim=0), torch.mean(tensor, dim=1) Out[31]: <pre>(tensor([0.2573, 0.2264, 0.7026]), tensor([0.5500, 0.2408]))</pre> <p>The <code>argmax</code> function returns the indices of maximum values along a determined dimension:</p> In\u00a0[32]: Copied! <pre>torch.argmax(tensor, dim=0), tensor.argmax(dim=0)\n</pre> torch.argmax(tensor, dim=0), tensor.argmax(dim=0) Out[32]: <pre>(tensor([0, 0, 0]), tensor([0, 0, 0]))</pre> In\u00a0[33]: Copied! <pre>tensor.argmax(dim=1)\n</pre> tensor.argmax(dim=1) Out[33]: <pre>tensor([2, 2])</pre> <p>These indices can be used to select elements or substructures of the tensor. For example:</p> In\u00a0[34]: Copied! <pre>tensor\n</pre> tensor Out[34]: <pre>tensor([[0.4426, 0.4323, 0.7752],\n        [0.0720, 0.0204, 0.6300]])</pre> In\u00a0[35]: Copied! <pre>tensor[:, tensor.argmax(dim=1)[0]]\n</pre> tensor[:, tensor.argmax(dim=1)[0]] Out[35]: <pre>tensor([0.7752, 0.6300])</pre> <p>Here all rows and the column corresponding to the largest value of the first row are selected, illustrating how to combine reduction operations with indexing.</p> <p>PyTorch supports a comprehensive set of mathematical operations between tensors. Understanding the differences between element-wise operations, matrix multiplication, and broadcasting is fundamental.</p> <p>Element-wise operations apply to corresponding elements of tensors with compatible shapes:</p> In\u00a0[36]: Copied! <pre>a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\nb = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n\n# Element-wise addition\nprint(\"Addition (a + b):\")\nprint(a + b)\n\n# Element-wise subtraction\nprint(\"\\nSubtraction (a - b):\")\nprint(a - b)\n\n# Element-wise multiplication\nprint(\"\\nElement-wise multiplication (a * b):\")\nprint(a * b)\n\n# Element-wise division\nprint(\"\\nElement-wise division (a / b):\")\nprint(a / b)\n\n# Element-wise power\nprint(\"\\nPower (a ** 2):\")\nprint(a**2)\n</pre> a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)  # Element-wise addition print(\"Addition (a + b):\") print(a + b)  # Element-wise subtraction print(\"\\nSubtraction (a - b):\") print(a - b)  # Element-wise multiplication print(\"\\nElement-wise multiplication (a * b):\") print(a * b)  # Element-wise division print(\"\\nElement-wise division (a / b):\") print(a / b)  # Element-wise power print(\"\\nPower (a ** 2):\") print(a**2) <pre>Addition (a + b):\ntensor([[ 6.,  8.],\n        [10., 12.]])\n\nSubtraction (a - b):\ntensor([[-4., -4.],\n        [-4., -4.]])\n\nElement-wise multiplication (a * b):\ntensor([[ 5., 12.],\n        [21., 32.]])\n\nElement-wise division (a / b):\ntensor([[0.2000, 0.3333],\n        [0.4286, 0.5000]])\n\nPower (a ** 2):\ntensor([[ 1.,  4.],\n        [ 9., 16.]])\n</pre> <p>PyTorch provides multiple ways to perform matrix multiplication, each with specific use cases:</p> In\u00a0[37]: Copied! <pre>a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\nb = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n\n# Method 1: torch.matmul (recommended, works with batched matrices)\nresult1 = torch.matmul(a, b)\nprint(\"torch.matmul(a, b):\")\nprint(result1)\n\n# Method 2: @ operator (syntactic sugar for matmul)\nresult2 = a @ b\nprint(\"\\na @ b:\")\nprint(result2)\n\n# Method 3: torch.mm (only for 2D matrices)\nresult3 = torch.mm(a, b)\nprint(\"\\ntorch.mm(a, b):\")\nprint(result3)\n\n# Verify all methods give the same result\nprint(\n    f\"\\nAll methods equivalent: {torch.equal(result1, result2) and torch.equal(result2, result3)}\"\n)\n</pre> a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)  # Method 1: torch.matmul (recommended, works with batched matrices) result1 = torch.matmul(a, b) print(\"torch.matmul(a, b):\") print(result1)  # Method 2: @ operator (syntactic sugar for matmul) result2 = a @ b print(\"\\na @ b:\") print(result2)  # Method 3: torch.mm (only for 2D matrices) result3 = torch.mm(a, b) print(\"\\ntorch.mm(a, b):\") print(result3)  # Verify all methods give the same result print(     f\"\\nAll methods equivalent: {torch.equal(result1, result2) and torch.equal(result2, result3)}\" ) <pre>torch.matmul(a, b):\ntensor([[19., 22.],\n        [43., 50.]])\n\na @ b:\ntensor([[19., 22.],\n        [43., 50.]])\n\ntorch.mm(a, b):\ntensor([[19., 22.],\n        [43., 50.]])\n\nAll methods equivalent: True\n</pre> <p>Key differences:</p> <ul> <li><code>torch.matmul</code> (or <code>@</code>): Most versatile, handles broadcasting and batched operations</li> <li><code>torch.mm</code>: Only for 2D matrices, slightly faster but less flexible</li> <li><code>*</code>: Element-wise multiplication (NOT matrix multiplication)</li> </ul> <p>Broadcasting allows operations between tensors of different shapes by automatically expanding dimensions:</p> In\u00a0[38]: Copied! <pre># Example 1: Scalar and tensor\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = 10\nresult = a + b  # b is broadcasted to match a's shape\nprint(\"Tensor + Scalar:\")\nprint(result)\n\n# Example 2: Vector and matrix\na = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\nb = torch.tensor([10, 20, 30])  # Shape: (3,)\nresult = a + b  # b is broadcasted along dim 0\nprint(\"\\nMatrix + Vector:\")\nprint(result)\n\n# Example 3: Broadcasting with unsqueeze\na = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\nb = torch.tensor([10, 20])  # Shape: (2,)\nresult = a + b  # Broadcasted to (3, 2)\nprint(\"\\nBroadcasting with different dimensions:\")\nprint(result)\nprint(f\"Result shape: {result.shape}\")\n</pre> # Example 1: Scalar and tensor a = torch.tensor([[1, 2, 3], [4, 5, 6]]) b = 10 result = a + b  # b is broadcasted to match a's shape print(\"Tensor + Scalar:\") print(result)  # Example 2: Vector and matrix a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3) b = torch.tensor([10, 20, 30])  # Shape: (3,) result = a + b  # b is broadcasted along dim 0 print(\"\\nMatrix + Vector:\") print(result)  # Example 3: Broadcasting with unsqueeze a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1) b = torch.tensor([10, 20])  # Shape: (2,) result = a + b  # Broadcasted to (3, 2) print(\"\\nBroadcasting with different dimensions:\") print(result) print(f\"Result shape: {result.shape}\") <pre>Tensor + Scalar:\ntensor([[11, 12, 13],\n        [14, 15, 16]])\n\nMatrix + Vector:\ntensor([[11, 22, 33],\n        [14, 25, 36]])\n\nBroadcasting with different dimensions:\ntensor([[11, 21],\n        [12, 22],\n        [13, 23]])\nResult shape: torch.Size([3, 2])\n</pre> <p>Broadcasting rules:</p> <ol> <li>Dimensions are aligned from right to left</li> <li>Dimensions must be equal, one of them must be 1, or one doesn't exist</li> <li>The result has the maximum size along each dimension</li> </ol> <p>Operations ending with an underscore (<code>_</code>) modify tensors in-place, saving memory but potentially causing issues with gradient computation:</p> In\u00a0[39]: Copied! <pre>x = torch.tensor([1, 2, 3], dtype=torch.float32)\nprint(f\"Original x: {x}\")\nprint(f\"Memory address: {x.data_ptr()}\")\n\n# In-place addition\nx.add_(5)\nprint(f\"After add_(5): {x}\")\nprint(f\"Memory address: {x.data_ptr()}\")  # Same address\n\n# In-place multiplication\nx.mul_(2)\nprint(f\"After mul_(2): {x}\")\n\n# Compare with non-in-place operations\ny = torch.tensor([1, 2, 3], dtype=torch.float32)\nprint(f\"\\nOriginal y: {y}\")\nprint(f\"Memory address: {y.data_ptr()}\")\n\ny = y + 5  # Creates new tensor\nprint(f\"After y = y + 5: {y}\")\nprint(f\"Memory address: {y.data_ptr()}\")  # Different address\n</pre> x = torch.tensor([1, 2, 3], dtype=torch.float32) print(f\"Original x: {x}\") print(f\"Memory address: {x.data_ptr()}\")  # In-place addition x.add_(5) print(f\"After add_(5): {x}\") print(f\"Memory address: {x.data_ptr()}\")  # Same address  # In-place multiplication x.mul_(2) print(f\"After mul_(2): {x}\")  # Compare with non-in-place operations y = torch.tensor([1, 2, 3], dtype=torch.float32) print(f\"\\nOriginal y: {y}\") print(f\"Memory address: {y.data_ptr()}\")  y = y + 5  # Creates new tensor print(f\"After y = y + 5: {y}\") print(f\"Memory address: {y.data_ptr()}\")  # Different address <pre>Original x: tensor([1., 2., 3.])\nMemory address: 94272452532224\nAfter add_(5): tensor([6., 7., 8.])\nMemory address: 94272452532224\nAfter mul_(2): tensor([12., 14., 16.])\n\nOriginal y: tensor([1., 2., 3.])\nMemory address: 94272342655616\nAfter y = y + 5: tensor([6., 7., 8.])\nMemory address: 94272455114176\n</pre> <p>Warning: Avoid in-place operations on tensors with <code>requires_grad=True</code> as they can cause errors during backpropagation.</p> <p>Understanding how to reshape and manipulate tensors is crucial for building neural networks, as layer outputs often need to be reshaped to match subsequent layer inputs.</p> In\u00a0[40]: Copied! <pre># Create a sample tensor\nx = torch.arange(12)\nprint(f\"Original tensor: {x}\")\nprint(f\"Shape: {x.shape}\")\n\n# Using view (shares memory with original tensor)\nx_view = x.view(3, 4)\nprint(f\"\\nView as (3, 4):\\n{x_view}\")\n\n# Using reshape (may create a copy if necessary)\nx_reshape = x.reshape(2, 6)\nprint(f\"\\nReshape as (2, 6):\\n{x_reshape}\")\n\n# Using -1 for automatic dimension calculation\nx_auto = x.view(3, -1)  # -1 automatically becomes 4\nprint(f\"\\nView as (3, -1):\\n{x_auto}\")\n\n# Flatten: convert to 1D\nx_flat = x.view(-1)\nprint(f\"\\nFlattened: {x_flat}\")\n\n# Flatten with method\nx_flat2 = x.flatten()\nprint(f\"Flatten method: {x_flat2}\")\n</pre> # Create a sample tensor x = torch.arange(12) print(f\"Original tensor: {x}\") print(f\"Shape: {x.shape}\")  # Using view (shares memory with original tensor) x_view = x.view(3, 4) print(f\"\\nView as (3, 4):\\n{x_view}\")  # Using reshape (may create a copy if necessary) x_reshape = x.reshape(2, 6) print(f\"\\nReshape as (2, 6):\\n{x_reshape}\")  # Using -1 for automatic dimension calculation x_auto = x.view(3, -1)  # -1 automatically becomes 4 print(f\"\\nView as (3, -1):\\n{x_auto}\")  # Flatten: convert to 1D x_flat = x.view(-1) print(f\"\\nFlattened: {x_flat}\")  # Flatten with method x_flat2 = x.flatten() print(f\"Flatten method: {x_flat2}\") <pre>Original tensor: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\nShape: torch.Size([12])\n\nView as (3, 4):\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\nReshape as (2, 6):\ntensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]])\n\nView as (3, -1):\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\nFlattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\nFlatten method: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n</pre> <p>Key differences:</p> <ul> <li><code>view()</code>: Returns a view of the original tensor (shares memory), requires contiguous memory</li> <li><code>reshape()</code>: May return a view or copy, more flexible but potentially less efficient</li> <li><code>flatten()</code>: Always returns a 1D tensor</li> </ul> In\u00a0[41]: Copied! <pre># Demonstrate the difference between view and reshape with transpose\nx = torch.arange(12).reshape(3, 4)\nprint(f\"Original:\\n{x}\")\nprint(f\"Is contiguous: {x.is_contiguous()}\")\n\n# Transpose makes tensor non-contiguous\nx_t = x.t()\nprint(f\"\\nTransposed:\\n{x_t}\")\nprint(f\"Is contiguous: {x_t.is_contiguous()}\")\n\n# view() fails on non-contiguous tensor\ntry:\n    x_t.view(12)\nexcept RuntimeError as e:\n    print(f\"\\nError with view(): {e}\")\n\n# reshape() works (creates a copy)\nx_t_reshaped = x_t.reshape(12)\nprint(f\"\\nReshape works: {x_t_reshaped}\")\n\n# Make contiguous explicitly\nx_t_cont = x_t.contiguous()\nx_t_view = x_t_cont.view(12)\nprint(f\"After contiguous(), view works: {x_t_view}\")\n</pre> # Demonstrate the difference between view and reshape with transpose x = torch.arange(12).reshape(3, 4) print(f\"Original:\\n{x}\") print(f\"Is contiguous: {x.is_contiguous()}\")  # Transpose makes tensor non-contiguous x_t = x.t() print(f\"\\nTransposed:\\n{x_t}\") print(f\"Is contiguous: {x_t.is_contiguous()}\")  # view() fails on non-contiguous tensor try:     x_t.view(12) except RuntimeError as e:     print(f\"\\nError with view(): {e}\")  # reshape() works (creates a copy) x_t_reshaped = x_t.reshape(12) print(f\"\\nReshape works: {x_t_reshaped}\")  # Make contiguous explicitly x_t_cont = x_t.contiguous() x_t_view = x_t_cont.view(12) print(f\"After contiguous(), view works: {x_t_view}\") <pre>Original:\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\nIs contiguous: True\n\nTransposed:\ntensor([[ 0,  4,  8],\n        [ 1,  5,  9],\n        [ 2,  6, 10],\n        [ 3,  7, 11]])\nIs contiguous: False\n\nError with view(): view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n\nReshape works: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\nAfter contiguous(), view works: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n</pre> <p>These operations add or remove dimensions of size 1:</p> In\u00a0[42]: Copied! <pre># Create a tensor with dimensions of size 1\nx = torch.rand(1, 3, 1, 4)\nprint(f\"Original shape: {x.shape}\")\n\n# Remove all dimensions of size 1\nx_squeezed = x.squeeze()\nprint(f\"After squeeze(): {x_squeezed.shape}\")\n\n# Remove specific dimension\nx_squeezed_dim = x.squeeze(dim=0)\nprint(f\"After squeeze(dim=0): {x_squeezed_dim.shape}\")\n\n# Add dimension\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"After unsqueeze(dim=0): {x_unsqueezed.shape}\")\n\n# Practical example: preparing batch dimension\nsingle_image = torch.rand(3, 224, 224)  # C, H, W\nbatched_image = single_image.unsqueeze(0)  # Add batch dimension: B, C, H, W\nprint(f\"\\nSingle image shape: {single_image.shape}\")\nprint(f\"Batched image shape: {batched_image.shape}\")\n</pre> # Create a tensor with dimensions of size 1 x = torch.rand(1, 3, 1, 4) print(f\"Original shape: {x.shape}\")  # Remove all dimensions of size 1 x_squeezed = x.squeeze() print(f\"After squeeze(): {x_squeezed.shape}\")  # Remove specific dimension x_squeezed_dim = x.squeeze(dim=0) print(f\"After squeeze(dim=0): {x_squeezed_dim.shape}\")  # Add dimension x_unsqueezed = x_squeezed.unsqueeze(dim=0) print(f\"After unsqueeze(dim=0): {x_unsqueezed.shape}\")  # Practical example: preparing batch dimension single_image = torch.rand(3, 224, 224)  # C, H, W batched_image = single_image.unsqueeze(0)  # Add batch dimension: B, C, H, W print(f\"\\nSingle image shape: {single_image.shape}\") print(f\"Batched image shape: {batched_image.shape}\") <pre>Original shape: torch.Size([1, 3, 1, 4])\nAfter squeeze(): torch.Size([3, 4])\nAfter squeeze(dim=0): torch.Size([3, 1, 4])\nAfter unsqueeze(dim=0): torch.Size([1, 3, 4])\n\nSingle image shape: torch.Size([3, 224, 224])\nBatched image shape: torch.Size([1, 3, 224, 224])\n</pre> <p>Combining multiple tensors is a common operation in deep learning:</p> In\u00a0[43]: Copied! <pre># Create sample tensors\na = torch.ones(2, 3)\nb = torch.zeros(2, 3)\nc = torch.full((2, 3), 0.5)\n\nprint(\"Tensor a:\")\nprint(a)\nprint(\"\\nTensor b:\")\nprint(b)\nprint(\"\\nTensor c:\")\nprint(c)\n\n# Concatenation along dimension 0 (rows)\ncat_dim0 = torch.cat([a, b, c], dim=0)\nprint(f\"\\nConcatenate along dim=0:\")\nprint(cat_dim0)\nprint(f\"Shape: {cat_dim0.shape}\")\n\n# Concatenation along dimension 1 (columns)\ncat_dim1 = torch.cat([a, b, c], dim=1)\nprint(f\"\\nConcatenate along dim=1:\")\nprint(cat_dim1)\nprint(f\"Shape: {cat_dim1.shape}\")\n\n# Stacking creates a new dimension\nstacked = torch.stack([a, b, c], dim=0)\nprint(f\"\\nStack along dim=0:\")\nprint(stacked)\nprint(f\"Shape: {stacked.shape}\")\n\n# Stack along different dimension\nstacked_dim1 = torch.stack([a, b, c], dim=1)\nprint(f\"\\nStack along dim=1 shape: {stacked_dim1.shape}\")\n</pre> # Create sample tensors a = torch.ones(2, 3) b = torch.zeros(2, 3) c = torch.full((2, 3), 0.5)  print(\"Tensor a:\") print(a) print(\"\\nTensor b:\") print(b) print(\"\\nTensor c:\") print(c)  # Concatenation along dimension 0 (rows) cat_dim0 = torch.cat([a, b, c], dim=0) print(f\"\\nConcatenate along dim=0:\") print(cat_dim0) print(f\"Shape: {cat_dim0.shape}\")  # Concatenation along dimension 1 (columns) cat_dim1 = torch.cat([a, b, c], dim=1) print(f\"\\nConcatenate along dim=1:\") print(cat_dim1) print(f\"Shape: {cat_dim1.shape}\")  # Stacking creates a new dimension stacked = torch.stack([a, b, c], dim=0) print(f\"\\nStack along dim=0:\") print(stacked) print(f\"Shape: {stacked.shape}\")  # Stack along different dimension stacked_dim1 = torch.stack([a, b, c], dim=1) print(f\"\\nStack along dim=1 shape: {stacked_dim1.shape}\") <pre>Tensor a:\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\nTensor b:\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\nTensor c:\ntensor([[0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000]])\n\nConcatenate along dim=0:\ntensor([[1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000]])\nShape: torch.Size([6, 3])\n\nConcatenate along dim=1:\ntensor([[1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n        [1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000]])\nShape: torch.Size([2, 9])\n\nStack along dim=0:\ntensor([[[1.0000, 1.0000, 1.0000],\n         [1.0000, 1.0000, 1.0000]],\n\n        [[0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000]],\n\n        [[0.5000, 0.5000, 0.5000],\n         [0.5000, 0.5000, 0.5000]]])\nShape: torch.Size([3, 2, 3])\n\nStack along dim=1 shape: torch.Size([2, 3, 3])\n</pre> <p>Key differences:</p> <ul> <li><code>torch.cat()</code>: Concatenates along an existing dimension</li> <li><code>torch.stack()</code>: Creates a new dimension and stacks along it</li> </ul> In\u00a0[44]: Copied! <pre># Create a tensor to split\nx = torch.arange(24).reshape(4, 6)\nprint(\"Original tensor:\")\nprint(x)\nprint(f\"Shape: {x.shape}\")\n\n# Split into equal chunks\nchunks = torch.split(x, split_size_or_sections=2, dim=0)\nprint(f\"\\nSplit into chunks of size 2 along dim=0:\")\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i} shape: {chunk.shape}\")\n    print(chunk)\n\n# Split into specific sizes\nsplit_sizes = [1, 2, 3]\ncustom_chunks = torch.split(x, split_sizes, dim=1)\nprint(f\"\\nSplit into custom sizes {split_sizes} along dim=1:\")\nfor i, chunk in enumerate(custom_chunks):\n    print(f\"Chunk {i} shape: {chunk.shape}\")\n\n# Chunk: split into specified number of chunks\nnum_chunks = 2\nchunked = torch.chunk(x, chunks=num_chunks, dim=0)\nprint(f\"\\nChunk into {num_chunks} parts:\")\nfor i, chunk in enumerate(chunked):\n    print(f\"Chunk {i} shape: {chunk.shape}\")\n</pre> # Create a tensor to split x = torch.arange(24).reshape(4, 6) print(\"Original tensor:\") print(x) print(f\"Shape: {x.shape}\")  # Split into equal chunks chunks = torch.split(x, split_size_or_sections=2, dim=0) print(f\"\\nSplit into chunks of size 2 along dim=0:\") for i, chunk in enumerate(chunks):     print(f\"Chunk {i} shape: {chunk.shape}\")     print(chunk)  # Split into specific sizes split_sizes = [1, 2, 3] custom_chunks = torch.split(x, split_sizes, dim=1) print(f\"\\nSplit into custom sizes {split_sizes} along dim=1:\") for i, chunk in enumerate(custom_chunks):     print(f\"Chunk {i} shape: {chunk.shape}\")  # Chunk: split into specified number of chunks num_chunks = 2 chunked = torch.chunk(x, chunks=num_chunks, dim=0) print(f\"\\nChunk into {num_chunks} parts:\") for i, chunk in enumerate(chunked):     print(f\"Chunk {i} shape: {chunk.shape}\") <pre>Original tensor:\ntensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]])\nShape: torch.Size([4, 6])\n\nSplit into chunks of size 2 along dim=0:\nChunk 0 shape: torch.Size([2, 6])\ntensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]])\nChunk 1 shape: torch.Size([2, 6])\ntensor([[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]])\n\nSplit into custom sizes [1, 2, 3] along dim=1:\nChunk 0 shape: torch.Size([4, 1])\nChunk 1 shape: torch.Size([4, 2])\nChunk 2 shape: torch.Size([4, 3])\n\nChunk into 2 parts:\nChunk 0 shape: torch.Size([2, 6])\nChunk 1 shape: torch.Size([2, 6])\n</pre> <p>PyTorch also allows performing advanced indexing operations and constructing submatrices through slicing techniques. Consider a random matrix of size <code>(4, 4)</code>:</p> In\u00a0[45]: Copied! <pre>matrix = torch.rand((4, 4))\nmatrix\n</pre> matrix = torch.rand((4, 4)) matrix Out[45]: <pre>tensor([[0.1311, 0.4175, 0.4872, 0.4283],\n        [0.8929, 0.6763, 0.2450, 0.6541],\n        [0.7383, 0.5073, 0.2931, 0.2258],\n        [0.7358, 0.0298, 0.4694, 0.7338]])</pre> <p>It is possible to extract submatrices taking one element out of every two in both dimensions:</p> In\u00a0[46]: Copied! <pre>submatrix_1 = matrix[0::2, 0::2]\nsubmatrix_2 = matrix[0::2, 1::2]\nsubmatrix_3 = matrix[1::2, 0::2]\nsubmatrix_4 = matrix[1::2, 1::2]\n\nsubmatrix_1, submatrix_2, submatrix_3, submatrix_4\n</pre> submatrix_1 = matrix[0::2, 0::2] submatrix_2 = matrix[0::2, 1::2] submatrix_3 = matrix[1::2, 0::2] submatrix_4 = matrix[1::2, 1::2]  submatrix_1, submatrix_2, submatrix_3, submatrix_4 Out[46]: <pre>(tensor([[0.1311, 0.4872],\n         [0.7383, 0.2931]]),\n tensor([[0.4175, 0.4283],\n         [0.5073, 0.2258]]),\n tensor([[0.8929, 0.2450],\n         [0.7358, 0.4694]]),\n tensor([[0.6763, 0.6541],\n         [0.0298, 0.7338]]))</pre> <p>These submatrices can be stacked along a new dimension using <code>torch.stack</code>:</p> In\u00a0[47]: Copied! <pre>submatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4])\nsubmatrices\n</pre> submatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4]) submatrices Out[47]: <pre>tensor([[[0.1311, 0.4872],\n         [0.7383, 0.2931]],\n\n        [[0.4175, 0.4283],\n         [0.5073, 0.2258]],\n\n        [[0.8929, 0.2450],\n         [0.7358, 0.4694]],\n\n        [[0.6763, 0.6541],\n         [0.0298, 0.7338]]])</pre> In\u00a0[48]: Copied! <pre>submatrices.shape\n</pre> submatrices.shape Out[48]: <pre>torch.Size([4, 2, 2])</pre> <p>The result is a tensor in which each submatrix occupies a position along the first dimension. If you want to add an additional dimension, you can use <code>unsqueeze</code>:</p> In\u00a0[49]: Copied! <pre>print(submatrices.shape)\nsubmatrices = submatrices.unsqueeze(dim=0)\nsubmatrices.shape\n</pre> print(submatrices.shape) submatrices = submatrices.unsqueeze(dim=0) submatrices.shape <pre>torch.Size([4, 2, 2])\n</pre> Out[49]: <pre>torch.Size([1, 4, 2, 2])</pre> <p>From this tensor, different operations can be performed. For example, we will calculate the Frobenius matrix norm (the Frobenius norm is equivalent to the square root of the sum of squares of all matrix elements) of each submatrix using <code>torch.linalg.matrix_norm</code>:</p> In\u00a0[50]: Copied! <pre>norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1))\nnorm\n</pre> norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1)) norm Out[50]: <pre>tensor([[0.9411, 0.8161, 1.2724, 1.1935]])</pre> <p>Once the norms are calculated, you can select the submatrix with the highest norm using <code>argmax</code> on the <code>norm</code> tensor:</p> In\u00a0[51]: Copied! <pre>submatrices[:, torch.argmax(norm), :, :]\n</pre> submatrices[:, torch.argmax(norm), :, :] Out[51]: <pre>tensor([[[0.8929, 0.2450],\n         [0.7358, 0.4694]]])</pre> <p>This example illustrates how to combine indexing, stacking, dimension insertion, and linear algebra operations to analyze and manipulate complex matrix structures within a tensor.</p> <p>Reproducibility constitutes a fundamental requirement in the development and evaluation of machine learning models. In this context, reproducibility is understood as the ability to obtain the same results when repeatedly executing an experiment under the same conditions: same code, same data, same hyperparameter configuration and, especially relevant, same random initialization.</p> <p>In PyTorch, an important part of model behavior depends on random processes, such as the initialization of neural network weights, the generation of tensors with random values, or random data sampling during training. If these processes are not controlled, small variations in initializations can produce different results in each execution, making it difficult to compare experiments and debug errors.</p> <p>To mitigate this problem, PyTorch provides mechanisms that allow fixing the seed of the random number generator. One of the most used is the instruction:</p> In\u00a0[52]: Copied! <pre>torch.manual_seed(42)\n</pre> torch.manual_seed(42) Out[52]: <pre>&lt;torch._C.Generator at 0x7f459c034bb0&gt;</pre> <p>This call initializes PyTorch's random number generator with a fixed seed, in this case the value <code>42</code>. From that moment on, all random operations that depend on this generator will produce the same sequence of values in successive executions, as long as the rest of the conditions (PyTorch version, hardware, operation order, etc.) remain constant. In this way, the creation of random tensors, the initialization of model parameters, and other stochastic processes associated with PyTorch become deterministic.</p> <p>The use of a fixed seed is especially important in experimentation and teaching environments. In a teaching context, it allows all students to obtain the same results when executing example notebooks, facilitating the follow-up of explanations and the detection of possible conceptual or implementation errors. In a research and development context, fixing the seed favors rigorous comparison between different models or configurations, as it reduces variability attributable solely to chance.</p> <p>It should be noted that, to achieve more complete reproducibility, it is often necessary to also fix the seeds of other random number generators used in the same environment, such as those from Python's standard libraries or NumPy.</p> <p>For complete reproducibility across all components of a PyTorch project, it is recommended to set seeds for all relevant libraries and configure deterministic behavior:</p> In\u00a0[53]: Copied! <pre># Standard libraries\nimport random\n\n# 3pps\nimport numpy as np\nimport torch\n\n\ndef set_seed(seed=42):\n    \"\"\"\n    Set seeds for reproducibility across all random number generators.\n\n    Args:\n        seed (int): The seed value to use for all RNGs\n    \"\"\"\n    # Python's built-in random module\n    random.seed(seed)\n\n    # NumPy random number generator\n    np.random.seed(seed)\n\n    # PyTorch random number generators\n    torch.manual_seed(seed)\n\n    # PyTorch CUDA random number generator (for GPU operations)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n\n    # Configure PyTorch to use deterministic algorithms\n    # Note: This may impact performance\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # For complete reproducibility in PyTorch &gt;= 1.8\n    # torch.use_deterministic_algorithms(True)  # Uncomment if needed\n\n\n# Apply reproducibility settings\nset_seed(42)\n\nprint(\"Reproducibility settings applied\")\nprint(f\"Random seed set to: 42\")\n</pre> # Standard libraries import random  # 3pps import numpy as np import torch   def set_seed(seed=42):     \"\"\"     Set seeds for reproducibility across all random number generators.      Args:         seed (int): The seed value to use for all RNGs     \"\"\"     # Python's built-in random module     random.seed(seed)      # NumPy random number generator     np.random.seed(seed)      # PyTorch random number generators     torch.manual_seed(seed)      # PyTorch CUDA random number generator (for GPU operations)     if torch.cuda.is_available():         torch.cuda.manual_seed(seed)         torch.cuda.manual_seed_all(seed)  # For multi-GPU setups      # Configure PyTorch to use deterministic algorithms     # Note: This may impact performance     torch.backends.cudnn.deterministic = True     torch.backends.cudnn.benchmark = False      # For complete reproducibility in PyTorch &gt;= 1.8     # torch.use_deterministic_algorithms(True)  # Uncomment if needed   # Apply reproducibility settings set_seed(42)  print(\"Reproducibility settings applied\") print(f\"Random seed set to: 42\") <pre>Reproducibility settings applied\nRandom seed set to: 42\n</pre> In\u00a0[54]: Copied! <pre># Test reproducibility with random tensor generation\n\n\ndef test_reproducibility():\n    \"\"\"Verify that setting the seed produces identical results\"\"\"\n\n    # First run\n    set_seed(42)\n    tensor1 = torch.rand(3, 3)\n    print(\"First run:\")\n    print(tensor1)\n\n    # Second run with same seed\n    set_seed(42)\n    tensor2 = torch.rand(3, 3)\n    print(\"\\nSecond run:\")\n    print(tensor2)\n\n    # Verify they are identical\n    print(f\"\\nTensors are identical: {torch.equal(tensor1, tensor2)}\")\n\n\ntest_reproducibility()\n</pre> # Test reproducibility with random tensor generation   def test_reproducibility():     \"\"\"Verify that setting the seed produces identical results\"\"\"      # First run     set_seed(42)     tensor1 = torch.rand(3, 3)     print(\"First run:\")     print(tensor1)      # Second run with same seed     set_seed(42)     tensor2 = torch.rand(3, 3)     print(\"\\nSecond run:\")     print(tensor2)      # Verify they are identical     print(f\"\\nTensors are identical: {torch.equal(tensor1, tensor2)}\")   test_reproducibility() <pre>First run:\ntensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009],\n        [0.2566, 0.7936, 0.9408]])\n\nSecond run:\ntensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009],\n        [0.2566, 0.7936, 0.9408]])\n\nTensors are identical: True\n</pre>"},{"location":"course/topic_02_mathematics/section_01_data_structures.html#data-structures-and-tensors-in-pytorch","title":"Data Structures and Tensors in PyTorch\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#gpu-availability","title":"GPU Availability\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#performance-comparison-cpu-vs-gpu","title":"Performance Comparison: CPU vs GPU\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#introduction-to-tensors","title":"Introduction to Tensors\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#pytorch-tensor-data-types","title":"PyTorch Tensor Data Types\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#creating-tensors-scalars-vectors-matrices-and-higher-order-tensors","title":"Creating Tensors: Scalars, Vectors, Matrices, and Higher-Order Tensors\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#creating-tensors-with-values","title":"Creating Tensors with Values\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#conversion-between-numpy-and-pytorch","title":"Conversion Between NumPy and PyTorch\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#data-types-devices-and-tensor-compatibility","title":"Data Types, Devices, and Tensor Compatibility\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#common-errors-and-best-practices","title":"Common Errors and Best Practices\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#1-incompatible-dimensions","title":"1. Incompatible Dimensions\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#2-device-mismatch","title":"2. Device Mismatch\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#3-data-type-incompatibility","title":"3. Data Type Incompatibility\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#4-gradient-related-issues","title":"4. Gradient-Related Issues\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#5-memory-leaks-with-gradients","title":"5. Memory Leaks with Gradients\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#basic-operations","title":"Basic Operations\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#mathematical-operations-between-tensors","title":"Mathematical Operations Between Tensors\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#element-wise-operations","title":"Element-wise Operations\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#matrix-multiplication","title":"Matrix Multiplication\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#in-place-operations","title":"In-place Operations\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#tensor-reshaping-and-manipulation","title":"Tensor Reshaping and Manipulation\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#view-reshape-and-flatten","title":"View, Reshape, and Flatten\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#memory-contiguity","title":"Memory Contiguity\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#squeeze-and-unsqueeze","title":"Squeeze and Unsqueeze\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#concatenation-and-stacking","title":"Concatenation and Stacking\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#splitting-tensors","title":"Splitting Tensors\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#indexing","title":"Indexing\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#reproducibility","title":"Reproducibility\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#comprehensive-reproducibility-setup","title":"Comprehensive Reproducibility Setup\u00b6","text":""},{"location":"course/topic_02_mathematics/section_01_data_structures.html#verifying-reproducibility","title":"Verifying Reproducibility\u00b6","text":""},{"location":"course/topic_02_mathematics/section_02_basic_maths.html","title":"Basic Mathematics and Automatic Differentiation","text":"<p>In this section, some fundamental concepts of differential calculus applied to machine learning are introduced, illustrating how PyTorch allows calculating gradients automatically through its autograd system. The objective is to connect the traditional mathematical formulation (symbolic calculus) with practical implementation in code, and show how these gradients are used in typical tasks such as linear regression, logistic regression, or multiclass classification.</p> <p>The central idea is as follows: A differentiable function is defined that depends on one or more tensors with <code>requires_grad=True</code>, a scalar value is calculated from them, and <code>backward()</code> is invoked. From that moment, PyTorch traverses the computational graph it has built internally and calculates the partial derivatives of the scalar output with respect to each of the differentiable inputs, storing them in the <code>.grad</code> attribute of the corresponding tensors.</p> <p>To illustrate the parallelism between symbolic calculus and automatic differentiation, consider the scalar function of two variables:</p> <p>$$ f(x_1, x_2) = x_1^2 + 3 x_1 x_2 + x_2^2. $$</p> <p>In PyTorch, a tensor <code>x</code> with two components is defined and gradient tracking is activated:</p> In\u00a0[1]: Copied! <pre># 3pps\nimport sympy as sp\nimport torch\n\n\n# Create input tensor with gradient tracking\nx = torch.tensor([2.0, 3.0], requires_grad=True)\n\n# Define the differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2\ny = x[0] ** 2 + 3 * x[0] * x[1] + x[1] ** 2\n\n# Calculate gradients\ny.backward()\n\n# Gradients with respect to each input\ngrad_x1 = x.grad[0]  # \u2202f/\u2202x1\ngrad_x2 = x.grad[1]  # \u2202f/\u2202x2\n\nprint(\"PyTorch gradients:\")\nprint(\"Gradient \u2202f/\u2202x1:\", grad_x1)\nprint(\"Gradient \u2202f/\u2202x2:\", grad_x2)\n</pre> # 3pps import sympy as sp import torch   # Create input tensor with gradient tracking x = torch.tensor([2.0, 3.0], requires_grad=True)  # Define the differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2 y = x[0] ** 2 + 3 * x[0] * x[1] + x[1] ** 2  # Calculate gradients y.backward()  # Gradients with respect to each input grad_x1 = x.grad[0]  # \u2202f/\u2202x1 grad_x2 = x.grad[1]  # \u2202f/\u2202x2  print(\"PyTorch gradients:\") print(\"Gradient \u2202f/\u2202x1:\", grad_x1) print(\"Gradient \u2202f/\u2202x2:\", grad_x2) <pre>PyTorch gradients:\nGradient \u2202f/\u2202x1: tensor(13.)\nGradient \u2202f/\u2202x2: tensor(12.)\n</pre> <p>PyTorch automatically constructs the operation graph that leads from <code>x</code> to <code>y</code> and, when invoking <code>y.backward()</code>, calculates the partial derivatives \u2202f/\u2202x\u2081 and \u2202f/\u2202x\u2082 at the specific point <code>x = [2, 3]</code>. These derivatives are stored in <code>x.grad</code>.</p> <p>In parallel, the same function can be represented symbolically with SymPy:</p> In\u00a0[2]: Copied! <pre># Define symbolic variables\nx1, x2 = sp.symbols(\"x1 x2\")\n\n# Define the same function symbolically\nf = x1**2 + 3 * x1 * x2 + x2**2\n\n# Calculate symbolic derivatives\ndf_dx1 = sp.diff(f, x1)\ndf_dx2 = sp.diff(f, x2)\n\nprint(\"SymPy derivative formulas:\")\nprint(\"\u2202f/\u2202x1 =\", df_dx1)\nprint(\"\u2202f/\u2202x2 =\", df_dx2)\n\n# Evaluate derivatives at point (x1=2, x2=3)\ngrad_x1_sym = df_dx1.evalf(subs={x1: 2, x2: 3})\ngrad_x2_sym = df_dx2.evalf(subs={x1: 2, x2: 3})\n\nprint(\"SymPy symbolic gradients evaluated at (x1=2, x2=3):\")\nprint(\"Gradient x1:\", grad_x1_sym)\nprint(\"Gradient x2:\", grad_x2_sym)\n</pre> # Define symbolic variables x1, x2 = sp.symbols(\"x1 x2\")  # Define the same function symbolically f = x1**2 + 3 * x1 * x2 + x2**2  # Calculate symbolic derivatives df_dx1 = sp.diff(f, x1) df_dx2 = sp.diff(f, x2)  print(\"SymPy derivative formulas:\") print(\"\u2202f/\u2202x1 =\", df_dx1) print(\"\u2202f/\u2202x2 =\", df_dx2)  # Evaluate derivatives at point (x1=2, x2=3) grad_x1_sym = df_dx1.evalf(subs={x1: 2, x2: 3}) grad_x2_sym = df_dx2.evalf(subs={x1: 2, x2: 3})  print(\"SymPy symbolic gradients evaluated at (x1=2, x2=3):\") print(\"Gradient x1:\", grad_x1_sym) print(\"Gradient x2:\", grad_x2_sym) <pre>SymPy derivative formulas:\n\u2202f/\u2202x1 = 2*x1 + 3*x2\n\u2202f/\u2202x2 = 3*x1 + 2*x2\nSymPy symbolic gradients evaluated at (x1=2, x2=3):\nGradient x1: 13.0000000000000\nGradient x2: 12.0000000000000\n</pre> <p>SymPy provides closed-form symbolic expressions for derivatives and allows evaluating them at specific points. The comparison between SymPy's results and PyTorch's shows how PyTorch's automatic differentiation matches analytical derivatives, which helps validate the implementation and understand the relationship between theory and practice.</p> <p>Below are several simple examples that illustrate how PyTorch calculates derivatives in different contexts: single-variable functions, multi-variable functions, chain rule application, and simple linear and logistic models. These examples allow intuitively understanding how the autograd system tracks operations and applies the rules of differential calculus.</p> In\u00a0[3]: Copied! <pre># 3pps\nimport torch\n\n\n# Example 1: Quadratic function\n# y = x\u00b2, dy/dx = 2x\nx = torch.tensor(3.0, requires_grad=True)\ny = x**2\ny.backward()\nprint(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n</pre> # 3pps import torch   # Example 1: Quadratic function # y = x\u00b2, dy/dx = 2x x = torch.tensor(3.0, requires_grad=True) y = x**2 y.backward() print(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\") <pre>y = x\u00b2 | x=3.0, dy/dx=6.0\n</pre> <p>In this first case, the function is one-dimensional and simple. PyTorch automatically applies the power rule for derivatives and obtains dy/dx = 2x evaluated at x = 3.</p> <p>In a scenario with multiple variables, PyTorch calculates partial gradients:</p> In\u00a0[4]: Copied! <pre># Example 2: Multiple variables\n# z = 2a + 3b, dz/da = 2, dz/db = 3\na = torch.tensor(4.0, requires_grad=True)\nb = torch.tensor(5.0, requires_grad=True)\nz = 2 * a + 3 * b\nz.backward()\nprint(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")\n</pre> # Example 2: Multiple variables # z = 2a + 3b, dz/da = 2, dz/db = 3 a = torch.tensor(4.0, requires_grad=True) b = torch.tensor(5.0, requires_grad=True) z = 2 * a + 3 * b z.backward() print(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\") <pre>z = 2a + 3b | dz/da=2.0, dz/db=3.0\n</pre> <p>Here, <code>a.grad</code> contains \u2202z/\u2202a and <code>b.grad</code> contains \u2202z/\u2202b, as expected from a linear function in two variables.</p> <p>The chain rule is applied implicitly when the function is composed of several intermediate operations:</p> In\u00a0[5]: Copied! <pre># Example 3: Chain rule\n# y = (2x + 1)\u00b2, dy/dx = 4(2x + 1)\nx = torch.tensor(3.0, requires_grad=True)\ny = (2 * x + 1) ** 2\ny.backward()\nprint(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n</pre> # Example 3: Chain rule # y = (2x + 1)\u00b2, dy/dx = 4(2x + 1) x = torch.tensor(3.0, requires_grad=True) y = (2 * x + 1) ** 2 y.backward() print(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\") <pre>y = (2x+1)\u00b2 | x=3.0, dy/dx=28.0\n</pre> <p>In this case, PyTorch internally decomposes the function into elementary steps (multiplication, addition, power) and combines their derivatives following the chain rule, without the user needing to do it explicitly.</p> <p>Derivatives acquire a central role when working with linear and logistic models, as they allow quantifying how the model output changes with small variations in the inputs or parameters. The following examples show how PyTorch calculates gradients with respect to inputs in simple configurations.</p> <p>In a linear model with two features, with weights <code>w</code> and bias <code>b</code>, the output is:</p> <p>$$ y = w_1 x_1 + w_2 x_2 + b, $$</p> <p>so the derivatives with respect to the inputs are \u2202y/\u2202x\u2081 = w\u2081 and \u2202y/\u2202x\u2082 = w\u2082:</p> In\u00a0[6]: Copied! <pre># Example 4: Linear regression\n# y = w\u00b7x + b, dy/dx = w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nw = torch.tensor([0.5, -1.0])\nb = 2.0\n\ny = w[0] * x[0] + w[1] * x[1] + b\ny.backward()\n\nprint(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")\n</pre> # Example 4: Linear regression # y = w\u00b7x + b, dy/dx = w x = torch.tensor([2.0, 3.0], requires_grad=True) w = torch.tensor([0.5, -1.0]) b = 2.0  y = w[0] * x[0] + w[1] * x[1] + b y.backward()  print(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\") <pre>Linear | dy/dx1=0.5, dy/dx2=-1.0\n</pre> <p>PyTorch exactly reproduces these derivatives: the gradient of the output with respect to each component of <code>x</code> matches the corresponding weight. This behavior is what generalizes when calculating gradients with respect to model parameters during training.</p> <p>In the case of logistic regression, a sigmoid function is applied over the linear combination:</p> <p>$$ z = w_1 x_1 + w_2 x_2 + b,\\\\ y = \\sigma(z) = \\frac{1}{1 + e^{-z}}. $$</p> <p>The derivative with respect to the inputs is given by the chain rule: $\\frac{\\partial y}{\\partial x_i} = \\sigma'(z)\\, w_i$, where $\\sigma'(z) = \\sigma(z)\\,(1 - \\sigma(z))$. PyTorch handles this composition automatically:</p> In\u00a0[7]: Copied! <pre># Example 5: Logistic regression\n# y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nz = w[0] * x[0] + w[1] * x[1] + b\ny = torch.sigmoid(z)\n\ny.backward()\nprint(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\")\n</pre> # Example 5: Logistic regression # y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w x = torch.tensor([2.0, 3.0], requires_grad=True) z = w[0] * x[0] + w[1] * x[1] + b y = torch.sigmoid(z)  y.backward() print(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\") <pre>Logistic | dy/dx1=0.1250, dy/dx2=-0.2500\n</pre> <p>The values contained in <code>x.grad</code> reflect the sensitivity of the predicted probability with respect to each of the input features, and illustrate how the nonlinear activation function (the sigmoid) affects the gradient.</p> <p>In multiclass classification tasks, it is common to use a linear layer followed by a softmax function. The linear layer calculates a score or logit for each class, and softmax transforms these scores into probabilities that sum to 1. Below is a simple example with three input features and three output classes.</p> <p>Consider an input vector <code>x</code> and a weight matrix <code>W</code>, where each column of <code>W</code> can be interpreted as the weight vector associated with a class. From them, the logits are obtained as a matrix product and softmax is applied:</p> In\u00a0[8]: Copied! <pre># 3pps\nimport torch\nimport torch.nn.functional as F\n\n\n# Input features\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Weight matrix for 3 classes\nW = torch.tensor(\n    [\n        [0.2, -0.5, 0.3],\n        [0.4, 0.1, -0.2],\n        [0.1, 0.3, 0.2],\n    ],\n    requires_grad=False,\n)\nb = torch.tensor([0.0, 0.0, 0.0])\n\n# Linear scores for each class: logits = W^T x + b\nlogits = torch.matmul(x, W) + b  # shape [3]\n\n# Apply Softmax to obtain probabilities\nprobs = F.softmax(logits, dim=0)\n\n# Select the probability of the predicted class (the highest)\npred_class_idx = probs.argmax()\ntop_prob = probs[pred_class_idx]\n\n# Calculate gradients with respect to the input\ntop_prob.backward()\n\nprint(\"Multiclass Classification | Probabilities:\", probs.detach().numpy())\nprint(\"Predicted class index:\", pred_class_idx.item())\nprint(\"Gradients inputs:\", x.grad.detach().numpy())\n</pre> # 3pps import torch import torch.nn.functional as F   # Input features x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # Weight matrix for 3 classes W = torch.tensor(     [         [0.2, -0.5, 0.3],         [0.4, 0.1, -0.2],         [0.1, 0.3, 0.2],     ],     requires_grad=False, ) b = torch.tensor([0.0, 0.0, 0.0])  # Linear scores for each class: logits = W^T x + b logits = torch.matmul(x, W) + b  # shape [3]  # Apply Softmax to obtain probabilities probs = F.softmax(logits, dim=0)  # Select the probability of the predicted class (the highest) pred_class_idx = probs.argmax() top_prob = probs[pred_class_idx]  # Calculate gradients with respect to the input top_prob.backward()  print(\"Multiclass Classification | Probabilities:\", probs.detach().numpy()) print(\"Predicted class index:\", pred_class_idx.item()) print(\"Gradients inputs:\", x.grad.detach().numpy()) <pre>Multiclass Classification | Probabilities: [0.51389724 0.25519383 0.23090893]\nPredicted class index: 0\nGradients inputs: [ 0.07993404  0.1105411  -0.03809503]\n</pre> <p>In this example, <code>logits</code> is a one-dimensional tensor of size 3 containing the linear score of each class. The <code>F.softmax</code> function transforms these logits into a probability vector. Next, the probability of the class with the highest value (<code>top_prob</code>) is selected and <code>backward()</code> is called to calculate the gradient of that probability with respect to the input vector <code>x</code>.</p> <p>The resulting values in <code>x.grad</code> indicate how the probability of the predicted class would vary if each component of the input were slightly perturbed. This information can be used, for example, to analyze the model's sensitivity to input features or as the basis for explanation techniques and adversarial example generation.</p>"},{"location":"course/topic_02_mathematics/section_02_basic_maths.html#basic-mathematics-and-automatic-differentiation","title":"Basic Mathematics and Automatic Differentiation\u00b6","text":""},{"location":"course/topic_02_mathematics/section_02_basic_maths.html#gradient-calculation-pytorch-versus-sympy","title":"Gradient Calculation: PyTorch versus SymPy\u00b6","text":""},{"location":"course/topic_02_mathematics/section_02_basic_maths.html#examples","title":"Examples\u00b6","text":""},{"location":"course/topic_02_mathematics/section_02_basic_maths.html#linear-regression-and-logistic-regression","title":"Linear Regression and Logistic Regression\u00b6","text":""},{"location":"course/topic_02_mathematics/section_02_basic_maths.html#multiclass-classification","title":"Multiclass Classification\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html","title":"Applied Mathematical Concepts","text":"<p>In this section, mathematical operations frequently used in machine learning and deep learning are introduced: distance metrics, cosine similarity, and the Hadamard product. All are formulated in matrix form and illustrated with practical examples that allow understanding their usefulness in the treatment of numerical data and, in particular, vector representations (embeddings) and images.</p> <p>Distance metrics quantify the dissimilarity between vectors in a feature space. Unlike similarity measures (which increase with greater resemblance), distance measures decrease as vectors become more alike. These metrics are fundamental in many machine learning algorithms such as k-Nearest Neighbors (k-NN), clustering (K-Means, DBSCAN), and dimensionality reduction techniques.</p> In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef pairwise_euclidean_distance(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Computes the pairwise Euclidean distance matrix efficiently.\n\n    Args:\n        X: Matrix of shape (n, d) where each row is a vector\n\n    Returns:\n        Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||\n    \"\"\"\n    # Compute squared norms for each vector (n, 1)\n    X_squared = np.sum(X**2, axis=1, keepdims=True)\n\n    # Use the identity: ||x-y||\u00b2 = ||x||\u00b2 + ||y||\u00b2 - 2x\u00b7y\n    distances_squared = X_squared + X_squared.T - 2 * (X @ X.T)\n\n    # Clip to avoid numerical errors with negative values\n    distances_squared = np.maximum(distances_squared, 0)\n\n    return np.sqrt(distances_squared)\n\n\ndef pairwise_manhattan_distance(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Computes the pairwise Manhattan distance matrix.\n\n    Args:\n        X: Matrix of shape (n, d) where each row is a vector\n\n    Returns:\n        Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||_1\n    \"\"\"\n    n = X.shape[0]\n    distances = np.zeros((n, n))\n\n    for i in range(n):\n        # Broadcasting: (1, d) - (n, d) = (n, d)\n        diff = np.abs(X[i : i + 1] - X)\n        distances[i] = np.sum(diff, axis=1)\n\n    return distances\n\n\n# Example usage\nX = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n        [1, 0, 0],\n        [0, 1, 0],\n    ],\n    dtype=float,\n)\n\nprint(\"Original vectors:\\n\", X)\n\neuclidean_dist = pairwise_euclidean_distance(X)\nprint(\"\\nPairwise Euclidean distances:\\n\", euclidean_dist)\n\nmanhattan_dist = pairwise_manhattan_distance(X)\nprint(\"\\nPairwise Manhattan distances:\\n\", manhattan_dist)\n\n# Compare distances for first two vectors\nprint(f\"\\nDistance between vectors 0 and 1:\")\nprint(f\"  Euclidean: {euclidean_dist[0, 1]:.3f}\")\nprint(f\"  Manhattan: {manhattan_dist[0, 1]:.3f}\")\n</pre> # 3pps import numpy as np   def pairwise_euclidean_distance(X: np.ndarray) -&gt; np.ndarray:     \"\"\"     Computes the pairwise Euclidean distance matrix efficiently.      Args:         X: Matrix of shape (n, d) where each row is a vector      Returns:         Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||     \"\"\"     # Compute squared norms for each vector (n, 1)     X_squared = np.sum(X**2, axis=1, keepdims=True)      # Use the identity: ||x-y||\u00b2 = ||x||\u00b2 + ||y||\u00b2 - 2x\u00b7y     distances_squared = X_squared + X_squared.T - 2 * (X @ X.T)      # Clip to avoid numerical errors with negative values     distances_squared = np.maximum(distances_squared, 0)      return np.sqrt(distances_squared)   def pairwise_manhattan_distance(X: np.ndarray) -&gt; np.ndarray:     \"\"\"     Computes the pairwise Manhattan distance matrix.      Args:         X: Matrix of shape (n, d) where each row is a vector      Returns:         Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||_1     \"\"\"     n = X.shape[0]     distances = np.zeros((n, n))      for i in range(n):         # Broadcasting: (1, d) - (n, d) = (n, d)         diff = np.abs(X[i : i + 1] - X)         distances[i] = np.sum(diff, axis=1)      return distances   # Example usage X = np.array(     [         [1, 2, 3],         [4, 5, 6],         [1, 0, 0],         [0, 1, 0],     ],     dtype=float, )  print(\"Original vectors:\\n\", X)  euclidean_dist = pairwise_euclidean_distance(X) print(\"\\nPairwise Euclidean distances:\\n\", euclidean_dist)  manhattan_dist = pairwise_manhattan_distance(X) print(\"\\nPairwise Manhattan distances:\\n\", manhattan_dist)  # Compare distances for first two vectors print(f\"\\nDistance between vectors 0 and 1:\") print(f\"  Euclidean: {euclidean_dist[0, 1]:.3f}\") print(f\"  Manhattan: {manhattan_dist[0, 1]:.3f}\") <pre>Original vectors:\n [[1. 2. 3.]\n [4. 5. 6.]\n [1. 0. 0.]\n [0. 1. 0.]]\n\nPairwise Euclidean distances:\n [[0.         5.19615242 3.60555128 3.31662479]\n [5.19615242 0.         8.36660027 8.24621125]\n [3.60555128 8.36660027 0.         1.41421356]\n [3.31662479 8.24621125 1.41421356 0.        ]]\n\nPairwise Manhattan distances:\n [[ 0.  9.  5.  5.]\n [ 9.  0. 14. 14.]\n [ 5. 14.  0.  2.]\n [ 5. 14.  2.  0.]]\n\nDistance between vectors 0 and 1:\n  Euclidean: 5.196\n  Manhattan: 9.000\n</pre> <p>Cosine similarity measures the degree of similarity between two vectors based on the angle they form, rather than their magnitude. Given a pair of vectors $u, v \\in \\mathbb{R}^d$, cosine similarity is defined as:</p> <p>$$ \\text{sim}(u, v) = \\frac{u \\cdot v}{\\|u\\|\\,\\|v\\|}, $$</p> <p>where $u \\cdot v$ denotes the dot product between $u$ and $v$, and $\\|\\cdot\\|$ is the Euclidean norm. The similarity value is in the range $[-1, 1]$, although in many practical contexts (for example, with non-negative embeddings) it usually takes values between 0 and 1. When the similarity is close to 1, the vectors point approximately in the same direction; when it is close to 0, they are almost orthogonal; and when it is negative, they point in opposite directions.</p> <p>In machine learning tasks, cosine similarity is routinely used to compare vector representations of elements such as words, documents, images, or users in recommendation systems. The idea is that vectors that are angularly close represent semantically similar entities.</p> <p>Below is an example in NumPy where the cosine similarity matrix is calculated for a set of vectors (embeddings):</p> In\u00a0[2]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef normalize_matrix(matrix: np.ndarray) -&gt; np.ndarray:\n    # Normalizes each row by dividing it by its Euclidean norm\n    return matrix / np.expand_dims(\n        np.sqrt(np.sum(np.power(matrix, 2), axis=1)), axis=-1\n    )\n\n\ndef cosine_similarity(matrix: np.ndarray) -&gt; np.ndarray:\n    # Assumes normalized rows: cosine similarity coincides with dot product\n    return matrix @ matrix.T\n\n\nX = np.array(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n        [1, 0, 0],\n        [0, 1, 0],\n    ],\n    dtype=float,\n)\nprint(\"Original embeddings:\\n\", X)\n\nX_normalized = normalize_matrix(matrix=X)\nprint(\"\\nNormalized embeddings:\\n\", X_normalized)\n\nsimilarity_matrix = cosine_similarity(matrix=X_normalized)\nprint(\"\\nSimilarity matrix:\\n\", similarity_matrix)\n</pre> # 3pps import numpy as np   def normalize_matrix(matrix: np.ndarray) -&gt; np.ndarray:     # Normalizes each row by dividing it by its Euclidean norm     return matrix / np.expand_dims(         np.sqrt(np.sum(np.power(matrix, 2), axis=1)), axis=-1     )   def cosine_similarity(matrix: np.ndarray) -&gt; np.ndarray:     # Assumes normalized rows: cosine similarity coincides with dot product     return matrix @ matrix.T   X = np.array(     [         [1, 2, 3],         [4, 5, 6],         [1, 0, 0],         [0, 1, 0],     ],     dtype=float, ) print(\"Original embeddings:\\n\", X)  X_normalized = normalize_matrix(matrix=X) print(\"\\nNormalized embeddings:\\n\", X_normalized)  similarity_matrix = cosine_similarity(matrix=X_normalized) print(\"\\nSimilarity matrix:\\n\", similarity_matrix) <pre>Original embeddings:\n [[1. 2. 3.]\n [4. 5. 6.]\n [1. 0. 0.]\n [0. 1. 0.]]\n\nNormalized embeddings:\n [[0.26726124 0.53452248 0.80178373]\n [0.45584231 0.56980288 0.68376346]\n [1.         0.         0.        ]\n [0.         1.         0.        ]]\n\nSimilarity matrix:\n [[1.         0.97463185 0.26726124 0.53452248]\n [0.97463185 1.         0.45584231 0.56980288]\n [0.26726124 0.45584231 1.         0.        ]\n [0.53452248 0.56980288 0.         1.        ]]\n</pre> <p>In this example, each row of <code>X</code> represents a feature vector. The <code>normalize_matrix</code> function normalizes each vector to have norm 1. Once normalized, the matrix product <code>matrix @ matrix.T</code> produces a square matrix in which entry $(i, j)$ coincides with the cosine similarity between the $i$-th vector and the $j$-th vector. The diagonal of this matrix contains values equal to 1, since each vector has maximum similarity with itself.</p> <p>This matrix formulation allows efficiently calculating all pairwise similarities between a set of vectors, which is especially useful in tasks such as nearest neighbor search, clustering, or similarity graph construction.</p> <p>The Hadamard product is an operation defined between matrices or tensors of equal shape, consisting of element-wise multiplication. If $A$ and $B$ are matrices of the same dimension, their Hadamard product $C = A \\odot B$ is defined by:</p> <p>$$C_{ij} = A_{ij} \\cdot B_{ij}, $$</p> <p>for all indices $i, j$. Unlike standard matrix multiplication, the Hadamard product does not combine rows and columns, but acts independently at each position.</p> <p>In machine learning and deep learning, the Hadamard product is frequently used to apply masks, combine features locally, or implement certain attention and gating mechanisms, where the contribution of each element of a tensor is modulated by multiplicative factors.</p> <p>The following example in PyTorch illustrates the use of the Hadamard product to apply a mask over a data matrix:</p> In\u00a0[3]: Copied! <pre># 3pps\nimport torch\n\n\nX = torch.tensor(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n        [1, 0, 0],\n        [0, 1, 0],\n    ],\n    dtype=float,\n)\nprint(X)\n\n# Create a mask of the same size as X\nmask = torch.zeros_like(X)\nmask[0::2, 0::2] = 1\nprint(mask)\n\n# Hadamard product (element-wise)\noutput_tensor = X * mask\nprint(mask)\n</pre> # 3pps import torch   X = torch.tensor(     [         [1, 2, 3],         [4, 5, 6],         [1, 0, 0],         [0, 1, 0],     ],     dtype=float, ) print(X)  # Create a mask of the same size as X mask = torch.zeros_like(X) mask[0::2, 0::2] = 1 print(mask)  # Hadamard product (element-wise) output_tensor = X * mask print(mask) <pre>tensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [1., 0., 0.],\n        [0., 1., 0.]], dtype=torch.float64)\ntensor([[1., 0., 1.],\n        [0., 0., 0.],\n        [1., 0., 1.],\n        [0., 0., 0.]], dtype=torch.float64)\ntensor([[1., 0., 1.],\n        [0., 0., 0.],\n        [1., 0., 1.],\n        [0., 0., 0.]], dtype=torch.float64)\n</pre> <p>In this code, <code>X</code> contains the original data and <code>mask</code> is a tensor of the same shape, initialized to zeros, in which ones are assigned at certain positions (rows and columns with step 2). The product <code>X * mask</code> applies the mask over the data: where the mask equals 1, the original value is preserved; where it equals 0, the result is nullified. This pattern is very common for selecting or attenuating specific regions in matrices or higher-dimensional tensors.</p> <p>The Hadamard product becomes especially interesting when working with images, which can be represented as tensors of size $(\\text{height}, \\text{width}, \\text{channels})$. A typical application consists of segmenting an image to isolate a region of interest and, subsequently, applying a mask over the original image to highlight or suppress that region.</p> <p>The following example illustrates a workflow that combines several techniques:</p> <ol> <li>Loading an image in RGB format and visualization.</li> <li>Conversion to grayscale and intensity normalization.</li> <li>Image segmentation using clustering (K-Means) with two classes.</li> <li>Refinement of the resulting mask using morphological operations (hole filling and closing).</li> <li>Application of the refined mask over the original image using a Hadamard product, generating a \"pseudo-attention mechanism\" effect.</li> </ol> In\u00a0[4]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom scipy import ndimage\nfrom sklearn.cluster import KMeans\n\n\n# Load the image\nimage = np.asarray(\n    Image.open(\n        \"../../assets/course/topic_02_mathematics/cat_image.jpg\"\n    )\n)\nprint(f\"Image shape: {image.shape}\")\nprint(image.max(), image.min())\nplt.imshow(image)\nplt.show()\n\n# Convert to grayscale and normalize to [0, 1]\ngray_scale_image = np.mean(image / 255.0, axis=-1)\nplt.imshow(gray_scale_image, vmin=0, vmax=1, cmap=\"Grays\")\nplt.colorbar()\nplt.show()\n\nprint(f\"gray_scale_image shape: {gray_scale_image.shape}\")\ngray_scale_image.max(), gray_scale_image.min()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np from PIL import Image from scipy import ndimage from sklearn.cluster import KMeans   # Load the image image = np.asarray(     Image.open(         \"../../assets/course/topic_02_mathematics/cat_image.jpg\"     ) ) print(f\"Image shape: {image.shape}\") print(image.max(), image.min()) plt.imshow(image) plt.show()  # Convert to grayscale and normalize to [0, 1] gray_scale_image = np.mean(image / 255.0, axis=-1) plt.imshow(gray_scale_image, vmin=0, vmax=1, cmap=\"Grays\") plt.colorbar() plt.show()  print(f\"gray_scale_image shape: {gray_scale_image.shape}\") gray_scale_image.max(), gray_scale_image.min() <pre>Image shape: (531, 612, 3)\n255 0\n</pre> <pre>gray_scale_image shape: (531, 612)\n</pre> Out[4]: <pre>(np.float64(1.0), np.float64(0.011764705882352941))</pre> <p>At this point, a grayscale image is available represented as a 2D matrix of values in the range $[0, 1]$. Next, K-Means is applied to group pixels into two clusters, which can be interpreted as background and object, depending on intensity:</p> In\u00a0[5]: Copied! <pre># Prepare data for K-Means: vectorize the image\nX = np.expand_dims(gray_scale_image.flatten(), axis=-1)\n\n# Fit a K-Means model with 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n\n# Reconstruct the cluster mask with the image shape\nkmeans_mask = kmeans.labels_.reshape(gray_scale_image.shape)\nkmeans_mask.max(), kmeans_mask.min()\n\nplt.imshow(kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\nplt.colorbar()\nplt.show()\n</pre> # Prepare data for K-Means: vectorize the image X = np.expand_dims(gray_scale_image.flatten(), axis=-1)  # Fit a K-Means model with 2 clusters kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)  # Reconstruct the cluster mask with the image shape kmeans_mask = kmeans.labels_.reshape(gray_scale_image.shape) kmeans_mask.max(), kmeans_mask.min()  plt.imshow(kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\") plt.colorbar() plt.show() <p>The resulting mask may contain fragmented regions or small holes. To improve its spatial coherence, morphological operations are applied using <code>scipy.ndimage</code>:</p> In\u00a0[6]: Copied! <pre># Fill holes in the mask\nfilled_kmeans_mask = ndimage.binary_fill_holes(kmeans_mask)\nplt.imshow(filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\nplt.colorbar()\nplt.show()\n\n# Closing operation to smooth the mask and eliminate imperfections\nstructure = np.ones((5, 5))  # A larger kernel implies more interpolation\nclosed_filled_kmeans_mask = ndimage.binary_closing(\n    filled_kmeans_mask,\n    structure=structure,\n    iterations=10,\n)\nplt.imshow(closed_filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\nplt.colorbar()\nplt.show()\n</pre> # Fill holes in the mask filled_kmeans_mask = ndimage.binary_fill_holes(kmeans_mask) plt.imshow(filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\") plt.colorbar() plt.show()  # Closing operation to smooth the mask and eliminate imperfections structure = np.ones((5, 5))  # A larger kernel implies more interpolation closed_filled_kmeans_mask = ndimage.binary_closing(     filled_kmeans_mask,     structure=structure,     iterations=10, ) plt.imshow(closed_filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\") plt.colorbar() plt.show() <p>Once the mask is refined, it is applied over the original image using a Hadamard product extended to the three color channels. To do this, the binary mask is replicated along the channel axis:</p> In\u00a0[7]: Copied! <pre># Replicate the binary mask in the 3 color channels\nmask_3channels = np.repeat(\n    np.expand_dims(closed_filled_kmeans_mask, -1),\n    3,\n    axis=-1,\n)\n\n# Apply the mask to the original image (pseudo-attention mechanism)\npseudo_attention_mechanism = image * mask_3channels\nplt.imshow(pseudo_attention_mechanism)\nplt.show()\n</pre> # Replicate the binary mask in the 3 color channels mask_3channels = np.repeat(     np.expand_dims(closed_filled_kmeans_mask, -1),     3,     axis=-1, )  # Apply the mask to the original image (pseudo-attention mechanism) pseudo_attention_mechanism = image * mask_3channels plt.imshow(pseudo_attention_mechanism) plt.show() <p>This procedure acts as a \"pseudo-attention mechanism\" because the binary mask highlights a region of the image (for example, the main figure, such as a cat) and attenuates the rest. Although this is not a learned attention mechanism, like those used in modern architectures (for example, Transformers), it illustrates how an operation as simple as the Hadamard product can be used to focus processing on specific areas of an input tensor.</p> <ul> <li>How to Convert an RGB Image to Grayscale</li> </ul>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#applied-mathematical-concepts","title":"Applied Mathematical Concepts\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#distance-metrics","title":"Distance Metrics\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#euclidean-distance","title":"Euclidean Distance\u00b6","text":"<p>The Euclidean distance is the most intuitive metric, corresponding to the straight-line distance between two points in Euclidean space. For two vectors $u, v \\in \\mathbb{R}^d$, it is defined as:</p> <p>$$ d_{\\text{Euclidean}}(u, v) = \\|u - v\\| = \\sqrt{\\sum_{i=1}^{d} (u_i - v_i)^2} $$</p> <p>This metric is sensitive to the magnitude of the vectors and assumes that all dimensions contribute equally to the distance. It is particularly useful when the scale of features is meaningful and when dealing with continuous data in geometric spaces.</p> <p>Properties:</p> <ul> <li>Always non-negative: $d(u, v) \\geq 0$</li> <li>Symmetric: $d(u, v) = d(v, u)$</li> <li>Satisfies the triangle inequality: $d(u, w) \\leq d(u, v) + d(v, w)$</li> </ul>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#manhattan-distance","title":"Manhattan Distance\u00b6","text":"<p>The Manhattan distance (also known as L1 distance or taxicab distance) measures the distance between two points by summing the absolute differences of their coordinates:</p> <p>$$ d_{\\text{Manhattan}}(u, v) = \\|u - v\\|_1 = \\sum_{i=1}^{d} |u_i - v_i| $$</p> <p>The name comes from the analogy of navigating a grid-like street pattern, where movement is restricted to orthogonal directions. This metric is less sensitive to outliers than Euclidean distance and is often preferred when working with high-dimensional sparse data or when features have different units.</p> <p>Use cases:</p> <ul> <li>Recommendation systems with categorical features</li> <li>Image processing (pixel-wise comparison)</li> <li>Feature spaces where diagonal movement is not meaningful</li> </ul>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#minkowski-distance","title":"Minkowski Distance\u00b6","text":"<p>The Minkowski distance is a generalization that encompasses both Euclidean and Manhattan distances as special cases. It is defined by a parameter $p \\geq 1$:</p> <p>$$ d_{\\text{Minkowski}}(u, v) = \\left(\\sum_{i=1}^{d} |u_i - v_i|^p\\right)^{1/p} $$</p> <p>Special cases:</p> <ul> <li>$p = 1$: Manhattan distance</li> <li>$p = 2$: Euclidean distance</li> <li>$p \\to \\infty$: Chebyshev distance (maximum absolute difference across any dimension)</li> </ul> <p>The choice of $p$ determines how much the metric emphasizes large differences between coordinates. Larger values of $p$ make the distance more sensitive to the largest coordinate difference.</p>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#pairwise-distance-matrix","title":"Pairwise Distance Matrix\u00b6","text":"<p>In practice, it is often necessary to compute distances between all pairs of vectors in a dataset. For a matrix $X \\in \\mathbb{R}^{n \\times d}$ where each row represents a vector, the pairwise distance matrix $D \\in \\mathbb{R}^{n \\times n}$ contains the distance between every pair of vectors.</p> <p>For Euclidean distance, an efficient matrix formulation uses the algebraic identity:</p> <p>$$ \\|u - v\\|^2 = \\|u\\|^2 + \\|v\\|^2 - 2u \\cdot v $$</p> <p>This allows computing all pairwise distances using matrix operations:</p>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#comparison-distance-vs-similarity","title":"Comparison: Distance vs. Similarity\u00b6","text":"<p>While distance metrics measure dissimilarity, cosine similarity (discussed in the next section) measures angular similarity. The key differences are:</p> Aspect Distance Metrics Cosine Similarity Range $[0, \\infty)$ $[-1, 1]$ Magnitude sensitivity Sensitive to vector magnitude Invariant to magnitude Interpretation Geometric separation Angular alignment Best for Continuous features, clustering Text embeddings, semantic similarity <p>Distance metrics are preferred when the absolute magnitudes of features matter (e.g., physical measurements), while cosine similarity is preferred when only the direction or relative proportions matter (e.g., document term frequencies, word embeddings).</p>"},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#cosine-similarity","title":"Cosine Similarity\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#hadamard-product","title":"Hadamard Product\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#advanced-example-segmentation-and-pseudo-attention-in-images","title":"Advanced Example: Segmentation and \"Pseudo-Attention\" in Images\u00b6","text":""},{"location":"course/topic_02_mathematics/section_03_mathematical_concepts.html#bibliography","title":"Bibliography\u00b6","text":""},{"location":"course/topic_03_applications/section_01_artificial_neuron.html","title":"Artificial Neuron and Linear Models","text":"<p>Before addressing Deep Learning, it is essential to understand intelligence as the ability to process information and make goal-oriented decisions. This perspective serves as the foundation for Artificial Intelligence (AI), understood as the development of computational systems capable of emulating aspects of human behavior: learning from experience, adapting to changes in the environment, and solving problems with minimal human intervention.</p> <p>Within AI, Machine Learning focuses on designing algorithms that learn from data. Instead of explicitly defining decision rules, an objective function is specified that quantifies model performance, and its parameters are optimized from labeled or unlabeled examples. This approach, often described as software 2.0, largely replaces manual programming with learning from data.</p> <p>Deep Learning constitutes a specialization of machine learning based on deep neural networks, capable of learning hierarchical representations of information and modeling highly nonlinear relationships. Thanks to these properties, deep learning has achieved outstanding results in computer vision, natural language processing, audio analysis, and, in general, in the treatment of unstructured or high-dimensional data.</p> <p>A key aspect in the recent advancement of Deep Learning is the so-called scaling laws, which show how model performance improves systematically by increasing data volume, computational capacity, and the number of parameters. This phenomenon has enabled training large-scale models, such as large language models (LLM), which exhibit emergent capabilities of reasoning, transfer, and generalization beyond direct training data. In parallel, computational efficiency is actively researched through lighter architectures, specialized hardware (GPU, TPU), and low-level numerical optimizations.</p> <p>Neural networks store knowledge in the form of implicit memory in their parameters (weights and biases). This poses important challenges related to generalization capacity, particularly the difference between behavior on data from the same distribution as training (in-distribution) and data outside that distribution (out-of-distribution). Likewise, in continuous learning contexts, problems such as catastrophic forgetting appear, where the model loses performance on previously learned tasks when incorporating new information. These issues have driven the development of foundation models, trained generally on large data corpora and subsequently adapted to specific tasks through fine-tuning or prompting techniques.</p> <p>From a formal perspective, learning is modeled as an optimization problem: A loss function is defined that measures prediction error, and the parameters that minimize an aggregated cost function are sought. For this, gradient-based algorithms are used, supported by automatic differentiation, which allows efficiently calculating derivatives in neural networks with millions or billions of parameters. In this context, data is transformed into continuous representations through embeddings, vectors in high-dimensional spaces that capture semantic or structural relationships between represented entities (words, images, users, products, etc.).</p> <p>Deep Learning uses specialized architectures depending on data type and task: dense networks (fully connected) for tabular or moderate-dimensional data, convolutional networks (CNN) for spatial data and images, recurrent networks (RNN) and Transformers for sequences, as well as multimodal models capable of integrating information from multiple sources (text, image, audio, video). While many problems with structured data can be effectively addressed with classical Machine Learning methods, unstructured data usually requires deep networks that automatically learn complex and meaningful representations from raw data.</p> <p>In this conceptual framework, the artificial neuron emerges as a mathematical abstraction inspired by the biological neuron. In simplified form, a neuron receives an input vector $\\mathbf{x}$, applies a linear combination parameterized by weights $\\mathbf{w}$ and a bias $b$, and finally passes the result through a nonlinear activation function $\\sigma$:</p> <p>$$ z = \\mathbf{w}^\\top \\mathbf{x} + b, \\\\ \\hat{y} = \\sigma(z). $$</p> <p>This structure constitutes the basic building block from which complete layers and deep neural networks are built. On this basis, classical models such as linear regression and logistic regression are developed, which can be interpreted as neurons with an appropriate activation (linear or sigmoid).</p> <p>Linear regression and logistic regression provide the conceptual foundation of deep learning by introducing the paradigm of differentiable models: models formed by linear transformations and differentiable nonlinear functions, which allows adjusting their parameters through gradient-based optimization algorithms. This principle is common to all modern neural network architectures.</p> <p>In both cases, the starting point is the calculation of a logit or linear combination of input features:</p> <p>$$ z = \\mathbf{w}^\\top \\mathbf{x} + b, $$</p> <p>where $\\mathbf{x} \\in \\mathbb{R}^n$ is the input vector (features), $\\mathbf{w} \\in \\mathbb{R}^n$ is the weight vector, and $b \\in \\mathbb{R}$ is the bias. This value $z$ constitutes the output of the linear part of the neuron.</p> <p>In a linear model for regression, the prediction is defined as</p> <p>$$ \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b, $$</p> <p>and can take unbounded real values. This type of model is used for regression, that is, to predict continuous variables such as prices, temperatures, or physical quantities.</p> <p>In classification problems, logits are transformed into probabilities through activation functions. In binary classification, the sigmoid function is used:</p> <p>$$ \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\\\ \\hat{y} = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b), $$</p> <p>so that $\\hat{y} \\in (0, 1)$ can be interpreted as the probability of belonging to the positive class. In multiclass classification, the Softmax function is used, which from a vector of logits $\\mathbf{z} \\in \\mathbb{R}^K$ produces a probability distribution over $K$ classes:</p> <p>$$ \\mathrm{softmax}(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}, \\\\ k = 1, \\dots, K. $$</p> <p>More generally, a neural network can be described as a composition of differentiable layers:</p> <p>$$ f(\\mathbf{x}) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(\\mathbf{x}), \\\\ f_\\ell(\\mathbf{x}) = \\sigma_\\ell(\\mathbf{W}_\\ell \\mathbf{x} + \\mathbf{b}_\\ell), $$</p> <p>where each layer applies a linear transformation $\\mathbf{W}_\\ell \\mathbf{x} + \\mathbf{b}_\\ell$ followed by a nonlinear activation function $\\sigma_\\ell$. This combination allows approximating highly complex and nonlinear functions, endowing the model with great expressive capacity.</p> <p>Logistic regression is a supervised method for binary classification that explicitly models the probability of class membership. Given labeled data $(\\mathbf{x}^{(i)}, y^{(i)})$, assumed independent and identically distributed, the model learns parameters $(\\mathbf{w}, b)$ that maximize the probability of observed labels. In applications such as image classification, inputs are represented as high-dimensional vectors obtained by flattening the pixel matrices. For example, an RGB image of $64 \\times 64$ pixels is represented as a vector in $\\mathbb{R}^{12288}$.</p> <p>Learning is formalized through a loss function $\\mathcal{L}(\\hat{y}, y)$, which measures the prediction error $\\hat{y}$ against the true label $y$, and a cost function defined as the average of losses over the training set:</p> <p>$$ J(\\mathbf{w}, b) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}), $$</p> <p>where $M$ is the number of examples. In logistic regression, the logarithmic loss or log-loss is commonly used:</p> <p>$$ \\mathcal{L}(\\hat{y}, y) = - \\big( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\big), $$</p> <p>which provides well-behaved gradients and favors the convergence of optimization algorithms. In regression problems, other losses are used, such as mean squared error (MSE):</p> <p>$$ \\mathrm{MSE} = \\frac{1}{M} \\sum_{i=1}^{M} \\big(\\hat{y}^{(i)} - y^{(i)}\\big)^2, $$</p> <p>MAE (mean absolute error), or Huber loss, depending on the desired trade-off between sensitivity to outliers and numerical stability.</p> <p>It is important to emphasize that low cost on the training set does not guarantee good performance on unseen data. Overfitting appears when the model memorizes training examples instead of learning generalizable patterns. This phenomenon is favored by small datasets, excessively complex architectures, or noisy data that poorly represents the distribution of interest.</p> <p>Gradient descent is one of the fundamental algorithms for training machine learning models. Its objective is to find parameter values that minimize a cost function, so that model predictions fit observed data as well as possible.</p> <p>In the case of logistic regression, the cost function $J(\\mathbf{w}, b)$ is defined from log-loss:</p> <p>$$ J(\\mathbf{w}, b) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = -\\frac{1}{M} \\sum_{i=1}^{M} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big]. $$</p> <p>To reduce the value of $J$, partial derivatives with respect to model parameters are calculated. These derivatives define the gradient, that is, the direction in which the cost function increases most rapidly. Since the objective is to minimize $J$, the algorithm adjusts parameters in the opposite direction to the gradient:</p> <p>$$ \\frac{\\partial J}{\\partial \\mathbf{w}} = \\mathbf{d w} = \\frac{1}{M} \\sum_{i=1}^{M} (\\hat{y}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}, \\\\ \\frac{\\partial J}{\\partial b} = d b = \\frac{1}{M} \\sum_{i=1}^{M} (\\hat{y}^{(i)} - y^{(i)}). $$</p> <p>These terms indicate in what direction and with what magnitude $\\mathbf{w}$ and $b$ should be modified to decrease error. The complete gradient descent procedure is developed iteratively and can be described as:</p> <ol> <li>Parameter initialization: Initial values are assigned to $\\mathbf{w}$ and $b$, often small and random or zeros, depending on the problem.</li> <li>Forward propagation: $\\hat{y}$ is calculated from input data $X$ and the loss function $\\mathcal{L}(\\hat{y}, y)$ and cost function $J(\\mathbf{w}, b)$ are evaluated.</li> <li>Backward propagation: Partial derivatives $\\mathbf{d w}$ and $d b$ are obtained through automatic differentiation or analytical derivation, which indicate how to adjust parameters.</li> <li>Parameter update: Values of $\\mathbf{w}$ and $b$ are updated according to the rule:</li> </ol> <p>$$ \\mathbf{w} := \\mathbf{w} - \\alpha \\cdot \\mathbf{d w}, \\\\ b := b - \\alpha \\cdot d b, $$</p> <p>where $\\alpha$ is the learning rate, a hyperparameter that controls step size at each iteration. If $\\alpha$ is too large, the algorithm may diverge; if it is too small, convergence will be very slow.</p> <p>This process is repeated until reaching an acceptable minimum of $J(\\mathbf{w}, b)$, which translates into more accurate predictions. In practice, gradient descent is implemented in a vectorized manner, leveraging matrix operations on all examples (or minibatches) in parallel, which simplifies code and allows exploiting GPU computational capacity.</p> <p>Activation functions introduce nonlinearity into neural networks and allow successive layers to capture complex relationships between input variables. Without nonlinear activation functions, a composition of linear layers would be equivalent to a single linear transformation, severely limiting model capacity.</p> <p>Below, several common activation functions are defined and their curves are shown through simple Python code. NumPy is used for calculation and matplotlib for visualization.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\nfrom typing import Callable\n\n# 3pps\n# 3rd party packages\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef sigmoid(input: np.ndarray) -&gt; np.ndarray:\n    return 1 / (1 + np.exp(-input))\n\n\ndef tanh(input: np.ndarray) -&gt; np.ndarray:\n    return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))\n\n\ndef relu(input: np.ndarray) -&gt; np.ndarray:\n    return np.maximum(0, input)\n\n\ndef leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    return np.maximum(alpha * input, input)\n\n\ndef elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:\n    return np.where(input &lt; 0, alpha * (np.exp(input) - 1), input)\n\n\ndef swish(input: np.ndarray) -&gt; np.ndarray:\n    return input * sigmoid(input)\n\n\ndef gelu(input: np.ndarray) -&gt; np.ndarray:\n    return (\n        0.5 * input * (1 + tanh(math.sqrt(2 / math.pi) * (input + 0.044715 * input**3)))\n    )\n\n\nsteps = np.arange(-10, 10, 0.1)\n\n# Create a figure with subplots\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfig.suptitle(\"Activation Functions\", fontsize=16, fontweight=\"bold\")\n\n# Flatten axes array for easier iteration\naxes = axes.flatten()\n\n# List of functions and their names\nfunctions = [\n    (\"Sigmoid\", sigmoid),\n    (\"Tanh\", tanh),\n    (\"ReLU\", relu),\n    (\"LeakyReLU\", leaky_relu),\n    (\"ELU\", elu),\n    (\"Swish\", swish),\n    (\"GELU\", gelu),\n]\n\n# Plot each function\nfor idx, (name, func) in enumerate(functions):\n    axes[idx].plot(steps, func(steps), linewidth=2)\n    axes[idx].set_title(f\"{name} function\")\n    axes[idx].grid(True, alpha=0.3)\n    axes[idx].set_xlabel(\"x\")\n    axes[idx].set_ylabel(\"f(x)\")\n\n# Hide unused subplots\nfor idx in range(len(functions), len(axes)):\n    axes[idx].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Standard libraries import math from typing import Callable  # 3pps # 3rd party packages import matplotlib.pyplot as plt import numpy as np   def sigmoid(input: np.ndarray) -&gt; np.ndarray:     return 1 / (1 + np.exp(-input))   def tanh(input: np.ndarray) -&gt; np.ndarray:     return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))   def relu(input: np.ndarray) -&gt; np.ndarray:     return np.maximum(0, input)   def leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:     return np.maximum(alpha * input, input)   def elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:     return np.where(input &lt; 0, alpha * (np.exp(input) - 1), input)   def swish(input: np.ndarray) -&gt; np.ndarray:     return input * sigmoid(input)   def gelu(input: np.ndarray) -&gt; np.ndarray:     return (         0.5 * input * (1 + tanh(math.sqrt(2 / math.pi) * (input + 0.044715 * input**3)))     )   steps = np.arange(-10, 10, 0.1)  # Create a figure with subplots fig, axes = plt.subplots(3, 3, figsize=(15, 12)) fig.suptitle(\"Activation Functions\", fontsize=16, fontweight=\"bold\")  # Flatten axes array for easier iteration axes = axes.flatten()  # List of functions and their names functions = [     (\"Sigmoid\", sigmoid),     (\"Tanh\", tanh),     (\"ReLU\", relu),     (\"LeakyReLU\", leaky_relu),     (\"ELU\", elu),     (\"Swish\", swish),     (\"GELU\", gelu), ]  # Plot each function for idx, (name, func) in enumerate(functions):     axes[idx].plot(steps, func(steps), linewidth=2)     axes[idx].set_title(f\"{name} function\")     axes[idx].grid(True, alpha=0.3)     axes[idx].set_xlabel(\"x\")     axes[idx].set_ylabel(\"f(x)\")  # Hide unused subplots for idx in range(len(functions), len(axes)):     axes[idx].axis(\"off\")  plt.tight_layout() plt.show() <p>Each of these functions has particular properties regarding saturation, derivatives, symmetry, and numerical behavior:</p> <ul> <li>Sigmoid: Compresses the input value to the interval $(0, 1)$. Suitable for probabilistic outputs in binary classification, although it can suffer from gradient saturation problems.</li> <li>Tanh: Similar to sigmoid, but centered at zero, with range $(-1, 1)$. Usually provides better gradients than pure sigmoid in intermediate layers.</li> <li>ReLU (Rectified Linear Unit): Defines $\\mathrm{ReLU}(x) = \\max(0, x)$. It is one of the most used activations due to its simplicity and good behavior in deep networks.</li> <li>Leaky ReLU and ELU: Introduce a small slope in the negative part to avoid completely inactive neurons and improve gradient propagation.</li> <li>Swish and GELU: Are smooth and nonlinear modern functions, used in recent architectures (for example, Transformers), which often offer empirical performance improvements over ReLU in certain contexts.</li> </ul> <p>These functions are implemented in a differentiable way, which allows PyTorch and other libraries to automatically calculate their gradients during the training phase.</p> <p>To illustrate how all the previous elements combine\u2014neurons, activation functions, loss functions, gradient descent, and automatic differentiation\u2014a binary classification example with PyTorch on a synthetic dataset is presented.</p> <p>In this example, data is generated using the <code>make_circles</code> function from <code>scikit-learn</code>, which produces two classes in the shape of concentric circles, a nonlinearly separable problem. Next, a simple neural network is defined, trained using stochastic gradient descent, and its performance is analyzed.</p> In\u00a0[2]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\n# 3rd party packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n\n\nclass BinaryClassifier(nn.Module):\n    def __init__(self, num_classes: int) -&gt; None:\n        super().__init__()\n        self.num_classes = num_classes\n\n        # Sequential model: hidden layer + GELU activation + output layer + sigmoid\n        self.model = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.GELU(),\n            nn.Linear(16, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(input_tensor)\n\n\n# Generate circle-shaped data\nn_samples = 1000\nX, y = make_circles(n_samples, noise=0.03, random_state=42)\nX.shape, y.shape\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\n\n# Define model, loss function, and optimizer\nmodel = BinaryClassifier(num_classes=2)\nloss_function = nn.BCELoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2)\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.3, random_state=42\n)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# Convert to PyTorch tensors\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n\nprint(y_train.min(), y_train.max(), y_train.dtype)\nprint(y_test.min(), y_test.max(), y_test.dtype)\n\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.show()\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\n</pre> # Standard libraries import math  # 3pps # 3rd party packages import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split from torch import nn   class BinaryClassifier(nn.Module):     def __init__(self, num_classes: int) -&gt; None:         super().__init__()         self.num_classes = num_classes          # Sequential model: hidden layer + GELU activation + output layer + sigmoid         self.model = nn.Sequential(             nn.Linear(2, 16),             nn.GELU(),             nn.Linear(16, 1),             nn.Sigmoid(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         return self.model(input_tensor)   # Generate circle-shaped data n_samples = 1000 X, y = make_circles(n_samples, noise=0.03, random_state=42) X.shape, y.shape  plt.scatter(X[:, 0], X[:, 1], c=y) plt.show()  # Define model, loss function, and optimizer model = BinaryClassifier(num_classes=2) loss_function = nn.BCELoss() optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2)  # Split into train and test X_train, X_test, y_train, y_test = train_test_split(     X, y, stratify=y, test_size=0.3, random_state=42 ) X_train.shape, X_test.shape, y_train.shape, y_test.shape  # Convert to PyTorch tensors X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32))  print(y_train.min(), y_train.max(), y_train.dtype) print(y_test.min(), y_test.max(), y_test.dtype)  plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train) plt.show()  plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show() <pre>tensor(0.) tensor(1.) torch.float32\ntensor(0.) tensor(1.) torch.float32\n</pre> <p>A training loop by epochs is defined, using minibatches and recording both loss and accuracy on training and test sets:</p> In\u00a0[3]: Copied! <pre>num_epochs = 20\nbatch_size = 32\nnum_batches = math.ceil(len(X_train) / batch_size)\nnum_batches_test = math.ceil(len(X_test) / batch_size)\n\nplot_loss_train = []\nplot_loss_test = []\nplot_acc_train = []\nplot_acc_test = []\n\nfor epoch in range(num_epochs):\n    loss_epoch_train = []\n    loss_epoch_test = []\n    accuracy_train = []\n    accuracy_test = []\n\n    # Training phase\n    model.train()\n    for i in range(num_batches):\n        X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n        y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n        optimizer.zero_grad()\n        predictions = model(X_batch)\n        loss = loss_function(predictions, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        loss_epoch_train.append(loss.item())\n        pred_labels = (predictions &gt;= 0.5).float()\n        acc = (pred_labels == y_batch).float().mean().item() * 100\n        accuracy_train.append(acc)\n\n    # Evaluation phase\n    model.eval()\n    with torch.inference_mode():\n        for i in range(num_batches_test):\n            X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]\n            y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n            predictions_inference = model(X_test_batch)\n            loss_test = loss_function(predictions_inference, y_test_batch)\n            loss_epoch_test.append(loss_test.item())\n\n            pred_labels_test = (predictions_inference &gt;= 0.5).float()\n            acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100\n            accuracy_test.append(acc_test)\n\n    # Epoch averages\n    train_loss_mean = np.mean(loss_epoch_train)\n    test_loss_mean = np.mean(loss_epoch_test)\n    train_acc_mean = np.mean(accuracy_train)\n    test_acc_mean = np.mean(accuracy_test)\n\n    print(\n        f\"Epoch: {epoch+1}, \"\n        f\"Train Loss: {train_loss_mean:.4f}, \"\n        f\"Test Loss: {test_loss_mean:.4f}, \"\n        f\"Train Acc: {train_acc_mean:.2f}%, \"\n        f\"Test Acc: {test_acc_mean:.2f}%\"\n    )\n\n    plot_loss_train.append(train_loss_mean)\n    plot_loss_test.append(test_loss_mean)\n    plot_acc_train.append(train_acc_mean)\n    plot_acc_test.append(test_acc_mean)\n</pre> num_epochs = 20 batch_size = 32 num_batches = math.ceil(len(X_train) / batch_size) num_batches_test = math.ceil(len(X_test) / batch_size)  plot_loss_train = [] plot_loss_test = [] plot_acc_train = [] plot_acc_test = []  for epoch in range(num_epochs):     loss_epoch_train = []     loss_epoch_test = []     accuracy_train = []     accuracy_test = []      # Training phase     model.train()     for i in range(num_batches):         X_batch = X_train[i * batch_size : (i + 1) * batch_size]         y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)          optimizer.zero_grad()         predictions = model(X_batch)         loss = loss_function(predictions, y_batch)         loss.backward()         optimizer.step()          loss_epoch_train.append(loss.item())         pred_labels = (predictions &gt;= 0.5).float()         acc = (pred_labels == y_batch).float().mean().item() * 100         accuracy_train.append(acc)      # Evaluation phase     model.eval()     with torch.inference_mode():         for i in range(num_batches_test):             X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]             y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)              predictions_inference = model(X_test_batch)             loss_test = loss_function(predictions_inference, y_test_batch)             loss_epoch_test.append(loss_test.item())              pred_labels_test = (predictions_inference &gt;= 0.5).float()             acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100             accuracy_test.append(acc_test)      # Epoch averages     train_loss_mean = np.mean(loss_epoch_train)     test_loss_mean = np.mean(loss_epoch_test)     train_acc_mean = np.mean(accuracy_train)     test_acc_mean = np.mean(accuracy_test)      print(         f\"Epoch: {epoch+1}, \"         f\"Train Loss: {train_loss_mean:.4f}, \"         f\"Test Loss: {test_loss_mean:.4f}, \"         f\"Train Acc: {train_acc_mean:.2f}%, \"         f\"Test Acc: {test_acc_mean:.2f}%\"     )      plot_loss_train.append(train_loss_mean)     plot_loss_test.append(test_loss_mean)     plot_acc_train.append(train_acc_mean)     plot_acc_test.append(test_acc_mean) <pre>Epoch: 1, Train Loss: 0.6902, Test Loss: 0.6757, Train Acc: 52.72%, Test Acc: 58.33%\n</pre> <pre>Epoch: 2, Train Loss: 0.6622, Test Loss: 0.6474, Train Acc: 58.83%, Test Acc: 58.44%\nEpoch: 3, Train Loss: 0.6126, Test Loss: 0.5602, Train Acc: 66.23%, Test Acc: 89.48%\nEpoch: 4, Train Loss: 0.5160, Test Loss: 0.4604, Train Acc: 88.35%, Test Acc: 92.50%\nEpoch: 5, Train Loss: 0.4216, Test Loss: 0.3620, Train Acc: 90.22%, Test Acc: 95.00%\nEpoch: 6, Train Loss: 0.3343, Test Loss: 0.2754, Train Acc: 94.58%, Test Acc: 99.38%\nEpoch: 7, Train Loss: 0.2597, Test Loss: 0.2124, Train Acc: 97.59%, Test Acc: 100.00%\nEpoch: 8, Train Loss: 0.2022, Test Loss: 0.1658, Train Acc: 99.15%, Test Acc: 100.00%\nEpoch: 9, Train Loss: 0.1597, Test Loss: 0.1322, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 10, Train Loss: 0.1292, Test Loss: 0.1081, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 11, Train Loss: 0.1070, Test Loss: 0.0905, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 12, Train Loss: 0.0903, Test Loss: 0.0774, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 13, Train Loss: 0.0776, Test Loss: 0.0676, Train Acc: 100.00%, Test Acc: 100.00%\n</pre> <pre>Epoch: 14, Train Loss: 0.0677, Test Loss: 0.0602, Train Acc: 100.00%, Test Acc: 100.00%\n</pre> <pre>Epoch: 15, Train Loss: 0.0599, Test Loss: 0.0545, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 16, Train Loss: 0.0538, Test Loss: 0.0499, Train Acc: 100.00%, Test Acc: 100.00%\nEpoch: 17, Train Loss: 0.0487, Test Loss: 0.0462, Train Acc: 100.00%, Test Acc: 99.69%\nEpoch: 18, Train Loss: 0.0444, Test Loss: 0.0431, Train Acc: 100.00%, Test Acc: 99.69%\nEpoch: 19, Train Loss: 0.0409, Test Loss: 0.0406, Train Acc: 100.00%, Test Acc: 99.69%\nEpoch: 20, Train Loss: 0.0378, Test Loss: 0.0384, Train Acc: 100.00%, Test Acc: 99.69%\n</pre> <p>After training, loss and accuracy curves throughout epochs are plotted and the model's ability to separate classes on the test set is visualized:</p> In\u00a0[4]: Copied! <pre># Loss evolution\nplt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\")\nplt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\")\nplt.legend()\nplt.show()\n\n# Accuracy evolution\nplt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\")\nplt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\")\nplt.legend()\nplt.show()\n\n# Original test data\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\n\n# Model predictions on test set\nwith torch.inference_mode():\n    predictions = model(X_test)\n\npredictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\nplt.show()\n</pre> # Loss evolution plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\") plt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\") plt.legend() plt.show()  # Accuracy evolution plt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\") plt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\") plt.legend() plt.show()  # Original test data plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show()  # Model predictions on test set with torch.inference_mode():     predictions = model(X_test)  predictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0) plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions) plt.show()"},{"location":"course/topic_03_applications/section_01_artificial_neuron.html#artificial-neuron-and-linear-models","title":"Artificial Neuron and Linear Models\u00b6","text":""},{"location":"course/topic_03_applications/section_01_artificial_neuron.html#linear-and-logistic-regression","title":"Linear and Logistic Regression\u00b6","text":""},{"location":"course/topic_03_applications/section_01_artificial_neuron.html#gradient-descent","title":"Gradient Descent\u00b6","text":""},{"location":"course/topic_03_applications/section_01_artificial_neuron.html#activation-functions","title":"Activation Functions\u00b6","text":""},{"location":"course/topic_03_applications/section_01_artificial_neuron.html#binary-classification-example-with-a-neural-network","title":"Binary Classification Example with a Neural Network\u00b6","text":""},{"location":"course/topic_03_applications/section_02_gradient_descent.html","title":"Gradient Descent","text":"<p>Gradient descent constitutes the core of training algorithms in machine learning and deep learning. In essence, it is an iterative procedure that adjusts model parameters in the direction opposite to the gradient of the cost function, with the objective of minimizing said function. This section first presents a purely numerical example in two dimensions, to visualize descent trajectories, and then several practical examples in PyTorch that show how the gradient is used to learn the parameters of simple models.</p> <p>In this first example, a nonlinear function of two variables is defined and its gradients are calculated analytically. From several random initial points, gradient descent is applied and the trajectories are visualized in the parameter plane, which provides a geometric idea of the optimization process.</p> <p>The function considered is:</p> <p>$$ f(x_1, x_2) = \\sin(x_1)\\cos(x_2) + \\sin(0.5\\, x_1)\\cos(0.5\\, x_2), $$</p> <p>implemented in NumPy as:</p> In\u00a0[1]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Function definition\ndef function(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"The input must contain 2 elements\"\n    return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(\n        0.5 * input[:, 0]\n    ) * np.cos(0.5 * input[:, 1])\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np   # Function definition def function(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"The input must contain 2 elements\"     return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(         0.5 * input[:, 0]     ) * np.cos(0.5 * input[:, 1]) <p>Next, the partial derivatives are defined analytically, that is, the gradient $\\nabla f(x_1, x_2) = (\\partial f/\\partial x_1, \\partial f/\\partial x_2)$:</p> In\u00a0[2]: Copied! <pre># Gradient calculation (partial derivatives)\n\n\ndef gradiente(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"The input must contain 2 elements\"\n\n    df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(\n        0.5 * input[:, 0]\n    ) * np.cos(0.5 * input[:, 1])\n    df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(\n        0.5 * input[:, 0]\n    ) * np.sin(0.5 * input[:, 1])\n\n    return np.stack([df_x1, df_x2], axis=1)\n</pre> # Gradient calculation (partial derivatives)   def gradiente(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"The input must contain 2 elements\"      df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(         0.5 * input[:, 0]     ) * np.cos(0.5 * input[:, 1])     df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(         0.5 * input[:, 0]     ) * np.sin(0.5 * input[:, 1])      return np.stack([df_x1, df_x2], axis=1) <p>The gradient descent algorithm is implemented as:</p> In\u00a0[3]: Copied! <pre># Gradient descent algorithm\n\n\ndef descenso_gradiente(\n    num_puntos: int = 10,\n    num_iteraciones: int = 30,\n    learning_rate: float = 1e-3,\n):\n    dim = 2\n    # Random initialization in the domain [0, 10] x [0, 10]\n    X = np.random.rand(num_puntos, dim) * 10\n    trayectorias = [X.copy()]\n\n    for _ in range(num_iteraciones):\n        X = X - learning_rate * gradiente(input=X)\n        trayectorias.append(X.copy())\n\n    return np.array(trayectorias)\n</pre> # Gradient descent algorithm   def descenso_gradiente(     num_puntos: int = 10,     num_iteraciones: int = 30,     learning_rate: float = 1e-3, ):     dim = 2     # Random initialization in the domain [0, 10] x [0, 10]     X = np.random.rand(num_puntos, dim) * 10     trayectorias = [X.copy()]      for _ in range(num_iteraciones):         X = X - learning_rate * gradiente(input=X)         trayectorias.append(X.copy())      return np.array(trayectorias) <p>The algorithm is executed for several initial points and their trajectories are plotted in the $(x_1, x_2)$ plane:</p> In\u00a0[4]: Copied! <pre># Execute gradient descent\ntrayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)\n\n# Visualize trajectories in 2D plane\nfor i in range(trayectoria.shape[1]):\n    plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Gradient Descent Trajectories\")\nplt.grid()\nplt.show()\n</pre> # Execute gradient descent trayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)  # Visualize trajectories in 2D plane for i in range(trayectoria.shape[1]):     plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.title(\"Gradient Descent Trajectories\") plt.grid() plt.show() <p>Each curve shows how a point moves iteratively in the descent direction of $f$. This example visually illustrates the fundamental idea: the gradient indicates the direction of maximum increase, and the algorithm moves in the opposite direction to approach function minima.</p> <p>In the second example, it is shown how to apply gradient descent in PyTorch to fit a quadratic function to synthetically generated data. A relationship between time and velocity is simulated that approximately follows a parabola, with added noise:</p> In\u00a0[5]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\n\n\n# Synthetic data\ntiempo = torch.arange(0, 20).float()\nvelocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1\n\nplt.scatter(tiempo, velocidad)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Velocity\")\nplt.title(\"Synthetic data (time vs. velocity)\")\nplt.show()\n\nvelocidad.shape, tiempo.shape\n</pre> # 3pps import matplotlib.pyplot as plt import torch   # Synthetic data tiempo = torch.arange(0, 20).float() velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1  plt.scatter(tiempo, velocidad) plt.xlabel(\"Time\") plt.ylabel(\"Velocity\") plt.title(\"Synthetic data (time vs. velocity)\") plt.show()  velocidad.shape, tiempo.shape Out[5]: <pre>(torch.Size([20]), torch.Size([20]))</pre> <p>The assumed model is a quadratic function of the form</p> <p>$$\\hat{v}(t) = a t^2 + b t + c, $$</p> <p>where $(a, b, c)$ are learnable parameters:</p> In\u00a0[6]: Copied! <pre>def funcion(instante_tiempo: torch.Tensor, parameters: torch.Tensor) -&gt; torch.Tensor:\n    a, b, c = parameters\n    return a * (instante_tiempo**2) + b * instante_tiempo + c\n\n\ndef loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:\n    return (real - predicted).square().mean()\n</pre> def funcion(instante_tiempo: torch.Tensor, parameters: torch.Tensor) -&gt; torch.Tensor:     a, b, c = parameters     return a * (instante_tiempo**2) + b * instante_tiempo + c   def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:     return (real - predicted).square().mean() <p>Parameters are initialized randomly and the initial prediction is observed:</p> In\u00a0[7]: Copied! <pre>parameters = torch.randn(3, requires_grad=True)\nparameters\n\npredicciones = funcion(instante_tiempo=tiempo, parameters=parameters)\npredicciones\n</pre> parameters = torch.randn(3, requires_grad=True) parameters  predicciones = funcion(instante_tiempo=tiempo, parameters=parameters) predicciones Out[7]: <pre>tensor([  -2.0211,   -3.1661,   -6.8923,  -13.1997,  -22.0884,  -33.5582,\n         -47.6093,  -64.2415,  -83.4550, -105.2497, -129.6256, -156.5827,\n        -186.1210, -218.2405, -252.9412, -290.2232, -330.0864, -372.5307,\n        -417.5563, -465.1631], grad_fn=&lt;AddBackward0&gt;)</pre> <p>To visualize the fit, an auxiliary function is defined:</p> In\u00a0[8]: Copied! <pre>def show_preds(tiempo, real, preds: torch.Tensor):\n    plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")\n    plt.scatter(\n        tiempo,\n        preds.detach().cpu().numpy(),\n        color=\"red\",\n        label=\"Predicted\",\n    )\n    plt.legend()\n    plt.show()\n\n\nshow_preds(tiempo, velocidad, predicciones)\n</pre> def show_preds(tiempo, real, preds: torch.Tensor):     plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")     plt.scatter(         tiempo,         preds.detach().cpu().numpy(),         color=\"red\",         label=\"Predicted\",     )     plt.legend()     plt.show()   show_preds(tiempo, velocidad, predicciones) <p>The initial loss is calculated as:</p> In\u00a0[9]: Copied! <pre>perdida = loss_function(predicciones, velocidad)\nperdida\n</pre> perdida = loss_function(predicciones, velocidad) perdida Out[9]: <pre>tensor(58429.8203, grad_fn=&lt;MeanBackward0&gt;)</pre> <p>Next, a manual gradient descent step is applied: the gradient is calculated using <code>backward()</code>, parameters are updated, and gradients are reset:</p> In\u00a0[10]: Copied! <pre># Calculate gradients\nperdida.backward()\nparameters.grad\n\n# Gradient descent step\nlr = 1e-5\nparameters.data = parameters.data - lr * parameters.grad.data\nparameters.grad = None\n\n# New prediction after update\npredicciones = funcion(instante_tiempo=tiempo, parameters=parameters)\nshow_preds(tiempo, velocidad, predicciones)\n</pre> # Calculate gradients perdida.backward() parameters.grad  # Gradient descent step lr = 1e-5 parameters.data = parameters.data - lr * parameters.grad.data parameters.grad = None  # New prediction after update predicciones = funcion(instante_tiempo=tiempo, parameters=parameters) show_preds(tiempo, velocidad, predicciones) <p>To repeat this process systematically, it is encapsulated in a function:</p> In\u00a0[11]: Copied! <pre>def apply_step_training(\n    tiempo,\n    parametros_aprendibles,\n    datos_a_predecir,\n    lr: float = 1e-5,\n):\n    predicciones = funcion(instante_tiempo=tiempo, parameters=parametros_aprendibles)\n    perdida = loss_function(predicted=predicciones, real=datos_a_predecir)\n    perdida.backward()\n\n    # Update parameters without gradient tracking\n    with torch.no_grad():\n        parametros_aprendibles -= lr * parametros_aprendibles.grad\n\n    # Reset gradients\n    parametros_aprendibles.grad.zero_()\n\n    show_preds(tiempo, datos_a_predecir, predicciones)\n    return predicciones, parametros_aprendibles, perdida\n</pre> def apply_step_training(     tiempo,     parametros_aprendibles,     datos_a_predecir,     lr: float = 1e-5, ):     predicciones = funcion(instante_tiempo=tiempo, parameters=parametros_aprendibles)     perdida = loss_function(predicted=predicciones, real=datos_a_predecir)     perdida.backward()      # Update parameters without gradient tracking     with torch.no_grad():         parametros_aprendibles -= lr * parametros_aprendibles.grad      # Reset gradients     parametros_aprendibles.grad.zero_()      show_preds(tiempo, datos_a_predecir, predicciones)     return predicciones, parametros_aprendibles, perdida <p>Training is executed for several epochs:</p> In\u00a0[12]: Copied! <pre># 3pps\nfrom tqdm import tqdm\n\n\nnum_epochs = 20\nparametros_aprendibles = torch.randn(3, requires_grad=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    predicciones, parametros_aprendibles, perdida = apply_step_training(\n        tiempo=tiempo,\n        parametros_aprendibles=parametros_aprendibles,\n        datos_a_predecir=velocidad,\n    )\n    print(f\"Epoch {epoch+1}, loss: {perdida}\")\n</pre> # 3pps from tqdm import tqdm   num_epochs = 20 parametros_aprendibles = torch.randn(3, requires_grad=True)  for epoch in tqdm(range(num_epochs)):     predicciones, parametros_aprendibles, perdida = apply_step_training(         tiempo=tiempo,         parametros_aprendibles=parametros_aprendibles,         datos_a_predecir=velocidad,     )     print(f\"Epoch {epoch+1}, loss: {perdida}\") <pre>\r  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> <pre>Epoch 1, loss: 3946.77734375\n</pre> <pre>\r 10%|\u2588         | 2/20 [00:00&lt;00:01,  9.97it/s]</pre> <pre>Epoch 2, loss: 1300.262451171875\n</pre> <pre>Epoch 3, loss: 799.4542236328125\n</pre> <pre>\r 20%|\u2588\u2588        | 4/20 [00:00&lt;00:01,  9.97it/s]</pre> <pre>Epoch 4, loss: 704.6792602539062\n</pre> <pre>\r 25%|\u2588\u2588\u258c       | 5/20 [00:00&lt;00:02,  7.45it/s]</pre> <pre>Epoch 5, loss: 686.7379760742188\n</pre> <pre>Epoch 6, loss: 683.3361206054688\n</pre> <pre>\r 35%|\u2588\u2588\u2588\u258c      | 7/20 [00:00&lt;00:01,  8.38it/s]</pre> <pre>Epoch 7, loss: 682.6856079101562\n</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588      | 8/20 [00:00&lt;00:01,  8.69it/s]</pre> <pre>Epoch 8, loss: 682.5556030273438\n</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u258c     | 9/20 [00:01&lt;00:01,  8.93it/s]</pre> <pre>Epoch 9, loss: 682.5242919921875\n</pre> <pre>Epoch 10, loss: 682.5114135742188\n</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 11/20 [00:01&lt;00:00,  9.40it/s]</pre> <pre>Epoch 11, loss: 682.5022583007812\n</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 12/20 [00:01&lt;00:00,  9.51it/s]</pre> <pre>Epoch 12, loss: 682.4935913085938\n</pre> <pre>Epoch 13, loss: 682.4852294921875\n</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 14/20 [00:01&lt;00:00,  9.68it/s]</pre> <pre>Epoch 14, loss: 682.476806640625\n</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 15/20 [00:01&lt;00:00,  9.73it/s]</pre> <pre>Epoch 15, loss: 682.4683837890625\n</pre> <pre>Epoch 16, loss: 682.4600830078125\n</pre> <pre>\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 17/20 [00:01&lt;00:00,  9.89it/s]</pre> <pre>Epoch 17, loss: 682.4515380859375\n</pre> <pre>Epoch 18, loss: 682.4432373046875\n</pre> <pre>\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:02&lt;00:00,  9.92it/s]</pre> <pre>Epoch 19, loss: 682.4347534179688\n</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02&lt;00:00,  9.92it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02&lt;00:00,  9.41it/s]</pre> <pre>Epoch 20, loss: 682.4263916015625\n</pre> <pre>\n</pre> <p>This flow illustrates the key training components in PyTorch:</p> <ul> <li>Definition of a differentiable function.</li> <li>Loss calculation.</li> <li>Call to <code>backward()</code> to obtain gradients.</li> <li>Manual parameter update within a <code>torch.no_grad()</code> context.</li> <li>Gradient reset before the next iteration.</li> </ul> <p>In this part, two complementary ideas are introduced: the abstraction of a linear layer and the implementation of a linear model in PyTorch as a subclass of <code>nn.Module</code>.</p> <p>First, a function that would represent a linear layer applied to an input is sketched:</p> In\u00a0[13]: Copied! <pre>def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:\n    # tensor_entrada: (B, N)\n    # w: (N,)\n    # b: scalar\n    return tensor_entrada @ w + b\n</pre> def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:     # tensor_entrada: (B, N)     # w: (N,)     # b: scalar     return tensor_entrada @ w + b <p>And a minimalist class:</p> In\u00a0[14]: Copied! <pre>class CapaLineal:\n    def __init__(self, shape_entrada: int) -&gt; None:\n        self.w = torch.randn()\n</pre> class CapaLineal:     def __init__(self, shape_entrada: int) -&gt; None:         self.w = torch.randn() <p>Although this is just a sketch, it serves to connect with PyTorch's standard implementation using <code>nn.Module</code>. Next, a fully functional linear model is proposed:</p> In\u00a0[15]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n\n\nclass Linear(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)\n        self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        return self.weight * input_tensor + self.bias\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.model_selection import train_test_split from torch import nn   class Linear(nn.Module):     def __init__(self) -&gt; None:         super().__init__()         self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)         self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         return self.weight * input_tensor + self.bias <p>The available device is checked:</p> In\u00a0[16]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[16]: <pre>'cpu'</pre> <p>Synthetic data following a linear relationship is generated:</p> In\u00a0[17]: Copied! <pre>start = 0\nend = 1\nsteps = 0.02\nX = np.arange(start, end, steps)\n\nbias = 0.3\nweight = 0.7\ny = weight * X + bias\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n\nplt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\")\nplt.legend()\nplt.show()\n\nplt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\")\nplt.legend()\nplt.show()\n</pre> start = 0 end = 1 steps = 0.02 X = np.arange(start, end, steps)  bias = 0.3 weight = 0.7 y = weight * X + bias  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.3, random_state=42 )  X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32))  plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\") plt.legend() plt.show()  plt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\") plt.legend() plt.show() <p>The model is initialized and its parameters are inspected:</p> In\u00a0[18]: Copied! <pre>linear_model = Linear()\nlist(linear_model.parameters())\nlinear_model.state_dict()\n</pre> linear_model = Linear() list(linear_model.parameters()) linear_model.state_dict() Out[18]: <pre>OrderedDict([('weight', tensor([0.7973])), ('bias', tensor([0.0796]))])</pre> <p>Before training, the model is evaluated on the test set:</p> In\u00a0[19]: Copied! <pre>linear_model.eval()\nwith torch.no_grad():\n    predictions = linear_model(X_test)\n\npredictions\n</pre> linear_model.eval() with torch.no_grad():     predictions = linear_model(X_test)  predictions Out[19]: <pre>tensor([0.2868, 0.7014, 0.5579, 0.7971, 0.3506, 0.8450, 0.4941, 0.4782, 0.5898,\n        0.3825, 0.2709, 0.1433, 0.6695, 0.2071, 0.1274])</pre> <p>Here an important distinction is introduced: <code>torch.no_grad()</code> and <code>torch.inference_mode()</code>. From PyTorch's documentation:</p> <ul> <li><code>no_grad</code> disables gradient tracking during the block, which avoids storing information for autograd.</li> <li><code>inference_mode</code> is analogous to <code>no_grad</code> but more strict and efficient: it also disables view tracking and version counting, and ensures that tensors created in this context are not subsequently used in computations with autograd.</li> </ul> <p>In practice, <code>inference_mode</code> is recommended for inference code, where it is known that the model will not be trained or updated. This reduces overhead and increases safety against accidental parameter modifications:</p> In\u00a0[20]: Copied! <pre>with torch.inference_mode():\n    predictions_2 = linear_model(X_test)\n\npredictions_2\n\nplt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions (no_grad)\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_2 = linear_model(X_test)  predictions_2  plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions (no_grad)\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() <p>A loss function and optimizer based on PyTorch are defined:</p> In\u00a0[21]: Copied! <pre>loss_fn = nn.L1Loss()  # Mean absolute error\noptimizer = torch.optim.SGD(linear_model.parameters())\n</pre> loss_fn = nn.L1Loss()  # Mean absolute error optimizer = torch.optim.SGD(linear_model.parameters()) <p>Next, the model is trained for several epochs, iterating over training data and evaluating on test data:</p> In\u00a0[22]: Copied! <pre>num_epochs: int = 50\n\nfor epoch in range(num_epochs):\n    epoch_losses_train = []\n    epoch_losses_test = []\n\n    # Training phase\n    linear_model.train()\n    for x, y_true in zip(X_train, y_train):\n        optimizer.zero_grad()\n\n        output_model = linear_model(x)\n        loss = loss_fn(output_model, y_true)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_losses_train.append(loss.item())\n\n    # Evaluation phase\n    linear_model.eval()\n    with torch.inference_mode():\n        for x, y_true in zip(X_test, y_test):\n            output_model = linear_model(x)\n            loss = loss_fn(output_model, y_true)\n            epoch_losses_test.append(loss.item())\n\n    print(\n        f\"Epoch: {epoch+1}, \"\n        f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"\n        f\"Test Loss: {np.mean(epoch_losses_test):.4f}\"\n    )\n</pre> num_epochs: int = 50  for epoch in range(num_epochs):     epoch_losses_train = []     epoch_losses_test = []      # Training phase     linear_model.train()     for x, y_true in zip(X_train, y_train):         optimizer.zero_grad()          output_model = linear_model(x)         loss = loss_fn(output_model, y_true)          loss.backward()         optimizer.step()          epoch_losses_train.append(loss.item())      # Evaluation phase     linear_model.eval()     with torch.inference_mode():         for x, y_true in zip(X_test, y_test):             output_model = linear_model(x)             loss = loss_fn(output_model, y_true)             epoch_losses_test.append(loss.item())      print(         f\"Epoch: {epoch+1}, \"         f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"         f\"Test Loss: {np.mean(epoch_losses_test):.4f}\"     ) <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:132: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.l1_loss(input, target, reduction=self.reduction)\n</pre> <pre>Epoch: 1, Train Loss: 0.1511, Test Loss: 0.1307\nEpoch: 2, Train Loss: 0.1075, Test Loss: 0.0875\nEpoch: 3, Train Loss: 0.0659, Test Loss: 0.0542\nEpoch: 4, Train Loss: 0.0473, Test Loss: 0.0439\nEpoch: 5, Train Loss: 0.0409, Test Loss: 0.0384\nEpoch: 6, Train Loss: 0.0385, Test Loss: 0.0361\nEpoch: 7, Train Loss: 0.0363, Test Loss: 0.0342\nEpoch: 8, Train Loss: 0.0345, Test Loss: 0.0324\nEpoch: 9, Train Loss: 0.0329, Test Loss: 0.0308\nEpoch: 10, Train Loss: 0.0311, Test Loss: 0.0292\nEpoch: 11, Train Loss: 0.0293, Test Loss: 0.0273\nEpoch: 12, Train Loss: 0.0276, Test Loss: 0.0257\nEpoch: 13, Train Loss: 0.0258, Test Loss: 0.0238\nEpoch: 14, Train Loss: 0.0241, Test Loss: 0.0222\nEpoch: 15, Train Loss: 0.0223, Test Loss: 0.0204\nEpoch: 16, Train Loss: 0.0206, Test Loss: 0.0188\nEpoch: 17, Train Loss: 0.0188, Test Loss: 0.0169\nEpoch: 18, Train Loss: 0.0172, Test Loss: 0.0153\nEpoch: 19, Train Loss: 0.0153, Test Loss: 0.0134\nEpoch: 20, Train Loss: 0.0137, Test Loss: 0.0118\nEpoch: 21, Train Loss: 0.0118, Test Loss: 0.0100\nEpoch: 22, Train Loss: 0.0102, Test Loss: 0.0084\n</pre> <pre>Epoch: 23, Train Loss: 0.0084, Test Loss: 0.0068\nEpoch: 24, Train Loss: 0.0066, Test Loss: 0.0049\nEpoch: 25, Train Loss: 0.0049, Test Loss: 0.0033\nEpoch: 26, Train Loss: 0.0031, Test Loss: 0.0019\nEpoch: 27, Train Loss: 0.0014, Test Loss: 0.0006\nEpoch: 28, Train Loss: 0.0008, Test Loss: 0.0004\nEpoch: 29, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 30, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 31, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 32, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 33, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 34, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 35, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 36, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 37, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 38, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 39, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 40, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 41, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 42, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 43, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 44, Train Loss: 0.0007, Test Loss: 0.0004\n</pre> <pre>Epoch: 45, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 46, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 47, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 48, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 49, Train Loss: 0.0007, Test Loss: 0.0004\nEpoch: 50, Train Loss: 0.0007, Test Loss: 0.0004\n</pre> <p>After training, final predictions are compared with real data:</p> In\u00a0[23]: Copied! <pre>with torch.inference_mode():\n    predictions_trained = linear_model(X_test)\n\nplt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_trained = linear_model(X_test)  plt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() <p>Finally, it is illustrated how to save and load the trained model:</p> In\u00a0[24]: Copied! <pre># Save only the state dict\ntorch.save(linear_model.state_dict(), \"linear_model_state.pth\")\n\n# Load the state dict\nlinear_model_loaded = Linear()  # Create a new instance\nlinear_model_loaded.load_state_dict(\n    torch.load(\"linear_model_state.pth\", weights_only=True)\n)\nlinear_model_loaded.eval()\n\nwith torch.inference_mode():\n    predictions_loaded = linear_model_loaded(X_test)\n\nplt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions (loaded)\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> # Save only the state dict torch.save(linear_model.state_dict(), \"linear_model_state.pth\")  # Load the state dict linear_model_loaded = Linear()  # Create a new instance linear_model_loaded.load_state_dict(     torch.load(\"linear_model_state.pth\", weights_only=True) ) linear_model_loaded.eval()  with torch.inference_mode():     predictions_loaded = linear_model_loaded(X_test)  plt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions (loaded)\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show()"},{"location":"course/topic_03_applications/section_02_gradient_descent.html#gradient-descent","title":"Gradient Descent\u00b6","text":""},{"location":"course/topic_03_applications/section_02_gradient_descent.html#example-1-gradient-descent-in-a-two-dimensional-landscape","title":"Example 1: Gradient Descent in a Two-Dimensional Landscape\u00b6","text":""},{"location":"course/topic_03_applications/section_02_gradient_descent.html#example-2-fitting-a-quadratic-function-in-pytorch","title":"Example 2: Fitting a Quadratic Function in PyTorch\u00b6","text":""},{"location":"course/topic_03_applications/section_02_gradient_descent.html#example-3-manually-implemented-linear-layer-and-simple-linear-module","title":"Example 3: Manually Implemented Linear Layer and Simple Linear Module\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html","title":"Regularization Techniques","text":"<p>This document presents an integrated and progressively developed explanation of regularization and normalization techniques in machine learning and deep learning, combining both theoretical foundations and a practical implementation in PyTorch. The central example consists of a binary classification problem solved with a small neural network trained on a synthetic dataset with circular structure. The code illustrates how to implement and apply L1 (Lasso) and L2 (Ridge) regularization in practice, while the theoretical sections place these techniques in the broader context of linear regression, overfitting control, and model stability.</p> <p>The exposition follows a linear and didactic structure. It starts from the analytical solution of linear regression using the Moore\u2013Penrose pseudoinverse and motivates the need for regularization from a numerical stability and generalization perspective. Subsequently, it formalizes L1 and L2 regularization, discusses other common regularization strategies (dropout, data augmentation, early stopping, input normalization), and explains activation normalization methods (Batch Normalization and Layer Normalization). Finally, it connects this theory with a complete PyTorch implementation, highlighting how the abstract concepts of Ridge and Lasso regularization translate into concrete training code.</p> <p>The text is intended to be accessible to readers with basic knowledge of linear algebra, probability, and Python programming, but it maintains a formal and technically precise tone.</p> <p>In the context of linear regression, the objective is to estimate a parameter vector $\\mathbf{w}$ that best fits a set of training examples. Given a design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times n}$, where each row represents an example and each column a feature, and a target vector $\\mathbf{y} \\in \\mathbb{R}^N$, the classical least-squares formulation seeks to minimize the mean squared error between the predictions $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}$ and the true targets $\\mathbf{y}$.</p> <p>In the simplest case, when the matrix $\\mathbf{X}^\\top \\mathbf{X}$ is invertible and well-conditioned, it is possible to obtain an analytical solution for the model weights using the Moore\u2013Penrose pseudoinverse. The closed-form expression for the optimal weights in the least-squares sense is given by:</p> <p>$$ \\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}. $$</p> <p>This expression corresponds to the normal equations and provides a direct solution without iterative optimization. However, in many practical situations the matrix $\\mathbf{X}^\\top \\mathbf{X}$ is not well-behaved. When it is nearly singular, that is, when some of its eigenvalues are very small or close to zero, the inverse $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$ becomes numerically unstable. Small perturbations in the data can then cause large variations in the estimated parameters $\\mathbf{w}$.</p> <p>This phenomenon has two main consequences. On the one hand, it produces highly sensitive models whose predictions change drastically in response to small input variations. On the other hand, it favours overfitting, that is, the model adapts too closely to the specific noise and peculiarities of the training data, displaying poor generalization when evaluated on new, unseen examples. To alleviate these issues, it is common to introduce regularization terms into the cost function.</p> <p>Regularization introduces additional terms into the loss function that penalize excessively large parameter values. From a statistical viewpoint, regularization reduces the variance of the estimator by constraining the hypothesis space. From a numerical viewpoint, it improves the conditioning of the optimization problem and stabilizes the solution.</p> <p>In the context of linear regression, the standard mean squared error loss is given by:</p> <p>$$ \\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2. $$</p> <p>Regularization augments this objective with a penalty on the weights. The two most widely used regularization schemes are L2 regularization (Ridge Regression) and L1 regularization (Lasso Regression).</p> <p>L2 regularization adds a penalty term proportional to the square of the Euclidean norm of the weights. The Ridge loss function takes the form:</p> <p>$$ \\mathcal{L}_{\\text{Ridge}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_2^2, $$</p> <p>where $\\|\\mathbf{w}\\|_2^2 = \\sum_j w_j^2$ and $\\lambda \\ge 0$ is a hyperparameter that controls the intensity of the penalization. A larger $\\lambda$ enforces a stronger restriction on the magnitude of the weights.</p> <p>From a geometric perspective, the L2 penalty discourages solutions with large coefficients and favours weight vectors with smaller and more evenly distributed components. This behaviour leads to smoother and more stable models that are less sensitive to noise. In terms of generalization, L2 regularization reduces overfitting by penalizing complex models with large parameters. In the context of gradient-based optimization, this penalty manifests as a shrinkage of the weights towards zero at each update, a phenomenon commonly known as weight decay.</p> <p>In addition, L2 regularization improves the conditioning of the matrix $\\mathbf{X}^\\top \\mathbf{X}$ by effectively adding $\\lambda \\mathbf{I}$ to it in the normal equations, which mitigates the problems associated with small eigenvalues. The resulting solution is more robust to perturbations in the data.</p> <p>L1 regularization, on the other hand, adds a penalty term based on the sum of the absolute values of the weights. The Lasso loss function is defined as:</p> <p>$$ \\mathcal{L}_{\\text{Lasso}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_1, $$</p> <p>where $\\|\\mathbf{w}\\|_1 = \\sum_j |w_j|$. As in the Ridge case, $\\lambda$ determines the strength of the regularization.</p> <p>The key difference between L1 and L2 regularization lies in the geometry of the penalty. The L1 norm induces sparsity: it tends to drive some coefficients exactly to zero. This effect leads to models in which only a subset of features remains active, effectively performing implicit feature selection. As a result, L1-regularized models are often simpler and more interpretable, as they rely on fewer variables.</p> <p>However, in optimization problems that are already non-convex due to the presence of deep neural architectures, the interaction of the L1 penalty with gradient descent can be less straightforward than in linear regression. The absolute value introduces non-differentiability at zero, and although subgradient methods and approximate techniques are commonly used, L1 regularization is less widespread in deep learning than L2 regularization and other regularization strategies.</p> <p>In deep learning, the capacity of neural networks to approximate highly complex functions substantially increases the risk of overfitting. Regularization and normalization techniques play a fundamental role in controlling this capacity, improving generalization, and stabilizing training dynamics.</p> <p>Regularization, in this broader sense, encompasses any technique designed to limit the model\u2019s dependence on the specifics of the training data, promoting representations that remain robust when confronted with unseen examples. While L1 and L2 regularization continue to be important, deep learning also includes complementary strategies such as dropout, data augmentation, early stopping, and input normalization.</p> <p>Several regularization techniques are widely used in practical deep learning systems.</p> <p>L2 regularization (Ridge) is implemented in neural networks as weight decay, typically through a term added to the loss or directly integrated into the optimizer. It penalizes the squared magnitude of the weights, preventing them from growing excessively and favouring smoother, more generalizable solutions. It stabilizes optimization and reduces the variance of the learned parameters.</p> <p>L1 regularization (Lasso) penalizes the absolute magnitude of the weights, driving many of them towards zero. In neural networks, this induces sparsity in the parameters and can be used as a form of implicit feature selection or connection pruning. Nonetheless, in deep models it is less common than L2 and often combined with other regularization methods.</p> <p>Dropout randomly deactivates a subset of neurons during training. At each forward pass, a random binary mask is applied to the activations of selected layers, forcing the network to distribute information and avoid strong co-adaptations between units. This mechanism acts as an ensemble of sub-models that share parameters and increases robustness by preventing the model from relying too heavily on any particular path through the network.</p> <p>However, dropout introduces stochasticity into the outputs. For a fixed input, two different forward passes can produce different predictions due to different sampled masks, and some mask realizations may be suboptimal (for example, with an unusually large number of deactivated units). After training, a deterministic behaviour is usually desired for inference. One strategy consists of estimating the expected output via Monte Carlo sampling, that is, performing multiple stochastic forward passes and averaging the results. This approach not only stabilizes predictions but also yields an empirical measure of predictive uncertainty. Nevertheless, it increases inference cost. A more common and efficient alternative is to replace the stochastic dropout operation with a deterministic scaling of the activations during inference, using the expected value of the random mask. Most deep learning libraries implement this approximation automatically when switching the model from training mode to evaluation mode.</p> <p>Data augmentation generates additional training examples from existing data through label-preserving transformations, such as rotations, translations, scaling, cropping, or changes in brightness and contrast in the case of images. This technique increases the diversity of the training set and encourages the model to learn features invariant to these transformations. In doing so, it reduces overfitting and improves generalization, especially when the original dataset is relatively small.</p> <p>Early stopping monitors the model\u2019s performance on a validation set and stops training when the validation error ceases to improve or begins to deteriorate. This technique prevents the network from continuing to adapt to noise and idiosyncrasies in the training set beyond the point of minimal validation error. It can be interpreted as a form of implicit regularization because it limits the effective capacity of the model by terminating training early.</p> <p>Input normalization preprocesses the features so that they share similar scales and are centred around zero, for example by subtracting the mean and dividing by the standard deviation of each feature. This operation improves numerical stability, accelerates convergence in gradient-based optimization, and prevents certain features with large magnitudes from dominating the learning process.</p> <p>In addition to regularizing the parameters, it is crucial to control the distribution of activations within a network. During training, the internal representations produced by intermediate layers can change significantly as earlier layers are updated, a phenomenon known as internal covariate shift. This variability complicates optimization, as each layer must constantly adapt to shifting input distributions.</p> <p>Activation normalization techniques address this issue by enforcing more stable and balanced distributions of activations across the network, thereby facilitating training and enabling the use of higher learning rates.</p> <p>Batch Normalization normalizes the activations of each layer using the mean and variance computed over the examples in a mini-batch. For each feature channel, it estimates the batch mean and variance, subtracts the mean from the activations, and divides by the standard deviation. Subsequently, it applies learnable scaling and shifting parameters that allow the network to recover any necessary distribution. Batch Normalization reduces internal covariate shift, speeds up convergence, and often reduces sensitivity to hyperparameters. However, its performance depends on the mini-batch size and composition. It can be less effective or unstable when batches are very small or when the data distribution varies strongly between batches, as in certain sequence modelling or streaming scenarios.</p> <p>Layer Normalization, in contrast, performs normalization at the level of individual samples rather than across the batch. It computes the mean and variance over the features of each sample and then normalizes and re-scales the activations. This property makes Layer Normalization especially suitable for architectures such as transformers and recurrent networks, as well as for distributed training settings, because it does not require sharing statistics across different examples in a mini-batch. It provides more consistent normalization in cases where batch composition can fluctuate substantially or where very small batch sizes are used.</p> <p>Both Batch Normalization and Layer Normalization can be interpreted as mechanisms that stabilize and regularize the training process by controlling the distribution of activations, although they are not regularization techniques in the same strict sense as L1 or L2 penalties on the parameters.</p> <p>To make the preceding concepts concrete, this section describes a complete implementation of a binary classification task using PyTorch, in which L1 and L2 regularization are explicitly incorporated into the training process. Although the problem is formulated as classification rather than regression, the underlying idea of penalizing parameter norms to improve generalization is the same as in Ridge and Lasso regression.</p> <p>The implementation starts by importing the necessary libraries for numerical computation, visualization, data generation, and model definition:</p> In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n</pre> # Standard libraries import math  # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split from torch import nn <p>The <code>math</code> module provides mathematical utilities such as <code>ceil</code>, which is used to compute the number of mini-batches. The <code>matplotlib.pyplot</code> and <code>numpy</code> libraries are used for data visualization and numerical operations, respectively. PyTorch (<code>torch</code>, <code>torch.nn</code>) supplies the infrastructure for defining and training neural networks. Finally, scikit-learn functions <code>make_circles</code> and <code>train_test_split</code> are used to generate a synthetic dataset and partition it into training and test sets.</p> <p>The model is a simple feed-forward neural network designed for binary classification with two-dimensional inputs:</p> In\u00a0[2]: Copied! <pre>class BinaryClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(nn.Linear(2, 16), nn.GELU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.model(x)\n</pre> class BinaryClassifier(nn.Module):     def __init__(self):         super().__init__()         self.model = nn.Sequential(nn.Linear(2, 16), nn.GELU(), nn.Linear(16, 1))      def forward(self, x):         return self.model(x) <p>The architecture comprises an input layer, a hidden layer, and an output layer. The input layer (<code>nn.Linear(2, 16)</code>) maps two-dimensional feature vectors to a 16-dimensional hidden representation. This choice is consistent with the synthetic dataset, where each example has two features. A GELU (Gaussian Error Linear Unit) activation introduces non-linearity between the hidden layer and the output layer. This non-linear transformation allows the network to learn complex decision boundaries that cannot be captured by linear models.</p> <p>The output layer (<code>nn.Linear(16, 1)</code>) produces a single scalar per input sample, known as a logit. In binary classification, logits are typically transformed into probabilities using the sigmoid function. In this example, the logits are directly passed to a loss function (<code>BCEWithLogitsLoss</code>) that internally combines the sigmoid operation with binary cross-entropy in a numerically stable manner.</p> <p>The <code>forward</code> method defines the computation performed at each call. The input tensor <code>x</code> has shape $(N, 2)$, where $N$ is the batch size, and the output tensor has shape $(N, 1)$. This convention aligns with the shape of the label tensors used during training.</p> <p>The dataset consists of two concentric circles, a classical example of a non-linearly separable problem:</p> In\u00a0[3]: Copied! <pre>n_samples = 1000\nX, y = make_circles(n_samples, noise=0.03, random_state=42)\n</pre> n_samples = 1000 X, y = make_circles(n_samples, noise=0.03, random_state=42) <p>The function <code>make_circles</code> generates $n_{\\text{samples}}$ two-dimensional points arranged in two circular clusters. The <code>noise</code> parameter introduces Gaussian noise into the data, making the classification task more realistic and preventing perfect separability. The <code>random_state</code> parameter fixes the random seed, ensuring reproducible data generation.</p> <p>The feature matrix <code>X</code> has shape $(1000, 2)$ and contains the coordinates of the points, while the label vector <code>y</code> has shape $(1000,)$ and encodes the class of each point (0 or 1). A scatter plot allows visual inspection of the data:</p> In\u00a0[4]: Copied! <pre>plt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\n</pre> plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() <p>In this plot, points are coloured according to their class, and the concentric circular structure becomes evident. Because the classes are not linearly separable, a linear classifier cannot find a straight decision boundary that separates them, which motivates the use of a neural network with a non-linear hidden layer.</p> <p>To evaluate the generalization performance of the model, the dataset is split into training and test subsets:</p> In\u00a0[5]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.3, random_state=42\n)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, stratify=y, test_size=0.3, random_state=42 ) <p>The argument <code>test_size=0.3</code> indicates that 30% of the data is reserved for testing, while 70% is used for training. The parameter <code>stratify=y</code> ensures that both subsets preserve the original class distribution, which is important to avoid biases in the evaluation. The <code>random_state</code> parameter ensures that the split is reproducible.</p> <p>Since PyTorch models operate on tensors, the NumPy arrays are converted to <code>torch.Tensor</code> objects with appropriate data types:</p> In\u00a0[6]: Copied! <pre>X_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1)\ny_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1)\n</pre> X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1) y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1) <p>The features are cast to 32-bit floating-point numbers, the usual precision for neural network training. The labels are also converted to <code>float32</code> and reshaped from shape $(N,)$ to $(N, 1)$ using <code>unsqueeze(1)</code>, so that their shape matches the model outputs. This alignment simplifies the use of scalar-valued loss functions that operate on tensors of identical shapes.</p> <p>It is common to verify the ranges and types of the label tensors:</p> In\u00a0[7]: Copied! <pre>print(y_train.min(), y_train.max(), y_train.dtype)\nprint(y_test.min(), y_test.max(), y_test.dtype)\n</pre> print(y_train.min(), y_train.max(), y_train.dtype) print(y_test.min(), y_test.max(), y_test.dtype) <pre>tensor(0.) tensor(1.) torch.float32\ntensor(0.) tensor(1.) torch.float32\n</pre> <p>These checks confirm that labels are correctly encoded (typically 0 and 1) and use a floating-point type, as required by <code>BCEWithLogitsLoss</code>.</p> <p>Separate scatter plots of the training and test sets allow verification that both subsets are representative of the overall data distribution and preserve the circular structure.</p> <p>To mirror Ridge and Lasso regularization within the neural network setting, two functions are defined that augment the loss with L2 and L1 penalties applied to the model parameters.</p> <p>L2 regularization (Ridge) is implemented as follows:</p> In\u00a0[8]: Copied! <pre>def ridge_regularization(model, loss, alpha):\n    l2_penalty = 0.0\n    for name, param in model.named_parameters():\n        if \"bias\" not in name:\n            l2_penalty += torch.sum(param**2)\n    return loss + alpha * l2_penalty\n</pre> def ridge_regularization(model, loss, alpha):     l2_penalty = 0.0     for name, param in model.named_parameters():         if \"bias\" not in name:             l2_penalty += torch.sum(param**2)     return loss + alpha * l2_penalty <p>This function iterates over the named parameters of the model using <code>model.named_parameters()</code>. For each parameter whose name does not contain the substring <code>'bias'</code>, it adds the sum of the squared values to <code>l2_penalty</code>. Bias terms are typically excluded from regularization, because penalizing them rarely improves generalization and may introduce unnecessary constraints. The function returns the original loss plus <code>alpha</code> times the accumulated penalty. The coefficient <code>alpha</code> plays the role of $\\lambda$ in the theoretical expressions, controlling the strength of the L2 regularization.</p> <p>L1 regularization (Lasso) is implemented analogously:</p> In\u00a0[9]: Copied! <pre>def lasso_regularization(model, loss, alpha):\n    l1_penalty = 0.0\n    for name, param in model.named_parameters():\n        if \"bias\" not in name:\n            l1_penalty += torch.sum(torch.abs(param))\n    return loss + alpha * l1_penalty\n</pre> def lasso_regularization(model, loss, alpha):     l1_penalty = 0.0     for name, param in model.named_parameters():         if \"bias\" not in name:             l1_penalty += torch.sum(torch.abs(param))     return loss + alpha * l1_penalty <p>Here, the penalty is the sum of the absolute values of the weights (excluding biases). As before, the final loss is the sum of the original loss and the scaled penalty. In theory, this L1 term encourages sparsity in the weights, although in small networks with dense fully connected layers, the degree of sparsity may be modest compared to linear models with large numbers of features.</p> <p>Conceptually, these functions transfer the idea of Ridge and Lasso from the linear regression setting (where they are applied to $\\mathbf{w}$) to a neural network setting (where they are applied to all or part of the learned parameters).</p> <p>The complete training procedure is encapsulated in the <code>train_model</code> function:</p> In\u00a0[10]: Copied! <pre>def train_model(\n    model, optimizer, loss_fn, reg_fn=None, alpha=0.0, num_epochs=20, batch_size=32\n):\n\n    num_batches = math.ceil(len(X_train) / batch_size)\n    num_batches_test = math.ceil(len(X_test) / batch_size)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_losses, train_accs = [], []\n\n        for i in range(num_batches):\n            X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n            y_batch = y_train[i * batch_size : (i + 1) * batch_size]\n\n            optimizer.zero_grad()\n            logits = model(X_batch)\n            loss = loss_fn(logits, y_batch)\n\n            if reg_fn is not None:\n                loss = reg_fn(model, loss, alpha)\n\n            loss.backward()\n            optimizer.step()\n\n            preds = (torch.sigmoid(logits) &gt;= 0.5).float()\n            acc = (preds == y_batch).float().mean().item() * 100\n\n            train_losses.append(loss.item())\n            train_accs.append(acc)\n\n        model.eval()\n        test_losses, test_accs = [], []\n\n        with torch.inference_mode():\n            for i in range(num_batches_test):\n                X_batch = X_test[i * batch_size : (i + 1) * batch_size]\n                y_batch = y_test[i * batch_size : (i + 1) * batch_size]\n\n                logits = model(X_batch)\n                loss = loss_fn(logits, y_batch)\n\n                preds = (torch.sigmoid(logits) &gt;= 0.5).float()\n                acc = (preds == y_batch).float().mean().item() * 100\n\n                test_losses.append(loss.item())\n                test_accs.append(acc)\n\n        print(\n            f\"Epoch {epoch+1:02d} | \"\n            f\"Train Loss: {np.mean(train_losses):.4f} | \"\n            f\"Test Loss: {np.mean(test_losses):.4f} | \"\n            f\"Train Acc: {np.mean(train_accs):.2f}% | \"\n            f\"Test Acc: {np.mean(test_accs):.2f}%\"\n        )\n</pre> def train_model(     model, optimizer, loss_fn, reg_fn=None, alpha=0.0, num_epochs=20, batch_size=32 ):      num_batches = math.ceil(len(X_train) / batch_size)     num_batches_test = math.ceil(len(X_test) / batch_size)      for epoch in range(num_epochs):         model.train()         train_losses, train_accs = [], []          for i in range(num_batches):             X_batch = X_train[i * batch_size : (i + 1) * batch_size]             y_batch = y_train[i * batch_size : (i + 1) * batch_size]              optimizer.zero_grad()             logits = model(X_batch)             loss = loss_fn(logits, y_batch)              if reg_fn is not None:                 loss = reg_fn(model, loss, alpha)              loss.backward()             optimizer.step()              preds = (torch.sigmoid(logits) &gt;= 0.5).float()             acc = (preds == y_batch).float().mean().item() * 100              train_losses.append(loss.item())             train_accs.append(acc)          model.eval()         test_losses, test_accs = [], []          with torch.inference_mode():             for i in range(num_batches_test):                 X_batch = X_test[i * batch_size : (i + 1) * batch_size]                 y_batch = y_test[i * batch_size : (i + 1) * batch_size]                  logits = model(X_batch)                 loss = loss_fn(logits, y_batch)                  preds = (torch.sigmoid(logits) &gt;= 0.5).float()                 acc = (preds == y_batch).float().mean().item() * 100                  test_losses.append(loss.item())                 test_accs.append(acc)          print(             f\"Epoch {epoch+1:02d} | \"             f\"Train Loss: {np.mean(train_losses):.4f} | \"             f\"Test Loss: {np.mean(test_losses):.4f} | \"             f\"Train Acc: {np.mean(train_accs):.2f}% | \"             f\"Test Acc: {np.mean(test_accs):.2f}%\"         ) <p>The function receives the model, an optimizer (for example, Adam), a loss function (here, <code>BCEWithLogitsLoss</code>), an optional regularization function (<code>reg_fn</code>), the corresponding regularization coefficient <code>alpha</code>, and the training hyperparameters <code>num_epochs</code> and <code>batch_size</code>.</p> <p>The training loop performs the following tasks. First, it computes the number of mini-batches for both training and test sets using <code>math.ceil</code>. For each epoch, the model is set to training mode with <code>model.train()</code>, and two lists are initialized to store batch-wise training losses and accuracies.</p> <p>The training dataset is then processed in mini-batches. For each batch, slices of <code>X_train</code> and <code>y_train</code> are selected. The gradients are reset via <code>optimizer.zero_grad()</code>, and the model produces logits for the batch. The primary loss is computed by applying <code>loss_fn</code> to the logits and labels. If a regularization function has been provided, the loss is augmented using <code>reg_fn(model, loss, alpha)</code>, effectively adding L1 or L2 penalties to the objective.</p> <p>After computing the final loss, <code>loss.backward()</code> is called to perform backpropagation, computing gradients with respect to all parameters. The optimizer then updates the model parameters via <code>optimizer.step()</code>.</p> <p>Accuracy for the batch is computed as the percentage of correctly classified examples. Both the loss and accuracy values are accumulated for later averaging.</p> <p>Once all training batches for the epoch have been processed, the model is switched to evaluation mode using <code>model.eval()</code>. In evaluation mode, certain layers such as dropout or batch normalization (if present) adjust their behaviour accordingly. The evaluation on the test set is carried out within a <code>with torch.inference_mode():</code> context, which disables gradient calculation, reducing memory consumption and computation time. The test data is also processed in mini-batches, and for each batch the loss and accuracy are computed in the same manner as during training, but without performing backpropagation or parameter updates.</p> <p>At the end of each epoch, the function prints a summary that includes the average training loss, test loss, training accuracy, and test accuracy. This information allows one to assess both how well the model fits the training data and how well it generalizes to unseen examples. The presence or absence of regularization will influence these metrics, typically reducing overfitting by sacrificing a small amount of training performance in exchange for better test performance.</p> <p>It is worth noting that, for simplicity, this implementation accesses <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, and <code>y_test</code> as global variables. In more modular designs, data loaders (<code>torch.utils.data.DataLoader</code>) are usually employed to encapsulate batching, shuffling, and parallel loading of data.</p> <p>Finally, the script demonstrates how to train the same model under two different regularization regimes.</p> <p>Training with L2 (Ridge) regularization is configured as follows:</p> In\u00a0[11]: Copied! <pre>model = BinaryClassifier()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\nloss_fn = nn.BCEWithLogitsLoss()\n\ntrain_model(model, optimizer, loss_fn, reg_fn=ridge_regularization, alpha=0.001)\n</pre> model = BinaryClassifier() optimizer = torch.optim.Adam(model.parameters(), lr=3e-2) loss_fn = nn.BCEWithLogitsLoss()  train_model(model, optimizer, loss_fn, reg_fn=ridge_regularization, alpha=0.001) <pre>Epoch 01 | Train Loss: 0.7028 | Test Loss: 0.6888 | Train Acc: 47.63% | Test Acc: 52.92%\nEpoch 02 | Train Loss: 0.6921 | Test Loss: 0.6815 | Train Acc: 54.16% | Test Acc: 70.52%\nEpoch 03 | Train Loss: 0.6815 | Test Loss: 0.6550 | Train Acc: 58.58% | Test Acc: 62.08%\nEpoch 04 | Train Loss: 0.6493 | Test Loss: 0.5922 | Train Acc: 69.40% | Test Acc: 77.50%\nEpoch 05 | Train Loss: 0.5884 | Test Loss: 0.5198 | Train Acc: 81.80% | Test Acc: 82.81%\nEpoch 06 | Train Loss: 0.5194 | Test Loss: 0.4299 | Train Acc: 86.51% | Test Acc: 88.75%\nEpoch 07 | Train Loss: 0.4576 | Test Loss: 0.3515 | Train Acc: 90.42% | Test Acc: 97.19%\nEpoch 08 | Train Loss: 0.4061 | Test Loss: 0.2967 | Train Acc: 95.15% | Test Acc: 96.35%\nEpoch 09 | Train Loss: 0.3634 | Test Loss: 0.2501 | Train Acc: 97.71% | Test Acc: 98.12%\nEpoch 10 | Train Loss: 0.3314 | Test Loss: 0.2124 | Train Acc: 99.01% | Test Acc: 99.06%\nEpoch 11 | Train Loss: 0.3089 | Test Loss: 0.1832 | Train Acc: 99.57% | Test Acc: 99.69%\n</pre> <pre>Epoch 12 | Train Loss: 0.2931 | Test Loss: 0.1607 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 13 | Train Loss: 0.2818 | Test Loss: 0.1434 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 14 | Train Loss: 0.2739 | Test Loss: 0.1301 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 15 | Train Loss: 0.2684 | Test Loss: 0.1200 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 16 | Train Loss: 0.2646 | Test Loss: 0.1123 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 17 | Train Loss: 0.2619 | Test Loss: 0.1064 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 18 | Train Loss: 0.2599 | Test Loss: 0.1019 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 19 | Train Loss: 0.2585 | Test Loss: 0.0983 | Train Acc: 99.86% | Test Acc: 100.00%\nEpoch 20 | Train Loss: 0.2575 | Test Loss: 0.0954 | Train Acc: 99.86% | Test Acc: 100.00%\n</pre> <p>A fresh instance of <code>BinaryClassifier</code> is created, and the Adam optimizer is initialized with a relatively high learning rate of $3 \\times 10^{-2}$. The <code>BCEWithLogitsLoss</code> loss function is used because the model outputs logits. The training loop is invoked with <code>ridge_regularization</code> as the regularization function, and the regularization coefficient is set to <code>alpha = 0.001</code>. During training, this configuration adds an L2 penalty on the weights to the loss at each parameter update, analogous to Ridge regression. One typically observes smoother weight trajectories and improved generalization compared to an unregularized model.</p> <p>The model is then re-trained from scratch with L1 (Lasso) regularization:</p> In\u00a0[12]: Copied! <pre>model = BinaryClassifier()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n\ntrain_model(model, optimizer, loss_fn, reg_fn=lasso_regularization, alpha=0.001)\n</pre> model = BinaryClassifier() optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)  train_model(model, optimizer, loss_fn, reg_fn=lasso_regularization, alpha=0.001) <pre>Epoch 01 | Train Loss: 0.7016 | Test Loss: 0.6747 | Train Acc: 53.12% | Test Acc: 72.19%\nEpoch 02 | Train Loss: 0.6735 | Test Loss: 0.6372 | Train Acc: 65.12% | Test Acc: 59.69%\n</pre> <pre>Epoch 03 | Train Loss: 0.6319 | Test Loss: 0.5741 | Train Acc: 72.14% | Test Acc: 78.54%\nEpoch 04 | Train Loss: 0.5774 | Test Loss: 0.5136 | Train Acc: 85.23% | Test Acc: 89.38%\nEpoch 05 | Train Loss: 0.5209 | Test Loss: 0.4675 | Train Acc: 89.20% | Test Acc: 84.27%\nEpoch 06 | Train Loss: 0.4612 | Test Loss: 0.4022 | Train Acc: 94.05% | Test Acc: 90.31%\nEpoch 07 | Train Loss: 0.4072 | Test Loss: 0.3423 | Train Acc: 95.47% | Test Acc: 94.38%\nEpoch 08 | Train Loss: 0.3609 | Test Loss: 0.2893 | Train Acc: 96.69% | Test Acc: 97.19%\nEpoch 09 | Train Loss: 0.3210 | Test Loss: 0.2457 | Train Acc: 97.71% | Test Acc: 98.75%\nEpoch 10 | Train Loss: 0.2870 | Test Loss: 0.2104 | Train Acc: 98.44% | Test Acc: 99.38%\nEpoch 11 | Train Loss: 0.2587 | Test Loss: 0.1825 | Train Acc: 99.57% | Test Acc: 99.38%\n</pre> <pre>Epoch 12 | Train Loss: 0.2353 | Test Loss: 0.1605 | Train Acc: 99.57% | Test Acc: 99.69%\nEpoch 13 | Train Loss: 0.2164 | Test Loss: 0.1429 | Train Acc: 99.86% | Test Acc: 99.69%\n</pre> <pre>Epoch 14 | Train Loss: 0.2008 | Test Loss: 0.1286 | Train Acc: 99.86% | Test Acc: 99.69%\nEpoch 15 | Train Loss: 0.1879 | Test Loss: 0.1167 | Train Acc: 100.00% | Test Acc: 99.69%\nEpoch 16 | Train Loss: 0.1770 | Test Loss: 0.1068 | Train Acc: 100.00% | Test Acc: 99.69%\nEpoch 17 | Train Loss: 0.1678 | Test Loss: 0.0984 | Train Acc: 100.00% | Test Acc: 99.69%\nEpoch 18 | Train Loss: 0.1600 | Test Loss: 0.0914 | Train Acc: 100.00% | Test Acc: 99.69%\nEpoch 19 | Train Loss: 0.1532 | Test Loss: 0.0853 | Train Acc: 100.00% | Test Acc: 99.69%\nEpoch 20 | Train Loss: 0.1473 | Test Loss: 0.0800 | Train Acc: 100.00% | Test Acc: 99.69%\n</pre> <p>The reinitialization step ensures that both experiments start from comparable initial conditions, making the comparison between L2 and L1 regularization more meaningful. The optimizer and loss function remain the same, but the regularization function is now <code>lasso_regularization</code>. L1 regularization tends to promote sparsity in the parameter space, potentially zeroing out some weights. In small networks, this effect may be less visually apparent than in large linear models, but it still encourages simpler solutions.</p> <p>By comparing the printed training and test losses and accuracies for both configurations, one can empirically observe the influence of L2 and L1 regularization on the learning dynamics and generalization performance of the model. In particular, it is often the case that L2 regularization yields smooth and stable decision boundaries, while L1 regularization, when appropriately tuned, can lead to slightly more compact or sparse parameter configurations.</p> <p>Monte Carlo Dropout constitutes an extension of the traditional use of Dropout in neural networks. This technique is simultaneously interpreted as a regularization mechanism during training and as an approximate Bayesian inference method during the prediction phase. Both perspectives are complementary and are supported by a common theoretical framework that connects deep learning with probabilistic models.</p> <p>In its classical formulation, Dropout is introduced as a regularization strategy aimed at reducing overfitting in deep neural networks. During training, certain neurons\u2014or, more precisely, their activations\u2014are randomly deactivated with a preset probability. This procedure modifies the effective architecture of the network in each forward pass, which induces behavior similar to training an ensemble of smaller models that share parameters.</p> <p>From a functional perspective, the random deactivation of neurons forces the network not to depend excessively on specific units, which reduces co-adaptation between them. The model is forced to distribute relevant information across multiple pathways and to learn internal representations that are redundant and robust against the absence of certain nodes. This effect is particularly beneficial in scenarios with a high number of parameters and limited-size datasets, where the risk of overfitting is .</p> <p>In some cases, it has been demonstrated that the use of Dropout can be interpreted as an approximate form of L2 regularization on the network's weights. This equivalence is established under certain hypotheses about the architecture and type of layers employed, and allows understanding Dropout as a mechanism that implicitly penalizes excessively complex parameter configurations, favoring simpler and better generalizable solutions.</p> <p>A basic  of implementing Dropout as a regularizer in a network defined with PyTorch is as follows:</p> In\u00a0[13]: Copied! <pre># 3pps\nimport torch.nn as nn\n\n\n# Classic Dropout in training\nmodel = nn.Sequential(\n    nn.Linear(100, 50), nn.Dropout(p=0.5), nn.Linear(50, 10)  # Regularization\n)\n</pre> # 3pps import torch.nn as nn   # Classic Dropout in training model = nn.Sequential(     nn.Linear(100, 50), nn.Dropout(p=0.5), nn.Linear(50, 10)  # Regularization ) <p>In this context, Dropout is used in a standard manner: During training, the random shutdown of neurons is activated, while during inference it is deactivated, using the network deterministically.</p> <p>The idea of Monte Carlo Dropout arises when one decides to keep Dropout active also during inference. Instead of making a single deterministic prediction with the complete network, multiple forward passes are executed over the same  data, applying Dropout in each of them. This produces a set of stochastic predictions that can be interpreted as samples from an approximate predictive distribution.</p> <p>From a practical point of view, this procedure allows  both an average prediction and an associated uncertainty measure. The mean of the stochastic predictions is used as the point output of the model, while the dispersion of said predictions (for , their standard deviation) offers an approximation to epistemic uncertainty, that is, the uncertainty derived from the model's lack of knowledge about the data.</p> <p>This  can be implemented in PyTorch in the following manner:</p> In\u00a0[14]: Copied! <pre># 3pps\nimport torch\n\n\n# Monte Carlo Dropout in inference\ndef mc_dropout_prediction(model, x, n_samples=100):\n    model.train()  # Keeps dropout active also in inference\n    predictions = []\n\n    for _ in range(n_samples):\n        with torch.no_grad():\n            predictions.append(model(x))\n\n    predictions = torch.stack(predictions)\n    mean = predictions.mean(dim=0)\n    uncertainty = predictions.std(dim=0)\n\n    return mean, uncertainty\n</pre> # 3pps import torch   # Monte Carlo Dropout in inference def mc_dropout_prediction(model, x, n_samples=100):     model.train()  # Keeps dropout active also in inference     predictions = []      for _ in range(n_samples):         with torch.no_grad():             predictions.append(model(x))      predictions = torch.stack(predictions)     mean = predictions.mean(dim=0)     uncertainty = predictions.std(dim=0)      return mean, uncertainty <p>In this code, the call to <code>model.train()</code> during the inference phase is intentional. Although under usual conditions this instruction is exclusively associated with training, here it is used so that the Dropout layers remain active and continue eliminating units randomly in each pass. In this way, each evaluation of the model on the same  generates a slightly different output, which allows constructing an empirical distribution of predictions.</p> <p>The mean of this distribution acts as a more robust point estimate, while the standard deviation provides a measure of dispersion that is interpreted as uncertainty. The greater this deviation, the greater the model's uncertainty regarding the prediction made.</p> <p>The relevance of Monte Carlo Dropout goes beyond its practical utility as a regularization tool or uncertainty estimation. Gal and Ghahramani (2016) demonstrate that training a neural network with Dropout and keeping it active during inference is mathematically equivalent to performing variational inference in a Bayesian model, specifically in an approximation of a Gaussian process over the functions represented by the network.</p> <p>In formal terms, the use of Dropout can be interpreted as the introduction of prior distributions over the network's weights, while the randomness induced in predictions during inference corresponds to the approximation of the posterior distribution over functions. The Monte Carlo procedure\u2014that is, the repetition of stochastic predictions\u2014allows approximately sampling from that posterior distribution, providing not only a point prediction, but also an explicit measure of the associated uncertainty.</p> <p>This equivalence situates Monte Carlo Dropout within the framework of variational Bayesian inference. The model ceases to be understood simply as a deterministic network with fixed weights and comes to be interpreted as a family of functions parameterized by stochastic latent variables. Inference is no longer limited to finding a single set of optimal parameters, but to approximating a distribution over said parameters or, equivalently, over the model's outputs.</p>"},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#regularization-techniques","title":"Regularization Techniques\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#linear-regression-and-the-moorepenrose-pseudoinverse","title":"Linear Regression and the Moore\u2013Penrose Pseudoinverse\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#regularization-in-linear-models","title":"Regularization in Linear Models\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#l2-regularization-ridge-regression","title":"L2 Regularization (Ridge Regression)\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#l1-regularization-lasso-regression","title":"L1 Regularization (Lasso Regression)\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#regularization-and-normalization","title":"Regularization and Normalization\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#common-regularization-techniques","title":"Common Regularization Techniques\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#activation-normalization-batch-and-layer-normalization","title":"Activation Normalization: Batch and Layer Normalization\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#binary-classification-with-regularization","title":"Binary Classification with Regularization\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#monte-carlo-dropout","title":"Monte Carlo Dropout\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#dropout","title":"Dropout\u00b6","text":""},{"location":"course/topic_03_applications/section_03_regularization_techniques.html#monte-carlo-dropout-as-approximate-bayesian-inference","title":"Monte Carlo Dropout as Approximate Bayesian Inference\u00b6","text":""},{"location":"course/topic_03_applications/section_04_optimizers.html","title":"Optimizers","text":"<p>In order to improve computational efficiency, gradient-based optimization in deep learning does not usually rely on the full dataset at each update step. Instead, it employs stochastic gradient descent (SGD), which uses small subsets of the data known as mini-batches. This strategy introduces stochasticity into the optimization process, reduces computational cost per update, and helps escape problematic regions of the loss landscape, such as saddle points, where the gradient vanishes without corresponding to a true minimum.</p> <p>Basic gradient descent can be inefficient or unstable in certain scenarios, particularly when the loss surface exhibits strong anisotropy or narrow valleys. For this reason, several variants have been developed to improve convergence speed and robustness. Among the most widely used are Momentum, RMSprop, and Adam, all of which build on the idea of adapting the update step based on past gradient information.</p> <p>Momentum augments SGD with an inertia term that accumulates gradient information over time, thereby smoothing the updates. It is defined by the following equations:</p> <p>$$ v_t = \\beta v_{t-1} + (1 - \\beta)\\,\\nabla_\\theta \\mathcal{L}(\\theta_t), $$</p> <p>$$ \\theta_{t+1} = \\theta_t - \\eta\\,v_t, $$</p> <p>where $v_t$ denotes the accumulated \"velocity\", $\\beta \\in [0, 1)$ is a decay coefficient, and $\\eta$ is the learning rate. In practice, $\\beta$ is often set to $0.9$. This mechanism reduces oscillations in directions of high curvature and accelerates convergence along narrow valleys by integrating information across multiple steps instead of relying solely on the current gradient.</p> <p>RMSprop is an adaptive learning rate method that rescales the gradients by a moving average of their squared values. It is defined as:</p> <p>$$ s_t = \\rho s_{t-1} + (1 - \\rho)\\left(\\nabla_\\theta \\mathcal{L}(\\theta_t)\\right)^2, $$</p> <p>$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}}\\,\\nabla_\\theta \\mathcal{L}(\\theta_t), $$</p> <p>where $\\rho \\approx 0.9$ and $\\epsilon \\approx 10^{-8}$ is a small constant introduced to avoid division by zero. In this formulation, parameters associated with consistently large gradients are updated with smaller steps, while those associated with small gradients receive relatively larger steps. This adaptive behaviour improves the stability of training and makes the optimizer more robust to poorly scaled loss landscapes.</p> <p>Adam (Adaptive Moment Estimation) combines the advantages of Momentum and RMSprop by maintaining both an exponential moving average of the gradients (first moment) and an exponential moving average of their squared values (second moment). The algorithm proceeds in four stages.</p> <p>First, it computes the exponentially weighted moving average of the gradients (first moment):</p> <p>$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\,\\nabla_\\theta \\mathcal{L}(\\theta_t). $$</p> <p>Second, it computes the exponentially weighted moving average of the squared gradients (second moment):</p> <p>$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)\\,\\left(\\nabla_\\theta \\mathcal{L}(\\theta_t)\\right)^2. $$</p> <p>Third, it performs a bias correction step to compensate for the initialization of $m_t$ and $v_t$ at zero:</p> <p>$$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}. $$</p> <p>Finally, it updates the parameters using the bias-corrected moments:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\,\\hat{m}_t. $$</p> <p>Typical recommended values are $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and $\\epsilon = 10^{-8}$. Adam is widely adopted in deep learning due to its fast convergence, numerical stability, and robustness to suboptimal hyperparameter configurations. It adapts the learning rate per parameter based on both the magnitude and the variance of the gradients, while also leveraging momentum-like smoothing.</p> <p>As an illustrative example, the following implementation compares these optimizers on the simple one-dimensional test function $f(\\theta) = \\theta^2$, whose global minimum is located at $\\theta = 0$. In this setting, all optimizers start from an initial value $\\theta = 5$ and attempt to reduce the loss. Although each algorithm follows a different trajectory through parameter space, they all tend toward the global minimum at $\\theta = 0$.</p> In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n\n\n# Loss function and gradient\nloss = lambda theta: theta**2\ngrad = lambda theta: 2 * theta\n\n# Initial value\ntheta_init = 5.0\n\n\n# Stochastic Gradient Descent (SGD)\ndef sgd(theta, grad, eta=0.1, steps=20):\n    for t in range(steps):\n        theta -= eta * grad(theta)\n    return theta\n\n\n# Momentum\ndef momentum(theta, grad, eta=0.1, beta=0.9, steps=20):\n    v = 0\n    for t in range(steps):\n        v = beta * v + (1 - beta) * grad(theta)\n        theta -= eta * v\n    return theta\n\n\n# RMSprop\ndef rmsprop(theta, grad, eta=0.1, rho=0.9, eps=1e-8, steps=20):\n    s = 0\n    for t in range(steps):\n        g = grad(theta)\n        s = rho * s + (1 - rho) * g**2\n        theta -= eta / (np.sqrt(s) + eps) * g\n    return theta\n\n\n# Adam\ndef adam(theta, grad, eta=0.1, beta1=0.9, beta2=0.999, eps=1e-8, steps=20):\n    m, v = 0, 0\n    for t in range(1, steps + 1):\n        g = grad(theta)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g**2\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        theta -= eta / (np.sqrt(v_hat) + eps) * m_hat\n    return theta\n\n\nprint(\"SGD:\", sgd(theta_init, grad))\nprint(\"Momentum:\", momentum(theta_init, grad))\nprint(\"RMSprop:\", rmsprop(theta_init, grad))\nprint(\"Adam:\", adam(theta_init, grad))\n</pre> # 3pps import numpy as np   # Loss function and gradient loss = lambda theta: theta**2 grad = lambda theta: 2 * theta  # Initial value theta_init = 5.0   # Stochastic Gradient Descent (SGD) def sgd(theta, grad, eta=0.1, steps=20):     for t in range(steps):         theta -= eta * grad(theta)     return theta   # Momentum def momentum(theta, grad, eta=0.1, beta=0.9, steps=20):     v = 0     for t in range(steps):         v = beta * v + (1 - beta) * grad(theta)         theta -= eta * v     return theta   # RMSprop def rmsprop(theta, grad, eta=0.1, rho=0.9, eps=1e-8, steps=20):     s = 0     for t in range(steps):         g = grad(theta)         s = rho * s + (1 - rho) * g**2         theta -= eta / (np.sqrt(s) + eps) * g     return theta   # Adam def adam(theta, grad, eta=0.1, beta1=0.9, beta2=0.999, eps=1e-8, steps=20):     m, v = 0, 0     for t in range(1, steps + 1):         g = grad(theta)         m = beta1 * m + (1 - beta1) * g         v = beta2 * v + (1 - beta2) * g**2         m_hat = m / (1 - beta1**t)         v_hat = v / (1 - beta2**t)         theta -= eta / (np.sqrt(v_hat) + eps) * m_hat     return theta   print(\"SGD:\", sgd(theta_init, grad)) print(\"Momentum:\", momentum(theta_init, grad)) print(\"RMSprop:\", rmsprop(theta_init, grad)) print(\"Adam:\", adam(theta_init, grad)) <pre>SGD: 0.057646075230342354\nMomentum: -1.3533566908039512\nRMSprop: 2.451835572498659\nAdam: 3.060338887144194\n</pre> <p>The following example demonstrates how different optimizers perform when training a small neural network on the circle dataset used in the regularization examples:</p> In\u00a0[2]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n\n\n# Generate and prepare data\nn_samples = 1000\nX, y = make_circles(n_samples, noise=0.03, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.3, random_state=42\n)\n\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1)\ny_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1)\n\n\n# Simple neural network\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# Training function\ndef train_with_optimizer(optimizer_name, learning_rate=0.01, num_epochs=100):\n    model = BinaryClassifier()\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    # Select optimizer\n    if optimizer_name == \"SGD\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    elif optimizer_name == \"Momentum\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    elif optimizer_name == \"RMSprop\":\n        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n    elif optimizer_name == \"Adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    train_losses = []\n    test_losses = []\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        optimizer.zero_grad()\n        logits = model(X_train)\n        loss = loss_fn(logits, y_train)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n        # Evaluation\n        model.eval()\n        with torch.no_grad():\n            test_logits = model(X_test)\n            test_loss = loss_fn(test_logits, y_test)\n            test_losses.append(test_loss.item())\n\n    return train_losses, test_losses\n\n\n# Compare optimizers\noptimizers = [\"SGD\", \"Momentum\", \"RMSprop\", \"Adam\"]\nresults = {}\n\nfor opt_name in optimizers:\n    print(f\"Training with {opt_name}...\")\n    train_loss, test_loss = train_with_optimizer(opt_name, learning_rate=0.01)\n    results[opt_name] = {\"train\": train_loss, \"test\": test_loss}\n    print(f\"  Final train loss: {train_loss[-1]:.4f}\")\n    print(f\"  Final test loss: {test_loss[-1]:.4f}\\n\")\n\n# Plot comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nfor opt_name in optimizers:\n    plt.plot(results[opt_name][\"train\"], label=opt_name)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss Comparison\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nfor opt_name in optimizers:\n    plt.plot(results[opt_name][\"test\"], label=opt_name)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Test Loss\")\nplt.title(\"Test Loss Comparison\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split from torch import nn   # Generate and prepare data n_samples = 1000 X, y = make_circles(n_samples, noise=0.03, random_state=42)  X_train, X_test, y_train, y_test = train_test_split(     X, y, stratify=y, test_size=0.3, random_state=42 )  X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1) y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1)   # Simple neural network class BinaryClassifier(nn.Module):     def __init__(self):         super().__init__()         self.model = nn.Sequential(             nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)         )      def forward(self, x):         return self.model(x)   # Training function def train_with_optimizer(optimizer_name, learning_rate=0.01, num_epochs=100):     model = BinaryClassifier()     loss_fn = nn.BCEWithLogitsLoss()      # Select optimizer     if optimizer_name == \"SGD\":         optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)     elif optimizer_name == \"Momentum\":         optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)     elif optimizer_name == \"RMSprop\":         optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)     elif optimizer_name == \"Adam\":         optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)     else:         raise ValueError(f\"Unknown optimizer: {optimizer_name}\")      train_losses = []     test_losses = []      for epoch in range(num_epochs):         # Training         model.train()         optimizer.zero_grad()         logits = model(X_train)         loss = loss_fn(logits, y_train)         loss.backward()         optimizer.step()         train_losses.append(loss.item())          # Evaluation         model.eval()         with torch.no_grad():             test_logits = model(X_test)             test_loss = loss_fn(test_logits, y_test)             test_losses.append(test_loss.item())      return train_losses, test_losses   # Compare optimizers optimizers = [\"SGD\", \"Momentum\", \"RMSprop\", \"Adam\"] results = {}  for opt_name in optimizers:     print(f\"Training with {opt_name}...\")     train_loss, test_loss = train_with_optimizer(opt_name, learning_rate=0.01)     results[opt_name] = {\"train\": train_loss, \"test\": test_loss}     print(f\"  Final train loss: {train_loss[-1]:.4f}\")     print(f\"  Final test loss: {test_loss[-1]:.4f}\\n\")  # Plot comparison plt.figure(figsize=(12, 5))  plt.subplot(1, 2, 1) for opt_name in optimizers:     plt.plot(results[opt_name][\"train\"], label=opt_name) plt.xlabel(\"Epoch\") plt.ylabel(\"Training Loss\") plt.title(\"Training Loss Comparison\") plt.legend() plt.grid(True)  plt.subplot(1, 2, 2) for opt_name in optimizers:     plt.plot(results[opt_name][\"test\"], label=opt_name) plt.xlabel(\"Epoch\") plt.ylabel(\"Test Loss\") plt.title(\"Test Loss Comparison\") plt.legend() plt.grid(True)  plt.tight_layout() plt.show() <pre>Training with SGD...\n</pre> <pre>  Final train loss: 0.6941\n  Final test loss: 0.6936\n\nTraining with Momentum...\n  Final train loss: 0.6916\n  Final test loss: 0.6911\n\nTraining with RMSprop...\n  Final train loss: 0.0686\n  Final test loss: 0.0672\n\nTraining with Adam...\n</pre> <pre>  Final train loss: 0.0249\n  Final test loss: 0.0248\n\n</pre>"},{"location":"course/topic_03_applications/section_04_optimizers.html#optimizers","title":"Optimizers\u00b6","text":""},{"location":"course/topic_03_applications/section_04_optimizers.html#simple-1d-function-optimization","title":"Simple 1D Function Optimization\u00b6","text":""},{"location":"course/topic_03_applications/section_04_optimizers.html#neural-network-training-with-different-optimizers","title":"Neural Network Training with Different Optimizers\u00b6","text":""},{"location":"course/topic_04_computer_vision/index.html","title":"Introduction","text":"<p>This topic systematically addresses the foundations and applications of computer vision systems based on deep learning. The objective is to understand how to process and model visual information, both in color image format (RGB format) and grayscale, and how these representation decisions influence the models' ability to learn relevant patterns.</p> <p>First, different image representation formats and their impact on subsequent processing are studied. It analyzes when it is convenient to work with color images, leveraging chromatic information, and when it is preferable to convert them to grayscale to simplify the problem, reduce dimensionality, or focus attention on luminance structures. The advantages and limitations of each approach and their relationship to the type of task (classification, segmentation, detection, etc.) are discussed.</p> <p>Next, various image processing techniques closely related to deep learning and, in many cases, signal processing are introduced. It explains how to normalize input data, both at the image and dataset level, and analyzes the influence of these normalizations on numerical stability, convergence speed, and final model performance. On this basis, normalization mechanisms applied within the architectures themselves are also studied, such as Batch Normalization and Layer Normalization, comparing their operating principles, their implicit assumptions, and their effect when normalized based on data statistics versus the use of internal model parameters.</p> <p>A central block of the topic is dedicated to the study of convolutional layers. It shows how to implement a convolution from scratch, analyzing in detail the filter sliding operation over the image, the local linear combination of pixels, and the obtaining of feature maps. Subsequently, it explains how to leverage primitives from libraries like PyTorch to define and train convolutional layers efficiently, maintaining intuition about what is happening at the mathematical and computational level.</p> <p>In relation to convolutions, properties such as translational invariance and their practical limitations are discussed with special attention. Although the ideal convolution presents certain translational invariance, this property is altered when strides greater than 1 or pooling layers are introduced. In particular, it analyzes how shifts (even circular ones) of the same image can produce significantly different responses due to these mechanisms, and reflects on why, despite this, pooling and stride are necessary to control spatial resolution, reduce computational cost, and increase the effective receptive field of deep layers.</p> <p>It is also emphasized that, even when convolutional layers do not require a fixed input size, image resolution significantly conditions the type of features the network is capable of learning. It discusses how the distribution of spatial frequencies and the scale of present patterns (edges, textures, global structures) influence the model's sensitivity to different levels of detail. This is essential to understand why the same architecture can behave differently depending on the size and quality of input images.</p> <p>From a historical perspective, the topic covers some of the most influential convolutional architectures. It starts with LeNet, developed by Yann LeCun and his team, used in a pioneering way in production environments for handwritten digit recognition in bank checks, and continues with subsequent architectures that explore the systematic increase in network depth and width. Models like VGG, which deepens the use of homogeneous convolutional blocks, and ResNet, which introduces residual connections to facilitate training of very deep networks, are analyzed. This review allows contextualizing progress in computer vision and understanding the motivations behind the design of increasingly complex networks.</p> <p>On this basis, transfer learning techniques are studied, which allow reusing convolutional models pretrained by large organizations (such as Meta, Google, or others) on massive datasets. It shows how to adapt these models to specific problems through fine-tuning of the last layers, partial freezing of parameters, or redefinition of classification heads, thus reducing training cost and improving performance in scenarios with limited data.</p> <p>In relation to efficiency and deployment on resource-constrained devices, methods such as knowledge distillation are introduced, which consist of transferring the behavior of large models (teachers) to smaller models (students). It discusses how these techniques allow compressing convolutional networks and adapting them to embedded environments, such as mobile phones, microcontrollers, or other edge computing systems, while maintaining a significant fraction of their predictive capacity.</p> <p>The topic also addresses autoencoders, both dense and convolutional, as tools for unsupervised learning of representations from images. It analyzes how these architectures allow extracting useful latent features for compression, anomaly detection, and out-of-distribution sample detection. The basic encoder-decoder structure, the reconstruction function, and the interpretation of latent variables are explained.</p> <p>At a more advanced level, some attention mechanisms applied to convolutional networks are presented, with special emphasis on channel attention. These mechanisms modulate the relative importance of each feature map, allowing the network to focus on spatially relevant patterns and enriching the extracted information without collapsing representations. It discusses how these attention blocks can be integrated into standard convolutional architectures and what benefits they provide in terms of performance and robustness.</p> <p>Subsequently, the application of Transformer-type architectures to the visual domain is introduced, through models like Vision Transformer (ViT). The central idea of dividing the image into patches, projecting them into an embedding space, and applying self-attention mechanisms similar to those used in natural language processing is explained. The original work proposed by Google is analyzed, as well as the implications of transferring the global attention paradigm to the computer vision context.</p> <p>Finally, the field of explainability and interpretability of vision models is addressed, with the aim of understanding why a model makes certain classification decisions. Techniques based on weights and gradients are explored, as well as the generation of heat maps that indicate the most relevant image regions for prediction. Among these techniques is Grad-CAM and related methods, which allow visualizing the effective attention of deep layers on the input image. These tools are essential for auditing models, detecting biases, debugging unexpected behaviors, and increasing confidence in systems deployed in sensitive environments.</p>"},{"location":"course/topic_04_computer_vision/section_01_image_processing.html","title":"Image Processing","text":"<p>Image processing constitutes a fundamental tool in computer vision and deep learning. From a digital image, transformations are applied that allow modifying its visual properties, extracting relevant information, or preparing data for training artificial intelligence models. This text focuses on practical aspects of image processing in Python, primarily using NumPy, PIL (Python Imaging Library), OpenCV, and PyTorch.</p> <p>The necessary steps are described progressively to: load an image and represent it as a matrix; apply basic color and brightness transformations; introduce noise; mask specific regions; convert to grayscale; combine images through Mixup; and finally, perform resizing and normalization for use in convolutional neural networks (CNNs). All of this is integrated into a narrative oriented toward both conceptual and practical understanding of each operation.</p> <p>A digital color image is typically represented as a three-dimensional tensor of size $H \\times W \\times C$, where $H$ is the height (in pixels), $W$ is the width, and $C$ is the number of color channels, typically $C = 3$ for RGB images (red, green, blue). Each pixel is stored as an integer value in the range $[0, 255]$ of type <code>uint8</code>, which allows describing the intensity of each channel.</p> <p>In Python, it is common to load an image and convert it to a NumPy array to be able to manipulate it in a vectorized manner. The following shows how to perform this operation with PIL and OpenCV, and how to visualize the image:</p> In\u00a0[1]: Copied! <pre># 3pps\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n\n# Method 1: Using PIL (Python Imaging Library)\nimage_pil = Image.open(\n    \"../../assets/course/topic_02_mathematics/cat_image.jpg\"\n)\nimage_array = np.array(image_pil)\n\n# Method 2: Using OpenCV\nimage_cv = cv2.imread(\n    \"../../assets/course/topic_02_mathematics/cat_image.jpg\"\n)\nimage_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n\n# Visualize the image\nplt.imshow(image_array)\nplt.axis(\"off\")\nplt.show()\n\nprint(f\"Image dimensions: {image_array.shape}\")\nprint(f\"Data type: {image_array.dtype}\")\n</pre> # 3pps import cv2 import matplotlib.pyplot as plt import numpy as np from PIL import Image   # Method 1: Using PIL (Python Imaging Library) image_pil = Image.open(     \"../../assets/course/topic_02_mathematics/cat_image.jpg\" ) image_array = np.array(image_pil)  # Method 2: Using OpenCV image_cv = cv2.imread(     \"../../assets/course/topic_02_mathematics/cat_image.jpg\" ) image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB  # Visualize the image plt.imshow(image_array) plt.axis(\"off\") plt.show()  print(f\"Image dimensions: {image_array.shape}\") print(f\"Data type: {image_array.dtype}\") <pre>Image dimensions: (531, 612, 3)\nData type: uint8\n</pre> <p>In this case, <code>image_array</code> is a NumPy array with typical shape <code>(height, width, 3)</code> and data type <code>uint8</code>. The conversion from BGR to RGB in the case of OpenCV is necessary because OpenCV uses the BGR format by default, while most visualization libraries and PIL handle RGB.</p> <p>This matrix representation is the foundation upon which the image processing transformations described in subsequent sections are built.</p> <p>Basic transformations allow modifying properties such as saturation, brightness, or noise level, as well as masking specific regions or converting the image to grayscale. These operations are essential both for classical preprocessing and for generating synthetic data variations in data augmentation contexts.</p> <p>Saturation controls the intensity of colors. An increase in saturation produces more vivid colors, while a decrease makes them more muted until reaching a grayscale image when saturation is null.</p> <p>In Python, a simple way to adjust saturation consists of using the <code>ImageEnhance</code> module from PIL:</p> In\u00a0[2]: Copied! <pre># 3pps\nfrom PIL import ImageEnhance\n\n\ndef cambiar_saturacion(image, factor=1.5):\n    \"\"\"\n    Factor &gt; 1: Increases saturation.\n    Factor &lt; 1: Decreases saturation.\n    Factor = 0: Grayscale image.\n    \"\"\"\n    image_pil = Image.fromarray(image)\n    enhancer = ImageEnhance.Color(image_pil)\n    image_saturada = enhancer.enhance(factor)\n    return np.array(image_saturada)\n\n\n# Usage examples\nimagen_mas_saturada = cambiar_saturacion(image_array, factor=2.0)\nimagen_menos_saturada = cambiar_saturacion(image_array, factor=0.5)\n</pre> # 3pps from PIL import ImageEnhance   def cambiar_saturacion(image, factor=1.5):     \"\"\"     Factor &gt; 1: Increases saturation.     Factor &lt; 1: Decreases saturation.     Factor = 0: Grayscale image.     \"\"\"     image_pil = Image.fromarray(image)     enhancer = ImageEnhance.Color(image_pil)     image_saturada = enhancer.enhance(factor)     return np.array(image_saturada)   # Usage examples imagen_mas_saturada = cambiar_saturacion(image_array, factor=2.0) imagen_menos_saturada = cambiar_saturacion(image_array, factor=0.5) In\u00a0[3]: Copied! <pre>plt.imshow(imagen_mas_saturada)\nplt.axis(\"off\")\nplt.show()\n\nplt.imshow(imagen_menos_saturada)\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(imagen_mas_saturada) plt.axis(\"off\") plt.show()  plt.imshow(imagen_menos_saturada) plt.axis(\"off\") plt.show() <p>The <code>factor</code> parameter controls the degree of modification. Values greater than 1 produce an increase in saturation, while values between 0 and 1 generate a progressively less saturated image.</p> <p>Brightness reflects the global luminosity of the image. It can be modified through PIL-based techniques or through arithmetic operations with NumPy.</p> <p>A first approach, using <code>ImageEnhance.Brightness</code>, is as follows:</p> In\u00a0[4]: Copied! <pre>def cambiar_brillo(image, factor=1.3):\n    \"\"\"\n    Factor &gt; 1: Increases brightness.\n    Factor &lt; 1: Decreases brightness.\n    \"\"\"\n    image_pil = Image.fromarray(image)\n    enhancer = ImageEnhance.Brightness(image_pil)\n    image_brillante = enhancer.enhance(factor)\n    return np.array(image_brillante)\n</pre> def cambiar_brillo(image, factor=1.3):     \"\"\"     Factor &gt; 1: Increases brightness.     Factor &lt; 1: Decreases brightness.     \"\"\"     image_pil = Image.fromarray(image)     enhancer = ImageEnhance.Brightness(image_pil)     image_brillante = enhancer.enhance(factor)     return np.array(image_brillante) <p>Alternatively, it is possible to adjust brightness directly with NumPy by adding a constant value to all pixels. In this case, it is important to avoid undesired saturations and maintain values in the valid range $[0, 255]$:</p> In\u00a0[5]: Copied! <pre># Alternative with NumPy\n\n\ndef ajustar_brillo_numpy(image, valor=50):\n    \"\"\"Add a constant value to all pixels.\"\"\"\n    image_ajustada = np.clip(image.astype(np.int16) + valor, 0, 255)\n    return image_ajustada.astype(np.uint8)\n\n\n# Usage examples\nimagen_mas_brillante = cambiar_brillo(image_array, factor=1.5)\nimagen_mas_oscura = cambiar_brillo(image_array, factor=0.7)\n</pre> # Alternative with NumPy   def ajustar_brillo_numpy(image, valor=50):     \"\"\"Add a constant value to all pixels.\"\"\"     image_ajustada = np.clip(image.astype(np.int16) + valor, 0, 255)     return image_ajustada.astype(np.uint8)   # Usage examples imagen_mas_brillante = cambiar_brillo(image_array, factor=1.5) imagen_mas_oscura = cambiar_brillo(image_array, factor=0.7) In\u00a0[6]: Copied! <pre>plt.imshow(imagen_mas_brillante)\nplt.axis(\"off\")\nplt.show()\n\nplt.imshow(imagen_mas_oscura)\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(imagen_mas_brillante) plt.axis(\"off\") plt.show()  plt.imshow(imagen_mas_oscura) plt.axis(\"off\") plt.show() <p>In the NumPy version, temporary conversion to <code>int16</code> avoids overflow problems when adding positive values to a <code>uint8</code> array. The <code>np.clip</code> function guarantees that the result remains within the allowed limits.</p> <p>Noise addition is a common technique in data augmentation to improve model robustness against perturbations. Gaussian noise is modeled through a normal distribution with mean $\\mu$ and standard deviation $\\sigma$, and is added to the value of each pixel:</p> In\u00a0[7]: Copied! <pre>def anadir_ruido_gaussiano(image, media=0, sigma=25):\n    \"\"\"\n    Adds Gaussian noise to the image.\n    sigma: Standard deviation of the noise (higher = more noise).\n    \"\"\"\n    ruido = np.random.normal(media, sigma, image.shape)\n    imagen_con_ruido = image.astype(np.float32) + ruido\n    imagen_con_ruido = np.clip(imagen_con_ruido, 0, 255)\n    return imagen_con_ruido.astype(np.uint8)\n\n\n# Usage example\nimagen_ruidosa = anadir_ruido_gaussiano(image_array, sigma=30)\n</pre> def anadir_ruido_gaussiano(image, media=0, sigma=25):     \"\"\"     Adds Gaussian noise to the image.     sigma: Standard deviation of the noise (higher = more noise).     \"\"\"     ruido = np.random.normal(media, sigma, image.shape)     imagen_con_ruido = image.astype(np.float32) + ruido     imagen_con_ruido = np.clip(imagen_con_ruido, 0, 255)     return imagen_con_ruido.astype(np.uint8)   # Usage example imagen_ruidosa = anadir_ruido_gaussiano(image_array, sigma=30) In\u00a0[8]: Copied! <pre>plt.imshow(imagen_ruidosa)\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(imagen_ruidosa) plt.axis(\"off\") plt.show() <p>A higher $\\sigma$ value increases noise intensity. As in the case of brightness, the operation is performed in floating point and the range is subsequently limited to obtain a valid image.</p> <p>Masking consists of nullifying or modifying specific regions of the image, typically replacing them with a constant color. This procedure is useful for simulating occlusions, applying techniques like Cutout, or highlighting certain areas.</p> <p>Rectangular masking can be implemented directly by indexing rows and columns:</p> In\u00a0[9]: Copied! <pre>def enmascarar_region(image, x1, y1, x2, y2, color=(0, 0, 0)):\n    \"\"\"\n    Masks a rectangular region of the image.\n    Coordinates (x1, y1) and (x2, y2) define the upper left and lower right corners.\n    \"\"\"\n    imagen_enmascarada = image.copy()\n    imagen_enmascarada[y1:y2, x1:x2] = color\n    return imagen_enmascarada\n</pre> def enmascarar_region(image, x1, y1, x2, y2, color=(0, 0, 0)):     \"\"\"     Masks a rectangular region of the image.     Coordinates (x1, y1) and (x2, y2) define the upper left and lower right corners.     \"\"\"     imagen_enmascarada = image.copy()     imagen_enmascarada[y1:y2, x1:x2] = color     return imagen_enmascarada <p>On the other hand, it is possible to mask a circular region using binary masks and OpenCV drawing functions:</p> In\u00a0[10]: Copied! <pre>def enmascarar_circular(image, centro, radio, color=(0, 0, 0)):\n    \"\"\"\n    Masks a circular region.\n    centro: Tuple (x, y) with the center of the circle.\n    radio: Radius of the circle in pixels.\n    \"\"\"\n    imagen_enmascarada = image.copy()\n    mascara = np.zeros(image.shape[:2], dtype=np.uint8)\n    cv2.circle(mascara, centro, radio, 255, -1)\n    imagen_enmascarada[mascara == 0] = color\n    return imagen_enmascarada\n\n\n# Example: Mask the center of the image with a rectangle\nh, w = image_array.shape[:2]\nimagen_con_mascara = enmascarar_region(\n    image_array, w // 4, h // 4, 3 * w // 4, 3 * h // 4, color=(128, 128, 128)\n)\n</pre> def enmascarar_circular(image, centro, radio, color=(0, 0, 0)):     \"\"\"     Masks a circular region.     centro: Tuple (x, y) with the center of the circle.     radio: Radius of the circle in pixels.     \"\"\"     imagen_enmascarada = image.copy()     mascara = np.zeros(image.shape[:2], dtype=np.uint8)     cv2.circle(mascara, centro, radio, 255, -1)     imagen_enmascarada[mascara == 0] = color     return imagen_enmascarada   # Example: Mask the center of the image with a rectangle h, w = image_array.shape[:2] imagen_con_mascara = enmascarar_region(     image_array, w // 4, h // 4, 3 * w // 4, 3 * h // 4, color=(128, 128, 128) ) In\u00a0[11]: Copied! <pre>plt.imshow(imagen_con_mascara)\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(imagen_con_mascara) plt.axis(\"off\") plt.show() <p>This type of masking not only has value as an augmentation technique but also for focusing analysis on regions of interest or for hiding sensitive information.</p> <p>Converting an RGB image to grayscale reduces the three color channels to a single dimension of luminous intensity. There are various methods to perform this transformation. A simple approach uses PIL's native functionality:</p> In\u00a0[12]: Copied! <pre>def convertir_blanco_negro(image):\n    \"\"\"Convert image to grayscale using PIL.\"\"\"\n    image_pil = Image.fromarray(image).convert(\"L\")\n    return np.array(image_pil)\n</pre> def convertir_blanco_negro(image):     \"\"\"Convert image to grayscale using PIL.\"\"\"     image_pil = Image.fromarray(image).convert(\"L\")     return np.array(image_pil) <p>Another option consists of applying a weighted combination of the red, green, and blue channels. A widely used standard formula is:</p> <p>$$ I = 0.299 R + 0.587 G + 0.114 B $$</p> <p>where $R$, $G$, and $B$ represent the intensities of each channel. This expression is implemented in a vectorized manner with NumPy:</p> In\u00a0[13]: Copied! <pre>def convertir_bn_ponderado(image):\n    \"\"\"\n    Manual weighted conversion to grayscale.\n    Standard formula: 0.299*R + 0.587*G + 0.114*B.\n    \"\"\"\n    return np.dot(image[..., :3], [0.299, 0.587, 0.114]).astype(np.uint8)\n\n\n# Usage example\nimagen_bn = convertir_blanco_negro(image_array)\nplt.imshow(imagen_bn, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n</pre> def convertir_bn_ponderado(image):     \"\"\"     Manual weighted conversion to grayscale.     Standard formula: 0.299*R + 0.587*G + 0.114*B.     \"\"\"     return np.dot(image[..., :3], [0.299, 0.587, 0.114]).astype(np.uint8)   # Usage example imagen_bn = convertir_blanco_negro(image_array) plt.imshow(imagen_bn, cmap=\"gray\") plt.axis(\"off\") plt.show() <p>Both methods generate a single-channel image that can be visualized through a grayscale color map. Grayscale conversion is used in applications where chromatic information is not essential or when reducing data dimensionality is desired.</p> <p>Data augmentation increases the effective diversity of the training set without the need to acquire new images. This increase in variability contributes to reducing overfitting and improves model generalization capacity, especially when the number of original samples is limited.</p> <p>Augmentation strategies can be grouped into several categories. Geometric transformations, such as horizontal or vertical flips, rotations, cropping, or scaling, modify the spatial arrangement of content. This promotes model invariance to changes in orientation, translation, or scale.</p> <p>Color-based transformations alter the chromatic characteristics of the image, including adjustments to brightness, contrast, saturation, and hue. These operations seek robustness against variations in illumination, capture devices, or environmental conditions.</p> <p>Noise-based techniques introduce stochastic perturbations, such as Gaussian or impulse noise, that prepare the model for less ideal acquisition conditions. Advanced methods like Cutout consist of randomly masking image regions, forcing the network not to depend exclusively on small discriminative areas. Mixup, described earlier, linearly combines multiple images and their labels, generating intermediate examples in the feature space.</p> <p>Together, these augmentation strategies allow models to learn more robust representations that are less sensitive to irrelevant data variations.</p> <p>Convolutional neural networks require input images to have fixed dimensions to be able to process them in batches and maintain a coherent structure throughout the convolutional layers. Standard values, such as $224 \\times 224$ pixels, are common in pretrained models (for example, in architectures trained on ImageNet), as they balance computational cost and sufficient spatial resolution to capture relevant details.</p> <p>Additionally, normalizing pixel intensities is essential for stabilizing training, ensuring that gradients are properly scaled, and facilitating optimizer convergence.</p> <p>Resizing can be carried out in various ways, depending on whether one wishes to preserve the original aspect ratio, add padding, or crop the image. An example of a function that implements several strategies is as follows:</p> In\u00a0[14]: Copied! <pre>def resize_image(image, target_size=(224, 224)):\n    \"\"\"Different resizing methods.\"\"\"\n\n    # Method 1: Simple resize (may distort aspect ratio)\n    resized_simple = cv2.resize(image, target_size)\n\n    # Method 2: Resize maintaining aspect ratio + padding\n    h, w = image.shape[:2]\n    target_h, target_w = target_size\n\n    scale = min(target_w / w, target_h / h)\n    new_w, new_h = int(w * scale), int(h * scale)\n\n    resized = cv2.resize(image, (new_w, new_h))\n\n    # Add padding to complete to target size\n    top = (target_h - new_h) // 2\n    bottom = target_h - new_h - top\n    left = (target_w - new_w) // 2\n    right = target_w - new_w - left\n\n    padded = cv2.copyMakeBorder(\n        resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]\n    )\n\n    # Method 3: Center crop (only if the image is larger than the target)\n    if h &gt; target_h or w &gt; target_w:\n        start_h = (h - target_h) // 2\n        start_w = (w - target_w) // 2\n        cropped = image[start_h : start_h + target_h, start_w : start_w + target_w]\n    else:\n        cropped = image\n\n    return {\n        \"simple_resize\": resized_simple,\n        \"resize_with_padding\": padded,\n        \"center_crop\": cropped,\n    }\n\n\n# Usage example\nresults = resize_image(image_array, target_size=(224, 224))\n</pre> def resize_image(image, target_size=(224, 224)):     \"\"\"Different resizing methods.\"\"\"      # Method 1: Simple resize (may distort aspect ratio)     resized_simple = cv2.resize(image, target_size)      # Method 2: Resize maintaining aspect ratio + padding     h, w = image.shape[:2]     target_h, target_w = target_size      scale = min(target_w / w, target_h / h)     new_w, new_h = int(w * scale), int(h * scale)      resized = cv2.resize(image, (new_w, new_h))      # Add padding to complete to target size     top = (target_h - new_h) // 2     bottom = target_h - new_h - top     left = (target_w - new_w) // 2     right = target_w - new_w - left      padded = cv2.copyMakeBorder(         resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]     )      # Method 3: Center crop (only if the image is larger than the target)     if h &gt; target_h or w &gt; target_w:         start_h = (h - target_h) // 2         start_w = (w - target_w) // 2         cropped = image[start_h : start_h + target_h, start_w : start_w + target_w]     else:         cropped = image      return {         \"simple_resize\": resized_simple,         \"resize_with_padding\": padded,         \"center_crop\": cropped,     }   # Usage example results = resize_image(image_array, target_size=(224, 224)) In\u00a0[15]: Copied! <pre>plt.imshow(results[\"simple_resize\"])\nplt.axis(\"off\")\nplt.show()\n\nplt.imshow(results[\"resize_with_padding\"])\nplt.axis(\"off\")\nplt.show()\n\nplt.imshow(results[\"center_crop\"])\nplt.axis(\"off\")\nplt.show()\n</pre> plt.imshow(results[\"simple_resize\"]) plt.axis(\"off\") plt.show()  plt.imshow(results[\"resize_with_padding\"]) plt.axis(\"off\") plt.show()  plt.imshow(results[\"center_crop\"]) plt.axis(\"off\") plt.show() <p>Simple resizing directly adjusts the image to the specified size but may distort geometry if the aspect ratio differs from the original. The approach that maintains the aspect ratio and adds padding avoids this distortion by adding uniform bands around the image. Finally, center cropping is useful when the image is larger and the most relevant information is considered to be near the center.</p> <p>In practice, preprocessing pipelines are defined that chain several transformations coherently. PyTorch, through the <code>torchvision.transforms</code> module, offers a compact way to define these workflows. The following shows a standard pipeline for models pretrained on ImageNet:</p> In\u00a0[16]: Copied! <pre># 3pps\nimport torch\nfrom torchvision import transforms\n\n\n# Standard preprocessing pipeline for pretrained models\npreprocess = transforms.Compose(\n    [\n        transforms.Resize(256),  # Resize to intermediate size\n        transforms.CenterCrop(224),  # Center crop to 224x224\n        transforms.ToTensor(),  # Convert to tensor [0, 1]\n        transforms.Normalize(  # Normalize with ImageNet mean and std\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),\n    ]\n)\n</pre> # 3pps import torch from torchvision import transforms   # Standard preprocessing pipeline for pretrained models preprocess = transforms.Compose(     [         transforms.Resize(256),  # Resize to intermediate size         transforms.CenterCrop(224),  # Center crop to 224x224         transforms.ToTensor(),  # Convert to tensor [0, 1]         transforms.Normalize(  # Normalize with ImageNet mean and std             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]         ),     ] ) <p>The <code>ToTensor</code> transformation converts the image to a PyTorch tensor with shape $(C, H, W)$ and automatically normalizes values to the range $[0, 1]$. Subsequently, <code>Normalize</code> applies a channel-wise affine transformation:</p> <p>$$ x' = \\frac{x - \\mu}{\\sigma} $$</p> <p>where $\\mu$ is the mean and $\\sigma$ is the standard deviation of pixel values for each channel in the original training set (in this case, ImageNet). This normalization homogenizes the input data scale, which facilitates optimization.</p> <p>During training, data augmentation is typically incorporated directly into the pipeline, while during validation a more conservative set of transformations is employed, without randomness. A typical example is as follows:</p> In\u00a0[17]: Copied! <pre># For training with augmentation\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n# For validation (without augmentation)\nval_transform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n</pre> # For training with augmentation train_transform = transforms.Compose(     [         transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),         transforms.RandomHorizontalFlip(),         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),         transforms.ToTensor(),         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),     ] )  # For validation (without augmentation) val_transform = transforms.Compose(     [         transforms.Resize(256),         transforms.CenterCrop(224),         transforms.ToTensor(),         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),     ] ) <p>In the training pipeline, <code>RandomResizedCrop</code> randomly selects a region from the original image and resizes it to $224 \\times 224$, varying both scale and position. <code>RandomHorizontalFlip</code> horizontally inverts the image with a predetermined probability, while <code>ColorJitter</code> randomly modifies brightness, contrast, and saturation within bounded ranges. In this way, each pass through the dataset generates slightly different versions of the original images, which increases the effective data diversity.</p> <p>In the validation pipeline, deterministic transformations are used to ensure that evaluations are reproducible and comparable across epochs. Size and normalization are kept consistent with the training pipeline, so that the model receives inputs in the same feature space.</p>"},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#image-processing","title":"Image Processing\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#loading-and-representing-images-in-python","title":"Loading and Representing Images in Python\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#basic-transformations-on-images","title":"Basic Transformations on Images\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#saturation-modification","title":"Saturation Modification\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#illumination-adjustment-brightness","title":"Illumination Adjustment (Brightness)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#introduction-of-gaussian-noise","title":"Introduction of Gaussian Noise\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#masking-image-regions","title":"Masking Image Regions\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#conversion-to-grayscale","title":"Conversion to Grayscale\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#data-augmentation-importance-and-strategies","title":"Data Augmentation: Importance and Strategies\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#image-resizing-and-normalization","title":"Image Resizing and Normalization\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#resizing-techniques","title":"Resizing Techniques\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_01_image_processing.html#complete-preprocessing-pipeline-for-cnns","title":"Complete Preprocessing Pipeline for CNNs\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html","title":"Normalization","text":"<p>Normalization constitutes a key stage both in input data preprocessing and in the internal design of neural network architectures. Its primary objective is to control the scale of numerical values, ensuring that different features are in comparable ranges and that training is stable, efficient, and less sensitive to initialization or hyperparameter choices.</p> <p>In the context of images, normalization can be divided into two major conceptual blocks. On one hand, input data normalization, which is applied before introducing images into the network. On the other hand, layer normalization, which is applied to the internal activations of the network during training. Although both categories pursue similar objectives, they are implemented at different stages of the data flow and with different mechanisms.</p> <p>Input data normalization is applied directly to images before they are processed by the network layers. In the case of images, one works with tensors or arrays where each pixel can be represented with raw values in the range $[0, 255]$ or, after prior conversion, with floating-point values.</p> <p>The purpose of this normalization is threefold. First, it provides numerical stability by avoiding excessively large or small values, which can cause uncontrolled or practically null gradients. Second, it accelerates training, as gradients propagate more uniformly through the network. Finally, it prevents one feature from dominating others simply due to its scale, favoring that all dimensions of the feature space contribute comparably to model learning.</p> <p>Input normalization fulfills several essential objectives. In terms of numerical stability, it prevents activations from reaching magnitudes that hinder the convergence of optimization algorithms. Additionally, input homogenization facilitates that gradients calculated during backpropagation have reasonable orders of magnitude, which allows using more aggressive learning rates without compromising convergence. Finally, by adjusting all features to similar ranges, a balancing effect is produced, so that the model is not biased toward those components with larger numerical values.</p> <p>Various standard techniques exist for normalizing images, each suitable for certain scenarios and architectures. A first technique consists of Min-Max normalization to the range $[0, 1]$. In this case, the image is linearly rescaled using its minimum and maximum values:</p> In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef normalize_min_max(image):\n    \"\"\"Brings the image to the range [0, 1].\"\"\"\n    image = image.astype(np.float32)\n    normalized = (image - image.min()) / (image.max() - image.min() + 1e-8)\n    return normalized\n</pre> # 3pps import numpy as np   def normalize_min_max(image):     \"\"\"Brings the image to the range [0, 1].\"\"\"     image = image.astype(np.float32)     normalized = (image - image.min()) / (image.max() - image.min() + 1e-8)     return normalized <p>This method is useful when one wants to work with values bounded between 0 and 1, for example in simple models or when one wishes to visualize or combine different data sources normalized to the same range.</p> <p>A widely used variant in deep neural networks consists of bringing values to the range $[-1, 1]$. For typical 8-bit images, a direct way to achieve this is to first divide by 255 and then apply a linear transformation:</p> In\u00a0[2]: Copied! <pre>def normalize_minus_one_to_one(image):\n    \"\"\"Brings the image to the range [-1, 1].\"\"\"\n    image = image.astype(np.float32) / 255.0\n    normalized = 2.0 * image - 1.0\n    return normalized\n</pre> def normalize_minus_one_to_one(image):     \"\"\"Brings the image to the range [-1, 1].\"\"\"     image = image.astype(np.float32) / 255.0     normalized = 2.0 * image - 1.0     return normalized <p>This type of normalization is common in architectures such as Generative Adversarial Networks (GANs), where it is preferable for input data to be centered around zero.</p> <p>Another fundamental approach is standardization or $z$-score normalization. In this case, the mean is subtracted and divided by the standard deviation of the data:</p> In\u00a0[3]: Copied! <pre>def standardize(image):\n    \"\"\"Standardizes: (x - mean) / standard deviation.\"\"\"\n    image = image.astype(np.float32)\n    mean = image.mean()\n    std = image.std()\n    standardized = (image - mean) / (std + 1e-8)\n    return standardized\n</pre> def standardize(image):     \"\"\"Standardizes: (x - mean) / standard deviation.\"\"\"     image = image.astype(np.float32)     mean = image.mean()     std = image.std()     standardized = (image - mean) / (std + 1e-8)     return standardized <p>This technique transforms data so that it has approximately zero mean and unit variance. In computer vision, it is frequently used at the channel level, utilizing precomputed means and standard deviations over large datasets, such as ImageNet.</p> <p>In practice, frameworks like PyTorch facilitate input normalization through predefined transformations. A typical example for models pretrained on ImageNet is as follows:</p> In\u00a0[4]: Copied! <pre># 3pps\nfrom torchvision import transforms\n\n\n# Standard transformation for pretrained models (ImageNet)\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),  # Converts to tensor and scales to [0, 1]\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],  # ImageNet mean per channel\n            std=[0.229, 0.224, 0.225],  # ImageNet standard deviation per channel\n        ),\n    ]\n)\n</pre> # 3pps from torchvision import transforms   # Standard transformation for pretrained models (ImageNet) transform = transforms.Compose(     [         transforms.ToTensor(),  # Converts to tensor and scales to [0, 1]         transforms.Normalize(             mean=[0.485, 0.456, 0.406],  # ImageNet mean per channel             std=[0.229, 0.224, 0.225],  # ImageNet standard deviation per channel         ),     ] ) <p>In this pipeline, the <code>ToTensor</code> function converts the image to a floating-point tensor and scales values to the range $[0, 1]$. Subsequently, <code>Normalize</code> applies channel-wise standardization using global statistics from the original training set. This practice ensures that pretrained models receive inputs in the same statistical regime for which they were optimized.</p> <p>Layer normalization is performed within the network architecture, on the intermediate activations that are generated as data advances through different layers. Unlike input data normalization, which is fixed preprocessing, layer normalization is implemented as differentiable blocks that form part of the model and that, in many cases, contain learnable parameters.</p> <p>The general idea consists of normalizing activations according to certain dimensions (for example, over the batch, over channels, or over all elements of a sample), and then applying a linear transformation with scale and shift parameters that are learned during training. In this way, the so-called \"internal covariate shift\" is corrected and the distribution of activations is stabilized, which facilitates the training of deep networks.</p> <p>Local Response Normalization (LRN) is a technique introduced in early networks such as AlexNet. Its purpose is to perform normalization based on the response of neighboring channels, mimicking certain lateral inhibition mechanisms observed in the biological visual system. Although it is included here for historical completeness, in practice its current use is residual, as it has been widely displaced by more effective methods such as Batch Normalization or Layer Normalization.</p> <p>A schematic implementation of LRN in PyTorch can be structured as a class that receives parameters such as neighborhood size $n$, coefficients $\\alpha$ and $\\beta$, and a constant $k$:</p> In\u00a0[5]: Copied! <pre># 3pps\nimport torch\nimport torch.nn as nn\n\n\nclass LocalResponseNormalization(nn.Module):\n    def __init__(self, k=2.0, n=5, alpha=1e-4, beta=0.75):\n        super().__init__()\n        self.k = k\n        self.n = n\n        self.alpha = alpha\n        self.beta = beta\n\n    # The complete implementation would include the calculation of normalization\n    # over neighboring channels according to the above parameters.\n</pre> # 3pps import torch import torch.nn as nn   class LocalResponseNormalization(nn.Module):     def __init__(self, k=2.0, n=5, alpha=1e-4, beta=0.75):         super().__init__()         self.k = k         self.n = n         self.alpha = alpha         self.beta = beta      # The complete implementation would include the calculation of normalization     # over neighboring channels according to the above parameters. <p>Although LRN had relevance in early works with deep CNNs, its current impact is very limited and it is not considered a recommendable choice for modern architectures.</p> <p>Global Response Normalization (GRN) is proposed as a more recent alternative to local normalization. Instead of normalizing with respect to neighboring channels, GRN considers the global response of all channels for each spatial position and regulates the magnitude of activations per channel from that global information. The objective is to prevent certain channels from becoming redundant or systematically dominating the representation, promoting a more balanced distribution of energy across channels.</p> <p>A typical GRN implementation in PyTorch can take the following form:</p> In\u00a0[6]: Copied! <pre>class GlobalResponseNormalization(nn.Module):\n    def __init__(self, num_channels, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        self.eps = eps\n\n    def forward(self, x):\n        # Calculate global norm per channel (p=2 over spatial dimensions)\n        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n        # Normalize with respect to the mean of global norms\n        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n        # Rescale and add residual component\n        return self.gamma * (x * nx) + self.beta + x\n</pre> class GlobalResponseNormalization(nn.Module):     def __init__(self, num_channels, eps=1e-6):         super().__init__()         self.gamma = nn.Parameter(torch.zeros(1, num_channels, 1, 1))         self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))         self.eps = eps      def forward(self, x):         # Calculate global norm per channel (p=2 over spatial dimensions)         gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)         # Normalize with respect to the mean of global norms         nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)         # Rescale and add residual component         return self.gamma * (x * nx) + self.beta + x <p>In this block, an $L^2$ norm per channel is first calculated by aggregating over spatial dimensions. Subsequently, this norm is normalized with respect to its mean and used to rescale the original activations through the learnable parameters $\\gamma$ and $\\beta$, to which the input itself is also added as a residual term. This type of normalization has been explored in modern convolutional architectures and in masked autoencoder models.</p> <p>Batch Normalization (BN) is one of the most influential internal normalization techniques in deep networks. Its central idea consists of normalizing activations using statistics (mean and variance) calculated over the training batch itself for each channel.</p> <p>For an activation tensor $x$ of size $(N, C, H, W)$, where $N$ is the batch size, $C$ is the number of channels, and $(H, W)$ is the spatial dimension, the mean and variance per channel are calculated in training mode:</p> <p>$$ \\mu*c = \\frac{1}{N H W} \\sum*{n,h,w} x*{n,c,h,w}, \\quad \\sigma_c^2 = \\frac{1}{N H W} \\sum*{n,h,w} (x\\_{n,c,h,w} - \\mu_c)^2. $$</p> <p>Next, normalization is performed:</p> <p>$$ \\hat{x}_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}, $$</p> <p>and an affine transformation is applied with learnable parameters $\\gamma_c$ and $\\beta_c$:</p> <p>$$ y*{n,c,h,w} = \\gamma_c \\hat{x}*{n,c,h,w} + \\beta_c. $$</p> <p>A simplified implementation of two-dimensional Batch Normalization can be expressed in PyTorch as follows:</p> In\u00a0[7]: Copied! <pre>class BatchNormalization2D(nn.Module):\n    def __init__(self, num_channels, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        self.eps = eps\n        self.momentum = momentum\n\n        # Accumulated statistics for inference\n        self.register_buffer(\"running_mean\", torch.zeros(1, num_channels, 1, 1))\n        self.register_buffer(\"running_var\", torch.ones(1, num_channels, 1, 1))\n\n    def forward(self, x):\n        if self.training:\n            mean = x.mean(dim=(0, 2, 3), keepdim=True)\n            var = x.var(dim=(0, 2, 3), keepdim=True)\n\n            # Update accumulated statistics\n            self.running_mean = (\n                1 - self.momentum\n            ) * self.running_mean + self.momentum * mean\n            self.running_var = (\n                1 - self.momentum\n            ) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n</pre> class BatchNormalization2D(nn.Module):     def __init__(self, num_channels, eps=1e-5, momentum=0.1):         super().__init__()         self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))         self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))         self.eps = eps         self.momentum = momentum          # Accumulated statistics for inference         self.register_buffer(\"running_mean\", torch.zeros(1, num_channels, 1, 1))         self.register_buffer(\"running_var\", torch.ones(1, num_channels, 1, 1))      def forward(self, x):         if self.training:             mean = x.mean(dim=(0, 2, 3), keepdim=True)             var = x.var(dim=(0, 2, 3), keepdim=True)              # Update accumulated statistics             self.running_mean = (                 1 - self.momentum             ) * self.running_mean + self.momentum * mean             self.running_var = (                 1 - self.momentum             ) * self.running_var + self.momentum * var         else:             mean = self.running_mean             var = self.running_var          x_norm = (x - mean) / torch.sqrt(var + self.eps)         return self.gamma * x_norm + self.beta <p>During training, batch statistics are used and accumulated means and variances are updated with a certain momentum. During inference, batch statistics are no longer used and accumulated means and variances are employed instead, which guarantees deterministic behavior.</p> <p>Among the main advantages of Batch Normalization are training acceleration, the possibility of using higher learning rates, and reduced dependence on weight initialization. In many architectures, BN also contributes to reducing the need for additional regularization techniques such as Dropout. However, it also presents limitations. In particular, its performance degrades when the batch size is very small, as mean and variance estimates become noisy, and its behavior differs between training and inference modes, which requires careful management of <code>train</code> and <code>eval</code> modes.</p> <p>Layer Normalization (LN) is designed to overcome some limitations of BN, especially in contexts where batch size is small or where the model structure does not adapt well to batch normalization, such as in recurrent networks or Transformers. In LN, normalization is performed independently for each sample, aggregating over all its feature dimensions.</p> <p>If one considers an input tensor $x$ associated with an individual sample, LN calculates the mean and variance over all relevant dimensions (for example, over channels and spatial positions) for each batch element, and normalizes analogously to BN but without depending on other samples in the batch. Thus, the normalization behavior is identical in training and inference, and does not depend on batch size.</p> <p>A schematic implementation of Layer Normalization for tensors of type $(N, C, H, W)$ can be written as follows:</p> In\u00a0[8]: Copied! <pre>class LayerNormalization2D(nn.Module):\n    def __init__(self, num_channels=None, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        if num_channels is not None:\n            self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n            self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        else:\n            self.gamma = None\n            self.beta = None\n\n    def forward(self, x):\n        mean = x.mean(dim=(1, 2, 3), keepdim=True)\n        var = x.var(dim=(1, 2, 3), keepdim=True)\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        if self.gamma is not None and self.beta is not None:\n            return self.gamma * x_norm + self.beta\n        return x_norm\n</pre> class LayerNormalization2D(nn.Module):     def __init__(self, num_channels=None, eps=1e-6):         super().__init__()         self.eps = eps         if num_channels is not None:             self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))             self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))         else:             self.gamma = None             self.beta = None      def forward(self, x):         mean = x.mean(dim=(1, 2, 3), keepdim=True)         var = x.var(dim=(1, 2, 3), keepdim=True)         x_norm = (x - mean) / torch.sqrt(var + self.eps)         if self.gamma is not None and self.beta is not None:             return self.gamma * x_norm + self.beta         return x_norm <p>This normalization is especially suitable for attention-based architectures, such as Transformers, and for recurrent networks, where dependence on batch statistics could introduce undesired noise. Additionally, by not differentiating between training and inference modes, it simplifies the operational flow of the model and facilitates the use of very small batch sizes, even equal to one.</p>"},{"location":"course/topic_04_computer_vision/section_02_normalization.html#normalization","title":"Normalization\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#input-data-normalization","title":"Input Data Normalization\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#motivation-for-normalizing-input","title":"Motivation for Normalizing Input\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#input-normalization-techniques","title":"Input Normalization Techniques\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#layer-normalization-in-neural-networks","title":"Layer Normalization in Neural Networks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#local-response-normalization-lrn","title":"Local Response Normalization (LRN)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#global-response-normalization-grn","title":"Global Response Normalization (GRN)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#batch-normalization-bn","title":"Batch Normalization (BN)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_02_normalization.html#layer-normalization-ln","title":"Layer Normalization (LN)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html","title":"Convolutional Layers","text":"<p>Convolutional layers constitute the core of Convolutional Neural Networks (CNNs) used in computer vision. A convolution applies a filter or kernel over an image to extract local features such as edges, textures, or shapes. This process is performed in a sliding manner over the image, generating an activation map that highlights those regions where the pattern defined by the filter is present with greater intensity.</p> <p>To understand how a convolution works, it is useful to start from an explicit implementation in NumPy on a grayscale image (2D matrix) and a two-dimensional kernel as well.</p> In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef convolve2d(image, kernel, padding=0, stride=1):\n    \"\"\"\n    Applies a 2D convolution.\n\n    image: Input image of shape (H, W).\n    kernel: Filter of shape (K_H, K_W).\n    padding: Zero padding around the image.\n    stride: Step with which the filter is displaced.\n    \"\"\"\n    # Apply padding if necessary\n    if padding &gt; 0:\n        image = np.pad(image, padding, mode=\"constant\")\n\n    h, w = image.shape\n    kh, kw = kernel.shape\n\n    # Calculate output size\n    out_h = (h - kh) // stride + 1\n    out_w = (w - kw) // stride + 1\n\n    output = np.zeros((out_h, out_w))\n\n    # Apply convolution\n    for i in range(out_h):\n        for j in range(out_w):\n            h_start = i * stride\n            w_start = j * stride\n            region = image[h_start : h_start + kh, w_start : w_start + kw]\n            output[i, j] = np.sum(region * kernel)\n\n    return output\n\n\n# Usage example\nimage = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n\nkernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n\nresultado = convolve2d(image, kernel)\nprint(resultado)\n</pre> # 3pps import numpy as np   def convolve2d(image, kernel, padding=0, stride=1):     \"\"\"     Applies a 2D convolution.      image: Input image of shape (H, W).     kernel: Filter of shape (K_H, K_W).     padding: Zero padding around the image.     stride: Step with which the filter is displaced.     \"\"\"     # Apply padding if necessary     if padding &gt; 0:         image = np.pad(image, padding, mode=\"constant\")      h, w = image.shape     kh, kw = kernel.shape      # Calculate output size     out_h = (h - kh) // stride + 1     out_w = (w - kw) // stride + 1      output = np.zeros((out_h, out_w))      # Apply convolution     for i in range(out_h):         for j in range(out_w):             h_start = i * stride             w_start = j * stride             region = image[h_start : h_start + kh, w_start : w_start + kw]             output[i, j] = np.sum(region * kernel)      return output   # Usage example image = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])  kernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])  resultado = convolve2d(image, kernel) print(resultado) <pre>[[-6. -6.]\n [-6. -6.]]\n</pre> <p>In this implementation, the kernel slides over the image from left to right and from top to bottom. At each position, a local region of the same size as the kernel is taken, element-wise multiplication is performed, and the results are summed, producing a value in the output map.</p> <p>Before training neural networks, image processing used manually designed filters to detect specific patterns. Many of these filters remain useful for illustrating the effect of a convolution.</p> In\u00a0[2]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom scipy.signal import convolve2d\n\n\n# Predefined filters\nfiltros = {\n    # Detects vertical edges\n    \"vertical\": np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n    # Detects horizontal edges\n    \"horizontal\": np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),\n    # Sobel X (enhanced vertical edges)\n    \"sobel_x\": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n    # Sobel Y (enhanced horizontal edges)\n    \"sobel_y\": np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n    # Blur\n    \"blur\": np.ones((3, 3)) / 9.0,\n    # Edge detection (Laplacian)\n    \"laplacian\": np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n}\n\n# Load image\nimage_pil = Image.open(\n    \"../../assets/course/topic_02_mathematics/cat_image.jpg\"\n).convert(\"L\")\nimagen = np.array(image_pil)\n\n# Apply all filters\nresultados = {}\nfor nombre, filtro in filtros.items():\n    resultados[nombre] = convolve2d(imagen, filtro, mode=\"same\", boundary=\"symm\")\n\n# Display results\nfig, axes = plt.subplots(2, 4, figsize=(15, 8))\naxes = axes.flatten()\n\n# Original image\naxes[0].imshow(imagen, cmap=\"gray\")\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\n# Filtered images\nfor i, (nombre, resultado) in enumerate(resultados.items(), 1):\n    axes[i].imshow(resultado, cmap=\"gray\")\n    axes[i].set_title(nombre.capitalize())\n    axes[i].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np from PIL import Image from scipy.signal import convolve2d   # Predefined filters filtros = {     # Detects vertical edges     \"vertical\": np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),     # Detects horizontal edges     \"horizontal\": np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),     # Sobel X (enhanced vertical edges)     \"sobel_x\": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),     # Sobel Y (enhanced horizontal edges)     \"sobel_y\": np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),     # Blur     \"blur\": np.ones((3, 3)) / 9.0,     # Edge detection (Laplacian)     \"laplacian\": np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]), }  # Load image image_pil = Image.open(     \"../../assets/course/topic_02_mathematics/cat_image.jpg\" ).convert(\"L\") imagen = np.array(image_pil)  # Apply all filters resultados = {} for nombre, filtro in filtros.items():     resultados[nombre] = convolve2d(imagen, filtro, mode=\"same\", boundary=\"symm\")  # Display results fig, axes = plt.subplots(2, 4, figsize=(15, 8)) axes = axes.flatten()  # Original image axes[0].imshow(imagen, cmap=\"gray\") axes[0].set_title(\"Original\") axes[0].axis(\"off\")  # Filtered images for i, (nombre, resultado) in enumerate(resultados.items(), 1):     axes[i].imshow(resultado, cmap=\"gray\")     axes[i].set_title(nombre.capitalize())     axes[i].axis(\"off\")  plt.tight_layout() plt.show() <p>Filters like Sobel or Laplacian enhance abrupt intensity transitions, that is, edges. Blur filters average local values, smoothing noise and fine details.</p> <p>Padding adds rows and columns of zeros around the input image. Its main purpose is to control the size of the output map and preserve information at the edges.</p> <p>Without padding, the size is reduced. For example:</p> <ul> <li>Input: $5 \\times 5$, Kernel: $3 \\times 3$, Stride $= 1$ Output: $3 \\times 3$</li> </ul> <p>With <code>padding = 1</code>, the effective input becomes $7 \\times 7$, so that:</p> <ul> <li>Extended input: $7 \\times 7$, Kernel: $3 \\times 3$, Stride $= 1$ Output: $5 \\times 5$</li> </ul> <p>In practice:</p> <ul> <li><code>padding = 0</code>: Used when one wishes to progressively reduce spatial size.</li> <li><code>padding = (kernel_size - 1) // 2</code>: Used to maintain the same input and output size when <code>stride = 1</code>.</li> </ul> <p>The stride controls the displacement of the kernel over the image. A stride of 1 traverses all adjacent pixels; a larger stride skips positions, reducing the spatial resolution of the output.</p> <p>Examples (with padding $=0$):</p> <ul> <li><code>stride = 1</code>: Input $8 \\times 8$, Kernel $3 \\times 3$ \u2192 Output $6 \\times 6$.</li> <li><code>stride = 2</code>: Input $8 \\times 8$, Kernel $3 \\times 3$ \u2192 Output $3 \\times 3$.</li> </ul> <p>Typical usage:</p> <ul> <li><code>stride = 1</code>: Used to capture all spatial details.</li> <li><code>stride = 2</code>: Used to reduce spatial size and computational cost, acting similarly to pooling.</li> </ul> <p>The kernel size determines the local field of view of the convolution:</p> <ul> <li>Small kernel, such as $3 \\times 3$: Most common, efficient, and sufficient in most modern architectures.</li> <li>Large kernel, such as $5 \\times 5$ or $7 \\times 7$: Covers more context in a single operation but introduces many more parameters.</li> </ul> <p>It is more efficient to stack several $3 \\times 3$ convolutions than to use a single convolution with a large kernel. For example:</p> <ul> <li>A $5 \\times 5$ layer uses $25$ parameters per channel.</li> <li>Two consecutive $3 \\times 3$ layers use $18$ parameters per channel and achieve a similar or superior effective receptive field, while also introducing more nonlinearity.</li> </ul> <p>Pooling reduces the spatial resolution of activation maps by aggregating information in local regions. It is used to decrease the size of intermediate tensors, control overfitting, and gain invariance to small translations.</p> <p>Max pooling takes the maximum value within each region.</p> In\u00a0[3]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef max_pool2d(image, pool_size=2):\n    h, w = image.shape\n    out_h, out_w = h // pool_size, w // pool_size\n    output = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[\n                i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size\n            ]\n            output[i, j] = np.max(region)\n\n    return output\n\n\n# Simple example with a 4x4 matrix\nsimple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])\n\nprint(\"Original image (4x4):\")\nprint(simple_image)\nprint()\n\npooled = max_pool2d(simple_image, pool_size=2)\n\nprint(\"After max pooling (2x2):\")\nprint(pooled)\nprint()\n\n# Step by step explanation\nprint(\"Step by step:\")\nprint(\n    f\"Top-left region: {simple_image[0:2, 0:2].flatten()} \u2192 max = {np.max(simple_image[0:2, 0:2])}\"\n)\nprint(\n    f\"Top-right region: {simple_image[0:2, 2:4].flatten()} \u2192 max = {np.max(simple_image[0:2, 2:4])}\"\n)\nprint(\n    f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} \u2192 max = {np.max(simple_image[2:4, 0:2])}\"\n)\nprint(\n    f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} \u2192 max = {np.max(simple_image[2:4, 2:4])}\"\n)\n</pre> # 3pps import numpy as np   def max_pool2d(image, pool_size=2):     h, w = image.shape     out_h, out_w = h // pool_size, w // pool_size     output = np.zeros((out_h, out_w))      for i in range(out_h):         for j in range(out_w):             region = image[                 i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size             ]             output[i, j] = np.max(region)      return output   # Simple example with a 4x4 matrix simple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])  print(\"Original image (4x4):\") print(simple_image) print()  pooled = max_pool2d(simple_image, pool_size=2)  print(\"After max pooling (2x2):\") print(pooled) print()  # Step by step explanation print(\"Step by step:\") print(     f\"Top-left region: {simple_image[0:2, 0:2].flatten()} \u2192 max = {np.max(simple_image[0:2, 0:2])}\" ) print(     f\"Top-right region: {simple_image[0:2, 2:4].flatten()} \u2192 max = {np.max(simple_image[0:2, 2:4])}\" ) print(     f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} \u2192 max = {np.max(simple_image[2:4, 0:2])}\" ) print(     f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} \u2192 max = {np.max(simple_image[2:4, 2:4])}\" ) <pre>Original image (4x4):\n[[ 1  2  5  6]\n [ 3  4  7  8]\n [ 9 10 13 14]\n [11 12 15 16]]\n\nAfter max pooling (2x2):\n[[ 4.  8.]\n [12. 16.]]\n\nStep by step:\nTop-left region: [1 2 3 4] \u2192 max = 4\nTop-right region: [5 6 7 8] \u2192 max = 8\nBottom-left region: [ 9 10 11 12] \u2192 max = 12\nBottom-right region: [13 14 15 16] \u2192 max = 16\n</pre> <p>Max pooling preserves the strongest responses in each region, emphasizing the presence of prominent features.</p> <p>Average pooling calculates the average in each region, smoothing the information.</p> In\u00a0[4]: Copied! <pre># 3pps\nimport numpy as np\n\n\ndef avg_pool2d(image, pool_size=2):\n    h, w = image.shape\n    out_h, out_w = h // pool_size, w // pool_size\n    output = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[\n                i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size\n            ]\n            output[i, j] = np.mean(region)\n\n    return output\n\n\n# Simple example with a 4x4 matrix\nsimple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])\n\nprint(\"Original image (4x4):\")\nprint(simple_image)\nprint()\n\npooled = avg_pool2d(simple_image, pool_size=2)\n\nprint(\"After average pooling (2x2):\")\nprint(pooled)\nprint()\n\n# Step by step explanation\nprint(\"Step by step:\")\nprint(\n    f\"Top-left region: {simple_image[0:2, 0:2].flatten()} \u2192 mean = {np.mean(simple_image[0:2, 0:2])}\"\n)\nprint(\n    f\"Top-right region: {simple_image[0:2, 2:4].flatten()} \u2192 mean = {np.mean(simple_image[0:2, 2:4])}\"\n)\nprint(\n    f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} \u2192 mean = {np.mean(simple_image[2:4, 0:2])}\"\n)\nprint(\n    f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} \u2192 mean = {np.mean(simple_image[2:4, 2:4])}\"\n)\n</pre> # 3pps import numpy as np   def avg_pool2d(image, pool_size=2):     h, w = image.shape     out_h, out_w = h // pool_size, w // pool_size     output = np.zeros((out_h, out_w))      for i in range(out_h):         for j in range(out_w):             region = image[                 i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size             ]             output[i, j] = np.mean(region)      return output   # Simple example with a 4x4 matrix simple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])  print(\"Original image (4x4):\") print(simple_image) print()  pooled = avg_pool2d(simple_image, pool_size=2)  print(\"After average pooling (2x2):\") print(pooled) print()  # Step by step explanation print(\"Step by step:\") print(     f\"Top-left region: {simple_image[0:2, 0:2].flatten()} \u2192 mean = {np.mean(simple_image[0:2, 0:2])}\" ) print(     f\"Top-right region: {simple_image[0:2, 2:4].flatten()} \u2192 mean = {np.mean(simple_image[0:2, 2:4])}\" ) print(     f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} \u2192 mean = {np.mean(simple_image[2:4, 0:2])}\" ) print(     f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} \u2192 mean = {np.mean(simple_image[2:4, 2:4])}\" ) <pre>Original image (4x4):\n[[ 1  2  5  6]\n [ 3  4  7  8]\n [ 9 10 13 14]\n [11 12 15 16]]\n\nAfter average pooling (2x2):\n[[ 2.5  6.5]\n [10.5 14.5]]\n\nStep by step:\nTop-left region: [1 2 3 4] \u2192 mean = 2.5\nTop-right region: [5 6 7 8] \u2192 mean = 6.5\nBottom-left region: [ 9 10 11 12] \u2192 mean = 10.5\nBottom-right region: [13 14 15 16] \u2192 mean = 14.5\n</pre> <p>Comparatively, max pooling is more aggressive and focused on prominent features, while average pooling provides a more smoothed representation, which can be useful in tasks where a more stable global response is desired.</p> <p>In modern efficiency-oriented architectures, especially on mobile devices, variants of standard convolution are used to drastically reduce the number of parameters and computational cost.</p> <p>The $1 \\times 1$ convolution acts on each spatial position independently and only mixes channels. It does not change the spatial size $(H, W)$, but it does change the number of channels.</p> In\u00a0[5]: Copied! <pre># 3pps\n# In PyTorch\nimport torch.nn as nn\n\n\nconv1x1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1)\n# Input: (B, 64, H, W) \u2192 Output: (B, 32, H, W)\n</pre> # 3pps # In PyTorch import torch.nn as nn   conv1x1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1) # Input: (B, 64, H, W) \u2192 Output: (B, 32, H, W) <p>It is used to reduce or increase the number of channels (bottleneck blocks in ResNet, for example) and to introduce nonlinearity between linear combinations of channels at very low cost. The number of parameters is:</p> <p>$$ \\text{parameters} = 64 \\times 32 \\times 1 \\times 1 = 2048. $$</p> <p>Depthwise convolution applies one filter per channel independently, without mixing them. In PyTorch, it is implemented using the <code>groups</code> parameter equal to the number of channels.</p> In\u00a0[6]: Copied! <pre>depthwise = nn.Conv2d(\n    in_channels=64,\n    out_channels=64,  # Same number of channels\n    kernel_size=3,\n    padding=1,\n    groups=64,  # One group per channel\n)\n</pre> depthwise = nn.Conv2d(     in_channels=64,     out_channels=64,  # Same number of channels     kernel_size=3,     padding=1,     groups=64,  # One group per channel ) <p>In a standard $3 \\times 3$ convolution from 64 to 64 channels, the number of parameters would be:</p> <p>$$ 64 \\times 64 \\times 3 \\times 3 = 36{,}864. $$</p> <p>In a depthwise convolution, only:</p> <p>$$ 64 \\times 3 \\times 3 = 576. $$</p> <p>This represents a massive cost reduction, although by itself it does not mix information between channels.</p> <p>Depthwise-separable convolution combines two steps:</p> <ol> <li>Depthwise convolution: Spatially filters each channel independently.</li> <li>Pointwise convolution ($1 \\times 1$): Mixes channels and adjusts the number of output channels.</li> </ol> In\u00a0[7]: Copied! <pre>depthwise_separable = nn.Sequential(\n    # Step 1: Depthwise (spatial filtering per channel)\n    nn.Conv2d(64, 64, kernel_size=3, groups=64, padding=1),\n    # Step 2: Pointwise (mix channels)\n    nn.Conv2d(64, 128, kernel_size=1),\n)\n</pre> depthwise_separable = nn.Sequential(     # Step 1: Depthwise (spatial filtering per channel)     nn.Conv2d(64, 64, kernel_size=3, groups=64, padding=1),     # Step 2: Pointwise (mix channels)     nn.Conv2d(64, 128, kernel_size=1), ) <p>Overall, the number of parameters is approximately 8\u20139 times smaller than that of an equivalent standard convolution, while maintaining competitive performance. This approach is widely used in efficient architectures such as MobileNet and EfficientNet.</p> <p>Groupwise convolution divides channels into several groups. Each group is processed independently, but within each group channels are mixed.</p> In\u00a0[8]: Copied! <pre>groupwise = nn.Conv2d(\n    in_channels=64,\n    out_channels=128,\n    kernel_size=3,\n    padding=1,\n    groups=4,  # Divides channels into 4 groups\n)\n</pre> groupwise = nn.Conv2d(     in_channels=64,     out_channels=128,     kernel_size=3,     padding=1,     groups=4,  # Divides channels into 4 groups ) <p>In this case, each group processes 16 input channels and produces 32 output channels. This technique allows balancing efficiency and modeling capacity and appears in architectures such as ResNeXt and ShuffleNet.</p> <p>In PyTorch, convolutional and pooling layers are typically combined with normalization and activation functions to form basic blocks.</p> In\u00a0[9]: Copied! <pre># 3pps\nimport torch.nn as nn\n\n\nstandard = nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n)\n</pre> # 3pps import torch.nn as nn   standard = nn.Sequential(     nn.Conv2d(3, 64, kernel_size=3, padding=1),     nn.BatchNorm2d(64),     nn.ReLU(inplace=True), ) <p>This block applies a standard $3 \\times 3$ convolution, normalizes activations with Batch Normalization, and applies a ReLU activation function.</p> In\u00a0[10]: Copied! <pre>efficient = nn.Sequential(\n    # Depthwise\n    nn.Conv2d(3, 3, kernel_size=3, groups=3, padding=1),\n    nn.BatchNorm2d(3),\n    nn.ReLU(inplace=True),\n    # Pointwise\n    nn.Conv2d(3, 64, kernel_size=1),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n)\n\n# Pooling\npooling = nn.MaxPool2d(kernel_size=2, stride=2)\n</pre> efficient = nn.Sequential(     # Depthwise     nn.Conv2d(3, 3, kernel_size=3, groups=3, padding=1),     nn.BatchNorm2d(3),     nn.ReLU(inplace=True),     # Pointwise     nn.Conv2d(3, 64, kernel_size=1),     nn.BatchNorm2d(64),     nn.ReLU(inplace=True), )  # Pooling pooling = nn.MaxPool2d(kernel_size=2, stride=2) <p>This block reduces computational cost while maintaining good representational power and is complemented with a pooling layer to reduce spatial resolution.</p> <p>MobileNetV2 uses inverted blocks with expansion and contraction of channels, combining $1 \\times 1$ and depthwise convolutions.</p> In\u00a0[11]: Copied! <pre>class MobileBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n\n        # 1. Expansion (1x1 conv)\n        self.expand = nn.Conv2d(in_ch, in_ch * 6, kernel_size=1)\n\n        # 2. Depthwise (3x3)\n        self.depthwise = nn.Conv2d(\n            in_ch * 6,\n            in_ch * 6,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            groups=in_ch * 6,\n        )\n\n        # 3. Projection (1x1 conv)\n        self.project = nn.Conv2d(in_ch * 6, out_ch, kernel_size=1)\n\n    def forward(self, x):\n        out = self.expand(x)\n        out = self.depthwise(out)\n        out = self.project(out)\n        return out\n</pre> class MobileBlock(nn.Module):     def __init__(self, in_ch, out_ch, stride=1):         super().__init__()          # 1. Expansion (1x1 conv)         self.expand = nn.Conv2d(in_ch, in_ch * 6, kernel_size=1)          # 2. Depthwise (3x3)         self.depthwise = nn.Conv2d(             in_ch * 6,             in_ch * 6,             kernel_size=3,             stride=stride,             padding=1,             groups=in_ch * 6,         )          # 3. Projection (1x1 conv)         self.project = nn.Conv2d(in_ch * 6, out_ch, kernel_size=1)      def forward(self, x):         out = self.expand(x)         out = self.depthwise(out)         out = self.project(out)         return out <p>This type of block allows building highly efficient deep networks on limited hardware.</p> <p>For a 2D convolution with input size $\\text{in\\_size}$, kernel size $k$, padding $p$, and stride $s$, the unidimensional output size (per axis) is given by:</p> <p>$$ \\text{out\\_size} = \\left\\lfloor \\frac{\\text{in\\_size} + 2p - k}{s} \\right\\rfloor + 1 $$</p> <p>Example:</p> <ul> <li>Input: $32$, Kernel: $3$, Padding: $1$, Stride: $1$ $$ \\text{out\\_size} = \\left\\lfloor \\frac{32 + 2 \\times 1 - 3}{1} \\right\\rfloor + 1 = 32 $$ that is, spatial size is maintained.</li> </ul>"},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#convolutional-layers","title":"Convolutional Layers\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#two-dimensional-convolution-from-scratch","title":"Two-Dimensional Convolution from Scratch\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#predefined-filters-for-feature-detection","title":"Predefined Filters for Feature Detection\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#fundamental-components-of-convolution","title":"Fundamental Components of Convolution\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#padding","title":"Padding\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#stride","title":"Stride\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#kernel-size","title":"Kernel Size\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#pooling-spatial-reduction","title":"Pooling: Spatial Reduction\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#max-pooling","title":"Max Pooling\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#average-pooling","title":"Average Pooling\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#types-of-efficient-convolutions","title":"Types of Efficient Convolutions\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#1-times-1-convolution-pointwise","title":"$1 \\times 1$ Convolution (Pointwise)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#depthwise-convolution","title":"Depthwise Convolution\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#depthwise-separable-convolution","title":"Depthwise-Separable Convolution\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#groupwise-convolution","title":"Groupwise Convolution\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#implementation-of-convolutional-blocks-in-pytorch","title":"Implementation of Convolutional Blocks in PyTorch\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#standard-convolutional-block","title":"Standard Convolutional Block\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#efficient-depthwise-separable-block","title":"Efficient Depthwise-Separable Block\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#mobilenetv2-type-block","title":"MobileNetV2-Type Block\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_03_convolutional_layer.html#comparison-of-convolutions-and-output-size-formula","title":"Comparison of Convolutions and Output Size Formula\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html","title":"LeNet","text":"<p>The LeNet-5 architecture, developed by Yann LeCun and his collaborators between 1988 and 1998, constitutes one of the earliest and most influential convolutional neural network architectures. It was specifically designed to address the problem of automatic handwritten character recognition, a task of considerable practical importance at the time. LeNet-5 was not merely a theoretical contribution, but was successfully deployed in real-world industrial systems, most notably for automatic check processing in the United States. For this reason, it is widely regarded as one of the first concrete and large-scale applications of deep learning in an operational setting.</p> <p>A central contribution of LeNet-5 lies in its demonstration that hierarchical feature representations can be learned directly from raw image data. By progressively transforming the input through multiple layers, the network captures increasingly abstract patterns while preserving the underlying spatial organization of the image. This approach leads to a significant reduction in the number of trainable parameters when compared to traditional fully connected multilayer perceptrons. In earlier architectures, each pixel was treated as an independent input feature, resulting in a loss of spatial information and a rapid growth in the number of parameters as input resolution increased. LeNet-5 overcomes these limitations by explicitly leveraging the two-dimensional structure of images and the strong local correlations that exist between neighboring pixels.</p> <p>The architecture illustrates how the coordinated use of convolutional layers, subsampling operations, and nonlinear activation functions enables the construction of models that are both expressive and computationally efficient. Convolutional layers enforce local connectivity and parameter sharing, subsampling layers progressively reduce spatial resolution while increasing robustness, and nonlinearities allow the network to model complex decision boundaries. Together, these components yield systems that are resilient to variations in position, scale, and moderate geometric deformations of handwritten characters, without incurring prohibitive computational costs.</p> <p>Through these design choices, LeNet-5 established a set of architectural principles that continue to underpin modern convolutional neural networks used in computer vision today. Its emphasis on spatial locality, hierarchical feature learning, and efficiency makes it a direct conceptual ancestor of many contemporary deep learning models, and a foundational milestone in the historical development of neural network architectures.</p> <p>Before the introduction of LeNet-5, image recognition tasks were predominantly addressed using fully connected multilayer perceptrons. This approach exhibits a fundamental structural limitation: the two-dimensional image is flattened into a one-dimensional vector before being processed by the network. As a consequence, all information about the relative spatial arrangement of pixels is lost. Pixels that are neighbors in the original image are treated in the same way as pixels that are far apart, preventing the model from exploiting spatial locality. This representation makes the system highly sensitive to small translations, local deformations, or changes in the position of the object within the image. Moreover, as image resolution increases, the number of parameters grows rapidly, leading to high computational costs, difficulties during training, and a strong tendency toward overfitting, particularly when only limited training data are available.</p> <p>LeNet-5 introduced a decisive conceptual shift by combining convolutional operations, subsampling mechanisms, and systematic weight sharing. Convolutional layers are designed to detect local patterns, such as edges, corners, or elementary strokes, while explicitly preserving the two-dimensional structure of the input image. Each convolutional filter is applied by sliding it across the image, acting as a specialized detector that responds strongly to a specific visual pattern wherever it appears. In this way, local features are extracted consistently across the entire spatial extent of the image.</p> <p>Subsampling layers, implemented in LeNet-5 through average pooling operations, further transform these feature maps by progressively reducing their spatial resolution. This dimensionality reduction introduces a degree of invariance to small translations and minor geometric deformations, as the precise location of a feature becomes less critical at coarser scales. At the same time, subsampling significantly decreases computational complexity and reduces the number of parameters required in subsequent layers, thereby improving efficiency and stability during training.</p> <p>A further key design principle is weight sharing. Instead of learning a distinct set of weights for each spatial position, the same convolutional filter is reused across all locations in the image. This approach drastically reduces the total number of parameters and enforces the idea that a meaningful visual pattern, such as a vertical stroke or an edge, should be recognized consistently regardless of its position. As a result, weight sharing enhances generalization and contributes to the robustness of the learned representations.</p> <p>The combined effect of convolution, subsampling, and weight sharing enables LeNet-5 to learn hierarchical representations in a progressive and structured manner. Early layers capture simple, localized features, while deeper layers integrate these elements into increasingly abstract and task-specific representations. This hierarchical organization of features remains a fundamental principle in contemporary convolutional neural network architectures, forming the conceptual foundation for models ranging from AlexNet to more recent vision systems, including transformer-based architectures.</p> <p>This section presents a modern, functional implementation inspired by LeNet using PyTorch. The goal is to have a complete workflow, executable end-to-end and directly convertible into a Jupyter Notebook. The MNIST dataset is used as the reference dataset for handwritten digit classification.</p> <p>First, the necessary libraries from Python\u2019s standard library and third-party packages are imported, including modules for model construction, data handling, visualization, and embedding analysis.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nfrom typing import Any\n\n# 3pps\n# Third-party libraries\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.manifold import TSNE\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> # Standard libraries from typing import Any  # 3pps # Third-party libraries import matplotlib.pyplot as plt import torch from sklearn.manifold import TSNE from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm   print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.9.1+cu128\nCUDA available: False\n</pre> <p>The device is automatically selected based on GPU availability. If CUDA is available, the GPU is used; otherwise, the model runs on the CPU.</p> In\u00a0[2]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device used: {device}\")\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Device used: {device}\") <pre>Device used: cpu\n</pre> <p>A helper function is defined to visually inspect examples from the dataset, displaying a set of images with their corresponding labels. This helps quickly verify that preprocessing is correct and samples are interpreted properly.</p> In\u00a0[3]: Copied! <pre>def show_images(images, labels):\n    fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n    if len(images) == 1:\n        axes = [axes]\n\n    for img, label, ax in zip(images, labels, axes):\n        ax.imshow(img.squeeze(), cmap=\"gray\")\n        ax.set_title(f\"Digit: {label}\")\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> def show_images(images, labels):     fig, axes = plt.subplots(1, len(images), figsize=(15, 3))     if len(images) == 1:         axes = [axes]      for img, label, ax in zip(images, labels, axes):         ax.imshow(img.squeeze(), cmap=\"gray\")         ax.set_title(f\"Digit: {label}\")         ax.axis(\"off\")      plt.tight_layout()     plt.show() <p>The MNIST dataset contains grayscale images of handwritten digits sized $28 \\times 28$. Preprocessing includes normalization using the mean $\\mu = 0.1307$ and standard deviation $\\sigma = 0.3081$, estimated from the dataset itself. Normalization is defined as:</p> <p>$$ x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}. $$</p> <p>This centers the data and scales it, facilitating and stabilizing the training of deep networks by improving the numerical conditioning of optimization operations.</p> In\u00a0[4]: Copied! <pre>transform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=transform\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n</pre> transform = transforms.Compose(     [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] )  train_dataset = datasets.MNIST(     root=\"./data\", train=True, download=True, transform=transform )  test_dataset = datasets.MNIST(     root=\"./data\", train=False, download=True, transform=transform )  print(f\"Training samples: {len(train_dataset)}\") print(f\"Test samples: {len(test_dataset)}\") <pre>\r  0%|          | 0.00/9.91M [00:00&lt;?, ?B/s]</pre> <pre>\r  1%|          | 98.3k/9.91M [00:00&lt;00:14, 689kB/s]</pre> <pre>\r  4%|\u258d         | 426k/9.91M [00:00&lt;00:05, 1.62MB/s]</pre> <pre>\r 17%|\u2588\u258b        | 1.70M/9.91M [00:00&lt;00:01, 5.45MB/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u258d      | 3.44M/9.91M [00:00&lt;00:00, 9.34MB/s]</pre> <pre>\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 9.50M/9.91M [00:00&lt;00:00, 24.7MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00&lt;00:00, 16.0MB/s]</pre> <pre>\n</pre> <pre>\r  0%|          | 0.00/28.9k [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 419kB/s]</pre> <pre>\n</pre> <pre>\r  0%|          | 0.00/1.65M [00:00&lt;?, ?B/s]</pre> <pre>\r  6%|\u258c         | 98.3k/1.65M [00:00&lt;00:02, 726kB/s]</pre> <pre>\r 26%|\u2588\u2588\u258c       | 426k/1.65M [00:00&lt;00:00, 1.72MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00&lt;00:00, 4.81MB/s]</pre> <pre>\n</pre> <pre>\r  0%|          | 0.00/4.54k [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 9.69MB/s]</pre> <pre>Training samples: 60000\nTest samples: 10000\n</pre> <pre>\n</pre> <p>DataLoaders are created from the training and test sets to batch samples, shuffle training examples, and efficiently handle data transfer to the computation device.</p> In\u00a0[5]: Copied! <pre>BATCH_SIZE = 32\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n)\n\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n)\n</pre> BATCH_SIZE = 32  train_dataloader = DataLoader(     train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 )  test_dataloader = DataLoader(     test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2 ) <p>Before training, it is useful to inspect some training samples. The mean and standard deviation of a batch are also computed to verify proper normalization.</p> In\u00a0[6]: Copied! <pre>images, labels = next(iter(train_dataloader))\nshow_images(images[:10], labels[:10])\n\nprint(f\"Batch mean: {images.mean():.3f}\")\nprint(f\"Batch standard deviation: {images.std():.3f}\")\n</pre> images, labels = next(iter(train_dataloader)) show_images(images[:10], labels[:10])  print(f\"Batch mean: {images.mean():.3f}\") print(f\"Batch standard deviation: {images.std():.3f}\") <pre>Batch mean: 0.022\nBatch standard deviation: 1.023\n</pre> <p>A modern, simplified version of LeNet is defined, adapted to MNIST and current deep learning practices. While not exactly replicating the original LeNet-5, it preserves the design spirit: a convolutional part for spatial feature extraction and a fully connected part for classification. Batch normalization and ReLU activation are included, standard in contemporary architectures, improving convergence speed and training stability.</p> In\u00a0[7]: Copied! <pre>class LeNet(nn.Module):\n    def __init__(self, input_channels: int = 1):\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channels, 16, kernel_size=4, stride=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n\n        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(32, 10))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n</pre> class LeNet(nn.Module):     def __init__(self, input_channels: int = 1):         super().__init__()          self.features = nn.Sequential(             nn.Conv2d(input_channels, 16, kernel_size=4, stride=2),             nn.BatchNorm2d(16),             nn.ReLU(),             nn.Conv2d(16, 32, kernel_size=4, stride=2),             nn.BatchNorm2d(32),             nn.ReLU(),             nn.AdaptiveAvgPool2d((1, 1)),         )          self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(32, 10))      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.features(x)         x = self.classifier(x)         return x <p>The <code>features</code> block applies two convolutional layers with spatial resolution reduction via <code>stride=2</code>, followed by batch normalization and ReLU activation. <code>nn.AdaptiveAvgPool2d((1, 1))</code> then adaptively reduces each feature map to size $1 \\times 1$ per channel, making the architecture robust to small spatial input variations. The <code>classifier</code> block flattens the features and applies a linear layer to produce logits for the 10 digit classes, later interpreted by <code>CrossEntropyLoss</code>, which internally applies softmax.</p> <p>The model is instantiated, moved to the selected device, and <code>torchinfo.summary</code> provides a structured architecture overview, including input/output dimensions and parameter counts.</p> In\u00a0[8]: Copied! <pre>model = LeNet().to(device)\n\nsummary(model, input_size=(BATCH_SIZE, 1, 28, 28), device=str(device))\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total trainable parameters: {total_params:,}\")\n</pre> model = LeNet().to(device)  summary(model, input_size=(BATCH_SIZE, 1, 28, 28), device=str(device))  total_params = sum(p.numel() for p in model.parameters()) print(f\"Total trainable parameters: {total_params:,}\") <pre>Total trainable parameters: 8,922\n</pre> <p>Training hyperparameters, optimizer, and loss function are defined. AdamW is used, combining Adam\u2019s advantages with explicit weight decay regularization. The chosen loss is <code>CrossEntropyLoss</code>, suitable for multi-class classification with integer labels.</p> In\u00a0[9]: Copied! <pre>NUM_EPOCHS = 2\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-4\n\noptimizer = torch.optim.AdamW(\n    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\n\nloss_function = nn.CrossEntropyLoss()\n</pre> NUM_EPOCHS = 2 LEARNING_RATE = 1e-3 WEIGHT_DECAY = 1e-4  optimizer = torch.optim.AdamW(     model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )  loss_function = nn.CrossEntropyLoss() <p>Training is organized in epochs. Each epoch updates model parameters on the training set, then evaluates performance on the test set without updating parameters. Loss and accuracy are recorded for both sets to analyze learning progression and detect issues such as overfitting.</p> In\u00a0[10]: Copied! <pre>train_losses, test_losses = [], []\ntrain_accuracies, test_accuracies = [], []\n\nfor epoch in range(NUM_EPOCHS):\n\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in tqdm(\n        train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\"\n    ):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n\n    train_losses.append(running_loss / len(train_dataloader))\n    train_accuracies.append(100 * correct / total)\n\n    model.eval()\n    test_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(\n            test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TEST]\"\n        ):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n\n    test_losses.append(test_loss / len(test_dataloader))\n    test_accuracies.append(100 * correct / total)\n\n    print(f\"Epoch {epoch+1}\")\n    print(f\"  Train \u2192 Loss: {train_losses[-1]:.4f} | Acc: {train_accuracies[-1]:.2f}%\")\n    print(f\"  Test  \u2192 Loss: {test_losses[-1]:.4f} | Acc: {test_accuracies[-1]:.2f}%\")\n</pre> train_losses, test_losses = [], [] train_accuracies, test_accuracies = [], []  for epoch in range(NUM_EPOCHS):      model.train()     running_loss, correct, total = 0.0, 0, 0      for images, labels in tqdm(         train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\"     ):         images, labels = images.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(images)         loss = loss_function(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()         _, preds = torch.max(outputs, 1)         total += labels.size(0)         correct += (preds == labels).sum().item()      train_losses.append(running_loss / len(train_dataloader))     train_accuracies.append(100 * correct / total)      model.eval()     test_loss, correct, total = 0.0, 0, 0      with torch.no_grad():         for images, labels in tqdm(             test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TEST]\"         ):             images, labels = images.to(device), labels.to(device)             outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             _, preds = torch.max(outputs, 1)             total += labels.size(0)             correct += (preds == labels).sum().item()      test_losses.append(test_loss / len(test_dataloader))     test_accuracies.append(100 * correct / total)      print(f\"Epoch {epoch+1}\")     print(f\"  Train \u2192 Loss: {train_losses[-1]:.4f} | Acc: {train_accuracies[-1]:.2f}%\")     print(f\"  Test  \u2192 Loss: {test_losses[-1]:.4f} | Acc: {test_accuracies[-1]:.2f}%\") <pre>\rEpoch 1/2 [TRAIN]:   0%|          | 0/1875 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   0%|          | 8/1875 [00:00&lt;00:23, 78.16it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   1%|\u258f         | 24/1875 [00:00&lt;00:14, 125.36it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   2%|\u258f         | 40/1875 [00:00&lt;00:13, 138.91it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   3%|\u258e         | 57/1875 [00:00&lt;00:12, 147.97it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   4%|\u258d         | 74/1875 [00:00&lt;00:11, 154.36it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   5%|\u258d         | 92/1875 [00:00&lt;00:11, 160.45it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   6%|\u258c         | 110/1875 [00:00&lt;00:10, 165.18it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   7%|\u258b         | 128/1875 [00:00&lt;00:10, 168.44it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   8%|\u258a         | 146/1875 [00:00&lt;00:10, 169.76it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:   9%|\u2589         | 165/1875 [00:01&lt;00:09, 173.97it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  10%|\u2589         | 183/1875 [00:01&lt;00:09, 175.28it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  11%|\u2588         | 201/1875 [00:01&lt;00:09, 175.33it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  12%|\u2588\u258f        | 219/1875 [00:01&lt;00:09, 175.15it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  13%|\u2588\u258e        | 237/1875 [00:01&lt;00:09, 175.48it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  14%|\u2588\u258e        | 255/1875 [00:01&lt;00:09, 175.62it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  15%|\u2588\u258d        | 274/1875 [00:01&lt;00:08, 177.90it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  16%|\u2588\u258c        | 292/1875 [00:01&lt;00:08, 176.89it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  17%|\u2588\u258b        | 310/1875 [00:01&lt;00:08, 175.75it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  17%|\u2588\u258b        | 328/1875 [00:01&lt;00:08, 174.20it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  18%|\u2588\u258a        | 346/1875 [00:02&lt;00:08, 174.34it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  19%|\u2588\u2589        | 364/1875 [00:02&lt;00:08, 175.46it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  20%|\u2588\u2588        | 382/1875 [00:02&lt;00:08, 175.95it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  21%|\u2588\u2588\u258f       | 400/1875 [00:02&lt;00:08, 176.42it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  22%|\u2588\u2588\u258f       | 418/1875 [00:02&lt;00:08, 177.07it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  23%|\u2588\u2588\u258e       | 436/1875 [00:02&lt;00:08, 177.72it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  24%|\u2588\u2588\u258d       | 454/1875 [00:02&lt;00:08, 177.12it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  25%|\u2588\u2588\u258c       | 472/1875 [00:02&lt;00:07, 176.48it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  26%|\u2588\u2588\u258c       | 490/1875 [00:02&lt;00:07, 174.89it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  27%|\u2588\u2588\u258b       | 508/1875 [00:02&lt;00:07, 173.89it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  28%|\u2588\u2588\u258a       | 526/1875 [00:03&lt;00:07, 174.53it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  29%|\u2588\u2588\u2589       | 545/1875 [00:03&lt;00:07, 176.72it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  30%|\u2588\u2588\u2588       | 563/1875 [00:03&lt;00:07, 177.19it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  31%|\u2588\u2588\u2588       | 581/1875 [00:03&lt;00:07, 176.79it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  32%|\u2588\u2588\u2588\u258f      | 599/1875 [00:03&lt;00:07, 177.48it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  33%|\u2588\u2588\u2588\u258e      | 617/1875 [00:03&lt;00:07, 175.55it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  34%|\u2588\u2588\u2588\u258d      | 635/1875 [00:03&lt;00:07, 175.54it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  35%|\u2588\u2588\u2588\u258d      | 653/1875 [00:03&lt;00:07, 173.07it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  36%|\u2588\u2588\u2588\u258c      | 671/1875 [00:03&lt;00:06, 172.14it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  37%|\u2588\u2588\u2588\u258b      | 690/1875 [00:04&lt;00:06, 174.84it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 708/1875 [00:04&lt;00:06, 175.92it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  39%|\u2588\u2588\u2588\u2589      | 727/1875 [00:04&lt;00:06, 176.80it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  40%|\u2588\u2588\u2588\u2589      | 746/1875 [00:04&lt;00:06, 179.92it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  41%|\u2588\u2588\u2588\u2588      | 765/1875 [00:04&lt;00:06, 181.02it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258f     | 784/1875 [00:04&lt;00:06, 179.18it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  43%|\u2588\u2588\u2588\u2588\u258e     | 802/1875 [00:04&lt;00:06, 176.56it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  44%|\u2588\u2588\u2588\u2588\u258e     | 820/1875 [00:04&lt;00:05, 176.69it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258d     | 839/1875 [00:04&lt;00:05, 177.82it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  46%|\u2588\u2588\u2588\u2588\u258c     | 857/1875 [00:04&lt;00:05, 178.40it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  47%|\u2588\u2588\u2588\u2588\u258b     | 875/1875 [00:05&lt;00:05, 177.34it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 894/1875 [00:05&lt;00:05, 178.49it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  49%|\u2588\u2588\u2588\u2588\u258a     | 912/1875 [00:05&lt;00:05, 177.57it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2589     | 931/1875 [00:05&lt;00:05, 178.66it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  51%|\u2588\u2588\u2588\u2588\u2588     | 949/1875 [00:05&lt;00:05, 177.06it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 967/1875 [00:05&lt;00:05, 176.03it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 985/1875 [00:05&lt;00:05, 176.81it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 1004/1875 [00:05&lt;00:04, 178.51it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 1023/1875 [00:05&lt;00:04, 181.18it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 1042/1875 [00:05&lt;00:04, 179.86it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 1060/1875 [00:06&lt;00:04, 179.49it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 1079/1875 [00:06&lt;00:04, 180.74it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 1098/1875 [00:06&lt;00:04, 183.15it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 1117/1875 [00:06&lt;00:04, 184.69it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 1136/1875 [00:06&lt;00:04, 183.34it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 1155/1875 [00:06&lt;00:03, 185.10it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 1174/1875 [00:06&lt;00:03, 185.83it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 1193/1875 [00:06&lt;00:03, 183.41it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 1212/1875 [00:06&lt;00:03, 181.30it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 1231/1875 [00:07&lt;00:03, 182.10it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 1250/1875 [00:07&lt;00:03, 181.96it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 1269/1875 [00:07&lt;00:03, 182.22it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 1288/1875 [00:07&lt;00:03, 180.90it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 1307/1875 [00:07&lt;00:03, 183.15it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 1327/1875 [00:07&lt;00:02, 185.39it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 1346/1875 [00:07&lt;00:02, 186.07it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 1365/1875 [00:07&lt;00:02, 186.23it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 1384/1875 [00:07&lt;00:02, 186.83it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 1404/1875 [00:07&lt;00:02, 188.11it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 1423/1875 [00:08&lt;00:02, 187.97it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 1443/1875 [00:08&lt;00:02, 189.70it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 1462/1875 [00:08&lt;00:02, 189.38it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 1481/1875 [00:08&lt;00:02, 189.55it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 1500/1875 [00:08&lt;00:01, 189.30it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 1519/1875 [00:08&lt;00:01, 189.47it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1538/1875 [00:08&lt;00:01, 188.86it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1557/1875 [00:08&lt;00:01, 189.08it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 1576/1875 [00:08&lt;00:01, 188.51it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 1595/1875 [00:08&lt;00:01, 188.93it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 1614/1875 [00:09&lt;00:01, 188.04it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 1633/1875 [00:09&lt;00:01, 186.15it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 1652/1875 [00:09&lt;00:01, 184.91it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 1671/1875 [00:09&lt;00:01, 185.31it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1690/1875 [00:09&lt;00:01, 183.10it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1709/1875 [00:09&lt;00:00, 183.10it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 1728/1875 [00:09&lt;00:00, 180.97it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 1747/1875 [00:09&lt;00:00, 181.73it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 1766/1875 [00:09&lt;00:00, 180.91it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 1785/1875 [00:10&lt;00:00, 179.68it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 1804/1875 [00:10&lt;00:00, 180.25it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 1823/1875 [00:10&lt;00:00, 181.86it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 1842/1875 [00:10&lt;00:00, 183.57it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1861/1875 [00:10&lt;00:00, 181.68it/s]</pre> <pre>\rEpoch 1/2 [TRAIN]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:10&lt;00:00, 178.37it/s]</pre> <pre>\n</pre> <pre>\rEpoch 1/2 [TEST]:   0%|          | 0/313 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/2 [TEST]:   5%|\u258c         | 16/313 [00:00&lt;00:01, 159.60it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  13%|\u2588\u258e        | 42/313 [00:00&lt;00:01, 212.46it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  22%|\u2588\u2588\u258f       | 68/313 [00:00&lt;00:01, 228.37it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  30%|\u2588\u2588\u2588       | 94/313 [00:00&lt;00:00, 235.74it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 120/313 [00:00&lt;00:00, 238.83it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  46%|\u2588\u2588\u2588\u2588\u258b     | 145/313 [00:00&lt;00:00, 242.24it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 171/313 [00:00&lt;00:00, 246.12it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 197/313 [00:00&lt;00:00, 248.32it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 224/313 [00:00&lt;00:00, 253.68it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 250/313 [00:01&lt;00:00, 250.71it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 276/313 [00:01&lt;00:00, 246.10it/s]</pre> <pre>\rEpoch 1/2 [TEST]:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 304/313 [00:01&lt;00:00, 252.11it/s]</pre> <pre>\rEpoch 1/2 [TEST]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:01&lt;00:00, 240.26it/s]</pre> <pre>\n</pre> <pre>Epoch 1\n  Train \u2192 Loss: 0.8921 | Acc: 78.10%\n  Test  \u2192 Loss: 0.3603 | Acc: 92.15%\n</pre> <pre>\rEpoch 2/2 [TRAIN]:   0%|          | 0/1875 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   0%|          | 9/1875 [00:00&lt;00:21, 87.00it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   1%|\u258f         | 27/1875 [00:00&lt;00:13, 139.11it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   2%|\u258f         | 46/1875 [00:00&lt;00:11, 159.52it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   3%|\u258e         | 65/1875 [00:00&lt;00:10, 169.84it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   4%|\u258d         | 84/1875 [00:00&lt;00:10, 176.47it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   5%|\u258c         | 103/1875 [00:00&lt;00:09, 180.52it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   7%|\u258b         | 122/1875 [00:00&lt;00:09, 182.48it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   8%|\u258a         | 141/1875 [00:00&lt;00:09, 183.57it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:   9%|\u258a         | 161/1875 [00:00&lt;00:09, 187.44it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  10%|\u2589         | 181/1875 [00:01&lt;00:08, 189.95it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  11%|\u2588         | 202/1875 [00:01&lt;00:08, 193.68it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  12%|\u2588\u258f        | 223/1875 [00:01&lt;00:08, 196.47it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  13%|\u2588\u258e        | 244/1875 [00:01&lt;00:08, 197.81it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  14%|\u2588\u258d        | 264/1875 [00:01&lt;00:08, 197.17it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  15%|\u2588\u258c        | 284/1875 [00:01&lt;00:08, 194.66it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  16%|\u2588\u258c        | 304/1875 [00:01&lt;00:08, 192.81it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  17%|\u2588\u258b        | 324/1875 [00:01&lt;00:08, 193.43it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  18%|\u2588\u258a        | 344/1875 [00:01&lt;00:07, 192.78it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  19%|\u2588\u2589        | 364/1875 [00:01&lt;00:07, 191.79it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  20%|\u2588\u2588        | 384/1875 [00:02&lt;00:07, 190.29it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  22%|\u2588\u2588\u258f       | 404/1875 [00:02&lt;00:07, 188.59it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  23%|\u2588\u2588\u258e       | 423/1875 [00:02&lt;00:07, 187.00it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  24%|\u2588\u2588\u258e       | 442/1875 [00:02&lt;00:07, 184.88it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  25%|\u2588\u2588\u258d       | 462/1875 [00:02&lt;00:07, 186.72it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  26%|\u2588\u2588\u258c       | 481/1875 [00:02&lt;00:07, 187.63it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  27%|\u2588\u2588\u258b       | 501/1875 [00:02&lt;00:07, 188.46it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  28%|\u2588\u2588\u258a       | 520/1875 [00:02&lt;00:07, 188.78it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  29%|\u2588\u2588\u2589       | 540/1875 [00:02&lt;00:07, 190.43it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  30%|\u2588\u2588\u2589       | 560/1875 [00:03&lt;00:06, 189.45it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  31%|\u2588\u2588\u2588       | 579/1875 [00:03&lt;00:06, 187.92it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  32%|\u2588\u2588\u2588\u258f      | 598/1875 [00:03&lt;00:06, 186.52it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  33%|\u2588\u2588\u2588\u258e      | 617/1875 [00:03&lt;00:06, 183.70it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  34%|\u2588\u2588\u2588\u258d      | 636/1875 [00:03&lt;00:06, 182.50it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  35%|\u2588\u2588\u2588\u258d      | 655/1875 [00:03&lt;00:06, 181.55it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  36%|\u2588\u2588\u2588\u258c      | 674/1875 [00:03&lt;00:06, 181.82it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  37%|\u2588\u2588\u2588\u258b      | 694/1875 [00:03&lt;00:06, 184.74it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 713/1875 [00:03&lt;00:06, 185.54it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  39%|\u2588\u2588\u2588\u2589      | 732/1875 [00:03&lt;00:06, 184.62it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  40%|\u2588\u2588\u2588\u2588      | 751/1875 [00:04&lt;00:06, 184.86it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  41%|\u2588\u2588\u2588\u2588      | 770/1875 [00:04&lt;00:05, 186.12it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258f     | 789/1875 [00:04&lt;00:05, 186.07it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  43%|\u2588\u2588\u2588\u2588\u258e     | 808/1875 [00:04&lt;00:05, 186.29it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  44%|\u2588\u2588\u2588\u2588\u258d     | 827/1875 [00:04&lt;00:05, 185.29it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258c     | 846/1875 [00:04&lt;00:05, 186.43it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  46%|\u2588\u2588\u2588\u2588\u258c     | 865/1875 [00:04&lt;00:05, 187.06it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  47%|\u2588\u2588\u2588\u2588\u258b     | 885/1875 [00:04&lt;00:05, 188.31it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 904/1875 [00:04&lt;00:05, 188.70it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  49%|\u2588\u2588\u2588\u2588\u2589     | 923/1875 [00:04&lt;00:05, 187.97it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2588     | 943/1875 [00:05&lt;00:04, 188.87it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 963/1875 [00:05&lt;00:04, 190.17it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 983/1875 [00:05&lt;00:04, 190.18it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 1003/1875 [00:05&lt;00:04, 189.79it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 1022/1875 [00:05&lt;00:04, 189.15it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 1041/1875 [00:05&lt;00:04, 187.88it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 1060/1875 [00:05&lt;00:04, 187.51it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 1079/1875 [00:05&lt;00:04, 187.65it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 1098/1875 [00:05&lt;00:04, 184.91it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 1117/1875 [00:06&lt;00:04, 183.84it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 1136/1875 [00:06&lt;00:04, 184.24it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 1155/1875 [00:06&lt;00:03, 184.83it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 1174/1875 [00:06&lt;00:03, 184.75it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 1193/1875 [00:06&lt;00:03, 183.98it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 1212/1875 [00:06&lt;00:03, 183.94it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 1231/1875 [00:06&lt;00:03, 183.82it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 1250/1875 [00:06&lt;00:03, 184.97it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 1269/1875 [00:06&lt;00:03, 185.60it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 1288/1875 [00:06&lt;00:03, 186.53it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 1307/1875 [00:07&lt;00:03, 186.45it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 1326/1875 [00:07&lt;00:02, 185.18it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 1345/1875 [00:07&lt;00:02, 185.78it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 1364/1875 [00:07&lt;00:02, 185.89it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 1384/1875 [00:07&lt;00:02, 188.58it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 1404/1875 [00:07&lt;00:02, 190.42it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 1424/1875 [00:07&lt;00:02, 191.93it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 1444/1875 [00:07&lt;00:02, 191.21it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 1464/1875 [00:07&lt;00:02, 192.86it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 1484/1875 [00:07&lt;00:02, 193.76it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 1504/1875 [00:08&lt;00:01, 190.47it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1524/1875 [00:08&lt;00:01, 191.11it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1544/1875 [00:08&lt;00:01, 192.29it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1564/1875 [00:08&lt;00:01, 194.39it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 1584/1875 [00:08&lt;00:01, 195.76it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 1604/1875 [00:08&lt;00:01, 196.62it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 1624/1875 [00:08&lt;00:01, 195.80it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 1644/1875 [00:08&lt;00:01, 195.91it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 1664/1875 [00:08&lt;00:01, 195.39it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 1685/1875 [00:08&lt;00:00, 197.18it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1705/1875 [00:09&lt;00:00, 195.47it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 1725/1875 [00:09&lt;00:00, 194.95it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 1745/1875 [00:09&lt;00:00, 195.80it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 1766/1875 [00:09&lt;00:00, 197.41it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 1786/1875 [00:09&lt;00:00, 197.61it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 1806/1875 [00:09&lt;00:00, 195.88it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 1826/1875 [00:09&lt;00:00, 196.64it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 1847/1875 [00:09&lt;00:00, 198.33it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1867/1875 [00:09&lt;00:00, 197.89it/s]</pre> <pre>\rEpoch 2/2 [TRAIN]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:09&lt;00:00, 188.24it/s]</pre> <pre>\n</pre> <pre>\rEpoch 2/2 [TEST]:   0%|          | 0/313 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 2/2 [TEST]:   5%|\u258c         | 16/313 [00:00&lt;00:01, 156.66it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  13%|\u2588\u258e        | 42/313 [00:00&lt;00:01, 215.51it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  22%|\u2588\u2588\u258f       | 68/313 [00:00&lt;00:01, 233.66it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  30%|\u2588\u2588\u2588       | 94/313 [00:00&lt;00:00, 241.38it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 120/313 [00:00&lt;00:00, 246.43it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  47%|\u2588\u2588\u2588\u2588\u258b     | 146/313 [00:00&lt;00:00, 248.03it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 172/313 [00:00&lt;00:00, 249.70it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 198/313 [00:00&lt;00:00, 251.65it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 225/313 [00:00&lt;00:00, 256.57it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 251/313 [00:01&lt;00:00, 254.80it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 277/313 [00:01&lt;00:00, 253.56it/s]</pre> <pre>\rEpoch 2/2 [TEST]:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 303/313 [00:01&lt;00:00, 250.71it/s]</pre> <pre>\rEpoch 2/2 [TEST]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:01&lt;00:00, 242.42it/s]</pre> <pre>Epoch 2\n  Train \u2192 Loss: 0.3038 | Acc: 92.77%\n  Test  \u2192 Loss: 0.1965 | Acc: 95.45%\n</pre> <pre>\n</pre> <p><code>model.train()</code> activates training-specific behaviors, such as updating batch normalization statistics and applying dropout if present. <code>model.eval()</code> disables these behaviors for deterministic evaluation, and <code>torch.no_grad()</code> during validation avoids gradient computation, reducing memory usage and computation time.</p> <p>After training, loss and accuracy evolution for training and testing is plotted. This visual analysis helps identify overfitting, underfitting, or learning stagnation, guiding potential architecture or hyperparameter adjustments.</p> In\u00a0[11]: Copied! <pre>epochs = range(1, NUM_EPOCHS + 1)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Evolution\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\nplt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.title(\"Accuracy Evolution\")\n\nplt.tight_layout()\nplt.show()\n</pre> epochs = range(1, NUM_EPOCHS + 1)  plt.figure(figsize=(12, 5))  plt.subplot(1, 2, 1) plt.plot(epochs, train_losses, label=\"Train Loss\") plt.plot(epochs, test_losses, label=\"Test Loss\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.title(\"Loss Evolution\")  plt.subplot(1, 2, 2) plt.plot(epochs, train_accuracies, label=\"Train Accuracy\") plt.plot(epochs, test_accuracies, label=\"Test Accuracy\") plt.xlabel(\"Epoch\") plt.ylabel(\"Accuracy (%)\") plt.legend() plt.title(\"Accuracy Evolution\")  plt.tight_layout() plt.show() <p>Comparing training and testing curves provides insights into model generalization. For example, increasing training accuracy with stagnant or decreasing test accuracy usually indicates overfitting, while high loss on both sets suggests insufficient model capacity or training time.</p> <p>Finally, the structure of embeddings produced by the model is analyzed using t-SNE (t-distributed Stochastic Neighbor Embedding). The model\u2019s linear outputs (logits) are extracted as example representations. t-SNE projects these high-dimensional vectors into 2D space while preserving local neighborhood relations. This projection visually shows how the model separates different classes in feature space.</p> In\u00a0[12]: Copied! <pre>model.eval()\n\nmax_samples = 1000\nembeddings, all_labels = [], []\n\nwith torch.no_grad():\n    for i, (images, labels) in enumerate(train_dataloader):\n        if len(all_labels) * train_dataloader.batch_size &gt;= max_samples:\n            break\n        images = images.to(device)\n        outputs = model(images)\n        embeddings.append(outputs.cpu())\n        all_labels.append(labels)\n\nembeddings = torch.cat(embeddings).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\ntsne = TSNE(\n    n_components=2,\n    perplexity=30,\n    random_state=42,\n    max_iter=300,\n    learning_rate=200,\n    n_jobs=-1,\n)\nX_embedded = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(\n    X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap=\"tab10\", alpha=0.6, s=10\n)\nplt.colorbar(scatter, ticks=range(10))\nplt.title(\"t-SNE of embeddings learned by LeNet\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\n</pre> model.eval()  max_samples = 1000 embeddings, all_labels = [], []  with torch.no_grad():     for i, (images, labels) in enumerate(train_dataloader):         if len(all_labels) * train_dataloader.batch_size &gt;= max_samples:             break         images = images.to(device)         outputs = model(images)         embeddings.append(outputs.cpu())         all_labels.append(labels)  embeddings = torch.cat(embeddings).numpy() all_labels = torch.cat(all_labels).numpy()  tsne = TSNE(     n_components=2,     perplexity=30,     random_state=42,     max_iter=300,     learning_rate=200,     n_jobs=-1, ) X_embedded = tsne.fit_transform(embeddings)  plt.figure(figsize=(10, 8)) scatter = plt.scatter(     X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap=\"tab10\", alpha=0.6, s=10 ) plt.colorbar(scatter, ticks=range(10)) plt.title(\"t-SNE of embeddings learned by LeNet\") plt.xlabel(\"Component 1\") plt.ylabel(\"Component 2\") plt.show() <p>When the model has learned a good data representation, points corresponding to different classes tend to cluster relatively separately in 2D space. This visualization provides an intuitive perspective on how the model internally organizes information and distinguishes handwritten digit classes. Clear separation indicates that the network-induced feature space facilitates linear classification in the final layer, confirming that learned representations are discriminative and semantically meaningful.</p>"},{"location":"course/topic_04_computer_vision/section_04_lenet.html#lenet","title":"LeNet\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#historical-context-and-relevance-of-lenet-5","title":"Historical Context and Relevance of LeNet-5\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#conceptual-foundations-of-lenet-5","title":"Conceptual Foundations of LeNet-5\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#original-lenet-5-architecture","title":"Original LeNet-5 Architecture\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#structural-organization-of-the-lenet-5-architecture","title":"Structural Organization of the LeNet-5 Architecture\u00b6","text":"<p>The original LeNet-5 architecture is composed of seven trainable layers that combine convolutional operations, subsampling mechanisms, and fully connected transformations. The network is designed to process grayscale images of size $32 \\times 32$, a resolution that is slightly larger than the standard MNIST digit images of $28 \\times 28$. This deliberate enlargement introduces a uniform margin around the digit, which facilitates the application of convolutional filters near image boundaries and allows the model to handle small translations without discarding relevant edge information.</p> <p>From a structural perspective, the architecture can be divided into two main components: a convolutional feature extraction stage followed by a fully connected classification stage. In the convolutional part, convolutional layers and average pooling layers alternate, progressively transforming the input image into a set of compact and informative feature representations. The final part of the network consists of dense layers that operate on these extracted features to perform classification.</p> <p>The characteristic dimensions of the layers in the original LeNet-5 architecture are summarized in the following table, which illustrates how spatial resolution decreases while the number of feature channels increases as the data propagate through the network:</p> Layer Type Input Output C1 Convolution $32 \\times 32 \\times 1$ $28 \\times 28 \\times 6$ S2 Average Pooling $28 \\times 28 \\times 6$ $14 \\times 14 \\times 6$ C3 Convolution $14 \\times 14 \\times 6$ $10 \\times 10 \\times 16$ S4 Average Pooling $10 \\times 10 \\times 16$ $5 \\times 5 \\times 16$ C5 Convolution $5 \\times 5 \\times 16$ $1 \\times 1 \\times 120$ F6 Fully Connected 120 84 Output Fully Connected 84 10 <p>In the original implementation, LeNet-5 employs sigmoid or $\\tanh$ activation functions rather than the rectified linear units commonly used in modern architectures. Furthermore, subsampling is performed using average pooling instead of max pooling, reflecting both the theoretical preferences and computational constraints of the period in which the model was developed. Despite these differences, the total number of trainable parameters is approximately 60,000, which is relatively small when compared to fully connected networks designed to process inputs of similar dimensionality.</p>"},{"location":"course/topic_04_computer_vision/section_04_lenet.html#pytorch-implementation","title":"PyTorch Implementation\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#importing-libraries","title":"Importing Libraries\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#device-setup","title":"Device Setup\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#auxiliary-visualization-function","title":"Auxiliary Visualization Function\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#loading-and-preprocessing-the-mnist-dataset","title":"Loading and Preprocessing the MNIST Dataset\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#creating-dataloaders","title":"Creating DataLoaders\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#visual-inspection-of-the-dataset","title":"Visual Inspection of the Dataset\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#defining-a-modern-lenet-version-in-pytorch","title":"Defining a Modern LeNet Version in PyTorch\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#model-instantiation-and-analysis","title":"Model Instantiation and Analysis\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#training-setup","title":"Training Setup\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#training-and-validation-loop","title":"Training and Validation Loop\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#visualizing-metric-evolution","title":"Visualizing Metric Evolution\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_04_lenet.html#visualizing-embeddings-with-t-sne","title":"Visualizing Embeddings with t-SNE\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html","title":"VGG","text":"<p>The VGG architecture, developed by the Visual Geometry Group at the University of Oxford and presented in 2014, constitutes a milestone in the evolution of deep learning applied to computer vision. While LeNet-5 establishes the conceptual foundations of convolutional neural networks, VGG systematically demonstrates that increasing the depth of the network, combined with an extremely simple and homogeneous structure, leads to significant improvements in performance. The most influential variants of this family are VGG-16 and VGG-19, named according to the number of trainable layers that compose them.</p> <p>The design of VGG is characterized by its structural simplicity: a sequence of convolutions of fixed size, periodically separated by pooling operations, followed by a block of fully connected layers. This minimalist philosophy contrasts with later, more complex architectures, such as Inception, and turns VGG into a fundamental didactic reference for understanding deep convolutional networks. The architecture shows that highly discriminative visual representations can be obtained through the systematic repetition of basic components, without the need to introduce complex operations or specialized modules.</p> <p>The distinctive feature of the VGG architecture lies in its deliberately conservative and homogeneous design philosophy. Unlike other architectures that combine convolutional filters of different sizes (for example, $5 \\times 5$ or $7 \\times 7$), VGG strictly standardizes its components and maintains a highly regular structure throughout the network.</p> <p>The architecture employs only $3 \\times 3$ convolutional filters, which are applied repeatedly in sequence. As the network depth increases and the spatial resolution of the feature maps decreases, the number of filters is progressively increased, typically doubling this number after each max pooling operation. This strategy allows the network to incrementally enrich the representational capacity of the feature maps while controlling the computational cost in terms of spatial dimensions.</p> <p>In addition, VGG makes systematic use of convolutions with appropriate padding in order to preserve the spatial resolution within each block. At the end of each block, it applies $2 \\times 2$ max pooling operations with stride 2 to reduce the spatial dimensions of the feature maps. This alternation between groups of convolutions with preserved resolution and pooling layers with downsampling produces a hierarchical representation of the input.</p> <p>Within this hierarchy, the early layers capture local and low-level information, such as edges, corners, and simple textures. As the signal propagates through deeper layers, the successive application of $3 \\times 3$ convolutions over increasingly abstract feature maps enables the network to aggregate broader spatial context and encode more complex and semantically rich patterns. Consequently, the deeper layers represent higher-level, more abstract features, such as object parts or entire object configurations, which are especially useful for tasks like image classification and object recognition.</p> <p>The choice of $3 \\times 3$ convolutional filters is justified by a combination of theoretical and practical arguments. Although LeNet-5 employs $5 \\times 5$ filters, the designers of VGG demonstrate that several consecutive layers of $3 \\times 3$ filters can emulate the receptive field of larger filters while requiring fewer parameters and incorporating a greater number of intermediate nonlinearities.</p> <p>From the perspective of the receptive field, a single $k \\times k$ convolution can be approximated by a sequence of smaller convolutions, such as $3 \\times 3$, provided that padding is chosen appropriately. In particular, two $3 \\times 3$ layers possess an effective receptive field equivalent to a $5 \\times 5$ filter, and three $3 \\times 3$ layers approximate a $7 \\times 7$ receptive field. This decomposition proves advantageous both in terms of parameter efficiency and in terms of the expressive capacity of the model.</p> <p>To understand this equivalence, it is useful to consider the notion of effective receptive field of a neuron in a convolutional network. Intuitively, the receptive field indicates the number of pixels from the original image that influence the activation of a neuron in a given layer. In the simplest case, with stride 1 and appropriate padding, a convolutional layer with filters of size $5 \\times 5$ possesses a receptive field of $5 \\times 5 = 25$ pixels. In contrast, two consecutive $3 \\times 3$ layers produce an effective receptive field of $5 \\times 5$: the first layer observes $3 \\times 3$ pixels of the image, and the second layer observes $3 \\times 3$ neurons of the previous layer, whose receptive fields overlap in such a manner that, combined, they cover $5 \\times 5$ pixels of the original image.</p> <p>The VGG-16 variant receives color (RGB) input images of size $224 \\times 224$ pixels and is organized as a hierarchical sequence of five convolutional blocks, followed by a set of fully connected layers responsible for the final classification. Each block groups several $3 \\times 3$ convolutions followed by a $2 \\times 2$ max pooling operation.</p> <p>In its original configuration for ImageNet, the architecture is organized as follows:</p> Block Conv Layers Filters Typical Output Size Pooling Block 1 2 64 $112 \\times 112 \\times 64$ MaxPool $2 \\times 2$ Block 2 2 128 $56 \\times 56 \\times 128$ MaxPool $2 \\times 2$ Block 3 3 256 $28 \\times 28 \\times 256$ MaxPool $2 \\times 2$ Block 4 3 512 $14 \\times 14 \\times 512$ MaxPool $2 \\times 2$ Block 5 3 512 $7 \\times 7 \\times 512$ MaxPool $2 \\times 2$ FC6 - 4096 4096 - FC7 - 4096 4096 - FC8 - 1000 1000 Softmax <p>The first two convolutional blocks are composed of two convolutional layers each. The first block employs 64 filters, while the second employs 128 filters, always with filter size $3 \\times 3$. At the end of each block, a $2 \\times 2$ max pooling operation is applied, whose role is to halve the spatial resolution and concentrate the most relevant information. The third, fourth, and fifth blocks increase the effective depth of the network by means of three consecutive convolutional layers per block. The number of filters increases to 256 in the third block and to 512 in the last two blocks. At these deeper stages, the network learns highly abstract features, such as object parts, complex textures, and high-level visual configurations, which are crucial for class discrimination.</p> <p>After the convolutional blocks, the resulting feature maps are transformed into a one-dimensional vector that feeds the classifier layers. The final segment consists of two dense layers with 4096 neurons each, followed by an output layer with 1000 neurons, corresponding to the thousand categories of the ImageNet dataset. The Softmax activation makes it possible to interpret the output as a probability distribution over classes. Throughout the architecture, the ReLU activation function is used, which accelerates training and helps mitigate the vanishing gradient problem. The combination of these design decisions\u2014moderate depth, small filters, homogeneous structure\u2014positions VGG-16 as a high-performance model on ImageNet, though at the cost of a very large number of parameters.</p> <p>VGG-16 achieves second place in the 2014 ImageNet challenge, but its impact extends far beyond the competition. The scientific and technical community adopts this architecture as a reference due to its clear, regular, and easily interpretable design. This clarity makes it a fundamental tool both for research and teaching on deep convolutional networks, as well as for the development of numerous subsequent works in transfer learning and feature extraction.</p> <p>Among its advantages, it is worth highlighting its homogeneous and modular architecture, which facilitates implementation and experimentation; its excellent capacity for learning hierarchical features in images; and its suitability for transfer learning. The initial layers of VGG learn generic and robust representations focused on edges, textures, and local patterns that can be reused effectively in a wide variety of computer vision tasks by adapting only the final layers.</p> <p>However, VGG also presents significant limitations. The very large number of parameters (on the order of 138 million in VGG-16) implies considerable memory consumption (around 500 MB in 32-bit precision) and a high computational cost both in training and inference. These characteristics make the architecture unsuitable for devices with limited resources and increase the cost of large-scale production deployment. In addition, a substantial portion of the parameters is concentrated in the final fully connected layers, which has motivated later architectures to replace these layers with more efficient mechanisms such as global average pooling.</p> <p>These constraints have driven the development of subsequent architectures such as Inception, ResNet, or MobileNet, which aim to maintain or improve performance while reducing computational cost, facilitating the training of deeper networks, and adapting to resource-constrained environments. Despite this, VGG remains a classic reference model due to its conceptual transparency and its capacity to serve as a starting point in numerous practical applications.</p> <p>This section implements a variant of VGG-16 adapted to the CIFAR-10 dataset, which contains color images of $32 \\times 32$ pixels belonging to 10 categories. The original architecture, designed for ImageNet ($224 \\times 224$), is modified to accommodate the smaller input size and the reduced number of classes in CIFAR-10, while preserving the VGG design philosophy.</p> <p>The following modules are imported to implement VGG-16 and train it on CIFAR-10. The goal is to have at our disposal the PyTorch tools required to define the model, manage data, train the network, and analyze the results in a systematic way.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nimport time\nfrom typing import Any, List\n\n# 3pps\n# Third-party libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.manifold import TSNE\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n</pre> # Standard libraries import time from typing import Any, List  # 3pps # Third-party libraries import matplotlib.pyplot as plt import numpy as np import torch from sklearn.manifold import TSNE from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm   print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") if torch.cuda.is_available():     print(f\"CUDA device: {torch.cuda.get_device_name(0)}\") <pre>PyTorch version: 2.9.1+cu128\nCUDA available: False\n</pre> <p>This block verifies GPU compatibility when available and provides basic information about the execution environment, which is useful for reproducing experiments and diagnosing configuration issues.</p> <p>Constants used throughout the implementation are defined next, such as batch size, number of epochs, learning rate, and number of classes. Centralizing these parameters facilitates experimentation and model tuning.</p> In\u00a0[2]: Copied! <pre># Global configuration\nBATCH_SIZE: int = 128\nNUM_EPOCHS: int = 1\nLEARNING_RATE: float = 1e-3\nWEIGHT_DECAY: float = 5e-4\nNUM_CLASSES: int = 10  # CIFAR-10 has 10 classes\nINPUT_SIZE: int = 32  # CIFAR-10: 32\u00d732 images\n\n# CIFAR-10 class names\nCIFAR10_CLASSES = [\n    \"airplane\",\n    \"automobile\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n]\n\nprint(\"Configuration:\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Number of classes: {NUM_CLASSES}\")\n</pre> # Global configuration BATCH_SIZE: int = 128 NUM_EPOCHS: int = 1 LEARNING_RATE: float = 1e-3 WEIGHT_DECAY: float = 5e-4 NUM_CLASSES: int = 10  # CIFAR-10 has 10 classes INPUT_SIZE: int = 32  # CIFAR-10: 32\u00d732 images  # CIFAR-10 class names CIFAR10_CLASSES = [     \"airplane\",     \"automobile\",     \"bird\",     \"cat\",     \"deer\",     \"dog\",     \"frog\",     \"horse\",     \"ship\",     \"truck\", ]  print(\"Configuration:\") print(f\"  Batch size: {BATCH_SIZE}\") print(f\"  Epochs: {NUM_EPOCHS}\") print(f\"  Learning rate: {LEARNING_RATE}\") print(f\"  Number of classes: {NUM_CLASSES}\") <pre>Configuration:\n  Batch size: 128\n  Epochs: 1\n  Learning rate: 0.001\n  Number of classes: 10\n</pre> <p>This configuration provides a reasonable starting point for training VGG-16 on CIFAR-10, establishing a compromise between performance and computational cost that can be adjusted according to available resources.</p> <p>Visual exploration of the data helps to better understand the problem and to verify that preprocessing is applied correctly. A function is defined to visualize images with their labels and, optionally, with model predictions.</p> In\u00a0[3]: Copied! <pre>def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):\n    \"\"\"\n    Visualizes a set of images with their labels.\n\n    Args:\n        images: Image tensor [N, C, H, W]\n        labels: Label tensor [N]\n        predictions: Optional prediction tensor [N]\n        classes: List of class names\n    \"\"\"\n    n_images = len(images)\n    fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))\n\n    if n_images == 1:\n        axes = [axes]\n\n    for idx, (img, label, ax) in enumerate(zip(images, labels, axes)):\n        # Denormalize image for visualization\n        img = img / 2 + 0.5  # Revert normalization [-1, 1] -&gt; [0, 1]\n        img = img.numpy().transpose((1, 2, 0))\n\n        ax.imshow(img)\n\n        title = f\"True: {classes[label]}\"\n        if predictions is not None:\n            pred = predictions[idx]\n            color = \"green\" if pred == label else \"red\"\n            title += f\"\\nPred: {classes[pred]}\"\n            ax.set_title(title, fontsize=10, color=color, fontweight=\"bold\")\n        else:\n            ax.set_title(title, fontsize=10, fontweight=\"bold\")\n\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"Visualization function defined correctly\")\n</pre> def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):     \"\"\"     Visualizes a set of images with their labels.      Args:         images: Image tensor [N, C, H, W]         labels: Label tensor [N]         predictions: Optional prediction tensor [N]         classes: List of class names     \"\"\"     n_images = len(images)     fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))      if n_images == 1:         axes = [axes]      for idx, (img, label, ax) in enumerate(zip(images, labels, axes)):         # Denormalize image for visualization         img = img / 2 + 0.5  # Revert normalization [-1, 1] -&gt; [0, 1]         img = img.numpy().transpose((1, 2, 0))          ax.imshow(img)          title = f\"True: {classes[label]}\"         if predictions is not None:             pred = predictions[idx]             color = \"green\" if pred == label else \"red\"             title += f\"\\nPred: {classes[pred]}\"             ax.set_title(title, fontsize=10, color=color, fontweight=\"bold\")         else:             ax.set_title(title, fontsize=10, fontweight=\"bold\")          ax.axis(\"off\")      plt.tight_layout()     plt.show()   print(\"Visualization function defined correctly\") <pre>Visualization function defined correctly\n</pre> <p>This function is used later to inspect both samples from the dataset and correct and incorrect model predictions, providing an essential visual diagnostic tool.</p> <p>The CIFAR-10 data are loaded and the data augmentation and normalization transformations applied to the images are defined. CIFAR-10 contains 60,000 color images of $32 \\times 32$ pixels distributed across 10 classes and presents greater variability and complexity than MNIST due to the diversity of objects, backgrounds, and capture conditions.</p> In\u00a0[4]: Copied! <pre>from torch.utils.data import Subset\n\n# Transformations with data augmentation for training\ntransform_train = transforms.Compose(\n    [\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n        ),\n    ]\n)\n\n# Transformations for validation (no augmentation)\ntransform_test = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n    ]\n)\n\nprint(\"Downloading CIFAR-10 dataset...\")\n\n# Training set\ntrain_dataset_full = datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform_train\n)\n\n# Test set\ntest_dataset_full = datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform_test\n)\n\n# Limit samples\ntrain_dataset = Subset(train_dataset_full, range(5000))\ntest_dataset = Subset(test_dataset_full, range(1000))\n\nprint(\"\\nDataset statistics:\")\nprint(f\"  Training samples: {len(train_dataset):,}\")\nprint(f\"  Test samples: {len(test_dataset):,}\")\nprint(f\"  Number of classes: {len(train_dataset_full.classes)}\")\nprint(f\"  Classes: {', '.join(train_dataset_full.classes)}\")\nprint(\"  Image size: 32\u00d732 pixels (RGB)\")\n</pre> from torch.utils.data import Subset  # Transformations with data augmentation for training transform_train = transforms.Compose(     [         transforms.RandomCrop(32, padding=4),         transforms.RandomHorizontalFlip(),         transforms.ToTensor(),         transforms.Normalize(             (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)         ),     ] )  # Transformations for validation (no augmentation) transform_test = transforms.Compose(     [         transforms.ToTensor(),         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),     ] )  print(\"Downloading CIFAR-10 dataset...\")  # Training set train_dataset_full = datasets.CIFAR10(     root=\"./data\", train=True, download=True, transform=transform_train )  # Test set test_dataset_full = datasets.CIFAR10(     root=\"./data\", train=False, download=True, transform=transform_test )  # Limit samples train_dataset = Subset(train_dataset_full, range(5000)) test_dataset = Subset(test_dataset_full, range(1000))  print(\"\\nDataset statistics:\") print(f\"  Training samples: {len(train_dataset):,}\") print(f\"  Test samples: {len(test_dataset):,}\") print(f\"  Number of classes: {len(train_dataset_full.classes)}\") print(f\"  Classes: {', '.join(train_dataset_full.classes)}\") print(\"  Image size: 32\u00d732 pixels (RGB)\") <pre>Downloading CIFAR-10 dataset...\n</pre> <pre>\r  0%|          | 0.00/170M [00:00&lt;?, ?B/s]</pre> <pre>\r  0%|          | 32.8k/170M [00:00&lt;09:57, 285kB/s]</pre> <pre>\r  0%|          | 229k/170M [00:00&lt;02:37, 1.08MB/s]</pre> <pre>\r  0%|          | 852k/170M [00:00&lt;00:55, 3.04MB/s]</pre> <pre>\r  2%|\u258f         | 3.34M/170M [00:00&lt;00:16, 10.2MB/s]</pre> <pre>\r  4%|\u258d         | 6.85M/170M [00:00&lt;00:08, 18.3MB/s]</pre> <pre>\r  6%|\u258c         | 9.70M/170M [00:00&lt;00:07, 21.5MB/s]</pre> <pre>\r  7%|\u258b         | 12.0M/170M [00:00&lt;00:07, 21.5MB/s]</pre> <pre>\r  9%|\u2589         | 16.0M/170M [00:00&lt;00:05, 26.8MB/s]</pre> <pre>\r 11%|\u2588         | 18.8M/170M [00:00&lt;00:05, 27.3MB/s]</pre> <pre>\r 13%|\u2588\u258e        | 21.6M/170M [00:01&lt;00:06, 22.8MB/s]</pre> <pre>\r 14%|\u2588\u258d        | 24.5M/170M [00:01&lt;00:06, 24.2MB/s]</pre> <pre>\r 16%|\u2588\u258c        | 27.0M/170M [00:01&lt;00:06, 22.0MB/s]</pre> <pre>\r 17%|\u2588\u258b        | 29.4M/170M [00:01&lt;00:07, 19.8MB/s]</pre> <pre>\r 18%|\u2588\u258a        | 31.5M/170M [00:01&lt;00:06, 20.0MB/s]</pre> <pre>\r 20%|\u2588\u2589        | 33.6M/170M [00:01&lt;00:07, 19.3MB/s]</pre> <pre>\r 21%|\u2588\u2588        | 35.6M/170M [00:01&lt;00:07, 19.1MB/s]</pre> <pre>\r 22%|\u2588\u2588\u258f       | 37.5M/170M [00:01&lt;00:07, 18.8MB/s]</pre> <pre>\r 23%|\u2588\u2588\u258e       | 39.5M/170M [00:02&lt;00:06, 19.1MB/s]</pre> <pre>\r 24%|\u2588\u2588\u258d       | 41.5M/170M [00:02&lt;00:06, 18.8MB/s]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 43.4M/170M [00:02&lt;00:06, 18.8MB/s]</pre> <pre>\r 27%|\u2588\u2588\u258b       | 45.4M/170M [00:02&lt;00:06, 19.2MB/s]</pre> <pre>\r 28%|\u2588\u2588\u258a       | 47.3M/170M [00:02&lt;00:06, 19.2MB/s]</pre> <pre>\r 29%|\u2588\u2588\u2589       | 49.3M/170M [00:02&lt;00:06, 18.7MB/s]</pre> <pre>\r 30%|\u2588\u2588\u2588       | 51.4M/170M [00:02&lt;00:06, 19.5MB/s]</pre> <pre>\r 31%|\u2588\u2588\u2588\u258f      | 53.4M/170M [00:02&lt;00:06, 19.2MB/s]</pre> <pre>\r 32%|\u2588\u2588\u2588\u258f      | 55.3M/170M [00:02&lt;00:06, 18.8MB/s]</pre> <pre>\r 34%|\u2588\u2588\u2588\u258e      | 57.5M/170M [00:03&lt;00:05, 19.4MB/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u258d      | 59.6M/170M [00:03&lt;00:05, 19.8MB/s]</pre> <pre>\r 36%|\u2588\u2588\u2588\u258c      | 61.6M/170M [00:03&lt;00:05, 19.5MB/s]</pre> <pre>\r 37%|\u2588\u2588\u2588\u258b      | 63.8M/170M [00:03&lt;00:05, 20.1MB/s]</pre> <pre>\r 39%|\u2588\u2588\u2588\u258a      | 65.8M/170M [00:03&lt;00:05, 19.7MB/s]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2589      | 67.9M/170M [00:03&lt;00:05, 19.8MB/s]</pre> <pre>\r 41%|\u2588\u2588\u2588\u2588      | 70.0M/170M [00:03&lt;00:04, 20.2MB/s]</pre> <pre>\r 42%|\u2588\u2588\u2588\u2588\u258f     | 72.0M/170M [00:03&lt;00:04, 20.2MB/s]</pre> <pre>\r 43%|\u2588\u2588\u2588\u2588\u258e     | 74.1M/170M [00:03&lt;00:04, 20.2MB/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u258d     | 76.3M/170M [00:03&lt;00:04, 20.8MB/s]</pre> <pre>\r 46%|\u2588\u2588\u2588\u2588\u258c     | 78.4M/170M [00:04&lt;00:04, 20.1MB/s]</pre> <pre>\r 47%|\u2588\u2588\u2588\u2588\u258b     | 80.7M/170M [00:04&lt;00:04, 20.9MB/s]</pre> <pre>\r 49%|\u2588\u2588\u2588\u2588\u258a     | 82.8M/170M [00:04&lt;00:04, 21.0MB/s]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2589     | 85.0M/170M [00:04&lt;00:04, 21.2MB/s]</pre> <pre>\r 51%|\u2588\u2588\u2588\u2588\u2588     | 87.2M/170M [00:04&lt;00:03, 20.8MB/s]</pre> <pre>\r 53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 89.6M/170M [00:04&lt;00:03, 21.6MB/s]</pre> <pre>\r 54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 91.8M/170M [00:04&lt;00:03, 21.7MB/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 93.9M/170M [00:04&lt;00:03, 21.5MB/s]</pre> <pre>\r 56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 96.1M/170M [00:04&lt;00:03, 21.4MB/s]</pre> <pre>\r 58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 98.3M/170M [00:04&lt;00:03, 21.3MB/s]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 101M/170M [00:05&lt;00:03, 21.4MB/s] </pre> <pre>\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 103M/170M [00:05&lt;00:02, 22.5MB/s]</pre> <pre>\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 105M/170M [00:05&lt;00:02, 21.8MB/s]</pre> <pre>\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 108M/170M [00:05&lt;00:02, 21.7MB/s]</pre> <pre>\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 110M/170M [00:05&lt;00:02, 21.6MB/s]</pre> <pre>\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 112M/170M [00:05&lt;00:02, 21.7MB/s]</pre> <pre>\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 114M/170M [00:05&lt;00:02, 21.9MB/s]</pre> <pre>\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 117M/170M [00:05&lt;00:02, 22.2MB/s]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 119M/170M [00:05&lt;00:02, 21.9MB/s]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 121M/170M [00:06&lt;00:02, 21.8MB/s]</pre> <pre>\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 124M/170M [00:06&lt;00:02, 22.1MB/s]</pre> <pre>\r 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 126M/170M [00:06&lt;00:01, 22.4MB/s]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 128M/170M [00:06&lt;00:01, 22.2MB/s]</pre> <pre>\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 130M/170M [00:06&lt;00:01, 22.0MB/s]</pre> <pre>\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 133M/170M [00:06&lt;00:01, 22.3MB/s]</pre> <pre>\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 135M/170M [00:06&lt;00:01, 22.5MB/s]</pre> <pre>\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 137M/170M [00:06&lt;00:01, 22.3MB/s]</pre> <pre>\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 140M/170M [00:06&lt;00:01, 22.1MB/s]</pre> <pre>\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 142M/170M [00:06&lt;00:01, 21.7MB/s]</pre> <pre>\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 144M/170M [00:07&lt;00:01, 22.6MB/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 147M/170M [00:07&lt;00:01, 22.6MB/s]</pre> <pre>\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 149M/170M [00:07&lt;00:00, 22.0MB/s]</pre> <pre>\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 151M/170M [00:07&lt;00:00, 22.2MB/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 154M/170M [00:07&lt;00:00, 22.1MB/s]</pre> <pre>\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 156M/170M [00:07&lt;00:00, 22.6MB/s]</pre> <pre>\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 158M/170M [00:07&lt;00:00, 22.4MB/s]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 160M/170M [00:07&lt;00:00, 22.2MB/s]</pre> <pre>\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 163M/170M [00:07&lt;00:00, 21.8MB/s]</pre> <pre>\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 165M/170M [00:07&lt;00:00, 22.4MB/s]</pre> <pre>\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 168M/170M [00:08&lt;00:00, 22.8MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170M/170M [00:08&lt;00:00, 22.4MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:08&lt;00:00, 20.7MB/s]</pre> <pre>\n</pre> <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n  entry = pickle.load(f, encoding=\"latin1\")\n</pre> <pre>\nDataset statistics:\n  Training samples: 5,000\n  Test samples: 1,000\n  Number of classes: 10\n  Classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n  Image size: 32\u00d732 pixels (RGB)\n</pre> <p>Data augmentation is introduced to increase the generalization capacity of the model. RandomCrop with padding simulates variations in framing and object position, while RandomHorizontalFlip increases robustness to horizontal symmetries. Both mechanisms reduce overfitting by generating slightly different versions of each image at each epoch, effectively expanding the training set without requiring additional data.</p> <p>Normalization is performed using the mean and standard deviation of the complete CIFAR-10 dataset for each RGB channel:</p> <p>$$ \\mu = (0.4914, 0.4822, 0.4465), \\quad \\sigma = (0.2470, 0.2435, 0.2616). $$</p> <p>This operation centers and scales the values of each channel using the transformation:</p> <p>$$ x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}, $$</p> <p>which facilitates optimization and stabilizes training by improving the numerical conditioning of gradients.</p> <p>Once the datasets are defined, <code>DataLoader</code> objects are created to manage batch iteration during training and evaluation, including optimizations to accelerate data loading.</p> In\u00a0[5]: Copied! <pre>print(\"Configuring DataLoaders:\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\n\n# Training DataLoader\ntrain_dataloader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    persistent_workers=True,\n    pin_memory=True,\n)\n\n# Test DataLoader\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    persistent_workers=True,\n    pin_memory=True,\n)\n\nprint(f\"  Training batches: {len(train_dataloader)}\")\nprint(f\"  Test batches: {len(test_dataloader)}\")\n</pre> print(\"Configuring DataLoaders:\") print(f\"  Batch size: {BATCH_SIZE}\")  # Training DataLoader train_dataloader = DataLoader(     dataset=train_dataset,     batch_size=BATCH_SIZE,     shuffle=True,     num_workers=4,     persistent_workers=True,     pin_memory=True, )  # Test DataLoader test_dataloader = DataLoader(     dataset=test_dataset,     batch_size=BATCH_SIZE,     shuffle=False,     num_workers=4,     persistent_workers=True,     pin_memory=True, )  print(f\"  Training batches: {len(train_dataloader)}\") print(f\"  Test batches: {len(test_dataloader)}\") <pre>Configuring DataLoaders:\n  Batch size: 128\n  Training batches: 40\n  Test batches: 8\n</pre> <p>Common PyTorch optimizations are applied. The parameter <code>num_workers=4</code> allows data loading in parallel via auxiliary processes, taking advantage of multiple CPU cores. The use of <code>pin_memory=True</code> improves the speed of data transfer to the GPU by using pinned (non-pageable) memory. Finally, <code>persistent_workers=True</code> avoids recreating the worker processes at each epoch, reducing initialization overhead and accelerating the data pipeline.</p> <p>Before defining the model, it is useful to inspect some images from the training set and verify the tensor dimensions and the effect of normalization.</p> In\u00a0[6]: Copied! <pre># Obtain one data batch\ndata_iter = iter(train_dataloader)\ntrain_images, train_labels = next(data_iter)\n\nprint(\"Batch dimensions:\")\nprint(f\"  Images: {train_images.shape}\")\nprint(f\"  Labels: {train_labels.shape}\")\nprint(f\"\\n  Interpretation: {BATCH_SIZE} RGB images of size 32\u00d732 pixels\")\n\n# Visualize first 8 examples\nprint(\"\\nVisualizing first 8 samples...\")\nshow_images(train_images[:8], train_labels[:8])\n\n# Statistics of normalized images\nprint(\"\\nStatistics after normalization:\")\nprint(f\"  Min value: {train_images.min():.3f}\")\nprint(f\"  Max value: {train_images.max():.3f}\")\nprint(\"  Mean per channel:\")\nfor i, channel in enumerate([\"R\", \"G\", \"B\"]):\n    print(f\"    {channel}: {train_images[:, i, :, :].mean():.3f}\")\n</pre> # Obtain one data batch data_iter = iter(train_dataloader) train_images, train_labels = next(data_iter)  print(\"Batch dimensions:\") print(f\"  Images: {train_images.shape}\") print(f\"  Labels: {train_labels.shape}\") print(f\"\\n  Interpretation: {BATCH_SIZE} RGB images of size 32\u00d732 pixels\")  # Visualize first 8 examples print(\"\\nVisualizing first 8 samples...\") show_images(train_images[:8], train_labels[:8])  # Statistics of normalized images print(\"\\nStatistics after normalization:\") print(f\"  Min value: {train_images.min():.3f}\") print(f\"  Max value: {train_images.max():.3f}\") print(\"  Mean per channel:\") for i, channel in enumerate([\"R\", \"G\", \"B\"]):     print(f\"    {channel}: {train_images[:, i, :, :].mean():.3f}\") <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Batch dimensions:\n  Images: torch.Size([128, 3, 32, 32])\n  Labels: torch.Size([128])\n\n  Interpretation: 128 RGB images of size 32\u00d732 pixels\n\nVisualizing first 8 samples...\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.434294].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.3941419].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5068768].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.3780243].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5295547].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.1006963].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.3390331].\n</pre> <pre>\nStatistics after normalization:\n  Min value: -1.989\n  Max value: 2.126\n  Mean per channel:\n    R: -0.336\n    G: -0.311\n    B: -0.252\n</pre> <p>This analysis allows the verification that data are correctly loaded, preprocessing is applied appropriately, and data augmentation transformations produce reasonable variations without excessively distorting the images.</p> <p>A VGG-16 variant adapted to $32 \\times 32$ images is implemented next. The original architecture for ImageNet is designed for $224 \\times 224$, so adjustments in the final layers are required to adapt to the reduced input size and the smaller number of classes.</p> In\u00a0[7]: Copied! <pre>class VGG16(nn.Module):\n    \"\"\"\n    Implementation of VGG-16 adapted for CIFAR-10 (32\u00d732 pixels).\n\n    Architecture:\n    - 5 convolutional blocks with configuration [64, 128, 256, 512, 512]\n    - All filters are 3\u00d73\n    - 2\u00d72 MaxPooling after each block\n    - 3 fully connected layers at the end\n    - BatchNorm to stabilize training\n    - Dropout for regularization\n    \"\"\"\n\n    def __init__(self, num_classes: int = 10, **kwargs: Any) -&gt; None:\n        super().__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        # Block 1: 2 conv layers with 64 filters\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 32\u00d732 -&gt; 16\u00d716\n        )\n\n        # Block 2: 2 conv layers with 128 filters\n        self.block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 16\u00d716 -&gt; 8\u00d78\n        )\n\n        # Block 3: 3 conv layers with 256 filters\n        self.block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 8\u00d78 -&gt; 4\u00d74\n        )\n\n        # Block 4: 3 conv layers with 512 filters\n        self.block4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 4\u00d74 -&gt; 2\u00d72\n        )\n\n        # Block 5: 3 conv layers with 512 filters\n        self.block5 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 2\u00d72 -&gt; 1\u00d71\n        )\n\n        # Classifier layers\n        # For CIFAR-10 (32\u00d732), after 5 poolings: 32 / 2^5 = 1\n        # Therefore: 512 \u00d7 1 \u00d7 1 = 512 features\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 1 * 1, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes),\n        )\n\n        # Weight initialization\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"\n        Initializes weights using He initialization for layers with ReLU.\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of VGG-16.\n\n        Args:\n            x: Input tensor [B, 3, 32, 32]\n\n        Returns:\n            Classification logits [B, num_classes]\n        \"\"\"\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.classifier(x)\n        return x\n\n    def get_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts features before the final classification layer.\n        Useful for embedding visualization.\n        \"\"\"\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = torch.flatten(x, 1)\n        return x\n\n\nprint(\"VGG-16 architecture defined correctly\")\n</pre> class VGG16(nn.Module):     \"\"\"     Implementation of VGG-16 adapted for CIFAR-10 (32\u00d732 pixels).      Architecture:     - 5 convolutional blocks with configuration [64, 128, 256, 512, 512]     - All filters are 3\u00d73     - 2\u00d72 MaxPooling after each block     - 3 fully connected layers at the end     - BatchNorm to stabilize training     - Dropout for regularization     \"\"\"      def __init__(self, num_classes: int = 10, **kwargs: Any) -&gt; None:         super().__init__(**kwargs)          self.num_classes = num_classes          # Block 1: 2 conv layers with 64 filters         self.block1 = nn.Sequential(             nn.Conv2d(3, 64, kernel_size=3, padding=1),             nn.BatchNorm2d(64),             nn.ReLU(inplace=True),             nn.Conv2d(64, 64, kernel_size=3, padding=1),             nn.BatchNorm2d(64),             nn.ReLU(inplace=True),             nn.MaxPool2d(kernel_size=2, stride=2),  # 32\u00d732 -&gt; 16\u00d716         )          # Block 2: 2 conv layers with 128 filters         self.block2 = nn.Sequential(             nn.Conv2d(64, 128, kernel_size=3, padding=1),             nn.BatchNorm2d(128),             nn.ReLU(inplace=True),             nn.Conv2d(128, 128, kernel_size=3, padding=1),             nn.BatchNorm2d(128),             nn.ReLU(inplace=True),             nn.MaxPool2d(kernel_size=2, stride=2),  # 16\u00d716 -&gt; 8\u00d78         )          # Block 3: 3 conv layers with 256 filters         self.block3 = nn.Sequential(             nn.Conv2d(128, 256, kernel_size=3, padding=1),             nn.BatchNorm2d(256),             nn.ReLU(inplace=True),             nn.Conv2d(256, 256, kernel_size=3, padding=1),             nn.BatchNorm2d(256),             nn.ReLU(inplace=True),             nn.Conv2d(256, 256, kernel_size=3, padding=1),             nn.BatchNorm2d(256),             nn.ReLU(inplace=True),             nn.MaxPool2d(kernel_size=2, stride=2),  # 8\u00d78 -&gt; 4\u00d74         )          # Block 4: 3 conv layers with 512 filters         self.block4 = nn.Sequential(             nn.Conv2d(256, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.Conv2d(512, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.Conv2d(512, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.MaxPool2d(kernel_size=2, stride=2),  # 4\u00d74 -&gt; 2\u00d72         )          # Block 5: 3 conv layers with 512 filters         self.block5 = nn.Sequential(             nn.Conv2d(512, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.Conv2d(512, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.Conv2d(512, 512, kernel_size=3, padding=1),             nn.BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.MaxPool2d(kernel_size=2, stride=2),  # 2\u00d72 -&gt; 1\u00d71         )          # Classifier layers         # For CIFAR-10 (32\u00d732), after 5 poolings: 32 / 2^5 = 1         # Therefore: 512 \u00d7 1 \u00d7 1 = 512 features         self.classifier = nn.Sequential(             nn.Flatten(),             nn.Linear(512 * 1 * 1, 512),             nn.BatchNorm1d(512),             nn.ReLU(inplace=True),             nn.Dropout(0.5),             nn.Linear(512, 512),             nn.BatchNorm1d(512),             nn.ReLU(inplace=True),             nn.Dropout(0.5),             nn.Linear(512, num_classes),         )          # Weight initialization         self._initialize_weights()      def _initialize_weights(self):         \"\"\"         Initializes weights using He initialization for layers with ReLU.         \"\"\"         for m in self.modules():             if isinstance(m, nn.Conv2d):                 nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")                 if m.bias is not None:                     nn.init.constant_(m.bias, 0)             elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):                 nn.init.constant_(m.weight, 1)                 nn.init.constant_(m.bias, 0)             elif isinstance(m, nn.Linear):                 nn.init.normal_(m.weight, 0, 0.01)                 nn.init.constant_(m.bias, 0)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass of VGG-16.          Args:             x: Input tensor [B, 3, 32, 32]          Returns:             Classification logits [B, num_classes]         \"\"\"         x = self.block1(x)         x = self.block2(x)         x = self.block3(x)         x = self.block4(x)         x = self.block5(x)         x = self.classifier(x)         return x      def get_features(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Extracts features before the final classification layer.         Useful for embedding visualization.         \"\"\"         x = self.block1(x)         x = self.block2(x)         x = self.block3(x)         x = self.block4(x)         x = self.block5(x)         x = torch.flatten(x, 1)         return x   print(\"VGG-16 architecture defined correctly\") <pre>VGG-16 architecture defined correctly\n</pre> <p>Once the architecture is defined, the model is instantiated, moved to the appropriate device (CPU or GPU), and its structure and parameter count are analyzed to verify that the implementation is correct.</p> In\u00a0[8]: Copied! <pre># Create the model\nmodel = VGG16(num_classes=NUM_CLASSES)\n\n# Determine available device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Device used: {device}\")\nprint(f\"\\n{'=' * 70}\")\nprint(\"VGG-16 ARCHITECTURE SUMMARY\")\nprint(f\"{'=' * 70}\\n\")\n\n# Detailed architecture summary\nsummary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))\n\n# Parameter count per block\nprint(f\"\\n{'=' * 70}\")\nprint(\"PARAMETER ANALYSIS PER BLOCK\")\nprint(f\"{'=' * 70}\")\n\n\ndef count_parameters(module):\n    return sum(p.numel() for p in module.parameters())\n\n\nprint(f\"  Block 1: {count_parameters(model.block1):&gt;12,} parameters\")\nprint(f\"  Block 2: {count_parameters(model.block2):&gt;12,} parameters\")\nprint(f\"  Block 3: {count_parameters(model.block3):&gt;12,} parameters\")\nprint(f\"  Block 4: {count_parameters(model.block4):&gt;12,} parameters\")\nprint(f\"  Block 5: {count_parameters(model.block5):&gt;12,} parameters\")\nprint(f\"  Classifier: {count_parameters(model.classifier):&gt;12,} parameters\")\nprint(f\"  {'-' * 66}\")\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"  TOTAL: {total_params:&gt;12,} parameters\")\nprint(f\"  Trainable: {trainable_params:&gt;12,} parameters\")\nprint(f\"  Memory (float32): {total_params * 4 / (1024 ** 2):&gt;10.2f} MB\")\n</pre> # Create the model model = VGG16(num_classes=NUM_CLASSES)  # Determine available device device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model = model.to(device)  print(f\"Device used: {device}\") print(f\"\\n{'=' * 70}\") print(\"VGG-16 ARCHITECTURE SUMMARY\") print(f\"{'=' * 70}\\n\")  # Detailed architecture summary summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))  # Parameter count per block print(f\"\\n{'=' * 70}\") print(\"PARAMETER ANALYSIS PER BLOCK\") print(f\"{'=' * 70}\")   def count_parameters(module):     return sum(p.numel() for p in module.parameters())   print(f\"  Block 1: {count_parameters(model.block1):&gt;12,} parameters\") print(f\"  Block 2: {count_parameters(model.block2):&gt;12,} parameters\") print(f\"  Block 3: {count_parameters(model.block3):&gt;12,} parameters\") print(f\"  Block 4: {count_parameters(model.block4):&gt;12,} parameters\") print(f\"  Block 5: {count_parameters(model.block5):&gt;12,} parameters\") print(f\"  Classifier: {count_parameters(model.classifier):&gt;12,} parameters\") print(f\"  {'-' * 66}\")  total_params = sum(p.numel() for p in model.parameters()) trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  print(f\"  TOTAL: {total_params:&gt;12,} parameters\") print(f\"  Trainable: {trainable_params:&gt;12,} parameters\") print(f\"  Memory (float32): {total_params * 4 / (1024 ** 2):&gt;10.2f} MB\") <pre>Device used: cpu\n\n======================================================================\nVGG-16 ARCHITECTURE SUMMARY\n======================================================================\n\n</pre> <pre>\n======================================================================\nPARAMETER ANALYSIS PER BLOCK\n======================================================================\n  Block 1:       38,976 parameters\n  Block 2:      221,952 parameters\n  Block 3:    1,476,864 parameters\n  Block 4:    5,902,848 parameters\n  Block 5:    7,082,496 parameters\n  Classifier:      532,490 parameters\n  ------------------------------------------------------------------\n  TOTAL:   15,255,626 parameters\n  Trainable:   15,255,626 parameters\n  Memory (float32):      58.20 MB\n</pre> <p>In the original VGG-16 for ImageNet, about 14.7 million parameters correspond to the convolutional layers and around 123 million to the fully connected layers (approximately 89 % of the total). The CIFAR-10 variant drastically reduces the parameters of the dense layers by going from 4096 to 512 neurons, resulting in a more manageable model suited to this dataset, although it remains considerably large compared with more modern, efficient architectures.</p> <p>The optimizer, loss function, and a learning rate scheduler are now defined to improve convergence and adapt dynamically to the evolution of training.</p> In\u00a0[9]: Copied! <pre>print(\"TRAINING CONFIGURATION\")\nprint(f\"{'=' * 70}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Initial learning rate: {LEARNING_RATE}\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"{'=' * 70}\\n\")\n\n# Optimizer: SGD with momentum\noptimizer = torch.optim.SGD(\n    params=model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY\n)\n\n# Learning rate scheduler: reduces LR when progress plateaus\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=\"max\",  # Monitor accuracy (to be maximized)\n    factor=0.1,  # Reduce LR to 10 % of current value\n    patience=3,  # After 3 epochs without improvement\n    min_lr=1e-6,\n)\n\n# Loss function: Cross-Entropy\nloss_function = nn.CrossEntropyLoss()\n\nprint(\"Optimizer: SGD with momentum=0.9\")\nprint(\"  SGD with momentum accumulates gradients with exponential decay\")\nprint(\"  This helps escape local minima and accelerates convergence\")\nprint(\"\\nScheduler: ReduceLROnPlateau\")\nprint(\"  Reduces the learning rate when accuracy stops improving\")\nprint(\"  Reduction factor: 0.1 (LR \u2192 0.1 \u00d7 LR)\")\nprint(\"  Patience: 3 epochs\")\nprint(\"\\nLoss function: CrossEntropyLoss\")\n</pre> print(\"TRAINING CONFIGURATION\") print(f\"{'=' * 70}\") print(f\"  Epochs: {NUM_EPOCHS}\") print(f\"  Initial learning rate: {LEARNING_RATE}\") print(f\"  Weight decay (L2): {WEIGHT_DECAY}\") print(f\"  Batch size: {BATCH_SIZE}\") print(f\"{'=' * 70}\\n\")  # Optimizer: SGD with momentum optimizer = torch.optim.SGD(     params=model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY )  # Learning rate scheduler: reduces LR when progress plateaus scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(     optimizer,     mode=\"max\",  # Monitor accuracy (to be maximized)     factor=0.1,  # Reduce LR to 10 % of current value     patience=3,  # After 3 epochs without improvement     min_lr=1e-6, )  # Loss function: Cross-Entropy loss_function = nn.CrossEntropyLoss()  print(\"Optimizer: SGD with momentum=0.9\") print(\"  SGD with momentum accumulates gradients with exponential decay\") print(\"  This helps escape local minima and accelerates convergence\") print(\"\\nScheduler: ReduceLROnPlateau\") print(\"  Reduces the learning rate when accuracy stops improving\") print(\"  Reduction factor: 0.1 (LR \u2192 0.1 \u00d7 LR)\") print(\"  Patience: 3 epochs\") print(\"\\nLoss function: CrossEntropyLoss\") <pre>TRAINING CONFIGURATION\n======================================================================\n  Epochs: 1\n  Initial learning rate: 0.001\n  Weight decay (L2): 0.0005\n  Batch size: 128\n======================================================================\n\nOptimizer: SGD with momentum=0.9\n  SGD with momentum accumulates gradients with exponential decay\n  This helps escape local minima and accelerates convergence\n\nScheduler: ReduceLROnPlateau\n  Reduces the learning rate when accuracy stops improving\n  Reduction factor: 0.1 (LR \u2192 0.1 \u00d7 LR)\n  Patience: 3 epochs\n\nLoss function: CrossEntropyLoss\n</pre> <p>Historically, for architectures such as VGG, SGD with momentum has been very effective, especially when sufficient training time is available and the learning rate is carefully tuned. The momentum term accumulates past gradients as follows:</p> <p>$$ v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla L(\\theta_{t-1}), \\quad \\theta_t = \\theta_{t-1} - \\text{lr} \\cdot v_t, $$</p> <p>where $0 &lt; \\beta &lt; 1$ is the momentum coefficient, typically 0.9. This mechanism accelerates descent in directions of consistent gradient and damps oscillations in directions of high curvature, improving both the speed of convergence and training stability.</p> <p>The ReduceLROnPlateau scheduler adjusts the learning rate dynamically based on the evolution of performance on the validation set. When the monitored metric, in this case test accuracy, stops improving for a given number of epochs, the patience parameter, the scheduler reduces the learning rate, allowing a finer adjustment of parameters near a local optimum.</p> <p>The training loop consists of two phases per epoch: a training phase, during which the model parameters are updated, and a validation phase, during which performance is evaluated without modifying the weights. Losses and accuracies on both phases are recorded, as well as the evolution of the learning rate.</p> In\u00a0[10]: Copied! <pre>from tqdm import tqdm\nimport time\n\n# Lists to store metrics\ntrain_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\nlearning_rates = []\n\n\n# Auxiliary function to compute accuracy\ndef calculate_accuracy(outputs, labels):\n    _, predicted = torch.max(outputs, 1)\n    correct = (predicted == labels).sum().item()\n    total = labels.size(0)\n    return correct, total\n\n\nprint(\"STARTING TRAINING\\n\")\nprint(f\"{'=' * 70}\\n\")\n\n# Start time\nstart_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start_time = time.time()\n\n    # ============ TRAINING PHASE ============\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    train_loop = tqdm(\n        train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False\n    )\n\n    for batch_image, batch_label in train_loop:\n        batch_image = batch_image.to(device)\n        batch_label = batch_label.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(batch_image)\n        loss = loss_function(outputs, batch_label)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        batch_correct, batch_total = calculate_accuracy(outputs, batch_label)\n        correct += batch_correct\n        total += batch_total\n\n        train_loop.set_postfix(\n            {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n        )\n\n    epoch_train_loss = running_loss / len(train_dataloader)\n    epoch_train_acc = 100 * correct / total\n    train_losses.append(epoch_train_loss)\n    train_accuracies.append(epoch_train_acc)\n\n    # ============ VALIDATION PHASE ============\n    model.eval()\n    test_loss, correct_test, total_test = 0.0, 0, 0\n\n    test_loop = tqdm(\n        test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False\n    )\n\n    with torch.no_grad():\n        for images, labels in test_loop:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            batch_correct, batch_total = calculate_accuracy(outputs, labels)\n            correct_test += batch_correct\n            total_test += batch_total\n\n            test_loop.set_postfix(\n                {\n                    \"loss\": f\"{loss.item():.4f}\",\n                    \"acc\": f\"{100 * correct_test / total_test:.2f}%\",\n                }\n            )\n\n    epoch_test_loss = test_loss / len(test_dataloader)\n    epoch_test_acc = 100 * correct_test / total_test\n    test_losses.append(epoch_test_loss)\n    test_accuracies.append(epoch_test_acc)\n\n    # Update learning rate scheduler\n    scheduler.step(epoch_test_acc)\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    learning_rates.append(current_lr)\n\n    # Epoch time\n    epoch_time = time.time() - epoch_start_time\n\n    # Epoch report\n    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")\n    print(f\"  Train \u2192 Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")\n    print(f\"  Test  \u2192 Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")\n    print(f\"  LR: {current_lr:.6f}\")\n    print(f\"  {'\u2500' * 66}\\n\")\n\n# Total time\ntotal_time = time.time() - start_time\n\nprint(f\"\\n{'=' * 70}\")\nprint(\"TRAINING COMPLETED\")\nprint(f\"{'=' * 70}\")\nprint(f\"  Total time: {total_time / 60:.2f} minutes\")\nprint(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\")\nprint(f\"  Final accuracy (train): {train_accuracies[-1]:.2f}%\")\nprint(f\"  Final accuracy (test): {test_accuracies[-1]:.2f}%\")\nprint(f\"  Best accuracy (test): {max(test_accuracies):.2f}%\")\n\n# Save the model\ntorch.save(\n    {\n        \"epoch\": NUM_EPOCHS,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"train_losses\": train_losses,\n        \"train_accuracies\": train_accuracies,\n        \"test_losses\": test_losses,\n        \"test_accuracies\": test_accuracies,\n    },\n    \"vgg16_cifar10.pth\",\n)\n\nprint(\"\\nModel saved as 'vgg16_cifar10.pth'\")\n</pre> from tqdm import tqdm import time  # Lists to store metrics train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], [] learning_rates = []   # Auxiliary function to compute accuracy def calculate_accuracy(outputs, labels):     _, predicted = torch.max(outputs, 1)     correct = (predicted == labels).sum().item()     total = labels.size(0)     return correct, total   print(\"STARTING TRAINING\\n\") print(f\"{'=' * 70}\\n\")  # Start time start_time = time.time()  for epoch in range(NUM_EPOCHS):     epoch_start_time = time.time()      # ============ TRAINING PHASE ============     model.train()     running_loss, correct, total = 0.0, 0, 0      train_loop = tqdm(         train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False     )      for batch_image, batch_label in train_loop:         batch_image = batch_image.to(device)         batch_label = batch_label.to(device)          optimizer.zero_grad()          outputs = model(batch_image)         loss = loss_function(outputs, batch_label)          loss.backward()         optimizer.step()          running_loss += loss.item()         batch_correct, batch_total = calculate_accuracy(outputs, batch_label)         correct += batch_correct         total += batch_total          train_loop.set_postfix(             {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}         )      epoch_train_loss = running_loss / len(train_dataloader)     epoch_train_acc = 100 * correct / total     train_losses.append(epoch_train_loss)     train_accuracies.append(epoch_train_acc)      # ============ VALIDATION PHASE ============     model.eval()     test_loss, correct_test, total_test = 0.0, 0, 0      test_loop = tqdm(         test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False     )      with torch.no_grad():         for images, labels in test_loop:             images = images.to(device)             labels = labels.to(device)              outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             batch_correct, batch_total = calculate_accuracy(outputs, labels)             correct_test += batch_correct             total_test += batch_total              test_loop.set_postfix(                 {                     \"loss\": f\"{loss.item():.4f}\",                     \"acc\": f\"{100 * correct_test / total_test:.2f}%\",                 }             )      epoch_test_loss = test_loss / len(test_dataloader)     epoch_test_acc = 100 * correct_test / total_test     test_losses.append(epoch_test_loss)     test_accuracies.append(epoch_test_acc)      # Update learning rate scheduler     scheduler.step(epoch_test_acc)     current_lr = optimizer.param_groups[0][\"lr\"]     learning_rates.append(current_lr)      # Epoch time     epoch_time = time.time() - epoch_start_time      # Epoch report     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")     print(f\"  Train \u2192 Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")     print(f\"  Test  \u2192 Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")     print(f\"  LR: {current_lr:.6f}\")     print(f\"  {'\u2500' * 66}\\n\")  # Total time total_time = time.time() - start_time  print(f\"\\n{'=' * 70}\") print(\"TRAINING COMPLETED\") print(f\"{'=' * 70}\") print(f\"  Total time: {total_time / 60:.2f} minutes\") print(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\") print(f\"  Final accuracy (train): {train_accuracies[-1]:.2f}%\") print(f\"  Final accuracy (test): {test_accuracies[-1]:.2f}%\") print(f\"  Best accuracy (test): {max(test_accuracies):.2f}%\")  # Save the model torch.save(     {         \"epoch\": NUM_EPOCHS,         \"model_state_dict\": model.state_dict(),         \"optimizer_state_dict\": optimizer.state_dict(),         \"train_losses\": train_losses,         \"train_accuracies\": train_accuracies,         \"test_losses\": test_losses,         \"test_accuracies\": test_accuracies,     },     \"vgg16_cifar10.pth\", )  print(\"\\nModel saved as 'vgg16_cifar10.pth'\") <pre>STARTING TRAINING\n\n======================================================================\n\n</pre> <pre>\rEpoch 1/1 [TRAIN]:   0%|          | 0/40 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/1 [TRAIN]:   0%|          | 0/40 [00:02&lt;?, ?it/s, loss=2.3221, acc=8.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   2%|\u258e         | 1/40 [00:02&lt;01:20,  2.07s/it, loss=2.3221, acc=8.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   2%|\u258e         | 1/40 [00:04&lt;01:20,  2.07s/it, loss=2.3218, acc=8.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   5%|\u258c         | 2/40 [00:04&lt;01:16,  2.00s/it, loss=2.3218, acc=8.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   5%|\u258c         | 2/40 [00:05&lt;01:16,  2.00s/it, loss=2.3561, acc=8.07%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   8%|\u258a         | 3/40 [00:05&lt;01:12,  1.96s/it, loss=2.3561, acc=8.07%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   8%|\u258a         | 3/40 [00:07&lt;01:12,  1.96s/it, loss=2.3204, acc=8.20%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  10%|\u2588         | 4/40 [00:07&lt;01:08,  1.90s/it, loss=2.3204, acc=8.20%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  10%|\u2588         | 4/40 [00:09&lt;01:08,  1.90s/it, loss=2.3225, acc=8.91%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  12%|\u2588\u258e        | 5/40 [00:09&lt;01:05,  1.87s/it, loss=2.3225, acc=8.91%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  12%|\u2588\u258e        | 5/40 [00:11&lt;01:05,  1.87s/it, loss=2.3176, acc=8.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  15%|\u2588\u258c        | 6/40 [00:11&lt;01:02,  1.85s/it, loss=2.3176, acc=8.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  15%|\u2588\u258c        | 6/40 [00:13&lt;01:02,  1.85s/it, loss=2.3434, acc=8.93%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  18%|\u2588\u258a        | 7/40 [00:13&lt;01:00,  1.84s/it, loss=2.3434, acc=8.93%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  18%|\u2588\u258a        | 7/40 [00:14&lt;01:00,  1.84s/it, loss=2.3415, acc=8.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  20%|\u2588\u2588        | 8/40 [00:14&lt;00:58,  1.82s/it, loss=2.3415, acc=8.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  20%|\u2588\u2588        | 8/40 [00:16&lt;00:58,  1.82s/it, loss=2.3131, acc=8.68%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  22%|\u2588\u2588\u258e       | 9/40 [00:16&lt;00:56,  1.82s/it, loss=2.3131, acc=8.68%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  22%|\u2588\u2588\u258e       | 9/40 [00:18&lt;00:56,  1.82s/it, loss=2.3319, acc=9.22%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  25%|\u2588\u2588\u258c       | 10/40 [00:18&lt;00:54,  1.82s/it, loss=2.3319, acc=9.22%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  25%|\u2588\u2588\u258c       | 10/40 [00:20&lt;00:54,  1.82s/it, loss=2.2972, acc=9.38%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  28%|\u2588\u2588\u258a       | 11/40 [00:20&lt;00:52,  1.81s/it, loss=2.2972, acc=9.38%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  28%|\u2588\u2588\u258a       | 11/40 [00:22&lt;00:52,  1.81s/it, loss=2.2863, acc=9.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  30%|\u2588\u2588\u2588       | 12/40 [00:22&lt;00:50,  1.80s/it, loss=2.2863, acc=9.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  30%|\u2588\u2588\u2588       | 12/40 [00:23&lt;00:50,  1.80s/it, loss=2.2716, acc=9.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  32%|\u2588\u2588\u2588\u258e      | 13/40 [00:23&lt;00:48,  1.80s/it, loss=2.2716, acc=9.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  32%|\u2588\u2588\u2588\u258e      | 13/40 [00:25&lt;00:48,  1.80s/it, loss=2.2968, acc=10.32%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  35%|\u2588\u2588\u2588\u258c      | 14/40 [00:25&lt;00:46,  1.80s/it, loss=2.2968, acc=10.32%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  35%|\u2588\u2588\u2588\u258c      | 14/40 [00:27&lt;00:46,  1.80s/it, loss=2.2622, acc=10.52%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 15/40 [00:27&lt;00:44,  1.79s/it, loss=2.2622, acc=10.52%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 15/40 [00:29&lt;00:44,  1.79s/it, loss=2.2931, acc=10.50%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  40%|\u2588\u2588\u2588\u2588      | 16/40 [00:29&lt;00:43,  1.80s/it, loss=2.2931, acc=10.50%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  40%|\u2588\u2588\u2588\u2588      | 16/40 [00:31&lt;00:43,  1.80s/it, loss=2.2638, acc=10.80%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:31&lt;00:41,  1.79s/it, loss=2.2638, acc=10.80%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:32&lt;00:41,  1.79s/it, loss=2.2954, acc=11.24%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:32&lt;00:39,  1.79s/it, loss=2.2954, acc=11.24%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:34&lt;00:39,  1.79s/it, loss=2.2876, acc=11.18%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:34&lt;00:37,  1.79s/it, loss=2.2876, acc=11.18%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:36&lt;00:37,  1.79s/it, loss=2.2642, acc=11.41%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:36&lt;00:35,  1.79s/it, loss=2.2642, acc=11.41%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:38&lt;00:35,  1.79s/it, loss=2.3236, acc=11.24%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:38&lt;00:34,  1.79s/it, loss=2.3236, acc=11.24%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:40&lt;00:34,  1.79s/it, loss=2.2551, acc=11.58%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:40&lt;00:32,  1.79s/it, loss=2.2551, acc=11.58%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:41&lt;00:32,  1.79s/it, loss=2.2922, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:41&lt;00:30,  1.79s/it, loss=2.2922, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:43&lt;00:30,  1.79s/it, loss=2.2685, acc=11.85%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:43&lt;00:28,  1.79s/it, loss=2.2685, acc=11.85%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:45&lt;00:28,  1.79s/it, loss=2.3094, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:45&lt;00:26,  1.79s/it, loss=2.3094, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:47&lt;00:26,  1.79s/it, loss=2.2809, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:47&lt;00:25,  1.79s/it, loss=2.2809, acc=11.75%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:49&lt;00:25,  1.79s/it, loss=2.2477, acc=11.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:49&lt;00:23,  1.78s/it, loss=2.2477, acc=11.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:50&lt;00:23,  1.78s/it, loss=2.2567, acc=12.22%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:50&lt;00:21,  1.78s/it, loss=2.2567, acc=12.22%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:52&lt;00:21,  1.78s/it, loss=2.2674, acc=12.23%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:52&lt;00:19,  1.78s/it, loss=2.2674, acc=12.23%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:54&lt;00:19,  1.78s/it, loss=2.2511, acc=12.40%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:54&lt;00:17,  1.78s/it, loss=2.2511, acc=12.40%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:56&lt;00:17,  1.78s/it, loss=2.2650, acc=12.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [00:56&lt;00:16,  1.78s/it, loss=2.2650, acc=12.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [00:57&lt;00:16,  1.78s/it, loss=2.2591, acc=12.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [00:57&lt;00:14,  1.78s/it, loss=2.2591, acc=12.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [00:59&lt;00:14,  1.78s/it, loss=2.2775, acc=12.57%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [00:59&lt;00:12,  1.79s/it, loss=2.2775, acc=12.57%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [01:01&lt;00:12,  1.79s/it, loss=2.2755, acc=12.55%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [01:01&lt;00:10,  1.79s/it, loss=2.2755, acc=12.55%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [01:03&lt;00:10,  1.79s/it, loss=2.2345, acc=12.66%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [01:03&lt;00:08,  1.78s/it, loss=2.2345, acc=12.66%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [01:05&lt;00:08,  1.78s/it, loss=2.2192, acc=12.83%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [01:05&lt;00:07,  1.78s/it, loss=2.2192, acc=12.83%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [01:06&lt;00:07,  1.78s/it, loss=2.2226, acc=12.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [01:06&lt;00:05,  1.78s/it, loss=2.2226, acc=12.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [01:08&lt;00:05,  1.78s/it, loss=2.1975, acc=13.01%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [01:08&lt;00:03,  1.78s/it, loss=2.1975, acc=13.01%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [01:10&lt;00:03,  1.78s/it, loss=2.2415, acc=13.12%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [01:10&lt;00:01,  1.78s/it, loss=2.2415, acc=13.12%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [01:10&lt;00:01,  1.78s/it, loss=2.1555, acc=13.12%]</pre> <pre>\rEpoch 1/1 [TRAIN]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [01:10&lt;00:00,  1.29s/it, loss=2.1555, acc=13.12%]</pre> <pre>\r                                                                                           </pre> <pre>\r</pre> <pre>\rEpoch 1/1 [TEST]:   0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/1 [TEST]:   0%|          | 0/8 [00:00&lt;?, ?it/s, loss=2.2536, acc=14.06%]</pre> <pre>\rEpoch 1/1 [TEST]:  12%|\u2588\u258e        | 1/8 [00:00&lt;00:05,  1.27it/s, loss=2.2536, acc=14.06%]</pre> <pre>\rEpoch 1/1 [TEST]:  12%|\u2588\u258e        | 1/8 [00:01&lt;00:05,  1.27it/s, loss=2.2700, acc=12.50%]</pre> <pre>\rEpoch 1/1 [TEST]:  25%|\u2588\u2588\u258c       | 2/8 [00:01&lt;00:03,  1.50it/s, loss=2.2700, acc=12.50%]</pre> <pre>\rEpoch 1/1 [TEST]:  25%|\u2588\u2588\u258c       | 2/8 [00:01&lt;00:03,  1.50it/s, loss=2.2520, acc=13.28%]</pre> <pre>\rEpoch 1/1 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:01&lt;00:03,  1.61it/s, loss=2.2520, acc=13.28%]</pre> <pre>\rEpoch 1/1 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:02&lt;00:03,  1.61it/s, loss=2.2635, acc=14.45%]</pre> <pre>\rEpoch 1/1 [TEST]:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:02&lt;00:02,  1.67it/s, loss=2.2635, acc=14.45%]</pre> <pre>\rEpoch 1/1 [TEST]:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:03&lt;00:02,  1.67it/s, loss=2.2493, acc=15.31%]</pre> <pre>\rEpoch 1/1 [TEST]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:03&lt;00:01,  1.70it/s, loss=2.2493, acc=15.31%]</pre> <pre>\rEpoch 1/1 [TEST]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:03&lt;00:01,  1.70it/s, loss=2.2600, acc=15.36%]</pre> <pre>\rEpoch 1/1 [TEST]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:03&lt;00:01,  1.73it/s, loss=2.2600, acc=15.36%]</pre> <pre>\rEpoch 1/1 [TEST]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:04&lt;00:01,  1.73it/s, loss=2.2736, acc=15.40%]</pre> <pre>\rEpoch 1/1 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:04&lt;00:00,  1.74it/s, loss=2.2736, acc=15.40%]</pre> <pre>\rEpoch 1/1 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:04&lt;00:00,  1.74it/s, loss=2.2621, acc=15.50%]</pre> <pre>\rEpoch 1/1 [TEST]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:04&lt;00:00,  1.81it/s, loss=2.2621, acc=15.50%]</pre> <pre>\r                                                                                        </pre> <pre>Epoch [1/1] - Time: 75.23s\n  Train \u2192 Loss: 2.2802 | Acc: 13.12%\n  Test  \u2192 Loss: 2.2605 | Acc: 15.50%\n  LR: 0.001000\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n======================================================================\nTRAINING COMPLETED\n======================================================================\n  Total time: 1.25 minutes\n  Average time per epoch: 75.23 seconds\n  Final accuracy (train): 13.12%\n  Final accuracy (test): 15.50%\n  Best accuracy (test): 15.50%\n\nModel saved as 'vgg16_cifar10.pth'\n</pre> <pre>\r</pre> <p>VGG is computationally intensive due to the number of convolution operations and the large number of channels in the deeper layers. On a modern GPU (for example, an RTX 3080), each epoch may require on the order of tens of seconds with the described configuration, whereas on CPU the process can be an order of magnitude slower. The instruction <code>model.train()</code> activates training-specific behaviors, such as updating <code>BatchNorm</code> statistics and applying <code>Dropout</code>, while <code>model.eval()</code> deactivates these behaviors to ensure deterministic and reproducible evaluation.</p> <p>Inspecting the evolution of loss, accuracy, and learning rate across epochs makes it possible to identify potential issues such as overfitting, training stagnation, or inadequate learning rate schedules.</p> In\u00a0[11]: Copied! <pre>import os\n\nos.makedirs(\"results\", exist_ok=True)\n\nepochs_range = range(1, NUM_EPOCHS + 1)\n\n# Create figure with three subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss plot\nax1.plot(\n    epochs_range, train_losses, \"o-\", label=\"Train Loss\", linewidth=2, markersize=6\n)\nax1.plot(epochs_range, test_losses, \"s-\", label=\"Test Loss\", linewidth=2, markersize=6)\nax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\")\nax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\")\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\nax1.set_xticks(list(epochs_range))\n\n# Accuracy plot\nax2.plot(\n    epochs_range,\n    train_accuracies,\n    \"o-\",\n    label=\"Train Accuracy\",\n    linewidth=2,\n    markersize=6,\n)\nax2.plot(\n    epochs_range,\n    test_accuracies,\n    \"s-\",\n    label=\"Test Accuracy\",\n    linewidth=2,\n    markersize=6,\n)\nax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\")\nax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\")\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\nax2.set_xticks(list(epochs_range))\n\n# Learning rate plot\nax3.plot(epochs_range, learning_rates, \"o-\", color=\"red\", linewidth=2, markersize=6)\nax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\")\nax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\")\nax3.set_yscale(\"log\")\nax3.grid(True, alpha=0.3)\nax3.set_xticks(list(epochs_range))\n\nplt.tight_layout()\nplt.savefig(\"results/vgg16_training_history.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Quantitative analysis\nprint(\"\\nResult analysis:\")\ndiff = train_accuracies[-1] - test_accuracies[-1]\nprint(f\"  Overfitting detected: {'YES' if diff &gt; 10 else 'NO'}\")\nprint(f\"  Train-test gap: {diff:.2f}%\")\nprint(f\"  Best epoch (test acc): {np.argmax(test_accuracies) + 1}\")\nprint(f\"  Gain since epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\")\n</pre> import os  os.makedirs(\"results\", exist_ok=True)  epochs_range = range(1, NUM_EPOCHS + 1)  # Create figure with three subplots fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))  # Loss plot ax1.plot(     epochs_range, train_losses, \"o-\", label=\"Train Loss\", linewidth=2, markersize=6 ) ax1.plot(epochs_range, test_losses, \"s-\", label=\"Test Loss\", linewidth=2, markersize=6) ax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\") ax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\") ax1.legend(fontsize=11) ax1.grid(True, alpha=0.3) ax1.set_xticks(list(epochs_range))  # Accuracy plot ax2.plot(     epochs_range,     train_accuracies,     \"o-\",     label=\"Train Accuracy\",     linewidth=2,     markersize=6, ) ax2.plot(     epochs_range,     test_accuracies,     \"s-\",     label=\"Test Accuracy\",     linewidth=2,     markersize=6, ) ax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\") ax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\") ax2.legend(fontsize=11) ax2.grid(True, alpha=0.3) ax2.set_xticks(list(epochs_range))  # Learning rate plot ax3.plot(epochs_range, learning_rates, \"o-\", color=\"red\", linewidth=2, markersize=6) ax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\") ax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\") ax3.set_yscale(\"log\") ax3.grid(True, alpha=0.3) ax3.set_xticks(list(epochs_range))  plt.tight_layout() plt.savefig(\"results/vgg16_training_history.png\", dpi=300, bbox_inches=\"tight\") plt.show()  # Quantitative analysis print(\"\\nResult analysis:\") diff = train_accuracies[-1] - test_accuracies[-1] print(f\"  Overfitting detected: {'YES' if diff &gt; 10 else 'NO'}\") print(f\"  Train-test gap: {diff:.2f}%\") print(f\"  Best epoch (test acc): {np.argmax(test_accuracies) + 1}\") print(f\"  Gain since epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\") <pre>\nResult analysis:\n  Overfitting detected: NO\n  Train-test gap: -2.38%\n  Best epoch (test acc): 1\n  Gain since epoch 1: 0.00%\n</pre> <p>The loss and accuracy curves help detect typical behaviors: if training loss decreases while validation loss increases, overfitting is evident; if both remain high, the model may be underfitting or the learning rate may be inadequate. The learning-rate plot on a logarithmic scale shows the stepwise decreases produced by the scheduler, which usually coincide with refinement phases during which the model parameters are adjusted more precisely.</p> <p>The confusion matrix provides a detailed view of per-class performance and helps identify systematic error patterns, thereby clarifying which categories are more difficult for the model to distinguish.</p> In\u00a0[12]: Copied! <pre># 3pps\nimport os\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nos.makedirs(\"results\", exist_ok=True)\n\nprint(\"Generating confusion matrix...\")\n\n# Obtain all predictions\nmodel.eval()\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_dataloader, desc=\"Evaluating\"):\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\n\n# Visualize confusion matrix\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=CIFAR10_CLASSES,\n    yticklabels=CIFAR10_CLASSES,\n    cbar_kws={\"label\": \"Number of samples\"},\n)\nplt.xlabel(\"Prediction\", fontsize=12, fontweight=\"bold\")\nplt.ylabel(\"True Label\", fontsize=12, fontweight=\"bold\")\nplt.title(\"Confusion Matrix - VGG16 on CIFAR-10\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"results/vgg16_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(\"=\" * 70)\nprint(\n    classification_report(\n        all_labels, all_predictions, target_names=CIFAR10_CLASSES, digits=3\n    )\n)\n\n# Per-class accuracy analysis\nprint(\"\\nPer-Class Accuracy Analysis:\")\nprint(\"=\" * 70)\nclass_correct = cm.diagonal()\nclass_total = cm.sum(axis=1)\nclass_accuracy = 100 * class_correct / class_total\n\nfor idx, class_name in enumerate(CIFAR10_CLASSES):\n    print(\n        f\"  {class_name:12s}: {class_accuracy[idx]:6.2f}% \"\n        f\"({class_correct[idx]:4d}/{class_total[idx]:4d})\"\n    )\n\n# Most confused class pairs\nprint(\"\\nMost Confused Class Pairs:\")\nprint(\"=\" * 70)\nconfusion_pairs = []\nfor i in range(len(CIFAR10_CLASSES)):\n    for j in range(len(CIFAR10_CLASSES)):\n        if i != j:\n            confusion_pairs.append((cm[i, j], CIFAR10_CLASSES[i], CIFAR10_CLASSES[j]))\n\nconfusion_pairs.sort(reverse=True)\nfor count, true_class, pred_class in confusion_pairs[:5]:\n    print(f\"  {true_class:12s} \u2192 {pred_class:12s}: {count:4d} times\")\n</pre> # 3pps import os import seaborn as sns from sklearn.metrics import classification_report, confusion_matrix  os.makedirs(\"results\", exist_ok=True)  print(\"Generating confusion matrix...\")  # Obtain all predictions model.eval() all_predictions = [] all_labels = []  with torch.no_grad():     for images, labels in tqdm(test_dataloader, desc=\"Evaluating\"):         images = images.to(device)         outputs = model(images)         _, predicted = torch.max(outputs, 1)          all_predictions.extend(predicted.cpu().numpy())         all_labels.extend(labels.numpy())  # Compute confusion matrix cm = confusion_matrix(all_labels, all_predictions)  # Visualize confusion matrix plt.figure(figsize=(12, 10)) sns.heatmap(     cm,     annot=True,     fmt=\"d\",     cmap=\"Blues\",     xticklabels=CIFAR10_CLASSES,     yticklabels=CIFAR10_CLASSES,     cbar_kws={\"label\": \"Number of samples\"}, ) plt.xlabel(\"Prediction\", fontsize=12, fontweight=\"bold\") plt.ylabel(\"True Label\", fontsize=12, fontweight=\"bold\") plt.title(\"Confusion Matrix - VGG16 on CIFAR-10\", fontsize=14, fontweight=\"bold\") plt.tight_layout() plt.savefig(\"results/vgg16_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\") plt.show()  # Classification report print(\"\\nClassification Report:\") print(\"=\" * 70) print(     classification_report(         all_labels, all_predictions, target_names=CIFAR10_CLASSES, digits=3     ) )  # Per-class accuracy analysis print(\"\\nPer-Class Accuracy Analysis:\") print(\"=\" * 70) class_correct = cm.diagonal() class_total = cm.sum(axis=1) class_accuracy = 100 * class_correct / class_total  for idx, class_name in enumerate(CIFAR10_CLASSES):     print(         f\"  {class_name:12s}: {class_accuracy[idx]:6.2f}% \"         f\"({class_correct[idx]:4d}/{class_total[idx]:4d})\"     )  # Most confused class pairs print(\"\\nMost Confused Class Pairs:\") print(\"=\" * 70) confusion_pairs = [] for i in range(len(CIFAR10_CLASSES)):     for j in range(len(CIFAR10_CLASSES)):         if i != j:             confusion_pairs.append((cm[i, j], CIFAR10_CLASSES[i], CIFAR10_CLASSES[j]))  confusion_pairs.sort(reverse=True) for count, true_class, pred_class in confusion_pairs[:5]:     print(f\"  {true_class:12s} \u2192 {pred_class:12s}: {count:4d} times\") <pre>Generating confusion matrix...\n</pre> <pre>\rEvaluating:   0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>\rEvaluating:  12%|\u2588\u258e        | 1/8 [00:00&lt;00:04,  1.50it/s]</pre> <pre>\rEvaluating:  25%|\u2588\u2588\u258c       | 2/8 [00:01&lt;00:03,  1.63it/s]</pre> <pre>\rEvaluating:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:01&lt;00:02,  1.68it/s]</pre> <pre>\rEvaluating:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:02&lt;00:02,  1.71it/s]</pre> <pre>\rEvaluating:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:02&lt;00:01,  1.71it/s]</pre> <pre>\rEvaluating:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:03&lt;00:01,  1.73it/s]</pre> <pre>\rEvaluating:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:04&lt;00:00,  1.74it/s]</pre> <pre>\rEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:04&lt;00:00,  1.85it/s]</pre> <pre>\rEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:04&lt;00:00,  1.75it/s]</pre> <pre>\n</pre> <pre>\nClassification Report:\n======================================================================\n              precision    recall  f1-score   support\n\n    airplane      0.370     0.097     0.154       103\n  automobile      0.250     0.045     0.076        89\n        bird      0.087     0.070     0.078       100\n         cat      0.091     0.010     0.018       103\n        deer      0.126     0.733     0.215        90\n         dog      0.500     0.012     0.023        86\n        frog      0.203     0.107     0.140       112\n       horse      0.500     0.029     0.056       102\n        ship      0.149     0.274     0.193       106\n       truck      0.268     0.202     0.230       109\n\n    accuracy                          0.155      1000\n   macro avg      0.255     0.158     0.118      1000\nweighted avg      0.252     0.155     0.121      1000\n\n\nPer-Class Accuracy Analysis:\n======================================================================\n  airplane    :   9.71% (  10/ 103)\n  automobile  :   4.49% (   4/  89)\n  bird        :   7.00% (   7/ 100)\n  cat         :   0.97% (   1/ 103)\n  deer        :  73.33% (  66/  90)\n  dog         :   1.16% (   1/  86)\n  frog        :  10.71% (  12/ 112)\n  horse       :   2.94% (   3/ 102)\n  ship        :  27.36% (  29/ 106)\n  truck       :  20.18% (  22/ 109)\n\nMost Confused Class Pairs:\n======================================================================\n  frog         \u2192 deer        :   87 times\n  cat          \u2192 deer        :   67 times\n  bird         \u2192 deer        :   66 times\n  horse        \u2192 deer        :   57 times\n  ship         \u2192 deer        :   46 times\n</pre> <p>The main diagonal of the matrix reflects correct classifications, while off-diagonal elements quantify confusions between class pairs. On CIFAR-10 it is common to see confusions between <code>cat</code> and <code>dog</code>, <code>automobile</code> and <code>truck</code>, or <code>bird</code> and <code>airplane</code>, indicating that certain categories share similar visual patterns from the model\u2019s perspective. This analysis is valuable for identifying model limitations and guiding potential improvements, such as collecting additional data for problematic classes or applying class-balancing techniques.</p> <p>To better understand model behavior, it is useful to visualize some correct predictions and some errors, providing a qualitative perspective that complements quantitative metrics.</p> In\u00a0[13]: Copied! <pre>print(\"Visualizing model predictions...\\n\")\n\n# Obtain one test batch\ndata_iter = iter(test_dataloader)\ntest_images, test_labels = next(data_iter)\n\n# Make predictions\nmodel.eval()\nwith torch.no_grad():\n    test_images_device = test_images.to(device)\n    outputs = model(test_images_device)\n    _, predictions = torch.max(outputs, 1)\n    predictions = predictions.cpu()\n\n# Visualize first 8 predictions\nprint(\"First 8 predictions:\")\nshow_images(test_images[:8], test_labels[:8], predictions[:8])\n\n# Find misclassified examples\nincorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]\n\nif len(incorrect_indices) &gt;= 8:\n    print(\"\\nExamples of incorrect predictions:\")\n    error_indices = incorrect_indices[:8]\n    show_images(\n        test_images[error_indices],\n        test_labels[error_indices],\n        predictions[error_indices],\n    )\nelse:\n    print(f\"\\nOnly {len(incorrect_indices)} errors in this batch\")\n</pre> print(\"Visualizing model predictions...\\n\")  # Obtain one test batch data_iter = iter(test_dataloader) test_images, test_labels = next(data_iter)  # Make predictions model.eval() with torch.no_grad():     test_images_device = test_images.to(device)     outputs = model(test_images_device)     _, predictions = torch.max(outputs, 1)     predictions = predictions.cpu()  # Visualize first 8 predictions print(\"First 8 predictions:\") show_images(test_images[:8], test_labels[:8], predictions[:8])  # Find misclassified examples incorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]  if len(incorrect_indices) &gt;= 8:     print(\"\\nExamples of incorrect predictions:\")     error_indices = incorrect_indices[:8]     show_images(         test_images[error_indices],         test_labels[error_indices],         predictions[error_indices],     ) else:     print(f\"\\nOnly {len(incorrect_indices)} errors in this batch\") <pre>Visualizing model predictions...\n\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.39153773..1.5471393].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.4979501].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4629833..1.5354269].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.22441113..1.4005105].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4391681..1.2814069].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.45793372..1.1802652].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.48679847..1.557913].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.32803053..1.555192].\n</pre> <pre>First 8 predictions:\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.39153773..1.5471393].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4629833..1.5354269].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.22441113..1.4005105].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4391681..1.2814069].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.45793372..1.1802652].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.48679847..1.557913].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.32803053..1.555192].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.36130375..1.5310344].\n</pre> <pre>\nExamples of incorrect predictions:\n</pre> <p>This qualitative analysis helps detect systematic error patterns, such as consistently confusing one type of vehicle with another or certain animal classes with one another. Visual inspection of errors can also reveal issues in the data, such as incorrect labels or ambiguous images that would be difficult for a human to classify.</p> <p>One of VGG\u2019s strengths is its ability to learn hierarchical features in depth. Activations of intermediate layers can be inspected to better understand which types of patterns the network captures at each convolutional block.</p> In\u00a0[14]: Copied! <pre>import os\n\nos.makedirs(\"results\", exist_ok=True)\n\n\ndef get_activation_maps(model, image, layer_name):\n    \"\"\"\n    Extracts activation maps from a specific layer.\n    \"\"\"\n    activations = {}\n\n    def hook_fn(module, input, output):\n        activations[\"output\"] = output\n\n    # Register hook\n    layer = dict([*model.named_modules()])[layer_name]\n    hook = layer.register_forward_hook(hook_fn)\n\n    # Forward pass\n    model.eval()\n    with torch.no_grad():\n        _ = model(image.unsqueeze(0).to(device))\n\n    hook.remove()\n    return activations[\"output\"].squeeze().cpu()\n\n\n# Select one test image\ntest_image, test_label = test_dataset[0]\nprint(f\"Analyzing image of class: {CIFAR10_CLASSES[test_label]}\\n\")\n\n# Show original image\nplt.figure(figsize=(4, 4))\nimg_display = test_image / 2 + 0.5  # Denormalize\nplt.imshow(img_display.permute(1, 2, 0))\nplt.title(f\"Original Image: {CIFAR10_CLASSES[test_label]}\", fontweight=\"bold\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n# Select and visualize activations from different blocks\nlayers_to_visualize = {\n    \"block1\": \"block1.0\",  # First conv of block1\n    \"block2\": \"block2.0\",  # First conv of block2\n    \"block3\": \"block3.0\",  # First conv of block3\n    \"block5\": \"block5.0\",  # First conv of block5\n}\n\nfor block_name, layer_name in layers_to_visualize.items():\n    print(f\"Visualizing activations of {block_name}...\")\n\n    activations = get_activation_maps(model, test_image, layer_name)\n\n    # Select first 16 filters for visualization\n    num_filters = min(16, activations.shape[0])\n\n    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n    fig.suptitle(\n        f\"Activations of {block_name.upper()} - \" f\"{CIFAR10_CLASSES[test_label]}\",\n        fontsize=16,\n        fontweight=\"bold\",\n    )\n\n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; num_filters:\n            activation = activations[idx]\n            ax.imshow(activation, cmap=\"viridis\")\n            ax.set_title(f\"Filter {idx}\", fontsize=10)\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(f\"results/vgg16_activations_{block_name}.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\nprint(\"\\nInterpretation of block-wise activations:\")\nprint(\"=\" * 70)\nprint(\"  Block 1: Detects low-level features\")\nprint(\"           (edges, corners, simple color variations)\")\nprint(\"  Block 2-3: Detect mid-level patterns\")\nprint(\"             (textures, more complex shapes, repetitive patterns)\")\nprint(\"  Block 4-5: Detect high-level features\")\nprint(\"             (object parts, combinations of textures and shapes)\")\n</pre> import os  os.makedirs(\"results\", exist_ok=True)   def get_activation_maps(model, image, layer_name):     \"\"\"     Extracts activation maps from a specific layer.     \"\"\"     activations = {}      def hook_fn(module, input, output):         activations[\"output\"] = output      # Register hook     layer = dict([*model.named_modules()])[layer_name]     hook = layer.register_forward_hook(hook_fn)      # Forward pass     model.eval()     with torch.no_grad():         _ = model(image.unsqueeze(0).to(device))      hook.remove()     return activations[\"output\"].squeeze().cpu()   # Select one test image test_image, test_label = test_dataset[0] print(f\"Analyzing image of class: {CIFAR10_CLASSES[test_label]}\\n\")  # Show original image plt.figure(figsize=(4, 4)) img_display = test_image / 2 + 0.5  # Denormalize plt.imshow(img_display.permute(1, 2, 0)) plt.title(f\"Original Image: {CIFAR10_CLASSES[test_label]}\", fontweight=\"bold\") plt.axis(\"off\") plt.tight_layout() plt.show()  # Select and visualize activations from different blocks layers_to_visualize = {     \"block1\": \"block1.0\",  # First conv of block1     \"block2\": \"block2.0\",  # First conv of block2     \"block3\": \"block3.0\",  # First conv of block3     \"block5\": \"block5.0\",  # First conv of block5 }  for block_name, layer_name in layers_to_visualize.items():     print(f\"Visualizing activations of {block_name}...\")      activations = get_activation_maps(model, test_image, layer_name)      # Select first 16 filters for visualization     num_filters = min(16, activations.shape[0])      fig, axes = plt.subplots(4, 4, figsize=(12, 12))     fig.suptitle(         f\"Activations of {block_name.upper()} - \" f\"{CIFAR10_CLASSES[test_label]}\",         fontsize=16,         fontweight=\"bold\",     )      for idx, ax in enumerate(axes.flat):         if idx &lt; num_filters:             activation = activations[idx]             ax.imshow(activation, cmap=\"viridis\")             ax.set_title(f\"Filter {idx}\", fontsize=10)         ax.axis(\"off\")      plt.tight_layout()     plt.savefig(f\"results/vgg16_activations_{block_name}.png\", dpi=300, bbox_inches=\"tight\")     plt.show()  print(\"\\nInterpretation of block-wise activations:\") print(\"=\" * 70) print(\"  Block 1: Detects low-level features\") print(\"           (edges, corners, simple color variations)\") print(\"  Block 2-3: Detect mid-level patterns\") print(\"             (textures, more complex shapes, repetitive patterns)\") print(\"  Block 4-5: Detect high-level features\") print(\"             (object parts, combinations of textures and shapes)\") <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.39153773..1.5471393].\n</pre> <pre>Analyzing image of class: cat\n\n</pre> <pre>Visualizing activations of block1...\n</pre> <pre>Visualizing activations of block2...\n</pre> <pre>Visualizing activations of block3...\n</pre> <pre>Visualizing activations of block5...\n</pre> <pre>\nInterpretation of block-wise activations:\n======================================================================\n  Block 1: Detects low-level features\n           (edges, corners, simple color variations)\n  Block 2-3: Detect mid-level patterns\n             (textures, more complex shapes, repetitive patterns)\n  Block 4-5: Detect high-level features\n             (object parts, combinations of textures and shapes)\n</pre>"},{"location":"course/topic_04_computer_vision/section_05_vgg.html#vgg","title":"VGG\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#theoretical-introduction","title":"Theoretical Introduction\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#depth-and-simplicity-in-convolutional-neural-networks","title":"Depth and Simplicity in Convolutional Neural Networks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#design-principles","title":"Design Principles\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#justification-for-the-use-of-small-filters","title":"Justification for the Use of Small Filters\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#architectural-organization-of-vgg-16","title":"Architectural Organization of VGG-16\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#impact-advantages-and-limitations","title":"Impact, Advantages, and Limitations\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#practical-implementation-of-vgg-16-on-cifar-10","title":"Practical Implementation of VGG-16 on CIFAR-10\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#library-imports","title":"Library Imports\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#global-hyperparameter-configuration","title":"Global Hyperparameter Configuration\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#auxiliary-visualization-function","title":"Auxiliary Visualization Function\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#cifar-10-dataset-preparation","title":"CIFAR-10 Dataset Preparation\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#dataloader-creation","title":"DataLoader Creation\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#initial-visual-exploration-of-the-dataset","title":"Initial Visual Exploration of the Dataset\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#definition-of-the-vgg-16-architecture-for-cifar-10","title":"Definition of the VGG-16 Architecture for CIFAR-10\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#model-instantiation-and-complexity-analysis","title":"Model Instantiation and Complexity Analysis\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#training-configuration","title":"Training Configuration\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#training-loop","title":"Training Loop\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#visualization-of-training-metrics","title":"Visualization of Training Metrics\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#confusion-matrix-and-per-class-analysis","title":"Confusion Matrix and Per-Class Analysis\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#visualization-of-correct-and-incorrect-predictions","title":"Visualization of Correct and Incorrect Predictions\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_05_vgg.html#extraction-and-visualization-of-intermediate-features","title":"Extraction and Visualization of Intermediate Features\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html","title":"ResNet","text":"<p>The ResNet (Residual Network) architecture, introduced in 2015 by Kaiming He and his collaborators at Microsoft Research, represents a decisive turning point in the design of deep neural networks for computer vision. While LeNet-5 established the systematic use of convolutional layers and architectures such as VGG demonstrated that increasing depth can significantly improve performance, ResNet introduces a fundamentally new structural mechanism that enables the effective and stable training of extremely deep networks, including models with more than one hundred layers.</p> <p>The importance of ResNet lies in its ability to overcome the optimization challenges that arise as network depth increases. Prior to its introduction, very deep architectures often suffered from severe training difficulties, including slow convergence, numerical instability, and degradation of performance as additional layers were added. These issues made it impractical to exploit the full representational power theoretically offered by deep models. ResNet addresses these limitations by reformulating how layers learn transformations, allowing information and gradients to propagate more effectively through the network.</p> <p>The practical impact of this innovation was clearly demonstrated when ResNet won the ImageNet 2015 Large Scale Visual Recognition Challenge with a 152-layer variant. This depth was previously considered unattainable from a training perspective, given the known difficulties associated with optimizing such deep architectures. The success of ResNet not only established a new state of the art in image recognition performance, but also reshaped prevailing assumptions about the feasible depth of neural networks, paving the way for subsequent generations of very deep models in computer vision and beyond.</p> <p>Prior to the introduction of ResNet, it was widely assumed that increasing the number of layers in a neural network should, at least in principle, enhance its representational capacity and improve performance. Deeper networks can theoretically capture increasingly complex hierarchical features, enabling more sophisticated modeling of input data. However, empirical studies revealed a surprising and counterintuitive phenomenon: beyond a certain depth\u2014typically around twenty to thirty layers\u2014adding additional layers often degrades performance, even on the training set itself. This effect is distinct from overfitting, as it occurs during training rather than being a consequence of limited generalization.</p> <p>This phenomenon is referred to as degradation and arises from structural optimization difficulties inherent to very deep networks. During backpropagation, the error signal\u2014used to update network parameters\u2014tends to either vanish or become numerically unstable as it propagates backward through many layers. Consequently, layers that are close to the input receive gradients that are extremely small or dominated by noise. As a result, these early layers fail to update their parameters effectively, preventing the network from fully exploiting its representational capacity.</p> <p>The central innovation of ResNet lies in the introduction of shortcut connections, also referred to as skip connections, which form the fundamental building block known as the residual block. The underlying idea is conceptually straightforward yet profoundly impactful: rather than requiring each group of layers to learn a complete input-to-output mapping, the network is allowed to learn only the residual\u2014that is, the difference between the input to the block and the desired output.</p> <p>Mathematically, if the desired mapping is denoted by $H(x)$ and the input to a residual block is $x$, the block is designed to learn a function $F(x) = H(x) - x$. The output of the block is then expressed as:</p> <p>$$ y = F(x) + x $$</p> <p>This formulation offers two key advantages. First, it simplifies the learning problem: it is often easier for a block to learn small deviations from the identity mapping than to approximate a completely new transformation. Second, the shortcut connection provides a direct pathway for both the forward signal and the backward gradients to propagate through the network. This mechanism mitigates the vanishing gradient problem and enhances numerical stability, allowing extremely deep networks to be trained effectively.</p> <p>By enabling layers to focus on learning residual functions instead of full transformations, ResNet overcomes the degradation problem observed in very deep networks and establishes a structural principle that has become foundational in modern deep learning architectures. Residual blocks can be stacked to create networks with hundreds of layers, achieving remarkable performance while maintaining stable and efficient training dynamics.</p> <p>Residual learning provides several fundamental advantages. First, it facilitates optimization. If the optimal transformation in a certain block is close to the identity, it is easier for the network to learn a residual function with $F(x) \\approx 0$ than to learn a complete transformation $H(x) \\approx x$ from scratch. The function space of residuals tends to be closer to the origin and is therefore more accessible to gradient-based optimization methods.</p> <p>Second, gradient propagation improves significantly. During backpropagation, the gradient of the loss $L$ with respect to the input $x$ of a residual block satisfies, in simplified form,</p> <p>$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\left( \\frac{\\partial F}{\\partial x} + I \\right), $$</p> <p>where $I$ is the identity matrix. This expression guarantees that, even in the limiting case where $\\frac{\\partial F}{\\partial x}$ tends to zero, there is always a direct gradient path through the identity term $I$. In practice, this prevents the signal from vanishing completely and contributes to stabilizing the training of very deep networks.</p> <p>Third, residual connections introduce a form of adaptive depth. If a block is not necessary for the task, it can approximate $F(x) \\approx 0$ and effectively behave as an identity transformation, so that $y \\approx x$. The network thus retains the ability to neutralize blocks that do not provide improvements, without compromising the information flow. Taken together, these mechanisms make it possible to train networks with more than one hundred layers without suffering the severe degradation that affected previous architectures. The input signal can traverse the network without being distorted, and the gradient has alternative paths that mitigate vanishing effects.</p> <p>The ResNet family includes several configurations that differ mainly in depth and in the type of residual block used. Summarizing:</p> Model Layers Parameters Blocks per stage Block type ResNet-18 18 ~11 M [2, 2, 2, 2] Basic ResNet-34 34 ~21 M [3, 4, 6, 3] Basic ResNet-50 50 ~25 M [3, 4, 6, 3] Bottleneck ResNet-101 101 ~44 M [3, 4, 23, 3] Bottleneck ResNet-152 152 ~60 M [3, 8, 36, 3] Bottleneck <p>In the shallower versions, such as ResNet-18 and ResNet-34, the basic block is used. This block consists of two $3 \\times 3$ convolutions followed by batch normalization and ReLU activation, and a residual sum with the identity branch. The number of output channels matches that of the input, with an expansion factor of 1.</p> <p>In the deeper variants, such as ResNet-50, ResNet-101, and ResNet-152, the bottleneck block is used, whose purpose is to reduce computational cost while preserving representational capacity. This block combines three consecutive convolutions. The first, of size $1 \\times 1$, reduces the channel dimensionality, for example from 256 to 64 channels. The second, of size $3 \\times 3$, performs the main processing on a reduced number of channels. The third, again of size $1 \\times 1$, restores the original dimensionality, for example from 64 back to 256 channels. The typical expansion factor is 4: the number of output channels is four times the number of intermediate channels.</p> <p>ResNet is currently regarded as a reference architecture in both academic and industrial contexts. The balance between depth, training stability, and efficiency makes it the backbone of numerous systems for face recognition, autonomous driving, medical imaging diagnosis, and large-scale visual analysis in multiple domains.</p> <p>Its advantages include efficient parameter usage, for example ResNet-50 uses approximately five times fewer parameters than VGG-16, the ability to train networks with more than one hundred layers without severe gradient degradation, its suitability as a base structure for transfer learning, numerical stability during training, and versatility, which has inspired variants in vision, natural language processing, and other modalities. Beyond solving a specific technical problem, ResNet redefines deep architecture design by explicitly incorporating identity paths that facilitate the flow of information and gradients through the network.</p> <p>The following provides a complete and fully functional implementation of ResNet (ResNet-18, ResNet-34, and ResNet-50) for the CIFAR-10 dataset using PyTorch. The code is structured for easy conversion into a Jupyter Notebook and can be executed sequentially\u2014from data loading to training, evaluation, and result visualization.</p> <p>The first step is to import the necessary modules for defining the network architecture, handling data, performing training, and visualizing results.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nimport time\nfrom typing import Any, List, Type, Union\n\n# 3pps\n# Third-party libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n</pre> # Standard libraries import time from typing import Any, List, Type, Union  # 3pps # Third-party libraries import matplotlib.pyplot as plt import numpy as np import seaborn as sns import torch import torch.nn.functional as F from sklearn.manifold import TSNE from sklearn.metrics import classification_report, confusion_matrix from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm   print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") if torch.cuda.is_available():     print(f\"CUDA device: {torch.cuda.get_device_name(0)}\") <pre>PyTorch version: 2.9.1+cu128\nCUDA available: False\n</pre> <p>The constants and hyperparameters that will be used throughout the experiment are defined next.</p> In\u00a0[2]: Copied! <pre># Global configuration\nBATCH_SIZE: int = 128\nNUM_EPOCHS: int = 1\nLEARNING_RATE: float = 0.1\nWEIGHT_DECAY: float = 1e-4\nMOMENTUM: float = 0.9\nNUM_CLASSES: int = 10\nINPUT_SIZE: int = 32\n\n# CIFAR-10 class names\nCIFAR10_CLASSES = [\n    \"airplane\",\n    \"automobile\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n]\n\nprint(\"Configuration:\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Initial learning rate: {LEARNING_RATE}\")\nprint(f\"  Momentum: {MOMENTUM}\")\nprint(f\"  Weight decay: {WEIGHT_DECAY}\")\nprint(f\"  Number of classes: {NUM_CLASSES}\")\n</pre> # Global configuration BATCH_SIZE: int = 128 NUM_EPOCHS: int = 1 LEARNING_RATE: float = 0.1 WEIGHT_DECAY: float = 1e-4 MOMENTUM: float = 0.9 NUM_CLASSES: int = 10 INPUT_SIZE: int = 32  # CIFAR-10 class names CIFAR10_CLASSES = [     \"airplane\",     \"automobile\",     \"bird\",     \"cat\",     \"deer\",     \"dog\",     \"frog\",     \"horse\",     \"ship\",     \"truck\", ]  print(\"Configuration:\") print(f\"  Batch size: {BATCH_SIZE}\") print(f\"  Epochs: {NUM_EPOCHS}\") print(f\"  Initial learning rate: {LEARNING_RATE}\") print(f\"  Momentum: {MOMENTUM}\") print(f\"  Weight decay: {WEIGHT_DECAY}\") print(f\"  Number of classes: {NUM_CLASSES}\") <pre>Configuration:\n  Batch size: 128\n  Epochs: 1\n  Initial learning rate: 0.1\n  Momentum: 0.9\n  Weight decay: 0.0001\n  Number of classes: 10\n</pre> <p>The <code>show_images</code> function allows visualization of CIFAR-10 images together with theirground-truth labels and, optionally, model predictions.</p> In\u00a0[3]: Copied! <pre>def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):\n    \"\"\"\n    Visualize a set of images with their labels and predictions.\n\n    Args:\n        images: Image tensor [N, C, H, W].\n        labels: Label tensor [N].\n        predictions: Optional tensor of predictions [N].\n        classes: List of class names.\n    \"\"\"\n    n_images = min(len(images), 8)\n    fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))\n    if n_images == 1:\n        axes = [axes]\n\n    for idx in range(n_images):\n        img = images[idx]\n        label = labels[idx]\n        ax = axes[idx]\n\n        # Denormalize image (assuming standard normalization)\n        img = img / 2 + 0.5\n        img = img.numpy().transpose((1, 2, 0))\n        ax.imshow(img)\n\n        title = f\"True: {classes[label]}\"\n        if predictions is not None:\n            pred = predictions[idx]\n            color = \"green\" if pred == label else \"red\"\n            title += f\"\\nPred: {classes[pred]}\"\n            ax.set_title(title, fontsize=9, color=color, fontweight=\"bold\")\n        else:\n            ax.set_title(title, fontsize=9, fontweight=\"bold\")\n\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"Visualization function defined correctly\")\n</pre> def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):     \"\"\"     Visualize a set of images with their labels and predictions.      Args:         images: Image tensor [N, C, H, W].         labels: Label tensor [N].         predictions: Optional tensor of predictions [N].         classes: List of class names.     \"\"\"     n_images = min(len(images), 8)     fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))     if n_images == 1:         axes = [axes]      for idx in range(n_images):         img = images[idx]         label = labels[idx]         ax = axes[idx]          # Denormalize image (assuming standard normalization)         img = img / 2 + 0.5         img = img.numpy().transpose((1, 2, 0))         ax.imshow(img)          title = f\"True: {classes[label]}\"         if predictions is not None:             pred = predictions[idx]             color = \"green\" if pred == label else \"red\"             title += f\"\\nPred: {classes[pred]}\"             ax.set_title(title, fontsize=9, color=color, fontweight=\"bold\")         else:             ax.set_title(title, fontsize=9, fontweight=\"bold\")          ax.axis(\"off\")      plt.tight_layout()     plt.show()   print(\"Visualization function defined correctly\") <pre>Visualization function defined correctly\n</pre> <p>CIFAR-10 is then loaded and the preprocessing and data augmentation transformations for training and validation are defined.</p> In\u00a0[4]: Copied! <pre>from torch.utils.data import Subset\n\n# CIFAR-10 normalization statistics\nCIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\nCIFAR10_STD = (0.2470, 0.2435, 0.2616)\n\n# Training transformations with data augmentation\ntransform_train = transforms.Compose(\n    [\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n    ]\n)\n\n# Validation/test transformations\ntransform_test = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]\n)\n\nprint(\"Downloading CIFAR-10 dataset...\")\ntrain_dataset_full = datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform_train\n)\n\ntest_dataset_full = datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform_test\n)\n\n# Limit samples\ntrain_dataset = Subset(train_dataset_full, range(5000))\ntest_dataset = Subset(test_dataset_full, range(1000))\n\nprint(\"\\nDataset statistics:\")\nprint(f\"  Training samples: {len(train_dataset):,}\")\nprint(f\"  Test samples: {len(test_dataset):,}\")\nprint(f\"  Number of classes: {len(train_dataset_full.classes)}\")\nprint(\"  Image size: 32\u00d732 pixels (RGB)\")\n</pre> from torch.utils.data import Subset  # CIFAR-10 normalization statistics CIFAR10_MEAN = (0.4914, 0.4822, 0.4465) CIFAR10_STD = (0.2470, 0.2435, 0.2616)  # Training transformations with data augmentation transform_train = transforms.Compose(     [         transforms.RandomCrop(32, padding=4),         transforms.RandomHorizontalFlip(),         transforms.ToTensor(),         transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),     ] )  # Validation/test transformations transform_test = transforms.Compose(     [transforms.ToTensor(), transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)] )  print(\"Downloading CIFAR-10 dataset...\") train_dataset_full = datasets.CIFAR10(     root=\"./data\", train=True, download=True, transform=transform_train )  test_dataset_full = datasets.CIFAR10(     root=\"./data\", train=False, download=True, transform=transform_test )  # Limit samples train_dataset = Subset(train_dataset_full, range(5000)) test_dataset = Subset(test_dataset_full, range(1000))  print(\"\\nDataset statistics:\") print(f\"  Training samples: {len(train_dataset):,}\") print(f\"  Test samples: {len(test_dataset):,}\") print(f\"  Number of classes: {len(train_dataset_full.classes)}\") print(\"  Image size: 32\u00d732 pixels (RGB)\") <pre>Downloading CIFAR-10 dataset...\n</pre> <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n  entry = pickle.load(f, encoding=\"latin1\")\n</pre> <pre>\nDataset statistics:\n  Training samples: 5,000\n  Test samples: 1,000\n  Number of classes: 10\n  Image size: 32\u00d732 pixels (RGB)\n</pre> <p>Data augmentation introduces random cropping and horizontal flipping to enhance generalization. Normalizing each channel using the CIFAR-10 mean and standard deviation centers and scales the data, promoting faster and more stable convergence.</p> <p>The training and test <code>DataLoader</code> objects are then defined, configuring the number ofworker processes and other performance-oriented options.</p> In\u00a0[5]: Copied! <pre>train_dataloader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    persistent_workers=True,\n)\n\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    persistent_workers=True,\n)\n\nprint(\"DataLoaders configured:\")\nprint(f\"  Training batches: {len(train_dataloader)}\")\nprint(f\"  Test batches: {len(test_dataloader)}\")\n</pre> train_dataloader = DataLoader(     dataset=train_dataset,     batch_size=BATCH_SIZE,     shuffle=True,     num_workers=4,     pin_memory=True,     persistent_workers=True, )  test_dataloader = DataLoader(     dataset=test_dataset,     batch_size=BATCH_SIZE,     shuffle=False,     num_workers=4,     pin_memory=True,     persistent_workers=True, )  print(\"DataLoaders configured:\") print(f\"  Training batches: {len(train_dataloader)}\") print(f\"  Test batches: {len(test_dataloader)}\") <pre>DataLoaders configured:\n  Training batches: 40\n  Test batches: 8\n</pre> <p>Before training the model, it is helpful to inspect a batch of images to ensure that data loading and preprocessing are correctly configured.</p> In\u00a0[6]: Copied! <pre># Get one training batch\ndata_iter = iter(train_dataloader)\ntrain_images, train_labels = next(data_iter)\n\nprint(\"\\nBatch dimensions:\")\nprint(f\"  Images: {train_images.shape}\")\nprint(f\"  Labels: {train_labels.shape}\")\n\nprint(\"\\nDisplaying first 8 samples...\")\nshow_images(train_images[:8], train_labels[:8])\n</pre> # Get one training batch data_iter = iter(train_dataloader) train_images, train_labels = next(data_iter)  print(\"\\nBatch dimensions:\") print(f\"  Images: {train_images.shape}\") print(f\"  Labels: {train_labels.shape}\")  print(\"\\nDisplaying first 8 samples...\") show_images(train_images[:8], train_labels[:8]) <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>\nBatch dimensions:\n  Images: torch.Size([128, 3, 32, 32])\n  Labels: torch.Size([128])\n\nDisplaying first 8 samples...\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.4080058].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.2914028].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.4104787].\n</pre> <p>The BasicBlock class implements the basic residual block used in ResNet-18 and ResNet-34. This block consists of two $3 \\times 3$ convolutions with batch normalization and a shortcut connection that adds the input to the output.</p> In\u00a0[7]: Copied! <pre>class BasicBlock(nn.Module):\n    \"\"\"\n    Basic residual block for ResNet-18 and ResNet-34.\n    \"\"\"\n\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        downsample: nn.Module = None,\n    ) -&gt; None:\n        super().__init__()\n\n        # First 3\u00d73 convolution\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        # Second 3\u00d73 convolution\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut branch (dimensionality adjustment if needed)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n</pre> class BasicBlock(nn.Module):     \"\"\"     Basic residual block for ResNet-18 and ResNet-34.     \"\"\"      expansion: int = 1      def __init__(         self,         in_channels: int,         out_channels: int,         stride: int = 1,         downsample: nn.Module = None,     ) -&gt; None:         super().__init__()          # First 3\u00d73 convolution         self.conv1 = nn.Conv2d(             in_channels,             out_channels,             kernel_size=3,             stride=stride,             padding=1,             bias=False,         )         self.bn1 = nn.BatchNorm2d(out_channels)          # Second 3\u00d73 convolution         self.conv2 = nn.Conv2d(             out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False         )         self.bn2 = nn.BatchNorm2d(out_channels)          # Shortcut branch (dimensionality adjustment if needed)         self.downsample = downsample         self.stride = stride      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         identity = x          out = self.conv1(x)         out = self.bn1(out)         out = F.relu(out)          out = self.conv2(out)         out = self.bn2(out)          if self.downsample is not None:             identity = self.downsample(x)          out += identity         out = F.relu(out)          return out <p>The Bottleneck class implements the bottleneck block used in deeper ResNet variants such as ResNet-50, ResNet-101, and ResNet-152. This block uses three convolutions with an expansion factor of 4 to reduce computational cost while maintaining representational capacity.</p> In\u00a0[8]: Copied! <pre>class Bottleneck(nn.Module):\n    \"\"\"\n    Bottleneck block for ResNet-50, ResNet-101, and ResNet-152.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        downsample: nn.Module = None,\n    ) -&gt; None:\n        super().__init__()\n\n        # 1\u00d71 conv to reduce dimensionality\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        # 3\u00d73 conv for main processing\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # 1\u00d71 conv to restore dimensionality\n        self.conv3 = nn.Conv2d(\n            out_channels, out_channels * self.expansion, kernel_size=1, bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n</pre> class Bottleneck(nn.Module):     \"\"\"     Bottleneck block for ResNet-50, ResNet-101, and ResNet-152.     \"\"\"      expansion: int = 4      def __init__(         self,         in_channels: int,         out_channels: int,         stride: int = 1,         downsample: nn.Module = None,     ) -&gt; None:         super().__init__()          # 1\u00d71 conv to reduce dimensionality         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)         self.bn1 = nn.BatchNorm2d(out_channels)          # 3\u00d73 conv for main processing         self.conv2 = nn.Conv2d(             out_channels,             out_channels,             kernel_size=3,             stride=stride,             padding=1,             bias=False,         )         self.bn2 = nn.BatchNorm2d(out_channels)          # 1\u00d71 conv to restore dimensionality         self.conv3 = nn.Conv2d(             out_channels, out_channels * self.expansion, kernel_size=1, bias=False         )         self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)          self.downsample = downsample         self.stride = stride      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         identity = x          out = self.conv1(x)         out = self.bn1(out)         out = F.relu(out)          out = self.conv2(out)         out = self.bn2(out)         out = F.relu(out)          out = self.conv3(out)         out = self.bn3(out)          if self.downsample is not None:             identity = self.downsample(x)          out += identity         out = F.relu(out)          return out <p>The ResNet class implements the complete ResNet architecture adapted for CIFAR-10. The main differences from the original ImageNet version include a smaller initial convolution, no initial max pooling, and a final classification layer adapted to 10 classes.</p> In\u00a0[9]: Copied! <pre>class ResNet(nn.Module):\n    \"\"\"\n    ResNet implementation adapted for CIFAR-10.\n\n    Differences with respect to the original ImageNet version:\n      - First layer: Conv 3\u00d73 instead of Conv 7\u00d77.\n      - No initial MaxPooling (images are 32\u00d732).\n      - Final classification layer adapted to CIFAR-10.\n\n    Args:\n        block: Block type (BasicBlock or Bottleneck).\n        layers: List with the number of blocks per stage.\n        num_classes: Number of output classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 10,\n    ) -&gt; None:\n        super().__init__()\n        self.in_channels = 64\n\n        # Initial layer adapted to CIFAR-10 (32\u00d732)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Four stages of residual blocks\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        # Global pooling and final classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        # Weight initialization\n        self._initialize_weights()\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        out_channels: int,\n        num_blocks: int,\n        stride: int = 1,\n    ) -&gt; nn.Sequential:\n        \"\"\"\n        Build one stage of residual blocks.\n\n        Args:\n            block: Residual block type.\n            out_channels: Number of output channels.\n            num_blocks: Number of blocks in the stage.\n            stride: Stride of the first block (downsampling).\n\n        Returns:\n            nn.Sequential containing the stage blocks.\n        \"\"\"\n        downsample = None\n\n        # Dimensionality adjustment in the shortcut branch\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.in_channels,\n                    out_channels * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n\n        for _ in range(1, num_blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"\n        Initialize weights using He (Kaiming) initialization.\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        ResNet forward pass.\n\n        Args:\n            x: Input tensor [B, 3, 32, 32].\n\n        Returns:\n            Classification logits [B, num_classes].\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)  # 32\u00d732 \u2192 32\u00d732\n        x = self.layer2(x)  # 32\u00d732 \u2192 16\u00d716\n        x = self.layer3(x)  # 16\u00d716 \u2192  8\u00d78\n        x = self.layer4(x)  #  8\u00d78 \u2192  4\u00d74\n\n        x = self.avgpool(x)  # 4\u00d74 \u2192 1\u00d71\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def get_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Extract features before the classification layer.\n        Useful for visualization of embeddings and transfer learning.\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        return x\n</pre> class ResNet(nn.Module):     \"\"\"     ResNet implementation adapted for CIFAR-10.      Differences with respect to the original ImageNet version:       - First layer: Conv 3\u00d73 instead of Conv 7\u00d77.       - No initial MaxPooling (images are 32\u00d732).       - Final classification layer adapted to CIFAR-10.      Args:         block: Block type (BasicBlock or Bottleneck).         layers: List with the number of blocks per stage.         num_classes: Number of output classes.     \"\"\"      def __init__(         self,         block: Type[Union[BasicBlock, Bottleneck]],         layers: List[int],         num_classes: int = 10,     ) -&gt; None:         super().__init__()         self.in_channels = 64          # Initial layer adapted to CIFAR-10 (32\u00d732)         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)         self.bn1 = nn.BatchNorm2d(64)         self.relu = nn.ReLU(inplace=True)          # Four stages of residual blocks         self.layer1 = self._make_layer(block, 64, layers[0], stride=1)         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)         self.layer4 = self._make_layer(block, 512, layers[3], stride=2)          # Global pooling and final classifier         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))         self.fc = nn.Linear(512 * block.expansion, num_classes)          # Weight initialization         self._initialize_weights()      def _make_layer(         self,         block: Type[Union[BasicBlock, Bottleneck]],         out_channels: int,         num_blocks: int,         stride: int = 1,     ) -&gt; nn.Sequential:         \"\"\"         Build one stage of residual blocks.          Args:             block: Residual block type.             out_channels: Number of output channels.             num_blocks: Number of blocks in the stage.             stride: Stride of the first block (downsampling).          Returns:             nn.Sequential containing the stage blocks.         \"\"\"         downsample = None          # Dimensionality adjustment in the shortcut branch         if stride != 1 or self.in_channels != out_channels * block.expansion:             downsample = nn.Sequential(                 nn.Conv2d(                     self.in_channels,                     out_channels * block.expansion,                     kernel_size=1,                     stride=stride,                     bias=False,                 ),                 nn.BatchNorm2d(out_channels * block.expansion),             )          layers = []         layers.append(block(self.in_channels, out_channels, stride, downsample))         self.in_channels = out_channels * block.expansion          for _ in range(1, num_blocks):             layers.append(block(self.in_channels, out_channels))          return nn.Sequential(*layers)      def _initialize_weights(self) -&gt; None:         \"\"\"         Initialize weights using He (Kaiming) initialization.         \"\"\"         for m in self.modules():             if isinstance(m, nn.Conv2d):                 nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")             elif isinstance(m, nn.BatchNorm2d):                 nn.init.constant_(m.weight, 1)                 nn.init.constant_(m.bias, 0)             elif isinstance(m, nn.Linear):                 nn.init.normal_(m.weight, 0, 0.01)                 nn.init.constant_(m.bias, 0)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         ResNet forward pass.          Args:             x: Input tensor [B, 3, 32, 32].          Returns:             Classification logits [B, num_classes].         \"\"\"         x = self.conv1(x)         x = self.bn1(x)         x = self.relu(x)          x = self.layer1(x)  # 32\u00d732 \u2192 32\u00d732         x = self.layer2(x)  # 32\u00d732 \u2192 16\u00d716         x = self.layer3(x)  # 16\u00d716 \u2192  8\u00d78         x = self.layer4(x)  #  8\u00d78 \u2192  4\u00d74          x = self.avgpool(x)  # 4\u00d74 \u2192 1\u00d71         x = torch.flatten(x, 1)         x = self.fc(x)          return x      def get_features(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Extract features before the classification layer.         Useful for visualization of embeddings and transfer learning.         \"\"\"         x = self.conv1(x)         x = self.bn1(x)         x = self.relu(x)          x = self.layer1(x)         x = self.layer2(x)         x = self.layer3(x)         x = self.layer4(x)          x = self.avgpool(x)         x = torch.flatten(x, 1)          return x <p>Factory functions are defined to instantiate the different ResNet variants with the appropriate block types and layer configurations:</p> In\u00a0[10]: Copied! <pre>def resnet18(num_classes: int = 10) -&gt; ResNet:\n    \"\"\"Construct a ResNet-18 for the given number of classes.\"\"\"\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n\n\ndef resnet34(num_classes: int = 10) -&gt; ResNet:\n    \"\"\"Construct a ResNet-34 for the given number of classes.\"\"\"\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n\n\ndef resnet50(num_classes: int = 10) -&gt; ResNet:\n    \"\"\"Construct a ResNet-50 for the given number of classes.\"\"\"\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n\n\nprint(\"ResNet architecture defined correctly\")\n</pre> def resnet18(num_classes: int = 10) -&gt; ResNet:     \"\"\"Construct a ResNet-18 for the given number of classes.\"\"\"     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)   def resnet34(num_classes: int = 10) -&gt; ResNet:     \"\"\"Construct a ResNet-34 for the given number of classes.\"\"\"     return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)   def resnet50(num_classes: int = 10) -&gt; ResNet:     \"\"\"Construct a ResNet-50 for the given number of classes.\"\"\"     return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)   print(\"ResNet architecture defined correctly\") <pre>ResNet architecture defined correctly\n</pre> <p>This implementation clearly separates the fundamental components: <code>BasicBlock</code> forResNet-18/34, <code>Bottleneck</code> for ResNet-50/101/152, and the <code>ResNet</code> class, which assemblesthe stages, manages downsampling, and applies global average pooling before the finalclassification.</p> <p>A ResNet-18 instance adapted to CIFAR-10 is created and its structure and parameter countare examined using <code>torchinfo.summary</code>.</p> In\u00a0[11]: Copied! <pre># Create ResNet-18\nmodel = resnet18(num_classes=NUM_CLASSES)\n\n# Select device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Device used: {device}\")\nprint(f\"\\n{'='*70}\")\nprint(\"RESNET-18 ARCHITECTURE SUMMARY\")\nprint(f\"{'='*70}\\n\")\n\nsummary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))\n\n\ndef count_parameters(module: nn.Module) -&gt; int:\n    return sum(p.numel() for p in module.parameters())\n\n\nprint(f\"\\n{'='*70}\")\nprint(\"PARAMETER ANALYSIS BY COMPONENT\")\nprint(f\"{'='*70}\")\nprint(f\"  Initial conv:     {count_parameters(model.conv1):&gt;12,} parameters\")\nprint(f\"  Layer 1 (64 ch.): {count_parameters(model.layer1):&gt;12,} parameters\")\nprint(f\"  Layer 2 (128 ch.):{count_parameters(model.layer2):&gt;12,} parameters\")\nprint(f\"  Layer 3 (256 ch.):{count_parameters(model.layer3):&gt;12,} parameters\")\nprint(f\"  Layer 4 (512 ch.):{count_parameters(model.layer4):&gt;12,} parameters\")\nprint(f\"  FC classifier:    {count_parameters(model.fc):&gt;12,} parameters\")\nprint(f\"  {'-'*66}\")\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"  TOTAL:            {total_params:&gt;12,} parameters\")\nprint(f\"  Trainable:        {trainable_params:&gt;12,} parameters\")\nprint(f\"  Memory (float32): {total_params * 4 / (1024**2):&gt;10.2f} MB\")\n</pre> # Create ResNet-18 model = resnet18(num_classes=NUM_CLASSES)  # Select device device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model = model.to(device)  print(f\"Device used: {device}\") print(f\"\\n{'='*70}\") print(\"RESNET-18 ARCHITECTURE SUMMARY\") print(f\"{'='*70}\\n\")  summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))   def count_parameters(module: nn.Module) -&gt; int:     return sum(p.numel() for p in module.parameters())   print(f\"\\n{'='*70}\") print(\"PARAMETER ANALYSIS BY COMPONENT\") print(f\"{'='*70}\") print(f\"  Initial conv:     {count_parameters(model.conv1):&gt;12,} parameters\") print(f\"  Layer 1 (64 ch.): {count_parameters(model.layer1):&gt;12,} parameters\") print(f\"  Layer 2 (128 ch.):{count_parameters(model.layer2):&gt;12,} parameters\") print(f\"  Layer 3 (256 ch.):{count_parameters(model.layer3):&gt;12,} parameters\") print(f\"  Layer 4 (512 ch.):{count_parameters(model.layer4):&gt;12,} parameters\") print(f\"  FC classifier:    {count_parameters(model.fc):&gt;12,} parameters\") print(f\"  {'-'*66}\")  total_params = sum(p.numel() for p in model.parameters()) trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  print(f\"  TOTAL:            {total_params:&gt;12,} parameters\") print(f\"  Trainable:        {trainable_params:&gt;12,} parameters\") print(f\"  Memory (float32): {total_params * 4 / (1024**2):&gt;10.2f} MB\") <pre>Device used: cpu\n\n======================================================================\nRESNET-18 ARCHITECTURE SUMMARY\n======================================================================\n\n</pre> <pre>\n======================================================================\nPARAMETER ANALYSIS BY COMPONENT\n======================================================================\n  Initial conv:            1,728 parameters\n  Layer 1 (64 ch.):      147,968 parameters\n  Layer 2 (128 ch.):     525,568 parameters\n  Layer 3 (256 ch.):   2,099,712 parameters\n  Layer 4 (512 ch.):   8,393,728 parameters\n  FC classifier:           5,130 parameters\n  ------------------------------------------------------------------\n  TOTAL:              11,173,962 parameters\n  Trainable:          11,173,962 parameters\n  Memory (float32):      42.63 MB\n</pre> <p>The optimizer, learning rate scheduler, and loss function are set up. Stochastic Gradient Descent (SGD) with Nesterov momentum is used, along with a MultiStepLR scheduler that decreases the learning rate at predefined epochs.</p> In\u00a0[12]: Copied! <pre>print(\"TRAINING CONFIGURATION\")\nprint(f\"{'='*70}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Initial learning rate: {LEARNING_RATE}\")\nprint(f\"  Momentum: {MOMENTUM}\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"{'='*70}\\n\")\n\n# Optimizer: SGD with Nesterov momentum\noptimizer = torch.optim.SGD(\n    params=model.parameters(),\n    lr=LEARNING_RATE,\n    momentum=MOMENTUM,\n    weight_decay=WEIGHT_DECAY,\n    nesterov=True,\n)\n\n# Scheduler: MultiStepLR\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[60, 80], gamma=0.1\n)\n\n# Loss function\nloss_function = nn.CrossEntropyLoss()\n\nprint(\"Optimizer: SGD with Nesterov momentum\")\nprint(\"  Nesterov momentum adds a 'look-ahead' in the descent direction\")\nprint(\"\\nScheduler: MultiStepLR\")\nprint(\"  Reduces learning rate \u00d70.1 at epochs 60 and 80\")\nprint(\"  Classic strategy for training ResNet on CIFAR-10\")\nprint(\"\\nLoss function: CrossEntropyLoss\")\n</pre> print(\"TRAINING CONFIGURATION\") print(f\"{'='*70}\") print(f\"  Epochs: {NUM_EPOCHS}\") print(f\"  Initial learning rate: {LEARNING_RATE}\") print(f\"  Momentum: {MOMENTUM}\") print(f\"  Weight decay (L2): {WEIGHT_DECAY}\") print(f\"  Batch size: {BATCH_SIZE}\") print(f\"{'='*70}\\n\")  # Optimizer: SGD with Nesterov momentum optimizer = torch.optim.SGD(     params=model.parameters(),     lr=LEARNING_RATE,     momentum=MOMENTUM,     weight_decay=WEIGHT_DECAY,     nesterov=True, )  # Scheduler: MultiStepLR scheduler = torch.optim.lr_scheduler.MultiStepLR(     optimizer, milestones=[60, 80], gamma=0.1 )  # Loss function loss_function = nn.CrossEntropyLoss()  print(\"Optimizer: SGD with Nesterov momentum\") print(\"  Nesterov momentum adds a 'look-ahead' in the descent direction\") print(\"\\nScheduler: MultiStepLR\") print(\"  Reduces learning rate \u00d70.1 at epochs 60 and 80\") print(\"  Classic strategy for training ResNet on CIFAR-10\") print(\"\\nLoss function: CrossEntropyLoss\") <pre>TRAINING CONFIGURATION\n======================================================================\n  Epochs: 1\n  Initial learning rate: 0.1\n  Momentum: 0.9\n  Weight decay (L2): 0.0001\n  Batch size: 128\n======================================================================\n\nOptimizer: SGD with Nesterov momentum\n  Nesterov momentum adds a 'look-ahead' in the descent direction\n\nScheduler: MultiStepLR\n  Reduces learning rate \u00d70.1 at epochs 60 and 80\n  Classic strategy for training ResNet on CIFAR-10\n\nLoss function: CrossEntropyLoss\n</pre> <p>The training loop is implemented with metric tracking and includes saving the model that achieves the highest test accuracy.</p> In\u00a0[13]: Copied! <pre>from tqdm import tqdm\nimport time\n\n# Metric storage\ntrain_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\nlearning_rates = []\n\n# Variables for saving the best model\nbest_test_acc = 0.0\nbest_epoch = 0\n\n\ndef calculate_accuracy(outputs: torch.Tensor, labels: torch.Tensor):\n    _, predicted = torch.max(outputs, 1)\n    correct = (predicted == labels).sum().item()\n    total = labels.size(0)\n    return correct, total\n\n\nprint(\"STARTING TRAINING\\n\")\nprint(f\"{'='*70}\\n\")\n\nstart_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start_time = time.time()\n\n    # Training phase\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    train_loop = tqdm(\n        train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False\n    )\n\n    for batch_image, batch_label in train_loop:\n        batch_image = batch_image.to(device)\n        batch_label = batch_label.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(batch_image)\n        loss = loss_function(outputs, batch_label)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        batch_correct, batch_total = calculate_accuracy(outputs, batch_label)\n        correct += batch_correct\n        total += batch_total\n\n        train_loop.set_postfix(\n            {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n        )\n\n    epoch_train_loss = running_loss / len(train_dataloader)\n    epoch_train_acc = 100 * correct / total\n    train_losses.append(epoch_train_loss)\n    train_accuracies.append(epoch_train_acc)\n\n    # Validation phase\n    model.eval()\n    test_loss, correct_test, total_test = 0.0, 0, 0\n\n    test_loop = tqdm(\n        test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False\n    )\n\n    with torch.no_grad():\n        for images, labels in test_loop:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            batch_correct, batch_total = calculate_accuracy(outputs, labels)\n            correct_test += batch_correct\n            total_test += batch_total\n\n            test_loop.set_postfix(\n                {\n                    \"loss\": f\"{loss.item():.4f}\",\n                    \"acc\": f\"{100 * correct_test / total_test:.2f}%\",\n                }\n            )\n\n    epoch_test_loss = test_loss / len(test_dataloader)\n    epoch_test_acc = 100 * correct_test / total_test\n    test_losses.append(epoch_test_loss)\n    test_accuracies.append(epoch_test_acc)\n\n    # Update scheduler\n    scheduler.step()\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    learning_rates.append(current_lr)\n\n    # Save best model according to test accuracy\n    if epoch_test_acc &gt; best_test_acc:\n        best_test_acc = epoch_test_acc\n        best_epoch = epoch + 1\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"test_acc\": best_test_acc,\n            },\n            \"resnet18_cifar10_best.pth\",\n        )\n\n    epoch_time = time.time() - epoch_start_time\n    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")\n    print(f\"  Train \u2192 Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")\n    print(f\"  Test  \u2192 Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")\n    print(\n        f\"  LR: {current_lr:.6f} | Best test acc: {best_test_acc:.2f}% (epoch {best_epoch})\"\n    )\n    print(f\"  {'\u2500'*66}\\n\")\n\ntotal_time = time.time() - start_time\n\nprint(f\"\\n{'='*70}\")\nprint(\"TRAINING COMPLETED\")\nprint(f\"{'='*70}\")\nprint(f\"  Total time: {total_time / 60:.2f} minutes\")\nprint(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\")\nprint(f\"  Final test accuracy: {test_accuracies[-1]:.2f}%\")\nprint(f\"  Best test accuracy: {best_test_acc:.2f}% at epoch {best_epoch}\")\n\n# Save final model and metrics\ntorch.save(\n    {\n        \"epoch\": NUM_EPOCHS,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"train_losses\": train_losses,\n        \"train_accuracies\": train_accuracies,\n        \"test_losses\": test_losses,\n        \"test_accuracies\": test_accuracies,\n        \"best_test_acc\": best_test_acc,\n        \"best_epoch\": best_epoch,\n    },\n    \"resnet18_cifar10_final.pth\",\n)\n\nprint(\"\\nSaved models:\")\nprint(\"  - resnet18_cifar10_best.pth (best model)\")\nprint(\"  - resnet18_cifar10_final.pth (final model + metrics)\")\n</pre> from tqdm import tqdm import time  # Metric storage train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], [] learning_rates = []  # Variables for saving the best model best_test_acc = 0.0 best_epoch = 0   def calculate_accuracy(outputs: torch.Tensor, labels: torch.Tensor):     _, predicted = torch.max(outputs, 1)     correct = (predicted == labels).sum().item()     total = labels.size(0)     return correct, total   print(\"STARTING TRAINING\\n\") print(f\"{'='*70}\\n\")  start_time = time.time()  for epoch in range(NUM_EPOCHS):     epoch_start_time = time.time()      # Training phase     model.train()     running_loss, correct, total = 0.0, 0, 0      train_loop = tqdm(         train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False     )      for batch_image, batch_label in train_loop:         batch_image = batch_image.to(device)         batch_label = batch_label.to(device)          optimizer.zero_grad()         outputs = model(batch_image)         loss = loss_function(outputs, batch_label)         loss.backward()         optimizer.step()          running_loss += loss.item()         batch_correct, batch_total = calculate_accuracy(outputs, batch_label)         correct += batch_correct         total += batch_total          train_loop.set_postfix(             {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}         )      epoch_train_loss = running_loss / len(train_dataloader)     epoch_train_acc = 100 * correct / total     train_losses.append(epoch_train_loss)     train_accuracies.append(epoch_train_acc)      # Validation phase     model.eval()     test_loss, correct_test, total_test = 0.0, 0, 0      test_loop = tqdm(         test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False     )      with torch.no_grad():         for images, labels in test_loop:             images = images.to(device)             labels = labels.to(device)              outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             batch_correct, batch_total = calculate_accuracy(outputs, labels)             correct_test += batch_correct             total_test += batch_total              test_loop.set_postfix(                 {                     \"loss\": f\"{loss.item():.4f}\",                     \"acc\": f\"{100 * correct_test / total_test:.2f}%\",                 }             )      epoch_test_loss = test_loss / len(test_dataloader)     epoch_test_acc = 100 * correct_test / total_test     test_losses.append(epoch_test_loss)     test_accuracies.append(epoch_test_acc)      # Update scheduler     scheduler.step()     current_lr = optimizer.param_groups[0][\"lr\"]     learning_rates.append(current_lr)      # Save best model according to test accuracy     if epoch_test_acc &gt; best_test_acc:         best_test_acc = epoch_test_acc         best_epoch = epoch + 1         torch.save(             {                 \"epoch\": epoch,                 \"model_state_dict\": model.state_dict(),                 \"optimizer_state_dict\": optimizer.state_dict(),                 \"test_acc\": best_test_acc,             },             \"resnet18_cifar10_best.pth\",         )      epoch_time = time.time() - epoch_start_time     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")     print(f\"  Train \u2192 Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")     print(f\"  Test  \u2192 Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")     print(         f\"  LR: {current_lr:.6f} | Best test acc: {best_test_acc:.2f}% (epoch {best_epoch})\"     )     print(f\"  {'\u2500'*66}\\n\")  total_time = time.time() - start_time  print(f\"\\n{'='*70}\") print(\"TRAINING COMPLETED\") print(f\"{'='*70}\") print(f\"  Total time: {total_time / 60:.2f} minutes\") print(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\") print(f\"  Final test accuracy: {test_accuracies[-1]:.2f}%\") print(f\"  Best test accuracy: {best_test_acc:.2f}% at epoch {best_epoch}\")  # Save final model and metrics torch.save(     {         \"epoch\": NUM_EPOCHS,         \"model_state_dict\": model.state_dict(),         \"optimizer_state_dict\": optimizer.state_dict(),         \"train_losses\": train_losses,         \"train_accuracies\": train_accuracies,         \"test_losses\": test_losses,         \"test_accuracies\": test_accuracies,         \"best_test_acc\": best_test_acc,         \"best_epoch\": best_epoch,     },     \"resnet18_cifar10_final.pth\", )  print(\"\\nSaved models:\") print(\"  - resnet18_cifar10_best.pth (best model)\") print(\"  - resnet18_cifar10_final.pth (final model + metrics)\") <pre>STARTING TRAINING\n\n======================================================================\n\n</pre> <pre>\rEpoch 1/1 [TRAIN]:   0%|          | 0/40 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/1 [TRAIN]:   0%|          | 0/40 [00:03&lt;?, ?it/s, loss=2.2889, acc=10.16%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   2%|\u258e         | 1/40 [00:03&lt;02:07,  3.26s/it, loss=2.2889, acc=10.16%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   2%|\u258e         | 1/40 [00:06&lt;02:07,  3.26s/it, loss=3.2596, acc=8.98%] </pre> <pre>\rEpoch 1/1 [TRAIN]:   5%|\u258c         | 2/40 [00:06&lt;01:57,  3.09s/it, loss=3.2596, acc=8.98%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   5%|\u258c         | 2/40 [00:09&lt;01:57,  3.09s/it, loss=5.0238, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   8%|\u258a         | 3/40 [00:09&lt;01:53,  3.07s/it, loss=5.0238, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:   8%|\u258a         | 3/40 [00:12&lt;01:53,  3.07s/it, loss=6.8855, acc=11.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  10%|\u2588         | 4/40 [00:12&lt;01:49,  3.04s/it, loss=6.8855, acc=11.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  10%|\u2588         | 4/40 [00:15&lt;01:49,  3.04s/it, loss=5.0635, acc=11.09%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  12%|\u2588\u258e        | 5/40 [00:15&lt;01:46,  3.05s/it, loss=5.0635, acc=11.09%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  12%|\u2588\u258e        | 5/40 [00:18&lt;01:46,  3.05s/it, loss=15.9338, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  15%|\u2588\u258c        | 6/40 [00:18&lt;01:43,  3.04s/it, loss=15.9338, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  15%|\u2588\u258c        | 6/40 [00:21&lt;01:43,  3.04s/it, loss=14.0421, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  18%|\u2588\u258a        | 7/40 [00:21&lt;01:39,  3.03s/it, loss=14.0421, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  18%|\u2588\u258a        | 7/40 [00:24&lt;01:39,  3.03s/it, loss=8.2190, acc=11.13%] </pre> <pre>\rEpoch 1/1 [TRAIN]:  20%|\u2588\u2588        | 8/40 [00:24&lt;01:36,  3.01s/it, loss=8.2190, acc=11.13%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  20%|\u2588\u2588        | 8/40 [00:27&lt;01:36,  3.01s/it, loss=5.9687, acc=10.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  22%|\u2588\u2588\u258e       | 9/40 [00:27&lt;01:33,  3.02s/it, loss=5.9687, acc=10.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  22%|\u2588\u2588\u258e       | 9/40 [00:30&lt;01:33,  3.02s/it, loss=4.3718, acc=10.31%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  25%|\u2588\u2588\u258c       | 10/40 [00:30&lt;01:30,  3.02s/it, loss=4.3718, acc=10.31%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  25%|\u2588\u2588\u258c       | 10/40 [00:33&lt;01:30,  3.02s/it, loss=3.4020, acc=10.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  28%|\u2588\u2588\u258a       | 11/40 [00:33&lt;01:27,  3.01s/it, loss=3.4020, acc=10.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  28%|\u2588\u2588\u258a       | 11/40 [00:36&lt;01:27,  3.01s/it, loss=2.8951, acc=10.61%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  30%|\u2588\u2588\u2588       | 12/40 [00:36&lt;01:24,  3.03s/it, loss=2.8951, acc=10.61%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  30%|\u2588\u2588\u2588       | 12/40 [00:39&lt;01:24,  3.03s/it, loss=3.5613, acc=11.00%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  32%|\u2588\u2588\u2588\u258e      | 13/40 [00:39&lt;01:21,  3.02s/it, loss=3.5613, acc=11.00%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  32%|\u2588\u2588\u2588\u258e      | 13/40 [00:42&lt;01:21,  3.02s/it, loss=3.2099, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  35%|\u2588\u2588\u2588\u258c      | 14/40 [00:42&lt;01:18,  3.01s/it, loss=3.2099, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  35%|\u2588\u2588\u2588\u258c      | 14/40 [00:45&lt;01:18,  3.01s/it, loss=3.1809, acc=10.68%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 15/40 [00:45&lt;01:15,  3.01s/it, loss=3.1809, acc=10.68%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  38%|\u2588\u2588\u2588\u258a      | 15/40 [00:48&lt;01:15,  3.01s/it, loss=2.3707, acc=10.55%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  40%|\u2588\u2588\u2588\u2588      | 16/40 [00:48&lt;01:12,  3.01s/it, loss=2.3707, acc=10.55%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  40%|\u2588\u2588\u2588\u2588      | 16/40 [00:51&lt;01:12,  3.01s/it, loss=2.6897, acc=10.71%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:51&lt;01:09,  3.02s/it, loss=2.6897, acc=10.71%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:54&lt;01:09,  3.02s/it, loss=2.6305, acc=10.42%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:54&lt;01:06,  3.01s/it, loss=2.6305, acc=10.42%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:57&lt;01:06,  3.01s/it, loss=2.5261, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:57&lt;01:03,  3.02s/it, loss=2.5261, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [01:00&lt;01:03,  3.02s/it, loss=2.3636, acc=10.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [01:00&lt;01:00,  3.02s/it, loss=2.3636, acc=10.59%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [01:03&lt;01:00,  3.02s/it, loss=2.3625, acc=10.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [01:03&lt;00:57,  3.02s/it, loss=2.3625, acc=10.45%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [01:06&lt;00:57,  3.02s/it, loss=2.4476, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [01:06&lt;00:54,  3.01s/it, loss=2.4476, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [01:09&lt;00:54,  3.01s/it, loss=2.4235, acc=10.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [01:09&lt;00:51,  3.04s/it, loss=2.4235, acc=10.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [01:12&lt;00:51,  3.04s/it, loss=2.4111, acc=10.16%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [01:12&lt;00:48,  3.03s/it, loss=2.4111, acc=10.16%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [01:15&lt;00:48,  3.03s/it, loss=2.3300, acc=10.19%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [01:15&lt;00:45,  3.01s/it, loss=2.3300, acc=10.19%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [01:18&lt;00:45,  3.01s/it, loss=2.3396, acc=10.13%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [01:18&lt;00:41,  2.99s/it, loss=2.3396, acc=10.13%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [01:21&lt;00:41,  2.99s/it, loss=2.3108, acc=10.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [01:21&lt;00:38,  2.99s/it, loss=2.3108, acc=10.33%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [01:24&lt;00:38,  2.99s/it, loss=2.3084, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [01:24&lt;00:35,  2.99s/it, loss=2.3084, acc=10.44%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [01:27&lt;00:35,  2.99s/it, loss=2.2925, acc=10.64%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [01:27&lt;00:32,  3.00s/it, loss=2.2925, acc=10.64%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [01:30&lt;00:32,  3.00s/it, loss=2.3094, acc=10.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [01:30&lt;00:30,  3.01s/it, loss=2.3094, acc=10.86%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [01:33&lt;00:30,  3.01s/it, loss=2.3651, acc=10.91%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [01:33&lt;00:26,  3.00s/it, loss=2.3651, acc=10.91%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [01:36&lt;00:26,  3.00s/it, loss=2.3263, acc=10.89%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [01:36&lt;00:24,  3.00s/it, loss=2.3263, acc=10.89%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [01:39&lt;00:24,  3.00s/it, loss=2.3260, acc=10.84%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [01:39&lt;00:20,  2.98s/it, loss=2.3260, acc=10.84%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [01:42&lt;00:20,  2.98s/it, loss=2.3401, acc=10.80%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [01:42&lt;00:17,  2.97s/it, loss=2.3401, acc=10.80%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [01:45&lt;00:17,  2.97s/it, loss=2.3705, acc=10.74%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [01:45&lt;00:14,  2.98s/it, loss=2.3705, acc=10.74%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [01:48&lt;00:14,  2.98s/it, loss=2.2996, acc=10.66%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [01:48&lt;00:11,  2.98s/it, loss=2.2996, acc=10.66%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [01:51&lt;00:11,  2.98s/it, loss=2.3508, acc=10.54%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [01:51&lt;00:08,  2.99s/it, loss=2.3508, acc=10.54%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [01:54&lt;00:08,  2.99s/it, loss=2.3210, acc=10.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [01:54&lt;00:06,  3.00s/it, loss=2.3210, acc=10.51%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [01:57&lt;00:06,  3.00s/it, loss=2.2718, acc=10.52%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [01:57&lt;00:02,  2.99s/it, loss=2.2718, acc=10.52%]</pre> <pre>\rEpoch 1/1 [TRAIN]:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [01:57&lt;00:02,  2.99s/it, loss=2.5193, acc=10.52%]</pre> <pre>\rEpoch 1/1 [TRAIN]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [01:57&lt;00:00,  2.16s/it, loss=2.5193, acc=10.52%]</pre> <pre>\r                                                                                           </pre> <pre>\r</pre> <pre>\rEpoch 1/1 [TEST]:   0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/1 [TEST]:   0%|          | 0/8 [00:01&lt;?, ?it/s, loss=2.3901, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TEST]:  12%|\u2588\u258e        | 1/8 [00:01&lt;00:08,  1.22s/it, loss=2.3901, acc=10.94%]</pre> <pre>\rEpoch 1/1 [TEST]:  12%|\u2588\u258e        | 1/8 [00:02&lt;00:08,  1.22s/it, loss=2.3385, acc=8.59%] </pre> <pre>\rEpoch 1/1 [TEST]:  25%|\u2588\u2588\u258c       | 2/8 [00:02&lt;00:06,  1.04s/it, loss=2.3385, acc=8.59%]</pre> <pre>\rEpoch 1/1 [TEST]:  25%|\u2588\u2588\u258c       | 2/8 [00:03&lt;00:06,  1.04s/it, loss=2.3276, acc=8.33%]</pre> <pre>\rEpoch 1/1 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:03&lt;00:04,  1.03it/s, loss=2.3276, acc=8.33%]</pre> <pre>\rEpoch 1/1 [TEST]:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:03&lt;00:04,  1.03it/s, loss=2.3489, acc=9.38%]</pre> <pre>\rEpoch 1/1 [TEST]:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:03&lt;00:03,  1.06it/s, loss=2.3489, acc=9.38%]</pre> <pre>\rEpoch 1/1 [TEST]:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:04&lt;00:03,  1.06it/s, loss=2.3596, acc=9.84%]</pre> <pre>\rEpoch 1/1 [TEST]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:04&lt;00:02,  1.07it/s, loss=2.3596, acc=9.84%]</pre> <pre>\rEpoch 1/1 [TEST]:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:05&lt;00:02,  1.07it/s, loss=2.3584, acc=9.90%]</pre> <pre>\rEpoch 1/1 [TEST]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:05&lt;00:01,  1.08it/s, loss=2.3584, acc=9.90%]</pre> <pre>\rEpoch 1/1 [TEST]:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:06&lt;00:01,  1.08it/s, loss=2.3287, acc=10.27%]</pre> <pre>\rEpoch 1/1 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:06&lt;00:00,  1.10it/s, loss=2.3287, acc=10.27%]</pre> <pre>\rEpoch 1/1 [TEST]:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:07&lt;00:00,  1.10it/s, loss=2.4369, acc=10.00%]</pre> <pre>\rEpoch 1/1 [TEST]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:07&lt;00:00,  1.17it/s, loss=2.4369, acc=10.00%]</pre> <pre>\r                                                                                        </pre> <pre>Epoch [1/1] - Time: 125.15s\n  Train \u2192 Loss: 3.6728 | Acc: 10.52%\n  Test  \u2192 Loss: 2.3611 | Acc: 10.00%\n  LR: 0.100000 | Best test acc: 10.00% (epoch 1)\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n======================================================================\nTRAINING COMPLETED\n======================================================================\n  Total time: 2.09 minutes\n  Average time per epoch: 125.15 seconds\n  Final test accuracy: 10.00%\n  Best test accuracy: 10.00% at epoch 1\n\nSaved models:\n  - resnet18_cifar10_best.pth (best model)\n  - resnet18_cifar10_final.pth (final model + metrics)\n</pre> <pre>\r</pre> <p>Compared to architectures like VGG, ResNet training on CIFAR-10 is generally more stable and efficient at similar depths, thanks to residual connections and a more moderate number of parameters.</p> <p>Training metrics are visualized using four plots\u2014loss progression, accuracy progression, learning rate schedule, and the train-test accuracy gap\u2014to analyze model performance and identify potential overfitting.</p> In\u00a0[14]: Copied! <pre>import os\n\nos.makedirs(\"results\", exist_ok=True)\n\nepochs_range = range(1, NUM_EPOCHS + 1)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Loss\nax1.plot(\n    epochs_range,\n    train_losses,\n    \"o-\",\n    label=\"Train Loss\",\n    linewidth=2,\n    markersize=3,\n    alpha=0.7,\n)\nax1.plot(\n    epochs_range,\n    test_losses,\n    \"s-\",\n    label=\"Test Loss\",\n    linewidth=2,\n    markersize=3,\n    alpha=0.7,\n)\nax1.axvline(\n    x=best_epoch,\n    color=\"green\",\n    linestyle=\"--\",\n    alpha=0.5,\n    label=f\"Best epoch ({best_epoch})\",\n)\nax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\")\nax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\")\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\n# Accuracy\nax2.plot(\n    epochs_range,\n    train_accuracies,\n    \"o-\",\n    label=\"Train Accuracy\",\n    linewidth=2,\n    markersize=3,\n    alpha=0.7,\n)\nax2.plot(\n    epochs_range,\n    test_accuracies,\n    \"s-\",\n    label=\"Test Accuracy\",\n    linewidth=2,\n    markersize=3,\n    alpha=0.7,\n)\nax2.axvline(\n    x=best_epoch,\n    color=\"green\",\n    linestyle=\"--\",\n    alpha=0.5,\n    label=f\"Best epoch ({best_epoch})\",\n)\nax2.axhline(\n    y=best_test_acc,\n    color=\"red\",\n    linestyle=\"--\",\n    alpha=0.5,\n    label=f\"Best acc: {best_test_acc:.2f}%\",\n)\nax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\")\nax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\")\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# Learning rate\nax3.plot(\n    epochs_range,\n    learning_rates,\n    \"o-\",\n    color=\"red\",\n    linewidth=2,\n    markersize=3,\n    alpha=0.7,\n)\nax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\")\nax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\")\nax3.set_yscale(\"log\")\nax3.grid(True, alpha=0.3)\nax3.axvline(x=60, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"LR decay\")\nax3.axvline(x=80, color=\"orange\", linestyle=\"--\", alpha=0.5)\nax3.legend(fontsize=10)\n\n# Train\u2013test gap\ngap = np.array(train_accuracies) - np.array(test_accuracies)\nax4.plot(epochs_range, gap, \"o-\", color=\"purple\", linewidth=2, markersize=3, alpha=0.7)\nax4.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\nax4.axhline(\n    y=5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Overfitting threshold (5%)\"\n)\nax4.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\nax4.set_ylabel(\"Train\u2013Test Gap (%)\", fontsize=12, fontweight=\"bold\")\nax4.set_title(\"Train\u2013Test Accuracy Difference\", fontsize=14, fontweight=\"bold\")\nax4.legend(fontsize=10)\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"results/resnet18_training_history.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"\\nResult analysis:\")\nfinal_gap = train_accuracies[-1] - test_accuracies[-1]\nprint(f\"  Overfitting detected: {'YES' if final_gap &gt; 10 else 'NO'}\")\nprint(f\"  Final train\u2013test gap: {final_gap:.2f}%\")\nprint(f\"  Best epoch: {best_epoch}\")\nprint(f\"  Improvement from epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\")\n</pre> import os  os.makedirs(\"results\", exist_ok=True)  epochs_range = range(1, NUM_EPOCHS + 1)  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))  # Loss ax1.plot(     epochs_range,     train_losses,     \"o-\",     label=\"Train Loss\",     linewidth=2,     markersize=3,     alpha=0.7, ) ax1.plot(     epochs_range,     test_losses,     \"s-\",     label=\"Test Loss\",     linewidth=2,     markersize=3,     alpha=0.7, ) ax1.axvline(     x=best_epoch,     color=\"green\",     linestyle=\"--\",     alpha=0.5,     label=f\"Best epoch ({best_epoch})\", ) ax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\") ax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\") ax1.legend(fontsize=10) ax1.grid(True, alpha=0.3)  # Accuracy ax2.plot(     epochs_range,     train_accuracies,     \"o-\",     label=\"Train Accuracy\",     linewidth=2,     markersize=3,     alpha=0.7, ) ax2.plot(     epochs_range,     test_accuracies,     \"s-\",     label=\"Test Accuracy\",     linewidth=2,     markersize=3,     alpha=0.7, ) ax2.axvline(     x=best_epoch,     color=\"green\",     linestyle=\"--\",     alpha=0.5,     label=f\"Best epoch ({best_epoch})\", ) ax2.axhline(     y=best_test_acc,     color=\"red\",     linestyle=\"--\",     alpha=0.5,     label=f\"Best acc: {best_test_acc:.2f}%\", ) ax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\") ax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\") ax2.legend(fontsize=10) ax2.grid(True, alpha=0.3)  # Learning rate ax3.plot(     epochs_range,     learning_rates,     \"o-\",     color=\"red\",     linewidth=2,     markersize=3,     alpha=0.7, ) ax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\") ax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\") ax3.set_yscale(\"log\") ax3.grid(True, alpha=0.3) ax3.axvline(x=60, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"LR decay\") ax3.axvline(x=80, color=\"orange\", linestyle=\"--\", alpha=0.5) ax3.legend(fontsize=10)  # Train\u2013test gap gap = np.array(train_accuracies) - np.array(test_accuracies) ax4.plot(epochs_range, gap, \"o-\", color=\"purple\", linewidth=2, markersize=3, alpha=0.7) ax4.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5) ax4.axhline(     y=5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Overfitting threshold (5%)\" ) ax4.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\") ax4.set_ylabel(\"Train\u2013Test Gap (%)\", fontsize=12, fontweight=\"bold\") ax4.set_title(\"Train\u2013Test Accuracy Difference\", fontsize=14, fontweight=\"bold\") ax4.legend(fontsize=10) ax4.grid(True, alpha=0.3)  plt.tight_layout() plt.savefig(\"results/resnet18_training_history.png\", dpi=300, bbox_inches=\"tight\") plt.show()  print(\"\\nResult analysis:\") final_gap = train_accuracies[-1] - test_accuracies[-1] print(f\"  Overfitting detected: {'YES' if final_gap &gt; 10 else 'NO'}\") print(f\"  Final train\u2013test gap: {final_gap:.2f}%\") print(f\"  Best epoch: {best_epoch}\") print(f\"  Improvement from epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\") <pre>\nResult analysis:\n  Overfitting detected: NO\n  Final train\u2013test gap: 0.52%\n  Best epoch: 1\n  Improvement from epoch 1: 0.00%\n</pre> <p>Moderate train\u2013test accuracy gaps indicate a healthy balance between fitting the data and generalizing to new samples. Learning rate reductions at epochs 60 and 80 often coincide with shifts in network behavior and corresponding improvements in test accuracy.</p> <p>Finally, model predictions on the test set are visualized, including both correctlyclassified examples and some errors, allowing qualitative inspection of model behavior.</p> In\u00a0[15]: Copied! <pre>print(\"\\nVisualizing predictions of the best model...\")\n\n# Get one test batch\ndata_iter = iter(test_dataloader)\ntest_images, test_labels = next(data_iter)\n\nmodel.eval()\nwith torch.no_grad():\n    test_images_device = test_images.to(device)\n    outputs = model(test_images_device)\n    _, predictions = torch.max(outputs, 1)\n    predictions = predictions.cpu()\n\nprint(\"\\nFirst 8 predictions:\")\nshow_images(test_images[:8], test_labels[:8], predictions[:8])\n\n# Examples of misclassifications\nincorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]\n\nif len(incorrect_indices) &gt;= 8:\n    print(\"\\nExamples of incorrect predictions:\")\n    error_indices = incorrect_indices[:8]\n    show_images(\n        test_images[error_indices],\n        test_labels[error_indices],\n        predictions[error_indices],\n    )\nelse:\n    print(f\"\\nOnly {len(incorrect_indices)} misclassifications in this batch\")\n</pre> print(\"\\nVisualizing predictions of the best model...\")  # Get one test batch data_iter = iter(test_dataloader) test_images, test_labels = next(data_iter)  model.eval() with torch.no_grad():     test_images_device = test_images.to(device)     outputs = model(test_images_device)     _, predictions = torch.max(outputs, 1)     predictions = predictions.cpu()  print(\"\\nFirst 8 predictions:\") show_images(test_images[:8], test_labels[:8], predictions[:8])  # Examples of misclassifications incorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]  if len(incorrect_indices) &gt;= 8:     print(\"\\nExamples of incorrect predictions:\")     error_indices = incorrect_indices[:8]     show_images(         test_images[error_indices],         test_labels[error_indices],         predictions[error_indices],     ) else:     print(f\"\\nOnly {len(incorrect_indices)} misclassifications in this batch\") <pre>\nVisualizing predictions of the best model...\n</pre> <pre>\nFirst 8 predictions:\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.39153773..1.5471393].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.4979501].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4629833..1.5354269].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.22441113..1.4005105].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4391681..1.2814069].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.45793372..1.1802652].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.48679847..1.557913].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.32803053..1.555192].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.39153773..1.5471393].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4629833..1.5354269].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.22441113..1.4005105].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4391681..1.2814069].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.48679847..1.557913].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.32803053..1.555192].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.36130375..1.5310344].\n</pre> <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.3290937..1.5279315].\n</pre> <pre>\nExamples of incorrect predictions:\n</pre>"},{"location":"course/topic_04_computer_vision/section_06_resnet.html#resnet","title":"ResNet\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#theoretical-introduction","title":"Theoretical Introduction\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#the-degradation-problem-in-deep-networks","title":"The Degradation Problem in Deep Networks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#residual-connections-and-shortcut-blocks","title":"Residual Connections and Shortcut Blocks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#advantages-of-residual-learning","title":"Advantages of Residual Learning\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#architectural-variants-of-resnet","title":"Architectural Variants of ResNet\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#basic-block-versus-bottleneck-block","title":"Basic Block versus Bottleneck Block\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#current-impact-and-importance-of-resnet","title":"Current Impact and Importance of ResNet\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#practical-implementation-of-resnet-for-cifar-10","title":"Practical Implementation of ResNet for CIFAR-10\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#importing-libraries","title":"Importing Libraries\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#global-hyperparameter-configuration","title":"Global Hyperparameter Configuration\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#visualization-helper-function","title":"Visualization Helper Function\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#preparing-the-cifar-10-dataset","title":"Preparing the CIFAR-10 Dataset\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#creating-dataloaders","title":"Creating DataLoaders\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#visual-exploration-of-the-dataset","title":"Visual Exploration of the Dataset\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#resnet-implementation","title":"ResNet Implementation\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#model-instantiation-and-analysis","title":"Model Instantiation and Analysis\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#training-configuration","title":"Training Configuration\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#training-and-validation-loop","title":"Training and Validation Loop\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#visualization-of-training-metrics","title":"Visualization of Training Metrics\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_06_resnet.html#visualization-of-model-predictions","title":"Visualization of Model Predictions\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_07_transfer_learning.html","title":"Transfer Learning","text":"<p>Transfer learning constitutes a fundamental paradigm in modern deep learning that enables the reuse of knowledge acquired by a model during training on a large-scale dataset to solve a different but related task. Rather than initializing a neural network with random weights and training it from scratch, transfer learning leverages the internal representations previously learned by a model trained on extensive and diverse datasets such as ImageNet, which contains millions of labeled images spanning thousands of categories. These learned representations encode hierarchical visual patterns ranging from low-level features such as edges, corners, and textures in the initial layers, to increasingly abstract and complex compositions such as object parts and semantic structures in the deeper layers. By transferring this knowledge to a new task, the approach significantly reduces the amount of labeled data required, accelerates convergence during training, and typically achieves superior performance compared to training from scratch, particularly when the target dataset is limited in size. The underlying assumption is that the features learned from a large and generic dataset capture general visual properties that remain useful across different but related domains, requiring only specialization of the final layers to adapt to the specific characteristics of the new task.</p> <p>The practical application of transfer learning is organized around three principal strategies that differ in the extent to which the pretrained model parameters are updated during training on the target task. The selection of an appropriate strategy depends primarily on two factors: The size of the available target dataset and the degree of similarity between the source domain (the dataset on which the model was originally trained) and the target domain (the new task to be solved). These strategies are feature extraction, partial fine-tuning, and full fine-tuning, each offering distinct trade-offs between computational efficiency, risk of overfitting, and capacity for domain adaptation.</p> <p>In the feature extraction strategy, all parameters of the pretrained convolutional base are frozen, meaning their values remain fixed and are not updated during training. Only the final classification layer, which is replaced to match the number of classes in the target task, is trained from scratch. In this configuration, the pretrained network functions as a fixed feature extractor that transforms input images into high-dimensional feature vectors, and a lightweight classifier is trained on top of these representations to perform the specific classification task. This approach is particularly suitable when the target dataset is very small, typically containing fewer than approximately one thousand images, and when the visual domain of the target task is relatively similar to that of the pretraining dataset, such as natural scenes, everyday objects, or common textures. The primary advantage of feature extraction is computational efficiency and reduced risk of overfitting, since the vast majority of parameters remain fixed and only a small number of weights in the final layer are optimized. However, the limitation of this strategy is its reduced capacity to adapt to domains that differ substantially from the pretraining data, because the internal feature representations cannot be modified to capture domain-specific patterns.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nmodel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nnum_classes = 2\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n</pre> import torch import torch.nn as nn from torchvision import models  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  for param in model.parameters():     param.requires_grad = False  num_classes = 2 model.fc = nn.Linear(model.fc.in_features, num_classes) <pre>Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/runner/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n</pre> <pre>\r  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]</pre> <pre>\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 41.4M/44.7M [00:00&lt;00:00, 433MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44.7M/44.7M [00:00&lt;00:00, 433MB/s]</pre> <pre>\n</pre> <p>In the partial fine-tuning strategy, the earliest layers of the network, which are closest to the input and capture low-level visual features, are kept frozen, while several of the deeper convolutional layers, together with the final classification layer, are unfrozen and updated during training. The rationale behind this approach is that the initial layers of a convolutional network learn generic features such as edges, gradients, and simple textures that are broadly applicable across different visual domains, whereas the deeper layers encode more task-specific and abstract representations. By freezing the early layers and fine-tuning only the later ones, the model preserves the general-purpose features while adapting the high-level representations to the characteristics of the new task. This strategy is appropriate for medium-sized datasets, typically ranging from one thousand to ten thousand images, and when the target domain is moderately different from the source domain. An illustrative example is medical image analysis, where the images may exhibit different visual characteristics compared to natural images, such as specific anatomical structures or imaging modalities, but still share certain fundamental visual patterns such as textures, shapes, and spatial relationships. Partial fine-tuning offers a balanced compromise between flexibility and protection against overfitting, as only a subset of the network parameters is modified. Additionally, it is common practice to employ differentiated learning rates, assigning a lower learning rate to the pretrained layers being fine-tuned and a higher learning rate to the newly initialized classification layer, thereby preventing abrupt changes to the pretrained weights while allowing the new layer to adapt more rapidly.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nmodel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\nfor name, param in model.named_parameters():\n    if \"layer4\" not in name and \"fc\" not in name:\n        param.requires_grad = False\n\nnum_classes = 2\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\noptimizer = torch.optim.Adam([\n    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},\n    {\"params\": model.fc.parameters(), \"lr\": 1e-3}\n])\n</pre> import torch import torch.nn as nn from torchvision import models  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  for name, param in model.named_parameters():     if \"layer4\" not in name and \"fc\" not in name:         param.requires_grad = False  num_classes = 2 model.fc = nn.Linear(model.fc.in_features, num_classes)  optimizer = torch.optim.Adam([     {\"params\": model.layer4.parameters(), \"lr\": 1e-4},     {\"params\": model.fc.parameters(), \"lr\": 1e-3} ]) <p>In the full fine-tuning strategy, all parameters of the pretrained model, including both the convolutional base and the classification layer, are updated during training on the target task. This approach aims to adapt the entire network architecture to the new domain while preserving the beneficial initialization provided by the pretrained weights, thereby avoiding the phenomenon known as catastrophic forgetting, in which the knowledge acquired during pretraining is abruptly destroyed. To mitigate this risk, full fine-tuning is typically performed using a relatively low learning rate, which allows gradual adaptation of the weights without causing drastic changes that would eliminate the useful representations learned during pretraining. This strategy is recommended when a large target dataset is available, typically containing more than ten thousand images, or when the target domain differs significantly from the source domain, such as highly specialized medical imaging modalities, satellite or aerial imagery, industrial inspection images, or any other domain with distinctive visual characteristics not well represented in standard pretraining datasets. Although full fine-tuning entails higher computational cost and increased risk of overfitting if the dataset is insufficient, it generally provides the highest achievable performance, as the entire network architecture is optimized to capture the specific patterns and structures present in the target task.</p> In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nmodel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\nnum_classes = 2\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</pre> import torch import torch.nn as nn from torchvision import models  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  num_classes = 2 model.fc = nn.Linear(model.fc.in_features, num_classes)  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) <p>The following section presents a complete and executable workflow that demonstrates the application of transfer learning using the feature extraction strategy on the CIFAR-10 dataset. CIFAR-10 is a widely used benchmark dataset consisting of 60,000 color images of size $32 \\times 32$ pixels, divided into 10 classes representing common objects such as airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. For the purpose of this example, the task is simplified to binary classification by selecting only two classes from the dataset. The workflow encompasses all essential steps, including data preparation with appropriate transformations, model configuration using a pretrained ResNet-18 architecture, training loop implementation, and evaluation of the trained model. The code is designed to be fully functional and executable, providing a practical template that can be adapted to other classification tasks.</p> In\u00a0[4]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ntest_dataset_full = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\nselected_classes = [3, 5]\ntrain_indices = [i for i, (_, label) in enumerate(train_dataset_full) if label in selected_classes]\ntest_indices = [i for i, (_, label) in enumerate(test_dataset_full) if label in selected_classes]\n\ntrain_indices = train_indices[:1000]\ntest_indices = test_indices[:200]\n\ntrain_dataset = Subset(train_dataset_full, train_indices)\ntest_dataset = Subset(test_dataset_full, test_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n</pre> import torch import torch.nn as nn from torch.utils.data import DataLoader, Subset from torchvision import datasets, transforms, models import numpy as np  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  transform = transforms.Compose([     transforms.Resize(224),     transforms.ToTensor(),     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])  train_dataset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform) test_dataset_full = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)  selected_classes = [3, 5] train_indices = [i for i, (_, label) in enumerate(train_dataset_full) if label in selected_classes] test_indices = [i for i, (_, label) in enumerate(test_dataset_full) if label in selected_classes]  train_indices = train_indices[:1000] test_indices = test_indices[:200]  train_dataset = Subset(train_dataset_full, train_indices) test_dataset = Subset(test_dataset_full, test_indices)  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) <pre>/home/runner/work/unie-deep-learning/unie-deep-learning/.venv/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n  entry = pickle.load(f, encoding=\"latin1\")\n</pre> <p>The data preparation phase involves defining appropriate image transformations that ensure compatibility with the pretrained model. The images are resized to $224 \\times 224$ pixels, which is the standard input size for ResNet architectures, converted to tensors, and normalized using the mean and standard deviation values computed on the ImageNet dataset. This normalization is critical because the pretrained model was trained with these specific statistics, and applying the same normalization to the target data ensures that the input distribution matches the distribution the model expects. The CIFAR-10 dataset is loaded using the torchvision datasets module, and a binary classification task is created by selecting two classes (cats and dogs, corresponding to class indices 3 and 5). The dataset is filtered to retain only samples belonging to these two classes, and separate data loaders are created for training and testing.</p> In\u00a0[5]: Copied! <pre>model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n</pre> model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  for param in model.parameters():     param.requires_grad = False  model.fc = nn.Linear(model.fc.in_features, 2) model = model.to(device)  criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3) <p>The model configuration follows the feature extraction strategy. A ResNet-18 architecture pretrained on ImageNet is loaded, and all parameters in the convolutional base are frozen by setting requires_grad to False. The final fully connected layer is replaced with a new linear layer that outputs two logits corresponding to the two classes in the binary classification task. Only the parameters of this new layer are trainable, and the model is transferred to the appropriate device (GPU if available, otherwise CPU). The loss function is cross-entropy loss, which is standard for multi-class classification tasks, and the optimizer is Adam with a learning rate of $10^{-3}$.</p> In\u00a0[6]: Copied! <pre>from tqdm import tqdm\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Add tqdm progress bar\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for images, labels in pbar:\n        images, labels = images.to(device), labels.to(device)\n        labels = torch.tensor([selected_classes.index(label.item()) for label in labels], device=device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Update progress bar with current metrics\n        pbar.set_postfix({\n            'loss': f'{running_loss/total:.4f}',\n            'acc': f'{100 * correct / total:.2f}%'\n        })\n\n    train_accuracy = 100 * correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n</pre> from tqdm import tqdm  num_epochs = 1  for epoch in range(num_epochs):     model.train()     running_loss = 0.0     correct = 0     total = 0      # Add tqdm progress bar     pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")          for images, labels in pbar:         images, labels = images.to(device), labels.to(device)         labels = torch.tensor([selected_classes.index(label.item()) for label in labels], device=device)          optimizer.zero_grad()         outputs = model(images)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()         _, predicted = torch.max(outputs, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item()                  # Update progress bar with current metrics         pbar.set_postfix({             'loss': f'{running_loss/total:.4f}',             'acc': f'{100 * correct / total:.2f}%'         })      train_accuracy = 100 * correct / total     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\") <pre>\rEpoch 1/1:   0%|          | 0/32 [00:00&lt;?, ?it/s]</pre> <pre>\rEpoch 1/1:   0%|          | 0/32 [00:00&lt;?, ?it/s, loss=0.0253, acc=40.62%]</pre> <pre>\rEpoch 1/1:   3%|\u258e         | 1/32 [00:00&lt;00:28,  1.08it/s, loss=0.0253, acc=40.62%]</pre> <pre>\rEpoch 1/1:   3%|\u258e         | 1/32 [00:01&lt;00:28,  1.08it/s, loss=0.0251, acc=45.31%]</pre> <pre>\rEpoch 1/1:   6%|\u258b         | 2/32 [00:01&lt;00:27,  1.09it/s, loss=0.0251, acc=45.31%]</pre> <pre>\rEpoch 1/1:   6%|\u258b         | 2/32 [00:02&lt;00:27,  1.09it/s, loss=0.0250, acc=45.83%]</pre> <pre>\rEpoch 1/1:   9%|\u2589         | 3/32 [00:02&lt;00:26,  1.09it/s, loss=0.0250, acc=45.83%]</pre> <pre>\rEpoch 1/1:   9%|\u2589         | 3/32 [00:03&lt;00:26,  1.09it/s, loss=0.0255, acc=46.88%]</pre> <pre>\rEpoch 1/1:  12%|\u2588\u258e        | 4/32 [00:03&lt;00:25,  1.09it/s, loss=0.0255, acc=46.88%]</pre> <pre>\rEpoch 1/1:  12%|\u2588\u258e        | 4/32 [00:04&lt;00:25,  1.09it/s, loss=0.0263, acc=45.00%]</pre> <pre>\rEpoch 1/1:  16%|\u2588\u258c        | 5/32 [00:04&lt;00:24,  1.09it/s, loss=0.0263, acc=45.00%]</pre> <pre>\rEpoch 1/1:  16%|\u2588\u258c        | 5/32 [00:05&lt;00:24,  1.09it/s, loss=0.0257, acc=46.35%]</pre> <pre>\rEpoch 1/1:  19%|\u2588\u2589        | 6/32 [00:05&lt;00:23,  1.09it/s, loss=0.0257, acc=46.35%]</pre> <pre>\rEpoch 1/1:  19%|\u2588\u2589        | 6/32 [00:06&lt;00:23,  1.09it/s, loss=0.0247, acc=50.45%]</pre> <pre>\rEpoch 1/1:  22%|\u2588\u2588\u258f       | 7/32 [00:06&lt;00:22,  1.09it/s, loss=0.0247, acc=50.45%]</pre> <pre>\rEpoch 1/1:  22%|\u2588\u2588\u258f       | 7/32 [00:07&lt;00:22,  1.09it/s, loss=0.0244, acc=50.39%]</pre> <pre>\rEpoch 1/1:  25%|\u2588\u2588\u258c       | 8/32 [00:07&lt;00:22,  1.09it/s, loss=0.0244, acc=50.39%]</pre> <pre>\rEpoch 1/1:  25%|\u2588\u2588\u258c       | 8/32 [00:08&lt;00:22,  1.09it/s, loss=0.0238, acc=53.12%]</pre> <pre>\rEpoch 1/1:  28%|\u2588\u2588\u258a       | 9/32 [00:08&lt;00:21,  1.09it/s, loss=0.0238, acc=53.12%]</pre> <pre>\rEpoch 1/1:  28%|\u2588\u2588\u258a       | 9/32 [00:09&lt;00:21,  1.09it/s, loss=0.0237, acc=53.75%]</pre> <pre>\rEpoch 1/1:  31%|\u2588\u2588\u2588\u258f      | 10/32 [00:09&lt;00:20,  1.08it/s, loss=0.0237, acc=53.75%]</pre> <pre>\rEpoch 1/1:  31%|\u2588\u2588\u2588\u258f      | 10/32 [00:10&lt;00:20,  1.08it/s, loss=0.0235, acc=54.26%]</pre> <pre>\rEpoch 1/1:  34%|\u2588\u2588\u2588\u258d      | 11/32 [00:10&lt;00:19,  1.08it/s, loss=0.0235, acc=54.26%]</pre> <pre>\rEpoch 1/1:  34%|\u2588\u2588\u2588\u258d      | 11/32 [00:11&lt;00:19,  1.08it/s, loss=0.0233, acc=54.43%]</pre> <pre>\rEpoch 1/1:  38%|\u2588\u2588\u2588\u258a      | 12/32 [00:11&lt;00:18,  1.08it/s, loss=0.0233, acc=54.43%]</pre> <pre>\rEpoch 1/1:  38%|\u2588\u2588\u2588\u258a      | 12/32 [00:11&lt;00:18,  1.08it/s, loss=0.0232, acc=54.33%]</pre> <pre>\rEpoch 1/1:  41%|\u2588\u2588\u2588\u2588      | 13/32 [00:11&lt;00:17,  1.08it/s, loss=0.0232, acc=54.33%]</pre> <pre>\rEpoch 1/1:  41%|\u2588\u2588\u2588\u2588      | 13/32 [00:12&lt;00:17,  1.08it/s, loss=0.0231, acc=54.69%]</pre> <pre>\rEpoch 1/1:  44%|\u2588\u2588\u2588\u2588\u258d     | 14/32 [00:12&lt;00:16,  1.09it/s, loss=0.0231, acc=54.69%]</pre> <pre>\rEpoch 1/1:  44%|\u2588\u2588\u2588\u2588\u258d     | 14/32 [00:13&lt;00:16,  1.09it/s, loss=0.0229, acc=55.00%]</pre> <pre>\rEpoch 1/1:  47%|\u2588\u2588\u2588\u2588\u258b     | 15/32 [00:13&lt;00:15,  1.09it/s, loss=0.0229, acc=55.00%]</pre> <pre>\rEpoch 1/1:  47%|\u2588\u2588\u2588\u2588\u258b     | 15/32 [00:14&lt;00:15,  1.09it/s, loss=0.0228, acc=54.88%]</pre> <pre>\rEpoch 1/1:  50%|\u2588\u2588\u2588\u2588\u2588     | 16/32 [00:14&lt;00:14,  1.09it/s, loss=0.0228, acc=54.88%]</pre> <pre>\rEpoch 1/1:  50%|\u2588\u2588\u2588\u2588\u2588     | 16/32 [00:15&lt;00:14,  1.09it/s, loss=0.0227, acc=54.78%]</pre> <pre>\rEpoch 1/1:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 17/32 [00:15&lt;00:13,  1.09it/s, loss=0.0227, acc=54.78%]</pre> <pre>\rEpoch 1/1:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 17/32 [00:16&lt;00:13,  1.09it/s, loss=0.0226, acc=55.38%]</pre> <pre>\rEpoch 1/1:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 18/32 [00:16&lt;00:12,  1.09it/s, loss=0.0226, acc=55.38%]</pre> <pre>\rEpoch 1/1:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 18/32 [00:17&lt;00:12,  1.09it/s, loss=0.0225, acc=55.10%]</pre> <pre>\rEpoch 1/1:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 19/32 [00:17&lt;00:11,  1.09it/s, loss=0.0225, acc=55.10%]</pre> <pre>\rEpoch 1/1:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 19/32 [00:18&lt;00:11,  1.09it/s, loss=0.0223, acc=56.25%]</pre> <pre>\rEpoch 1/1:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 20/32 [00:18&lt;00:11,  1.09it/s, loss=0.0223, acc=56.25%]</pre> <pre>\rEpoch 1/1:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 20/32 [00:19&lt;00:11,  1.09it/s, loss=0.0220, acc=56.99%]</pre> <pre>\rEpoch 1/1:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 21/32 [00:19&lt;00:10,  1.09it/s, loss=0.0220, acc=56.99%]</pre> <pre>\rEpoch 1/1:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 21/32 [00:20&lt;00:10,  1.09it/s, loss=0.0219, acc=57.95%]</pre> <pre>\rEpoch 1/1:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 22/32 [00:20&lt;00:09,  1.09it/s, loss=0.0219, acc=57.95%]</pre> <pre>\rEpoch 1/1:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 22/32 [00:21&lt;00:09,  1.09it/s, loss=0.0217, acc=58.29%]</pre> <pre>\rEpoch 1/1:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 23/32 [00:21&lt;00:08,  1.09it/s, loss=0.0217, acc=58.29%]</pre> <pre>\rEpoch 1/1:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 23/32 [00:22&lt;00:08,  1.09it/s, loss=0.0215, acc=58.59%]</pre> <pre>\rEpoch 1/1:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 24/32 [00:22&lt;00:07,  1.09it/s, loss=0.0215, acc=58.59%]</pre> <pre>\rEpoch 1/1:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 24/32 [00:22&lt;00:07,  1.09it/s, loss=0.0214, acc=58.88%]</pre> <pre>\rEpoch 1/1:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 25/32 [00:22&lt;00:06,  1.09it/s, loss=0.0214, acc=58.88%]</pre> <pre>\rEpoch 1/1:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 25/32 [00:23&lt;00:06,  1.09it/s, loss=0.0213, acc=59.13%]</pre> <pre>\rEpoch 1/1:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 26/32 [00:23&lt;00:05,  1.09it/s, loss=0.0213, acc=59.13%]</pre> <pre>\rEpoch 1/1:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 26/32 [00:24&lt;00:05,  1.09it/s, loss=0.0211, acc=59.72%]</pre> <pre>\rEpoch 1/1:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 27/32 [00:24&lt;00:04,  1.09it/s, loss=0.0211, acc=59.72%]</pre> <pre>\rEpoch 1/1:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 27/32 [00:25&lt;00:04,  1.09it/s, loss=0.0210, acc=59.82%]</pre> <pre>\rEpoch 1/1:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 28/32 [00:25&lt;00:03,  1.09it/s, loss=0.0210, acc=59.82%]</pre> <pre>\rEpoch 1/1:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 28/32 [00:26&lt;00:03,  1.09it/s, loss=0.0210, acc=60.13%]</pre> <pre>\rEpoch 1/1:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 29/32 [00:26&lt;00:02,  1.09it/s, loss=0.0210, acc=60.13%]</pre> <pre>\rEpoch 1/1:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 29/32 [00:27&lt;00:02,  1.09it/s, loss=0.0208, acc=60.52%]</pre> <pre>\rEpoch 1/1:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 30/32 [00:27&lt;00:01,  1.09it/s, loss=0.0208, acc=60.52%]</pre> <pre>\rEpoch 1/1:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 30/32 [00:28&lt;00:01,  1.09it/s, loss=0.0207, acc=61.09%]</pre> <pre>\rEpoch 1/1:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 31/32 [00:28&lt;00:00,  1.09it/s, loss=0.0207, acc=61.09%]</pre> <pre>\rEpoch 1/1:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 31/32 [00:28&lt;00:00,  1.09it/s, loss=0.0210, acc=61.20%]</pre> <pre>\rEpoch 1/1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:28&lt;00:00,  1.40it/s, loss=0.0210, acc=61.20%]</pre> <pre>\rEpoch 1/1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:28&lt;00:00,  1.11it/s, loss=0.0210, acc=61.20%]</pre> <pre>Epoch [1/1], Loss: 0.6578, Accuracy: 61.20%\n</pre> <pre>\n</pre> <p>The training loop iterates over the specified number of epochs, and for each epoch, the model processes all batches in the training dataset. The labels are remapped to binary indices (0 and 1) to match the output dimension of the modified classification layer. For each batch, the gradients are zeroed, the forward pass computes the model outputs, the loss is calculated, the backward pass computes the gradients, and the optimizer updates the trainable parameters. Training loss and accuracy are tracked and displayed at the end of each epoch to monitor the learning progress.</p> In\u00a0[7]: Copied! <pre>model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        labels = torch.tensor([selected_classes.index(label.item()) for label in labels], device=device)\n\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * correct / total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n</pre> model.eval() correct = 0 total = 0  with torch.no_grad():     for images, labels in test_loader:         images, labels = images.to(device), labels.to(device)         labels = torch.tensor([selected_classes.index(label.item()) for label in labels], device=device)          outputs = model(images)         _, predicted = torch.max(outputs, 1)         total += labels.size(0)         correct += (predicted == labels).sum().item()  test_accuracy = 100 * correct / total print(f\"Test Accuracy: {test_accuracy:.2f}%\") <pre>Test Accuracy: 76.00%\n</pre> <p>After training, the model is evaluated on the test dataset to assess its generalization performance. The model is set to evaluation mode, which disables dropout and batch normalization updates, and predictions are generated for all test samples without computing gradients. The test accuracy is calculated as the percentage of correctly classified samples and provides an estimate of the model's performance on unseen data. This complete workflow demonstrates how transfer learning enables effective training on a small dataset by leveraging pretrained representations, achieving reasonable performance with minimal computational resources and training time.</p> <p>Embeddings are high-dimensional numerical vectors that provide a compact and semantically meaningful representation of the input data. In the context of computer vision, an embedding captures the essential visual characteristics of an image in a fixed-length vector, typically extracted from one of the intermediate or final layers of a trained convolutional neural network. These representations are particularly valuable because they encode learned features that reflect the semantic content of the image, enabling their use in tasks such as similarity search, clustering, visualization, and retrieval. By removing the final classification layer from a pretrained model and extracting the output of the preceding layer, one obtains a feature vector that summarizes the image content in a form that is both discriminative and generalizable across different tasks.</p> In\u00a0[8]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\nmodel = nn.Sequential(*list(model.children())[:-1])\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef extract_embeddings(dataloader):\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for images, lbls in dataloader:\n            emb = model(images).squeeze()\n            embeddings.append(emb.numpy())\n            labels.append(lbls.numpy())\n    return np.vstack(embeddings), np.concatenate(labels)\n\nembeddings, labels = extract_embeddings(train_loader)\n\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    mask = labels == label\n    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], label=f'Class {label}', alpha=0.6)\nplt.legend()\nplt.title('t-SNE Visualization of Embeddings')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.show()\n</pre> import torch import torch.nn as nn from torchvision import models, transforms from sklearn.manifold import TSNE import matplotlib.pyplot as plt import numpy as np  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) model = nn.Sequential(*list(model.children())[:-1]) model.eval()  transform = transforms.Compose([     transforms.Resize(224),     transforms.ToTensor(),     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])  def extract_embeddings(dataloader):     embeddings = []     labels = []     with torch.no_grad():         for images, lbls in dataloader:             emb = model(images).squeeze()             embeddings.append(emb.numpy())             labels.append(lbls.numpy())     return np.vstack(embeddings), np.concatenate(labels)  embeddings, labels = extract_embeddings(train_loader)  tsne = TSNE(n_components=2, random_state=42) embeddings_2d = tsne.fit_transform(embeddings)  plt.figure(figsize=(10, 8)) for label in np.unique(labels):     mask = labels == label     plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], label=f'Class {label}', alpha=0.6) plt.legend() plt.title('t-SNE Visualization of Embeddings') plt.xlabel('Component 1') plt.ylabel('Component 2') plt.show() <p>The extraction of embeddings involves modifying the pretrained model by removing the final classification layer, thereby retaining only the convolutional base and the global average pooling layer. For ResNet-18, this results in a 512-dimensional feature vector for each input image. The model is set to evaluation mode to ensure consistent behavior, and the same preprocessing transformations used during training are applied to the input image. The extract_embedding function takes an image file path as input, loads and preprocesses the image, passes it through the modified model, and returns the resulting embedding as a NumPy array. These embeddings can subsequently be used for various downstream applications, such as computing pairwise similarities using cosine distance or Euclidean distance, performing dimensionality reduction for visualization using techniques such as t-SNE or UMAP, or training classifiers on top of the extracted features for new tasks without requiring access to the original training data or computational resources for full model training.</p>"},{"location":"course/topic_04_computer_vision/section_07_transfer_learning.html#transfer-learning","title":"Transfer Learning\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_07_transfer_learning.html#fundamental-strategies-in-transfer-learning","title":"Fundamental Strategies in Transfer Learning\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_07_transfer_learning.html#complete-practical-example-with-cifar-10","title":"Complete Practical Example with CIFAR-10\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_07_transfer_learning.html#embedding-extraction","title":"Embedding Extraction\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html","title":"Autoencoders","text":"<p>Autoencoders constitute a family of neural network architectures designed to learn compressed representations of data in an unsupervised manner. The fundamental structure of an autoencoder is organized into two main blocks: An encoder, which transforms the original input into a lower-dimensional latent representation, and a decoder, which takes this latent representation and reconstructs from it an approximation of the original input. The training objective consists of minimizing the discrepancy between the reconstructed output and the input, so that the model is forced to capture the most relevant characteristics of the data in the latent space.</p> <p>This document presents several variants of autoencoders, from basic dense architectures to more advanced models such as variational autoencoders (VAE), Beta-VAE, and VQ-VAE. All implementations are developed on the MNIST dataset and are provided as fully functional code, ready to be executed from start to finish in a Jupyter Notebook environment.</p> <p>The vanilla autoencoder uses exclusively dense (fully connected) layers to encode and decode MNIST images. Each image of size $28 \\times 28$ is flattened into a vector of dimension 784 and projected into a lower-dimensional latent space. The encoder applies a sequence of linear transformations and nonlinear activation functions until it reaches the latent space, whereas the decoder performs the inverse process to reconstruct the image.</p> <p>This configuration introduces the central idea of autoencoders but exhibits clear limitations. Dense layers do not explicitly exploit the spatial structure of the image, which leads to a large number of parameters due to the full connectivity between neurons. In addition, since local relationships between pixels are not modeled explicitly, reconstructions tend to be blurrier and less detailed.</p> <p>The following code presents a basic functional implementation on MNIST.</p> In\u00a0[1]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nclass VanillaAutoencoder(nn.Module):\n    def __init__(self, input_dim: int = 784, latent_dim: int = 32) -&gt; None:\n        super().__init__()\n        # Encoder: Progressively reduces dimensionality\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, latent_dim),\n        )\n        # Decoder: Reconstructs from the latent space\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_dim),\n            nn.Sigmoid(),  # Output in [0, 1]\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Flatten the image\n        x = x.view(x.size(0), -1)\n        # Encode\n        latent = self.encoder(x)\n        # Decode\n        reconstructed = self.decoder(latent)\n        # Return to image shape\n        return reconstructed.view(-1, 1, 28, 28)\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x.view(x.size(0), -1)\n        return self.encoder(x)\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader from torchvision import datasets, transforms   class VanillaAutoencoder(nn.Module):     def __init__(self, input_dim: int = 784, latent_dim: int = 32) -&gt; None:         super().__init__()         # Encoder: Progressively reduces dimensionality         self.encoder = nn.Sequential(             nn.Linear(input_dim, 256),             nn.ReLU(),             nn.Linear(256, 128),             nn.ReLU(),             nn.Linear(128, latent_dim),         )         # Decoder: Reconstructs from the latent space         self.decoder = nn.Sequential(             nn.Linear(latent_dim, 128),             nn.ReLU(),             nn.Linear(128, 256),             nn.ReLU(),             nn.Linear(256, input_dim),             nn.Sigmoid(),  # Output in [0, 1]         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         # Flatten the image         x = x.view(x.size(0), -1)         # Encode         latent = self.encoder(x)         # Decode         reconstructed = self.decoder(latent)         # Return to image shape         return reconstructed.view(-1, 1, 28, 28)      def encode(self, x: torch.Tensor) -&gt; torch.Tensor:         x = x.view(x.size(0), -1)         return self.encoder(x) In\u00a0[2]: Copied! <pre>def prepare_mnist_data(batch_size: int = 128):\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    train_dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    test_dataset = datasets.MNIST(\n        root=\"./data\", train=False, download=True, transform=transform\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n</pre> def prepare_mnist_data(batch_size: int = 128):     transform = transforms.Compose([transforms.ToTensor()])      train_dataset = datasets.MNIST(         root=\"./data\", train=True, download=True, transform=transform     )     test_dataset = datasets.MNIST(         root=\"./data\", train=False, download=True, transform=transform     )      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)      return train_loader, test_loader In\u00a0[3]: Copied! <pre>def train_autoencoder(\n    model: nn.Module,\n    train_loader: DataLoader,\n    num_epochs: int = 10,\n    device: str = \"cuda\",\n) -&gt; nn.Module:\n\n    model = model.to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for data, _ in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            reconstructed = model(data)\n            loss = criterion(reconstructed, data)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n\n    return model\n</pre> def train_autoencoder(     model: nn.Module,     train_loader: DataLoader,     num_epochs: int = 10,     device: str = \"cuda\", ) -&gt; nn.Module:      model = model.to(device)     criterion = nn.MSELoss()     optimizer = optim.Adam(model.parameters(), lr=1e-3)      model.train()     for epoch in range(num_epochs):         total_loss = 0.0         for data, _ in train_loader:             data = data.to(device)             optimizer.zero_grad()             reconstructed = model(data)             loss = criterion(reconstructed, data)             loss.backward()             optimizer.step()             total_loss += loss.item()          avg_loss = total_loss / len(train_loader)         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")      return model In\u00a0[4]: Copied! <pre>def visualize_reconstructions(\n    model: nn.Module,\n    test_loader: DataLoader,\n    num_images: int = 10,\n    device: str = \"cuda\",\n) -&gt; None:\n\n    model.eval()\n    data, _ = next(iter(test_loader))\n    data = data[:num_images].to(device)\n\n    with torch.no_grad():\n        reconstructed = model(data)\n\n    data = data.cpu()\n    reconstructed = reconstructed.cpu()\n\n    fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n    for i in range(num_images):\n        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[0, i].set_title(\"Original\")\n\n        axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n        axes[1, i].set_title(\"Reconstructed\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> def visualize_reconstructions(     model: nn.Module,     test_loader: DataLoader,     num_images: int = 10,     device: str = \"cuda\", ) -&gt; None:      model.eval()     data, _ = next(iter(test_loader))     data = data[:num_images].to(device)      with torch.no_grad():         reconstructed = model(data)      data = data.cpu()     reconstructed = reconstructed.cpu()      fig, axes = plt.subplots(2, num_images, figsize=(15, 3))     for i in range(num_images):         axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")         axes[0, i].axis(\"off\")         axes[0, i].set_title(\"Original\")          axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")         axes[1, i].axis(\"off\")         axes[1, i].set_title(\"Reconstructed\")      plt.tight_layout()     plt.show() In\u00a0[5]: Copied! <pre># Vanilla autoencoder execution\ntrain_loader, test_loader = prepare_mnist_data()\nvanilla_ae = VanillaAutoencoder(input_dim=784, latent_dim=32)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nvanilla_ae = train_autoencoder(vanilla_ae, train_loader, num_epochs=2, device=device)\nvisualize_reconstructions(vanilla_ae, test_loader, device=device)\n</pre> # Vanilla autoencoder execution train_loader, test_loader = prepare_mnist_data() vanilla_ae = VanillaAutoencoder(input_dim=784, latent_dim=32) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" vanilla_ae = train_autoencoder(vanilla_ae, train_loader, num_epochs=2, device=device) visualize_reconstructions(vanilla_ae, test_loader, device=device) <pre>Epoch [1/2], Loss: 0.051985\n</pre> <pre>Epoch [2/2], Loss: 0.024950\n</pre> <p>The denoising autoencoder extends the previous approach by introducing noise into the input during training. In this case, the encoder receives a corrupted version of the image, while the loss function compares the decoder output with the clean original image. This mechanism forces the model to learn robust latent representations that capture the underlying structure of the data, rather than merely approximating the identity function.</p> <p>Noise is usually introduced as additive Gaussian noise, and values are subsequently clipped to keep them in the range $[0, 1]$. In this way, the model learns to \"undo\" the corruption, acting as a filter that preserves relevant content and discards spurious details.</p> <p>The following code illustrates an implementation of this variant on MNIST.</p> In\u00a0[6]: Copied! <pre>class DenoisingAutoencoder(nn.Module):\n    def __init__(self, input_dim: int = 784, latent_dim: int = 32) -&gt; None:\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, latent_dim),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_dim),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x.view(x.size(0), -1)\n        latent = self.encoder(x)\n        reconstructed = self.decoder(latent)\n        return reconstructed.view(-1, 1, 28, 28)\n</pre> class DenoisingAutoencoder(nn.Module):     def __init__(self, input_dim: int = 784, latent_dim: int = 32) -&gt; None:         super().__init__()         self.encoder = nn.Sequential(             nn.Linear(input_dim, 256),             nn.ReLU(),             nn.Dropout(0.2),             nn.Linear(256, 128),             nn.ReLU(),             nn.Linear(128, latent_dim),         )         self.decoder = nn.Sequential(             nn.Linear(latent_dim, 128),             nn.ReLU(),             nn.Linear(128, 256),             nn.ReLU(),             nn.Linear(256, input_dim),             nn.Sigmoid(),         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = x.view(x.size(0), -1)         latent = self.encoder(x)         reconstructed = self.decoder(latent)         return reconstructed.view(-1, 1, 28, 28) In\u00a0[7]: Copied! <pre>def add_noise(images: torch.Tensor, noise_factor: float = 0.3) -&gt; torch.Tensor:\n    noisy = images + noise_factor * torch.randn_like(images)\n    noisy = torch.clip(noisy, 0.0, 1.0)\n    return noisy\n</pre> def add_noise(images: torch.Tensor, noise_factor: float = 0.3) -&gt; torch.Tensor:     noisy = images + noise_factor * torch.randn_like(images)     noisy = torch.clip(noisy, 0.0, 1.0)     return noisy In\u00a0[8]: Copied! <pre>def train_denoising_ae(\n    model: nn.Module,\n    train_loader: DataLoader,\n    num_epochs: int = 10,\n    device: str = \"cuda\",\n    noise_factor: float = 0.3,\n) -&gt; nn.Module:\n\n    model = model.to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for data, _ in train_loader:\n            clean_data = data.to(device)\n            noisy_data = add_noise(clean_data, noise_factor)\n\n            optimizer.zero_grad()\n            reconstructed = model(noisy_data)\n            loss = criterion(reconstructed, clean_data)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n\n    return model\n</pre> def train_denoising_ae(     model: nn.Module,     train_loader: DataLoader,     num_epochs: int = 10,     device: str = \"cuda\",     noise_factor: float = 0.3, ) -&gt; nn.Module:      model = model.to(device)     criterion = nn.MSELoss()     optimizer = optim.Adam(model.parameters(), lr=1e-3)      model.train()     for epoch in range(num_epochs):         total_loss = 0.0         for data, _ in train_loader:             clean_data = data.to(device)             noisy_data = add_noise(clean_data, noise_factor)              optimizer.zero_grad()             reconstructed = model(noisy_data)             loss = criterion(reconstructed, clean_data)             loss.backward()             optimizer.step()             total_loss += loss.item()          avg_loss = total_loss / len(train_loader)         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")      return model In\u00a0[9]: Copied! <pre>def visualize_denoising(\n    model: nn.Module,\n    test_loader: DataLoader,\n    noise_factor: float = 0.3,\n    num_images: int = 10,\n    device: str = \"cuda\",\n) -&gt; None:\n\n    model.eval()\n    data, _ = next(iter(test_loader))\n    data = data[:num_images].to(device)\n    noisy_data = add_noise(data, noise_factor)\n\n    with torch.no_grad():\n        reconstructed = model(noisy_data)\n\n    data = data.cpu()\n    noisy_data = noisy_data.cpu()\n    reconstructed = reconstructed.cpu()\n\n    fig, axes = plt.subplots(3, num_images, figsize=(15, 5))\n    for i in range(num_images):\n        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        if i == 0:\n            axes[0, i].set_ylabel(\"Original\", rotation=0, labelpad=40)\n\n        axes[1, i].imshow(noisy_data[i].squeeze(), cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n        if i == 0:\n            axes[1, i].set_ylabel(\"Noisy\", rotation=0, labelpad=40)\n\n        axes[2, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n        axes[2, i].axis(\"off\")\n        if i == 0:\n            axes[2, i].set_ylabel(\"Denoised\", rotation=0, labelpad=40)\n\n    plt.tight_layout()\n    plt.show()\n</pre> def visualize_denoising(     model: nn.Module,     test_loader: DataLoader,     noise_factor: float = 0.3,     num_images: int = 10,     device: str = \"cuda\", ) -&gt; None:      model.eval()     data, _ = next(iter(test_loader))     data = data[:num_images].to(device)     noisy_data = add_noise(data, noise_factor)      with torch.no_grad():         reconstructed = model(noisy_data)      data = data.cpu()     noisy_data = noisy_data.cpu()     reconstructed = reconstructed.cpu()      fig, axes = plt.subplots(3, num_images, figsize=(15, 5))     for i in range(num_images):         axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")         axes[0, i].axis(\"off\")         if i == 0:             axes[0, i].set_ylabel(\"Original\", rotation=0, labelpad=40)          axes[1, i].imshow(noisy_data[i].squeeze(), cmap=\"gray\")         axes[1, i].axis(\"off\")         if i == 0:             axes[1, i].set_ylabel(\"Noisy\", rotation=0, labelpad=40)          axes[2, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")         axes[2, i].axis(\"off\")         if i == 0:             axes[2, i].set_ylabel(\"Denoised\", rotation=0, labelpad=40)      plt.tight_layout()     plt.show() In\u00a0[10]: Copied! <pre># Denoising autoencoder execution\ndenoising_ae = DenoisingAutoencoder(input_dim=784, latent_dim=32)\ndenoising_ae = train_denoising_ae(\n    denoising_ae, train_loader, num_epochs=2, device=device\n)\nvisualize_denoising(denoising_ae, test_loader, device=device)\n</pre> # Denoising autoencoder execution denoising_ae = DenoisingAutoencoder(input_dim=784, latent_dim=32) denoising_ae = train_denoising_ae(     denoising_ae, train_loader, num_epochs=2, device=device ) visualize_denoising(denoising_ae, test_loader, device=device) <pre>Epoch [1/2], Loss: 0.057884\n</pre> <pre>Epoch [2/2], Loss: 0.030809\n</pre> <p>Convolutional autoencoders are better suited to image data because they explicitly exploit spatial structure. The encoder applies convolutions with shared weights and local filters; spatial dimensionality is reduced through stride and the stacking of layers. The decoder uses transposed convolutions to perform upsampling and reconstruct the original resolution.</p> <p>In this context, convolutions provide several advantages. They significantly reduce the number of parameters compared with dense layers, due to weight sharing across different spatial positions. They also capture local patterns and hierarchical structures (edges, digit parts, whole digits), which leads to sharper reconstructions that are more consistent with image content.</p> <p>The following implementation illustrates a convolutional autoencoder with a linear bottleneck.</p> In\u00a0[11]: Copied! <pre>class ConvAutoencoder(nn.Module):\n    def __init__(self, latent_dim: int = 128) -&gt; None:\n        super().__init__()\n        # Convolutional encoder\n        self.encoder = nn.Sequential(\n            # 28x28 -&gt; 14x14\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            # 14x14 -&gt; 7x7\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            # 7x7 -&gt; 4x4 (slight additional reduction)\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n        )\n\n        # Linear bottleneck\n        self.flatten = nn.Flatten()\n        self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)\n        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)\n        self.unflatten = nn.Unflatten(1, (128, 4, 4))\n\n        # Decoder with transposed convolutions\n        self.decoder = nn.Sequential(\n            # 4x4 -&gt; 7x7\n            nn.ConvTranspose2d(\n                128, 64, kernel_size=3, stride=2, padding=1, output_padding=0\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            # 7x7 -&gt; 14x14\n            nn.ConvTranspose2d(\n                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            # 14x14 -&gt; 28x28\n            nn.ConvTranspose2d(\n                32, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Encode\n        x = self.encoder(x)\n        x = self.flatten(x)\n        latent = self.fc_encode(x)\n        # Decode\n        x = self.fc_decode(latent)\n        x = self.unflatten(x)\n        reconstructed = self.decoder(x)\n        return reconstructed\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.encoder(x)\n        x = self.flatten(x)\n        return self.fc_encode(x)\n</pre> class ConvAutoencoder(nn.Module):     def __init__(self, latent_dim: int = 128) -&gt; None:         super().__init__()         # Convolutional encoder         self.encoder = nn.Sequential(             # 28x28 -&gt; 14x14             nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(32),             # 14x14 -&gt; 7x7             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(64),             # 7x7 -&gt; 4x4 (slight additional reduction)             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(128),         )          # Linear bottleneck         self.flatten = nn.Flatten()         self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)         self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)         self.unflatten = nn.Unflatten(1, (128, 4, 4))          # Decoder with transposed convolutions         self.decoder = nn.Sequential(             # 4x4 -&gt; 7x7             nn.ConvTranspose2d(                 128, 64, kernel_size=3, stride=2, padding=1, output_padding=0             ),             nn.ReLU(),             nn.BatchNorm2d(64),             # 7x7 -&gt; 14x14             nn.ConvTranspose2d(                 64, 32, kernel_size=3, stride=2, padding=1, output_padding=1             ),             nn.ReLU(),             nn.BatchNorm2d(32),             # 14x14 -&gt; 28x28             nn.ConvTranspose2d(                 32, 1, kernel_size=3, stride=2, padding=1, output_padding=1             ),             nn.Sigmoid(),         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         # Encode         x = self.encoder(x)         x = self.flatten(x)         latent = self.fc_encode(x)         # Decode         x = self.fc_decode(latent)         x = self.unflatten(x)         reconstructed = self.decoder(x)         return reconstructed      def encode(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.encoder(x)         x = self.flatten(x)         return self.fc_encode(x) In\u00a0[12]: Copied! <pre># Training the convolutional autoencoder\nconv_ae = ConvAutoencoder(latent_dim=128)\nconv_ae = train_autoencoder(conv_ae, train_loader, num_epochs=2, device=device)\nvisualize_reconstructions(conv_ae, test_loader, device=device)\n</pre> # Training the convolutional autoencoder conv_ae = ConvAutoencoder(latent_dim=128) conv_ae = train_autoencoder(conv_ae, train_loader, num_epochs=2, device=device) visualize_reconstructions(conv_ae, test_loader, device=device) <pre>Epoch [1/2], Loss: 0.031839\n</pre> <pre>Epoch [2/2], Loss: 0.003783\n</pre> <p>Transposed convolutions can introduce characteristic artifacts known as checkerboard artifacts, which arise when the combination of kernel size and stride produces uneven overlaps during the upsampling operation.</p> <p>To mitigate checkerboard artifacts, it is common to replace transposed convolutions with an upsampling strategy based on interpolation followed by standard convolutions. In this configuration, the spatial resolution is first increased by interpolation (bilinear, bicubic, etc.), and then a convolution is applied to refine the result and learn filters over the rescaled image.</p> <p>This procedure tends to produce smoother and visually more coherent reconstructions, significantly reducing undesired patterns at the cost of some additional computational cost.</p> <p>The following model preserves the same convolutional encoder as the previous autoencoder but replaces the <code>ConvTranspose2d</code>-based decoder with a decoder that combines <code>Upsample</code> and <code>Conv2d</code>.</p> In\u00a0[13]: Copied! <pre>class UpsamplingAutoencoder(nn.Module):\n    def __init__(self, latent_dim: int = 128) -&gt; None:\n        super().__init__()\n        # Encoder identical to the convolutional autoencoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n        )\n\n        self.flatten = nn.Flatten()\n        self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)\n        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)\n        self.unflatten = nn.Unflatten(1, (128, 4, 4))\n\n        # Decoder with upsampling + convolution\n        self.decoder = nn.Sequential(\n            # 4x4 -&gt; 7x7\n            nn.Upsample(size=(7, 7), mode=\"bilinear\", align_corners=False),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            # 7x7 -&gt; 14x14\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            # 14x14 -&gt; 28x28\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.encoder(x)\n        x = self.flatten(x)\n        latent = self.fc_encode(x)\n        x = self.fc_decode(latent)\n        x = self.unflatten(x)\n        reconstructed = self.decoder(x)\n        return reconstructed\n</pre> class UpsamplingAutoencoder(nn.Module):     def __init__(self, latent_dim: int = 128) -&gt; None:         super().__init__()         # Encoder identical to the convolutional autoencoder         self.encoder = nn.Sequential(             nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(32),             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(64),             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             nn.ReLU(),             nn.BatchNorm2d(128),         )          self.flatten = nn.Flatten()         self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)         self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)         self.unflatten = nn.Unflatten(1, (128, 4, 4))          # Decoder with upsampling + convolution         self.decoder = nn.Sequential(             # 4x4 -&gt; 7x7             nn.Upsample(size=(7, 7), mode=\"bilinear\", align_corners=False),             nn.Conv2d(128, 64, kernel_size=3, padding=1),             nn.ReLU(),             nn.BatchNorm2d(64),             # 7x7 -&gt; 14x14             nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),             nn.Conv2d(64, 32, kernel_size=3, padding=1),             nn.ReLU(),             nn.BatchNorm2d(32),             # 14x14 -&gt; 28x28             nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),             nn.Conv2d(32, 1, kernel_size=3, padding=1),             nn.Sigmoid(),         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.encoder(x)         x = self.flatten(x)         latent = self.fc_encode(x)         x = self.fc_decode(latent)         x = self.unflatten(x)         reconstructed = self.decoder(x)         return reconstructed In\u00a0[14]: Copied! <pre># Training and visualization\nupsampling_ae = UpsamplingAutoencoder(latent_dim=128)\nupsampling_ae = train_autoencoder(\n    upsampling_ae, train_loader, num_epochs=2, device=device\n)\nvisualize_reconstructions(upsampling_ae, test_loader, device=device)\n</pre> # Training and visualization upsampling_ae = UpsamplingAutoencoder(latent_dim=128) upsampling_ae = train_autoencoder(     upsampling_ae, train_loader, num_epochs=2, device=device ) visualize_reconstructions(upsampling_ae, test_loader, device=device) <pre>Epoch [1/2], Loss: 0.030231\n</pre> <pre>Epoch [2/2], Loss: 0.007590\n</pre> <p>The use of bilinear or bicubic interpolation followed by standard convolutions generally produces visually more pleasant reconstructions and significantly reduces checkerboard artifacts, while preserving the model's ability to capture high-level patterns.</p> <p>The variational autoencoder (VAE) introduces an important conceptual change with respect to deterministic autoencoders. Instead of learning a direct mapping from the input to a fixed latent vector, the encoder learns the parameters of a probability distribution over the latent space. It is usually assumed that each latent dimension follows an independent Gaussian distribution, so the encoder produces a mean $\\mu$ and a logarithm of the variance $\\log \\sigma^2$ for each dimension.</p> <p>During training, a sample $z$ is drawn from the latent space using the reparameterization trick:</p> <p>$$z = \\mu + \\sigma \\odot \\varepsilon$$</p> <p>where $\\varepsilon \\sim \\mathcal{N}(0, I)$ and</p> <p>$$\\sigma = \\exp\\left(\\tfrac{1}{2} \\log\\sigma^2\\right)$$</p> <p>This formulation allows gradients to be backpropagated through the sampling operation.</p> <p>The VAE loss function includes two terms. The first is the reconstruction loss, which measures the discrepancy between the original and reconstructed images (for example, using binary cross-entropy). The second is a regularization term based on the Kullback\u2013Leibler (KL) divergence between the learned latent distribution and a standard normal distribution $\\mathcal{N}(0, I)$:</p> <p>$$\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2}\\sum_{i} \\left(1 + \\log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2\\right)$$</p> <p>This term forces the latent space to adopt a well-structured distribution, facilitating sampling and the generation of new examples.</p> <p>The following code presents a convolutional VAE implementation for MNIST.</p> In\u00a0[15]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nclass VAE(nn.Module):\n    def __init__(self, latent_dim: int = 20) -&gt; None:\n        super().__init__()\n        # Convolutional encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 28x28 -&gt; 14x14\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 14x14 -&gt; 7x7\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n\n        # Decoder\n        self.fc_decode = nn.Linear(latent_dim, 64 * 7 * 7)\n        self.decoder = nn.Sequential(\n            nn.Unflatten(1, (64, 7, 7)),\n            nn.ConvTranspose2d(\n                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),  # 7x7 -&gt; 14x14\n            nn.ReLU(),\n            nn.ConvTranspose2d(\n                32, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),  # 14x14 -&gt; 28x28\n            nn.Sigmoid(),\n        )\n\n    def encode(self, x: torch.Tensor):\n        h = self.encoder(x)\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n        x = self.fc_decode(z)\n        return self.decoder(x)\n\n    def forward(self, x: torch.Tensor):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        reconstructed = self.decode(z)\n        return reconstructed, mu, logvar\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.optim as optim from sklearn.manifold import TSNE from torch.utils.data import DataLoader from torchvision import datasets, transforms   class VAE(nn.Module):     def __init__(self, latent_dim: int = 20) -&gt; None:         super().__init__()         # Convolutional encoder         self.encoder = nn.Sequential(             nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 28x28 -&gt; 14x14             nn.ReLU(),             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 14x14 -&gt; 7x7             nn.ReLU(),             nn.Flatten(),         )          self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)         self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)          # Decoder         self.fc_decode = nn.Linear(latent_dim, 64 * 7 * 7)         self.decoder = nn.Sequential(             nn.Unflatten(1, (64, 7, 7)),             nn.ConvTranspose2d(                 64, 32, kernel_size=3, stride=2, padding=1, output_padding=1             ),  # 7x7 -&gt; 14x14             nn.ReLU(),             nn.ConvTranspose2d(                 32, 1, kernel_size=3, stride=2, padding=1, output_padding=1             ),  # 14x14 -&gt; 28x28             nn.Sigmoid(),         )      def encode(self, x: torch.Tensor):         h = self.encoder(x)         return self.fc_mu(h), self.fc_logvar(h)      def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:         std = torch.exp(0.5 * logvar)         eps = torch.randn_like(std)         return mu + eps * std      def decode(self, z: torch.Tensor) -&gt; torch.Tensor:         x = self.fc_decode(z)         return self.decoder(x)      def forward(self, x: torch.Tensor):         mu, logvar = self.encode(x)         z = self.reparameterize(mu, logvar)         reconstructed = self.decode(z)         return reconstructed, mu, logvar In\u00a0[16]: Copied! <pre>def vae_loss(\n    reconstructed: torch.Tensor,\n    original: torch.Tensor,\n    mu: torch.Tensor,\n    logvar: torch.Tensor,\n) -&gt; torch.Tensor:\n\n    recon_loss = nn.functional.binary_cross_entropy(\n        reconstructed, original, reduction=\"sum\"\n    )\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_loss\n</pre> def vae_loss(     reconstructed: torch.Tensor,     original: torch.Tensor,     mu: torch.Tensor,     logvar: torch.Tensor, ) -&gt; torch.Tensor:      recon_loss = nn.functional.binary_cross_entropy(         reconstructed, original, reduction=\"sum\"     )     kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())     return recon_loss + kl_loss In\u00a0[17]: Copied! <pre>def train_vae(\n    model: nn.Module,\n    train_loader: DataLoader,\n    num_epochs: int = 10,\n    device: str = \"cuda\",\n) -&gt; nn.Module:\n\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        for data, _ in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            reconstructed, mu, logvar = model(data)\n            loss = vae_loss(reconstructed, data, mu, logvar)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], \"\n            f\"Loss: {total_loss / len(train_loader.dataset):.4f}\"\n        )\n\n    return model\n</pre> def train_vae(     model: nn.Module,     train_loader: DataLoader,     num_epochs: int = 10,     device: str = \"cuda\", ) -&gt; nn.Module:      model = model.to(device)     optimizer = optim.Adam(model.parameters(), lr=1e-3)      for epoch in range(num_epochs):         model.train()         total_loss = 0.0         for data, _ in train_loader:             data = data.to(device)             optimizer.zero_grad()             reconstructed, mu, logvar = model(data)             loss = vae_loss(reconstructed, data, mu, logvar)             loss.backward()             optimizer.step()             total_loss += loss.item()          print(             f\"Epoch [{epoch+1}/{num_epochs}], \"             f\"Loss: {total_loss / len(train_loader.dataset):.4f}\"         )      return model In\u00a0[18]: Copied! <pre>def visualize_latent_space_tsne(\n    model: VAE, data_loader: DataLoader, device: str = \"cuda\", n_samples: int = 5000\n) -&gt; None:\n    \"\"\"Visualize the latent space using t-SNE.\"\"\"\n    model.eval()\n    latent_vectors = []\n    labels = []\n\n    with torch.no_grad():\n        for data, label in data_loader:\n            data = data.to(device)\n            mu, _ = model.encode(data)\n            latent_vectors.append(mu.cpu().numpy())\n            labels.append(label.numpy())\n            if len(latent_vectors) * data.size(0) &gt;= n_samples:\n                break\n\n    latent_vectors = np.concatenate(latent_vectors, axis=0)[:n_samples]\n    labels = np.concatenate(labels, axis=0)[:n_samples]\n\n    print(\"Applying t-SNE...\")\n    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n    latent_2d = tsne.fit_transform(latent_vectors)\n\n    plt.figure(figsize=(12, 10))\n    scatter = plt.scatter(\n        latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap=\"tab10\", alpha=0.6, s=5\n    )\n    plt.colorbar(scatter, label=\"Digit\")\n    plt.title(\"t-SNE Visualization of the VAE Latent Space\")\n    plt.xlabel(\"t-SNE Dimension 1\")\n    plt.ylabel(\"t-SNE Dimension 2\")\n    plt.tight_layout()\n    plt.show()\n</pre> def visualize_latent_space_tsne(     model: VAE, data_loader: DataLoader, device: str = \"cuda\", n_samples: int = 5000 ) -&gt; None:     \"\"\"Visualize the latent space using t-SNE.\"\"\"     model.eval()     latent_vectors = []     labels = []      with torch.no_grad():         for data, label in data_loader:             data = data.to(device)             mu, _ = model.encode(data)             latent_vectors.append(mu.cpu().numpy())             labels.append(label.numpy())             if len(latent_vectors) * data.size(0) &gt;= n_samples:                 break      latent_vectors = np.concatenate(latent_vectors, axis=0)[:n_samples]     labels = np.concatenate(labels, axis=0)[:n_samples]      print(\"Applying t-SNE...\")     tsne = TSNE(n_components=2, random_state=42, perplexity=30)     latent_2d = tsne.fit_transform(latent_vectors)      plt.figure(figsize=(12, 10))     scatter = plt.scatter(         latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap=\"tab10\", alpha=0.6, s=5     )     plt.colorbar(scatter, label=\"Digit\")     plt.title(\"t-SNE Visualization of the VAE Latent Space\")     plt.xlabel(\"t-SNE Dimension 1\")     plt.ylabel(\"t-SNE Dimension 2\")     plt.tight_layout()     plt.show() In\u00a0[19]: Copied! <pre>def generate_samples(\n    model: VAE, num_samples: int = 16, latent_dim: int = 20, device: str = \"cuda\"\n) -&gt; None:\n\n    model.eval()\n    with torch.no_grad():\n        z = torch.randn(num_samples, latent_dim).to(device)\n        samples = model.decode(z).cpu()\n\n    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(samples[i].squeeze(), cmap=\"gray\")\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n</pre> def generate_samples(     model: VAE, num_samples: int = 16, latent_dim: int = 20, device: str = \"cuda\" ) -&gt; None:      model.eval()     with torch.no_grad():         z = torch.randn(num_samples, latent_dim).to(device)         samples = model.decode(z).cpu()      fig, axes = plt.subplots(4, 4, figsize=(8, 8))     for i, ax in enumerate(axes.flat):         ax.imshow(samples[i].squeeze(), cmap=\"gray\")         ax.axis(\"off\")     plt.tight_layout()     plt.show() In\u00a0[20]: Copied! <pre>def prepare_mnist_data(batch_size: int = 128):\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    train_dataset = datasets.MNIST(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    test_dataset = datasets.MNIST(\n        root=\"./data\", train=False, download=True, transform=transform\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n</pre> def prepare_mnist_data(batch_size: int = 128):     transform = transforms.Compose([transforms.ToTensor()])      train_dataset = datasets.MNIST(         root=\"./data\", train=True, download=True, transform=transform     )     test_dataset = datasets.MNIST(         root=\"./data\", train=False, download=True, transform=transform     )      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)      return train_loader, test_loader In\u00a0[21]: Copied! <pre># Prepare data\ntrain_loader, test_loader = prepare_mnist_data()\n\n# Train VAE\nvae = VAE(latent_dim=20)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nvae = train_vae(vae, train_loader, num_epochs=2, device=device)\n\n# Visualize latent space with t-SNE\nvisualize_latent_space_tsne(vae, test_loader, device=device)\n\n# Generate synthetic samples\ngenerate_samples(vae, latent_dim=20, device=device)\n\n# Visualize reconstructions\nwith torch.no_grad():\n    data, _ = next(iter(test_loader))\n    data = data[:10].to(device)\n    reconstructed, _, _ = vae(data)\n\n    fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axes[0, i].imshow(data[i].cpu().squeeze(), cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n\n    axes[0, 0].set_ylabel(\"Original\", size=12)\n    axes[1, 0].set_ylabel(\"Reconstructed\", size=12)\n    plt.tight_layout()\n    plt.show()\n</pre> # Prepare data train_loader, test_loader = prepare_mnist_data()  # Train VAE vae = VAE(latent_dim=20) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" vae = train_vae(vae, train_loader, num_epochs=2, device=device)  # Visualize latent space with t-SNE visualize_latent_space_tsne(vae, test_loader, device=device)  # Generate synthetic samples generate_samples(vae, latent_dim=20, device=device)  # Visualize reconstructions with torch.no_grad():     data, _ = next(iter(test_loader))     data = data[:10].to(device)     reconstructed, _, _ = vae(data)      fig, axes = plt.subplots(2, 10, figsize=(15, 3))     for i in range(10):         axes[0, i].imshow(data[i].cpu().squeeze(), cmap=\"gray\")         axes[0, i].axis(\"off\")         axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap=\"gray\")         axes[1, i].axis(\"off\")      axes[0, 0].set_ylabel(\"Original\", size=12)     axes[1, 0].set_ylabel(\"Reconstructed\", size=12)     plt.tight_layout()     plt.show() <pre>Epoch [1/2], Loss: 161.9175\n</pre> <pre>Epoch [2/2], Loss: 120.5446\n</pre> <pre>Applying t-SNE...\n</pre> <p>VAEs are particularly useful for generating synthetic data by direct sampling in the latent space and for anomaly detection by analyzing out-of-distribution examples. However, they can suffer from the posterior collapse phenomenon, in which the decoder largely ignores latent information and learns to reconstruct from local patterns alone, reducing the quality and informativeness of latent representations.</p> <p>The Beta-VAE introduces a hyperparameter $\\beta$ in the VAE loss function to weight the KL divergence term:</p> <p>$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathcal{L}_{\\text{recon}} + \\beta \\,\\mathcal{L}_{\\text{KL}}$$</p> <p>When $\\beta &gt; 1$, the model is forced to align the latent distribution more strongly with the standard normal distribution, which tends to produce more disentangled representations. In a disentangled latent space, each dimension preferentially captures an independent factor of variation in the data (for example, stroke thickness, slant, or size), improving interpretability and control over generated samples.</p> <p>Excessively high values of $\\beta$ can degrade reconstruction quality by penalizing latent code complexity too strongly.</p> <p>The following code shows how to adapt the loss and training procedure for a Beta-VAE using the VAE architecture defined above.</p> In\u00a0[22]: Copied! <pre>def beta_vae_loss(\n    reconstructed: torch.Tensor,\n    original: torch.Tensor,\n    mu: torch.Tensor,\n    logvar: torch.Tensor,\n    beta: float = 4.0,\n) -&gt; torch.Tensor:\n\n    recon_loss = nn.functional.binary_cross_entropy(\n        reconstructed, original, reduction=\"sum\"\n    )\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + beta * kl_loss\n</pre> def beta_vae_loss(     reconstructed: torch.Tensor,     original: torch.Tensor,     mu: torch.Tensor,     logvar: torch.Tensor,     beta: float = 4.0, ) -&gt; torch.Tensor:      recon_loss = nn.functional.binary_cross_entropy(         reconstructed, original, reduction=\"sum\"     )     kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())     return recon_loss + beta * kl_loss In\u00a0[23]: Copied! <pre>def train_beta_vae(\n    model: VAE,\n    train_loader: DataLoader,\n    num_epochs: int = 10,\n    beta: float = 4.0,\n    device: str = \"cuda\",\n) -&gt; VAE:\n\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    model.train()\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for data, _ in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            reconstructed, mu, logvar = model(data)\n            loss = beta_vae_loss(reconstructed, data, mu, logvar, beta)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    return model\n</pre> def train_beta_vae(     model: VAE,     train_loader: DataLoader,     num_epochs: int = 10,     beta: float = 4.0,     device: str = \"cuda\", ) -&gt; VAE:      model = model.to(device)     optimizer = optim.Adam(model.parameters(), lr=1e-3)     model.train()      for epoch in range(num_epochs):         total_loss = 0.0         for data, _ in train_loader:             data = data.to(device)             optimizer.zero_grad()             reconstructed, mu, logvar = model(data)             loss = beta_vae_loss(reconstructed, data, mu, logvar, beta)             loss.backward()             optimizer.step()             total_loss += loss.item()          avg_loss = total_loss / len(train_loader.dataset)         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")      return model <p>To explore the effect of controlled variations along individual latent dimensions, the latent traversal technique is used. It consists of systematically modifying a single latent coordinate while keeping the remaining ones fixed.</p> In\u00a0[24]: Copied! <pre>def visualize_latent_traversal(\n    model: VAE,\n    test_loader: DataLoader,\n    latent_dim: int = 20,\n    dim_to_vary: int = 0,\n    device: str = \"cuda\",\n) -&gt; None:\n\n    model.eval()\n    data, _ = next(iter(test_loader))\n    data = data[0:1].to(device)\n\n    with torch.no_grad():\n        mu, _ = model.encode(data)\n        values = torch.linspace(-3, 3, 10)\n        samples = []\n\n        for val in values:\n            z = mu.clone()\n            z[0, dim_to_vary] = val\n            reconstructed = model.decode(z)\n            samples.append(reconstructed)\n\n        samples = torch.cat(samples, dim=0)\n\n    samples = samples.cpu()\n    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(samples[i].squeeze(), cmap=\"gray\")\n        ax.axis(\"off\")\n        ax.set_title(f\"{values[i]:.1f}\")\n    plt.tight_layout()\n    plt.show()\n</pre> def visualize_latent_traversal(     model: VAE,     test_loader: DataLoader,     latent_dim: int = 20,     dim_to_vary: int = 0,     device: str = \"cuda\", ) -&gt; None:      model.eval()     data, _ = next(iter(test_loader))     data = data[0:1].to(device)      with torch.no_grad():         mu, _ = model.encode(data)         values = torch.linspace(-3, 3, 10)         samples = []          for val in values:             z = mu.clone()             z[0, dim_to_vary] = val             reconstructed = model.decode(z)             samples.append(reconstructed)          samples = torch.cat(samples, dim=0)      samples = samples.cpu()     fig, axes = plt.subplots(1, 10, figsize=(15, 2))     for i, ax in enumerate(axes.flat):         ax.imshow(samples[i].squeeze(), cmap=\"gray\")         ax.axis(\"off\")         ax.set_title(f\"{values[i]:.1f}\")     plt.tight_layout()     plt.show() In\u00a0[25]: Copied! <pre># Training the Beta-VAE\nbeta_vae = VAE(latent_dim=20)\nbeta_vae = train_beta_vae(\n    beta_vae, train_loader, num_epochs=2, beta=4.0, device=device\n)\n\n# Visualize variation of some latent dimensions\nfor dim in range(5):\n    visualize_latent_traversal(beta_vae, test_loader, dim_to_vary=dim, device=device)\n</pre> # Training the Beta-VAE beta_vae = VAE(latent_dim=20) beta_vae = train_beta_vae(     beta_vae, train_loader, num_epochs=2, beta=4.0, device=device )  # Visualize variation of some latent dimensions for dim in range(5):     visualize_latent_traversal(beta_vae, test_loader, dim_to_vary=dim, device=device) <pre>Epoch [1/2], Loss: 198.7401\n</pre> <pre>Epoch [2/2], Loss: 164.8172\n</pre> <p>Latent traversal enables inspection of the influence of each latent dimension on generated samples, facilitating the interpretation of disentangled representations and the design of controlled manipulations over specific attributes.</p> <p>VQ-VAE introduces a fundamental modification in the treatment of the latent space. Instead of continuous codes, it uses a discrete representation based on a learned codebook of embeddings. The encoder projects the input into a continuous latent tensor of dimension $C$; each latent vector is then quantized by selecting the closest embedding from the codebook, that is, by assigning a discrete index. The decoder receives the quantized embeddings and reconstructs the input.</p> <p>This discretization offers several advantages. It avoids the posterior collapse problem typical of some VAEs, as quantization forces the model to actively use the latent space. Moreover, the discrete representation is particularly well suited to be modeled later using autoregressive models (for example, transformers), which has been crucial in generative architectures such as DALL\u00b7E. In this context, latent indices act as tokens on which language-modeling techniques can be applied.</p> <p>The following code presents a simple VQ-VAE implementation for MNIST, including the vector quantization module.</p> In\u00a0[26]: Copied! <pre>\"\"\"VQ-VAE (Vector Quantized Variational Autoencoder) Implementation\"\"\"\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n</pre> \"\"\"VQ-VAE (Vector Quantized Variational Autoencoder) Implementation\"\"\"  # 3pps import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim In\u00a0[27]: Copied! <pre>class VectorQuantizer(nn.Module):\n    \"\"\"\n    Vector Quantizer layer for VQ-VAE.\n    Converts continuous latent vectors into discrete codes from the codebook.\n    \"\"\"\n\n    def __init__(\n        self, num_embeddings: int, embedding_dim: int, commitment_cost: float = 0.25\n    ) -&gt; None:\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n\n        # Codebook of embeddings\n        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n        self.embeddings.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n\n    def forward(self, inputs: torch.Tensor):\n        \"\"\"\n        Args:\n            inputs: Tensor of shape (B, C, H, W)\n        Returns:\n            quantized: Quantized tensor (B, C, H, W)\n            loss: Quantization loss (codebook + commitment)\n            encoding_indices: Indices of selected codebook vectors\n        \"\"\"\n        # Reorder to (B, H, W, C)\n        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n        input_shape = inputs.shape\n\n        # Flatten to (B*H*W, C)\n        flat_input = inputs.view(-1, self.embedding_dim)\n\n        # L2 distances to each codebook embedding\n        distances = (\n            torch.sum(flat_input**2, dim=1, keepdim=True)\n            + torch.sum(self.embeddings.weight**2, dim=1)\n            - 2 * torch.matmul(flat_input, self.embeddings.weight.t())\n        )\n\n        # Index of nearest embedding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n        # One-hot encoding\n        encodings = torch.zeros(\n            encoding_indices.shape[0], self.num_embeddings, device=inputs.device\n        )\n        encodings.scatter_(1, encoding_indices, 1)\n\n        # Quantization via codebook\n        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)\n\n        # VQ losses\n        e_latent_loss = nn.functional.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = nn.functional.mse_loss(quantized, inputs.detach())\n        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n\n        # Straight-through estimator\n        quantized = inputs + (quantized - inputs).detach()\n\n        # Back to (B, C, H, W)\n        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n\n        return quantized, loss, encoding_indices\n</pre> class VectorQuantizer(nn.Module):     \"\"\"     Vector Quantizer layer for VQ-VAE.     Converts continuous latent vectors into discrete codes from the codebook.     \"\"\"      def __init__(         self, num_embeddings: int, embedding_dim: int, commitment_cost: float = 0.25     ) -&gt; None:         super().__init__()         self.embedding_dim = embedding_dim         self.num_embeddings = num_embeddings         self.commitment_cost = commitment_cost          # Codebook of embeddings         self.embeddings = nn.Embedding(num_embeddings, embedding_dim)         self.embeddings.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)      def forward(self, inputs: torch.Tensor):         \"\"\"         Args:             inputs: Tensor of shape (B, C, H, W)         Returns:             quantized: Quantized tensor (B, C, H, W)             loss: Quantization loss (codebook + commitment)             encoding_indices: Indices of selected codebook vectors         \"\"\"         # Reorder to (B, H, W, C)         inputs = inputs.permute(0, 2, 3, 1).contiguous()         input_shape = inputs.shape          # Flatten to (B*H*W, C)         flat_input = inputs.view(-1, self.embedding_dim)          # L2 distances to each codebook embedding         distances = (             torch.sum(flat_input**2, dim=1, keepdim=True)             + torch.sum(self.embeddings.weight**2, dim=1)             - 2 * torch.matmul(flat_input, self.embeddings.weight.t())         )          # Index of nearest embedding         encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)          # One-hot encoding         encodings = torch.zeros(             encoding_indices.shape[0], self.num_embeddings, device=inputs.device         )         encodings.scatter_(1, encoding_indices, 1)          # Quantization via codebook         quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)          # VQ losses         e_latent_loss = nn.functional.mse_loss(quantized.detach(), inputs)         q_latent_loss = nn.functional.mse_loss(quantized, inputs.detach())         loss = q_latent_loss + self.commitment_cost * e_latent_loss          # Straight-through estimator         quantized = inputs + (quantized - inputs).detach()          # Back to (B, C, H, W)         quantized = quantized.permute(0, 3, 1, 2).contiguous()          return quantized, loss, encoding_indices In\u00a0[28]: Copied! <pre>class VQVAE(nn.Module):\n    \"\"\"\n    VQ-VAE model with encoder, vector quantizer, and decoder.\n    \"\"\"\n\n    def __init__(self, num_embeddings: int = 512, embedding_dim: int = 64) -&gt; None:\n        super().__init__()\n\n        # Encoder: (1, 28, 28) -&gt; (embedding_dim, 7, 7)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 28x28 -&gt; 14x14\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 14x14 -&gt; 7x7\n            nn.ReLU(),\n            nn.Conv2d(64, embedding_dim, kernel_size=1),  # 7x7, C=embedding_dim\n        )\n\n        # Vector Quantizer\n        self.vq = VectorQuantizer(num_embeddings, embedding_dim)\n\n        # Decoder: (embedding_dim, 7, 7) -&gt; (1, 28, 28)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(\n                embedding_dim, 64, kernel_size=4, stride=2, padding=1\n            ),  # 7x7 -&gt; 14x14\n            nn.ReLU(),\n            nn.ConvTranspose2d(\n                64, 32, kernel_size=4, stride=2, padding=1\n            ),  # 14x14 -&gt; 28x28\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 1, kernel_size=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: Input tensor (B, 1, 28, 28)\n        Returns:\n            reconstructed: Reconstruction (B, 1, 28, 28)\n            vq_loss: Vector quantization loss\n        \"\"\"\n        z = self.encoder(x)\n        quantized, vq_loss, _ = self.vq(z)\n        reconstructed = self.decoder(quantized)\n        return reconstructed, vq_loss\n\n    def encode(self, x: torch.Tensor):\n        \"\"\"Encode and quantize the input.\"\"\"\n        z = self.encoder(x)\n        quantized, _, indices = self.vq(z)\n        return quantized, indices\n\n    def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode a quantized latent tensor.\"\"\"\n        return self.decoder(z)\n</pre> class VQVAE(nn.Module):     \"\"\"     VQ-VAE model with encoder, vector quantizer, and decoder.     \"\"\"      def __init__(self, num_embeddings: int = 512, embedding_dim: int = 64) -&gt; None:         super().__init__()          # Encoder: (1, 28, 28) -&gt; (embedding_dim, 7, 7)         self.encoder = nn.Sequential(             nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 28x28 -&gt; 14x14             nn.ReLU(),             nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 14x14 -&gt; 7x7             nn.ReLU(),             nn.Conv2d(64, embedding_dim, kernel_size=1),  # 7x7, C=embedding_dim         )          # Vector Quantizer         self.vq = VectorQuantizer(num_embeddings, embedding_dim)          # Decoder: (embedding_dim, 7, 7) -&gt; (1, 28, 28)         self.decoder = nn.Sequential(             nn.ConvTranspose2d(                 embedding_dim, 64, kernel_size=4, stride=2, padding=1             ),  # 7x7 -&gt; 14x14             nn.ReLU(),             nn.ConvTranspose2d(                 64, 32, kernel_size=4, stride=2, padding=1             ),  # 14x14 -&gt; 28x28             nn.ReLU(),             nn.ConvTranspose2d(32, 1, kernel_size=1),             nn.Sigmoid(),         )      def forward(self, x: torch.Tensor):         \"\"\"         Args:             x: Input tensor (B, 1, 28, 28)         Returns:             reconstructed: Reconstruction (B, 1, 28, 28)             vq_loss: Vector quantization loss         \"\"\"         z = self.encoder(x)         quantized, vq_loss, _ = self.vq(z)         reconstructed = self.decoder(quantized)         return reconstructed, vq_loss      def encode(self, x: torch.Tensor):         \"\"\"Encode and quantize the input.\"\"\"         z = self.encoder(x)         quantized, _, indices = self.vq(z)         return quantized, indices      def decode(self, z: torch.Tensor) -&gt; torch.Tensor:         \"\"\"Decode a quantized latent tensor.\"\"\"         return self.decoder(z) In\u00a0[29]: Copied! <pre>def train_vqvae(\n    model: VQVAE,\n    train_loader: DataLoader,\n    num_epochs: int = 10,\n    lr: float = 1e-3,\n    device: str = \"cuda\",\n) -&gt; VQVAE:\n    \"\"\"\n    Train the VQ-VAE model.\n\n    Args:\n        model: VQVAE model.\n        train_loader: Training DataLoader.\n        num_epochs: Number of epochs.\n        lr: Learning rate.\n        device: \"cuda\" or \"cpu\".\n\n    Returns:\n        Trained model.\n    \"\"\"\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    model.train()\n\n    for epoch in range(num_epochs):\n        total_recon_loss = 0.0\n        total_vq_loss = 0.0\n\n        for data, _ in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n\n            reconstructed, vq_loss = model(data)\n            recon_loss = nn.functional.mse_loss(reconstructed, data)\n            loss = recon_loss + vq_loss\n\n            loss.backward()\n            optimizer.step()\n\n            total_recon_loss += recon_loss.item()\n            total_vq_loss += vq_loss.item()\n\n        avg_recon = total_recon_loss / len(train_loader)\n        avg_vq = total_vq_loss / len(train_loader)\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}] | \"\n            f\"Recon Loss: {avg_recon:.6f} | \"\n            f\"VQ Loss: {avg_vq:.6f}\"\n        )\n\n    return model\n</pre> def train_vqvae(     model: VQVAE,     train_loader: DataLoader,     num_epochs: int = 10,     lr: float = 1e-3,     device: str = \"cuda\", ) -&gt; VQVAE:     \"\"\"     Train the VQ-VAE model.      Args:         model: VQVAE model.         train_loader: Training DataLoader.         num_epochs: Number of epochs.         lr: Learning rate.         device: \"cuda\" or \"cpu\".      Returns:         Trained model.     \"\"\"     model = model.to(device)     optimizer = optim.Adam(model.parameters(), lr=lr)     model.train()      for epoch in range(num_epochs):         total_recon_loss = 0.0         total_vq_loss = 0.0          for data, _ in train_loader:             data = data.to(device)             optimizer.zero_grad()              reconstructed, vq_loss = model(data)             recon_loss = nn.functional.mse_loss(reconstructed, data)             loss = recon_loss + vq_loss              loss.backward()             optimizer.step()              total_recon_loss += recon_loss.item()             total_vq_loss += vq_loss.item()          avg_recon = total_recon_loss / len(train_loader)         avg_vq = total_vq_loss / len(train_loader)         print(             f\"Epoch [{epoch+1}/{num_epochs}] | \"             f\"Recon Loss: {avg_recon:.6f} | \"             f\"VQ Loss: {avg_vq:.6f}\"         )      return model In\u00a0[30]: Copied! <pre>def visualize_vqvae_reconstructions(\n    model: VQVAE, test_loader: DataLoader, device: str = \"cuda\", num_images: int = 8\n) -&gt; None:\n    \"\"\"\n    Visualize original and VQ-VAE reconstructed images.\n    \"\"\"\n    model.eval()\n    data, _ = next(iter(test_loader))\n    data = data[:num_images].to(device)\n\n    with torch.no_grad():\n        reconstructed, _ = model(data)\n\n    data = data.cpu()\n    reconstructed = reconstructed.cpu()\n\n    fig, axes = plt.subplots(2, num_images, figsize=(12, 3))\n    for i in range(num_images):\n        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n\n    axes[0, 0].set_ylabel(\"Original\", size=12)\n    axes[1, 0].set_ylabel(\"Reconstructed\", size=12)\n    plt.tight_layout()\n    plt.show()\n</pre> def visualize_vqvae_reconstructions(     model: VQVAE, test_loader: DataLoader, device: str = \"cuda\", num_images: int = 8 ) -&gt; None:     \"\"\"     Visualize original and VQ-VAE reconstructed images.     \"\"\"     model.eval()     data, _ = next(iter(test_loader))     data = data[:num_images].to(device)      with torch.no_grad():         reconstructed, _ = model(data)      data = data.cpu()     reconstructed = reconstructed.cpu()      fig, axes = plt.subplots(2, num_images, figsize=(12, 3))     for i in range(num_images):         axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")         axes[0, i].axis(\"off\")         axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")         axes[1, i].axis(\"off\")      axes[0, 0].set_ylabel(\"Original\", size=12)     axes[1, 0].set_ylabel(\"Reconstructed\", size=12)     plt.tight_layout()     plt.show() In\u00a0[31]: Copied! <pre># Main VQ-VAE execution\nNUM_EMBEDDINGS = 512\nEMBEDDING_DIM = 64\nNUM_EPOCHS = 2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvqvae = VQVAE(num_embeddings=NUM_EMBEDDINGS, embedding_dim=EMBEDDING_DIM)\nvqvae = train_vqvae(vqvae, train_loader, num_epochs=NUM_EPOCHS, device=DEVICE)\nvisualize_vqvae_reconstructions(vqvae, test_loader, device=DEVICE)\n</pre> # Main VQ-VAE execution NUM_EMBEDDINGS = 512 EMBEDDING_DIM = 64 NUM_EPOCHS = 2 DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  vqvae = VQVAE(num_embeddings=NUM_EMBEDDINGS, embedding_dim=EMBEDDING_DIM) vqvae = train_vqvae(vqvae, train_loader, num_epochs=NUM_EPOCHS, device=DEVICE) visualize_vqvae_reconstructions(vqvae, test_loader, device=DEVICE) <pre>Epoch [1/2] | Recon Loss: 0.034533 | VQ Loss: 0.162459\n</pre> <pre>Epoch [2/2] | Recon Loss: 0.008001 | VQ Loss: 0.010701\n</pre> <p>VQ-VAE provides a discrete latent space that is particularly suitable for integration into multimodal systems and complex generative models, in which tokenization of data is essential. Vector quantization offers a robust foundation for applying advanced sequential modeling techniques to image representations and facilitates integration with language architectures that operate on discrete sequences.</p>"},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#autoencoders","title":"Autoencoders\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#vanilla-autoencoder-with-dense-layers","title":"Vanilla Autoencoder with Dense Layers\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#denoising-autoencoder","title":"Denoising Autoencoder\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#convolutional-autoencoder","title":"Convolutional Autoencoder\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#autoencoder-with-interpolation-based-upsampling","title":"Autoencoder with Interpolation-Based Upsampling\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#variational-autoencoder-vae","title":"Variational Autoencoder (VAE)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#beta-vae","title":"Beta-VAE\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_08_autoencoders.html#vq-vae-vector-quantized-vae","title":"VQ-VAE (Vector Quantized VAE)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_09_attention.html","title":"Attention Mechanisms in Convolutional Neural Networks","text":"<p>Attention mechanisms in convolutional neural networks enable the model to adaptively focus on the most relevant features of the input signal, either at the channel level or at the spatial level. These modules learn to recalibrate intermediate activations by assigning differentiated importance weights, which increases the representational capacity of the model without drastically increasing the number of parameters or the computational cost.</p> <p>In modern architectures, attention is integrated in a modular way into existing convolutional blocks, such as the residual blocks of ResNet. The following sections describe and implement two of the most influential attention mechanisms in convolutional networks: The Squeeze-and-Excitation (SE) block and the Convolutional Block Attention Module (CBAM).</p> <p>The Squeeze-and-Excitation block, introduced in the work Squeeze-and-Excitation Networks, incorporates a channel-wise attention mechanism. The central idea is to explicitly model the dependency relationships between feature channels so that the network learns to emphasize those channels that are most informative for the task, while suppressing less relevant or redundant channels.</p> <p>The SE mechanism decomposes into two conceptual stages, commonly referred to as squeeze and excitation. In the squeeze phase, the spatial dimension of each feature map is reduced by means of global average pooling. In this way, each channel is compressed into a single scalar value that summarizes its global activation across the entire image. In the excitation phase, these aggregated values are fed into a small fully connected network that learns a channel-wise attention function. The output of this network is a vector of weights in the interval $(0, 1)$, which is applied multiplicatively to the original channels, recalibrating their relative importance.</p> <p>Let $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ be a feature tensor with batch size $B$, number of channels $C$, and spatial dimensions $H \\times W$. The squeeze operation computes, for each channel $c$,</p> <p>$$z_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_c(i, j)$$</p> <p>The compressed vector $z \\in \\mathbb{R}^{C}$ is processed by a two-layer fully connected network with an intermediate dimensionality reduction, which produces a vector of weights $s \\in (0, 1)^{C}$ after a sigmoid activation. The recalibration is implemented as</p> <p>$$\\tilde{X}_c(i, j) = s_c \\cdot X_c(i, j)$$</p> <p>The following code shows an implementation of the SE block and its integration into a basic residual block in PyTorch. The code is designed for direct use in a reproducible and fully executable workflow.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, in_channels: int, reduction_ratio: int = 16) -&gt; None:\n        super().__init__()\n        reduced_channels = max(in_channels // reduction_ratio, 1)\n        # Squeeze: Global average pooling per channel\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        # Excitation: Two fully connected (implemented as Linear) layers\n        self.excitation = nn.Sequential(\n            nn.Linear(in_channels, reduced_channels, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(reduced_channels, in_channels, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        batch_size, channels, _, _ = x.size()\n        # Squeeze: Global average pooling per channel\n        squeezed = self.squeeze(x).view(batch_size, channels)\n        # Excitation: Channel-wise weights in (0, 1)\n        excited = self.excitation(squeezed).view(batch_size, channels, 1, 1)\n        # Channel-wise recalibration\n        return x * excited\n\n\nclass SEResidualBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        reduction_ratio: int = 16,\n    ) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.se = SqueezeExcitation(out_channels, reduction_ratio)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(out_channels),\n            )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        identity = self.shortcut(x)\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += identity\n        out = F.relu(out)\n        return out\n</pre> import torch import torch.nn as nn import torch.nn.functional as F   class SqueezeExcitation(nn.Module):     def __init__(self, in_channels: int, reduction_ratio: int = 16) -&gt; None:         super().__init__()         reduced_channels = max(in_channels // reduction_ratio, 1)         # Squeeze: Global average pooling per channel         self.squeeze = nn.AdaptiveAvgPool2d(1)         # Excitation: Two fully connected (implemented as Linear) layers         self.excitation = nn.Sequential(             nn.Linear(in_channels, reduced_channels, bias=False),             nn.ReLU(inplace=True),             nn.Linear(reduced_channels, in_channels, bias=False),             nn.Sigmoid(),         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         batch_size, channels, _, _ = x.size()         # Squeeze: Global average pooling per channel         squeezed = self.squeeze(x).view(batch_size, channels)         # Excitation: Channel-wise weights in (0, 1)         excited = self.excitation(squeezed).view(batch_size, channels, 1, 1)         # Channel-wise recalibration         return x * excited   class SEResidualBlock(nn.Module):     def __init__(         self,         in_channels: int,         out_channels: int,         stride: int = 1,         reduction_ratio: int = 16,     ) -&gt; None:         super().__init__()         self.conv1 = nn.Conv2d(             in_channels,             out_channels,             kernel_size=3,             stride=stride,             padding=1,             bias=False,         )         self.bn1 = nn.BatchNorm2d(out_channels)         self.conv2 = nn.Conv2d(             out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False         )         self.bn2 = nn.BatchNorm2d(out_channels)         self.se = SqueezeExcitation(out_channels, reduction_ratio)         self.shortcut = nn.Sequential()         if stride != 1 or in_channels != out_channels:             self.shortcut = nn.Sequential(                 nn.Conv2d(                     in_channels, out_channels, kernel_size=1, stride=stride, bias=False                 ),                 nn.BatchNorm2d(out_channels),             )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         identity = self.shortcut(x)         out = F.relu(self.bn1(self.conv1(x)))         out = self.bn2(self.conv2(out))         out = self.se(out)         out += identity         out = F.relu(out)         return out <p>To verify the correct construction and behavior of the SE block, a small functional test can be defined. This test checks that input and output shapes match and reports the number of parameters of the SE module.</p> In\u00a0[2]: Copied! <pre>def test_se_block() -&gt; None:\n    x = torch.randn(2, 64, 32, 32)\n    se_block = SqueezeExcitation(in_channels=64, reduction_ratio=16)\n    output = se_block(x)\n    print(f\"Input shape:  {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"SE parameters: {sum(p.numel() for p in se_block.parameters())}\")\n    assert x.shape == output.shape, \"Shape mismatch\"\n    print(\"SE Block test passed\")\n\n\ntest_se_block()\n</pre> def test_se_block() -&gt; None:     x = torch.randn(2, 64, 32, 32)     se_block = SqueezeExcitation(in_channels=64, reduction_ratio=16)     output = se_block(x)     print(f\"Input shape:  {x.shape}\")     print(f\"Output shape: {output.shape}\")     print(f\"SE parameters: {sum(p.numel() for p in se_block.parameters())}\")     assert x.shape == output.shape, \"Shape mismatch\"     print(\"SE Block test passed\")   test_se_block() <pre>Input shape:  torch.Size([2, 64, 32, 32])\nOutput shape: torch.Size([2, 64, 32, 32])\nSE parameters: 512\nSE Block test passed\n</pre> <p>The SE block introduces a relatively moderate number of additional parameters, controlled by the hyperparameter <code>reduction_ratio</code>. This parameter determines the bottleneck size in the excitation network: Larger values reduce the capacity of the module but also decrease its computational cost. In practice, configurations such as <code>reduction_ratio = 16</code> usually provide a good balance between modeling capacity and efficiency.</p> <p>The Convolutional Block Attention Module (CBAM) extends the SE idea by sequentially incorporating attention both in the channel domain and in the spatial domain. First, it applies a channel attention module conceptually similar to SE, but combining information from global average pooling and global max pooling. Subsequently, it applies a spatial attention module that analyzes the distribution of activations across channels to determine which regions of the image are most relevant.</p> <p>The channel attention module in CBAM is built from two parallel paths. One path receives as input the output of a global average pooling and the other uses the output of a global max pooling, both computed over the spatial dimensions for each channel. Each of these summaries is processed by a small $1 \\times 1$ convolutional network that acts as a shared fully connected projection. The two resulting outputs are combined by element-wise addition and then passed through a sigmoid function to obtain a channel attention map that modulates the contribution of each channel.</p> <p>The spatial attention module is applied to the feature maps already recalibrated by channel. To this end, two single-channel spatial maps are computed by aggregating over the channel dimension using mean and maximum operations. These two maps are concatenated along the channel axis and processed by a convolution of size $k \\times k$, typically with $k = 7$, followed by a sigmoid activation. The result is a spatial attention map that is applied multiplicatively to the signal, modulating the importance of each spatial position $(i, j)$ in the image.</p> <p>The following code presents the implementation of CBAM (channel and spatial attention) and its integration into a residual block.</p> In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels: int, reduction_ratio: int = 16) -&gt; None:\n        super().__init__()\n        reduced_channels = max(in_channels // reduction_ratio, 1)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        # Shared MLP implemented with 1x1 convolutions\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, reduced_channels, kernel_size=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(reduced_channels, in_channels, kernel_size=1, bias=False),\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        attention = self.sigmoid(avg_out + max_out)\n        return x * attention\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size: int = 7) -&gt; None:\n        super().__init__()\n        padding = (kernel_size - 1) // 2\n        self.conv = nn.Conv2d(\n            2, 1, kernel_size=kernel_size, padding=padding, bias=False\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Channel-wise average and max projections\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        combined = torch.cat([avg_out, max_out], dim=1)\n        attention = self.sigmoid(self.conv(combined))\n        return x * attention\n\n\nclass CBAM(nn.Module):\n    def __init__(\n        self, in_channels: int, reduction_ratio: int = 16, kernel_size: int = 7\n    ) -&gt; None:\n        super().__init__()\n        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        return x\n\n\nclass CBAMResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.cbam = CBAM(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(out_channels),\n            )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        identity = self.shortcut(x)\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.cbam(out)\n        out += identity\n        out = F.relu(out)\n        return out\n</pre> import torch import torch.nn as nn import torch.nn.functional as F   class ChannelAttention(nn.Module):     def __init__(self, in_channels: int, reduction_ratio: int = 16) -&gt; None:         super().__init__()         reduced_channels = max(in_channels // reduction_ratio, 1)         self.avg_pool = nn.AdaptiveAvgPool2d(1)         self.max_pool = nn.AdaptiveMaxPool2d(1)         # Shared MLP implemented with 1x1 convolutions         self.fc = nn.Sequential(             nn.Conv2d(in_channels, reduced_channels, kernel_size=1, bias=False),             nn.ReLU(inplace=True),             nn.Conv2d(reduced_channels, in_channels, kernel_size=1, bias=False),         )         self.sigmoid = nn.Sigmoid()      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         avg_out = self.fc(self.avg_pool(x))         max_out = self.fc(self.max_pool(x))         attention = self.sigmoid(avg_out + max_out)         return x * attention   class SpatialAttention(nn.Module):     def __init__(self, kernel_size: int = 7) -&gt; None:         super().__init__()         padding = (kernel_size - 1) // 2         self.conv = nn.Conv2d(             2, 1, kernel_size=kernel_size, padding=padding, bias=False         )         self.sigmoid = nn.Sigmoid()      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         # Channel-wise average and max projections         avg_out = torch.mean(x, dim=1, keepdim=True)         max_out, _ = torch.max(x, dim=1, keepdim=True)         combined = torch.cat([avg_out, max_out], dim=1)         attention = self.sigmoid(self.conv(combined))         return x * attention   class CBAM(nn.Module):     def __init__(         self, in_channels: int, reduction_ratio: int = 16, kernel_size: int = 7     ) -&gt; None:         super().__init__()         self.channel_attention = ChannelAttention(in_channels, reduction_ratio)         self.spatial_attention = SpatialAttention(kernel_size)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.channel_attention(x)         x = self.spatial_attention(x)         return x   class CBAMResidualBlock(nn.Module):     def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -&gt; None:         super().__init__()         self.conv1 = nn.Conv2d(             in_channels,             out_channels,             kernel_size=3,             stride=stride,             padding=1,             bias=False,         )         self.bn1 = nn.BatchNorm2d(out_channels)         self.conv2 = nn.Conv2d(             out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False         )         self.bn2 = nn.BatchNorm2d(out_channels)         self.cbam = CBAM(out_channels)         self.shortcut = nn.Sequential()         if stride != 1 or in_channels != out_channels:             self.shortcut = nn.Sequential(                 nn.Conv2d(                     in_channels, out_channels, kernel_size=1, stride=stride, bias=False                 ),                 nn.BatchNorm2d(out_channels),             )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         identity = self.shortcut(x)         out = F.relu(self.bn1(self.conv1(x)))         out = self.bn2(self.conv2(out))         out = self.cbam(out)         out += identity         out = F.relu(out)         return out <p>The following code fragment performs a basic check of the CBAM module, analogous to the test applied in the case of the SE block. It validates that the input and output have the same shape and reports the number of parameters of the module.</p> In\u00a0[4]: Copied! <pre>import torch\n\n\ndef test_cbam() -&gt; None:\n    x = torch.randn(2, 64, 32, 32)  # Batch of 2, 64 channels, 32x32 feature map\n    cbam = CBAM(in_channels=64)\n\n    output = cbam(x)\n\n    print(f\"Input shape:  {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"CBAM parameters: {sum(p.numel() for p in cbam.parameters())}\")\n\n    assert x.shape == output.shape, \"Shape mismatch\"\n    print(\"CBAM test passed\")\n\n\ntest_cbam()\n</pre> import torch   def test_cbam() -&gt; None:     x = torch.randn(2, 64, 32, 32)  # Batch of 2, 64 channels, 32x32 feature map     cbam = CBAM(in_channels=64)      output = cbam(x)      print(f\"Input shape:  {x.shape}\")     print(f\"Output shape: {output.shape}\")     print(f\"CBAM parameters: {sum(p.numel() for p in cbam.parameters())}\")      assert x.shape == output.shape, \"Shape mismatch\"     print(\"CBAM test passed\")   test_cbam() <pre>Input shape:  torch.Size([2, 64, 32, 32])\nOutput shape: torch.Size([2, 64, 32, 32])\nCBAM parameters: 610\nCBAM test passed\n</pre> <p>In practice, CBAM often provides consistent improvements over SE, since it combines channel-level and spatial attention in a complementary way. Spatial attention is particularly useful in tasks where the localization of objects or discriminative regions plays a critical role, such as object detection, semantic and instance segmentation, or recognition in scenarios with multiple instances per image.</p>"},{"location":"course/topic_04_computer_vision/section_09_attention.html#attention-mechanisms-in-convolutional-neural-networks","title":"Attention Mechanisms in Convolutional Neural Networks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_09_attention.html#squeeze-and-excitation-se-block","title":"Squeeze-and-Excitation (SE) Block\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_09_attention.html#convolutional-block-attention-module-cbam","title":"Convolutional Block Attention Module (CBAM)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html","title":"Interpretability in Convolutional Neural Networks","text":"<p>Interpretability in Convolutional Neural Networks (CNNs) is essential to understand the decisions made by the model, detect potential biases, and improve robustness against perturbations or distributional shifts. Due to their depth and the combination of convolutions with nonlinear activation functions, CNNs behave as highly complex systems whose internal mechanisms are difficult to inspect directly. For this reason, specific techniques are developed to visualize which regions or features of the input signal contribute most significantly to the predictions.</p> <p>This document describes and implements several of the most widely used methodologies for interpreting CNNs: saliency maps, Grad-CAM, Guided Grad-CAM (based on Guided Backpropagation), occlusion analysis, and Integrated Gradients. A complete, functional implementation on CIFAR-10 using an adapted ResNet-18 model is then presented, organized linearly for step-by-step execution and easily convertible into a Jupyter Notebook.</p> <p>Saliency maps rely on computing the gradient of the model output with respect to each input pixel. Intuitively, if a small variation in a pixel produces a significant change in the output associated with a specific class, that pixel is considered important for the decision. The absolute value of this gradient is used as a local relevance measure.</p> <p>Given a model $f(\\cdot)$ and an input image $x$, the saliency map for a class $c$ is defined as</p> <p>$$S = \\left| \\frac{\\partial f_c(x)}{\\partial x} \\right|$$</p> <p>When the network processes inputs with multiple channels (for example, RGB images), it is common to aggregate the channel-wise information to construct a two-dimensional map. A simple strategy is to take the maximum over the channel dimension:</p> <p>$$S_{i,j} = \\max_{k} \\left| \\frac{\\partial f_c(x)}{\\partial x_{k,i,j}} \\right|$$</p> <p>This map provides, for each spatial position $(i,j)$, a sensitivity measure of the class score $f_c$ with respect to perturbations of the corresponding pixels. Saliency maps are conceptually simple and computationally efficient; however, the resulting visualizations are often noisy and do not always align clearly with semantically interpretable regions of the image.</p> <p>The following code initializes the environment, defines basic configuration, and implements a class that generates saliency maps based on gradients, together with a visualization function that overlays the resulting map on the original image.</p> In\u00a0[1]: Copied! <pre>\"\"\"Interpretability in Convolutional Neural Networks\n\nComplete functional implementation with CIFAR-10\n\nImplemented techniques:\n1. Saliency Maps\n2. Grad-CAM\n3. Guided Grad-CAM (based on Guided Backpropagation)\n4. Occlusion Analysis\n5. Integrated Gradients\n\"\"\"\n\n# Standard libraries\n# IMPORTS\nimport warnings\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom scipy.ndimage import zoom\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\\n\")\n\n# GLOBAL CONFIGURATION\nCONFIG = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"batch_size\": 32,\n}\n\nCIFAR10_CLASSES = [\n    \"airplane\",\n    \"automobile\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n]\n\nprint(f\"Configuration: {CONFIG}\\n\")\n</pre> \"\"\"Interpretability in Convolutional Neural Networks  Complete functional implementation with CIFAR-10  Implemented techniques: 1. Saliency Maps 2. Grad-CAM 3. Guided Grad-CAM (based on Guided Backpropagation) 4. Occlusion Analysis 5. Integrated Gradients \"\"\"  # Standard libraries # IMPORTS import warnings  # 3pps import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torchvision.models as models from scipy.ndimage import zoom from torch.utils.data import DataLoader from torchvision import datasets, transforms   warnings.filterwarnings(\"ignore\")  print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\\n\")  # GLOBAL CONFIGURATION CONFIG = {     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",     \"batch_size\": 32, }  CIFAR10_CLASSES = [     \"airplane\",     \"automobile\",     \"bird\",     \"cat\",     \"deer\",     \"dog\",     \"frog\",     \"horse\",     \"ship\",     \"truck\", ]  print(f\"Configuration: {CONFIG}\\n\") <pre>PyTorch version: 2.9.1+cu128\nCUDA available: False\n\nConfiguration: {'device': 'cpu', 'batch_size': 32}\n\n</pre> <p>To illustrate the interpretability techniques, the CIFAR-10 dataset is used. CIFAR-10 contains color images of size $32 \\times 32$ belonging to ten different classes. The following function downloads and prepares the test set, applying a standard normalization that is widely used for this dataset.</p> In\u00a0[2]: Copied! <pre>def prepare_cifar10_data():\n    \"\"\"\n    Downloads and prepares the CIFAR-10 test set\n    with standard normalization.\n    \"\"\"\n    print(\"Preparing CIFAR-10...\")\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n        ]\n    )\n\n    test_dataset = datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=transform\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2\n    )\n\n    print(f\"Test: {len(test_dataset)} images\\n\")\n    return test_loader, test_dataset\n</pre> def prepare_cifar10_data():     \"\"\"     Downloads and prepares the CIFAR-10 test set     with standard normalization.     \"\"\"     print(\"Preparing CIFAR-10...\")     transform = transforms.Compose(         [             transforms.ToTensor(),             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),         ]     )      test_dataset = datasets.CIFAR10(         root=\"./data\", train=False, download=True, transform=transform     )      test_loader = DataLoader(         test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2     )      print(f\"Test: {len(test_dataset)} images\\n\")     return test_loader, test_dataset <p>A ResNet-18 model pretrained on ImageNet is used as the base and adapted to the characteristics of CIFAR-10. The adaptation consists of modifying the first convolutional layer to work more appropriately with $32 \\times 32$ images and adjusting the final fully connected layer to the number of classes in CIFAR-10. Although the model is loaded with pretrained ImageNet weights, the final layer is initialized randomly, so the performance may not be optimal without fine-tuning. However, this limitation does not affect the main purpose of the code, which is to illustrate interpretability techniques in a functional manner.</p> In\u00a0[3]: Copied! <pre>def load_pretrained_model():\n    \"\"\"\n    Loads a ResNet-18 pretrained on ImageNet and adapts it to CIFAR-10.\n    \"\"\"\n    print(\"Loading pretrained model...\")\n    model = models.resnet18(pretrained=True)\n\n    # Adapt the first layer to 32x32 images (remove initial max-pooling)\n    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    model.maxpool = nn.Identity()\n\n    # Adapt final layer for 10 CIFAR-10 classes\n    model.fc = nn.Linear(model.fc.in_features, 10)\n\n    model = model.to(CONFIG[\"device\"])\n    model.eval()\n\n    print(f\"Model loaded on {CONFIG['device']}\\n\")\n    return model\n</pre> def load_pretrained_model():     \"\"\"     Loads a ResNet-18 pretrained on ImageNet and adapts it to CIFAR-10.     \"\"\"     print(\"Loading pretrained model...\")     model = models.resnet18(pretrained=True)      # Adapt the first layer to 32x32 images (remove initial max-pooling)     model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)     model.maxpool = nn.Identity()      # Adapt final layer for 10 CIFAR-10 classes     model.fc = nn.Linear(model.fc.in_features, 10)      model = model.to(CONFIG[\"device\"])     model.eval()      print(f\"Model loaded on {CONFIG['device']}\\n\")     return model <p>The following implementation computes saliency maps via gradients and includes a visualization routine that facilitates the direct analysis of which image regions contribute most to the model's prediction.</p> In\u00a0[4]: Copied! <pre>print(\"=\" * 70)\nprint(\"1. SALIENCY MAPS\")\nprint(\"=\" * 70)\nprint(\n    \"\"\"Saliency maps compute the gradient of the output with respect\nto each image pixel, indicating which regions have the largest\ninfluence on the prediction.\n\nAdvantages:\n- Simple and efficient computation.\n- Shows the direct influence of pixels.\n\nLimitations:\n- Visualizations are often noisy.\n- Do not always align with semantically clear regions.\n\"\"\"\n)\n\n\nclass SaliencyMapGenerator:\n    \"\"\"Generates saliency maps using gradients.\"\"\"\n\n    def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:\n        self.model = model.to(device)\n        self.model.eval()\n        self.device = device\n\n    def generate_saliency(self, image: torch.Tensor, target_class: int | None = None):\n        \"\"\"\n        Computes the saliency map for a single image.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            target_class: Target class index; if None, the model prediction is used.\n\n        Returns:\n            2D saliency map (numpy array).\n        \"\"\"\n        image = image.to(self.device)\n        image.requires_grad = True\n\n        output = self.model(image)\n\n        if target_class is None:\n            target_class = output.argmax(dim=1)\n\n        self.model.zero_grad()\n        output[0, target_class].backward()\n\n        saliency = image.grad.data.abs()\n        # Channel aggregation: maximum along the channel axis\n        saliency, _ = torch.max(saliency, dim=1)\n\n        return saliency.squeeze().cpu().numpy()\n\n    def visualize_saliency(\n        self,\n        image: torch.Tensor,\n        original_image: np.ndarray,\n        target_class: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Visualizes the saliency map and its overlay on the original image.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            original_image: Denormalized image [H, W, 3] in [0, 1].\n            target_class: Target class; if None, the model prediction is used.\n        \"\"\"\n        saliency = self.generate_saliency(image, target_class)\n\n        # Normalize to [0, 1] for visualization\n        saliency = (saliency - saliency.min()) / (\n            saliency.max() - saliency.min() + 1e-8\n        )\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(original_image)\n        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n        axes[0].axis(\"off\")\n\n        axes[1].imshow(saliency, cmap=\"hot\")\n        axes[1].set_title(\"Saliency Map\", fontsize=12, fontweight=\"bold\")\n        axes[1].axis(\"off\")\n\n        axes[2].imshow(original_image)\n        axes[2].imshow(saliency, cmap=\"hot\", alpha=0.5)\n        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n        axes[2].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n</pre> print(\"=\" * 70) print(\"1. SALIENCY MAPS\") print(\"=\" * 70) print(     \"\"\"Saliency maps compute the gradient of the output with respect to each image pixel, indicating which regions have the largest influence on the prediction.  Advantages: - Simple and efficient computation. - Shows the direct influence of pixels.  Limitations: - Visualizations are often noisy. - Do not always align with semantically clear regions. \"\"\" )   class SaliencyMapGenerator:     \"\"\"Generates saliency maps using gradients.\"\"\"      def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:         self.model = model.to(device)         self.model.eval()         self.device = device      def generate_saliency(self, image: torch.Tensor, target_class: int | None = None):         \"\"\"         Computes the saliency map for a single image.          Args:             image: Tensor [1, 3, H, W] normalized.             target_class: Target class index; if None, the model prediction is used.          Returns:             2D saliency map (numpy array).         \"\"\"         image = image.to(self.device)         image.requires_grad = True          output = self.model(image)          if target_class is None:             target_class = output.argmax(dim=1)          self.model.zero_grad()         output[0, target_class].backward()          saliency = image.grad.data.abs()         # Channel aggregation: maximum along the channel axis         saliency, _ = torch.max(saliency, dim=1)          return saliency.squeeze().cpu().numpy()      def visualize_saliency(         self,         image: torch.Tensor,         original_image: np.ndarray,         target_class: int | None = None,     ) -&gt; None:         \"\"\"         Visualizes the saliency map and its overlay on the original image.          Args:             image: Tensor [1, 3, H, W] normalized.             original_image: Denormalized image [H, W, 3] in [0, 1].             target_class: Target class; if None, the model prediction is used.         \"\"\"         saliency = self.generate_saliency(image, target_class)          # Normalize to [0, 1] for visualization         saliency = (saliency - saliency.min()) / (             saliency.max() - saliency.min() + 1e-8         )          fig, axes = plt.subplots(1, 3, figsize=(15, 5))          axes[0].imshow(original_image)         axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")         axes[0].axis(\"off\")          axes[1].imshow(saliency, cmap=\"hot\")         axes[1].set_title(\"Saliency Map\", fontsize=12, fontweight=\"bold\")         axes[1].axis(\"off\")          axes[2].imshow(original_image)         axes[2].imshow(saliency, cmap=\"hot\", alpha=0.5)         axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")         axes[2].axis(\"off\")          plt.tight_layout()         plt.show() <pre>======================================================================\n1. SALIENCY MAPS\n======================================================================\nSaliency maps compute the gradient of the output with respect\nto each image pixel, indicating which regions have the largest\ninfluence on the prediction.\n\nAdvantages:\n- Simple and efficient computation.\n- Shows the direct influence of pixels.\n\nLimitations:\n- Visualizations are often noisy.\n- Do not always align with semantically clear regions.\n\n</pre> <p>Grad-CAM generates heatmaps that localize the regions of an image that contribute most strongly to the prediction for a specific class. Instead of operating directly on the pixels, Grad-CAM works on the activation maps of an internal convolutional layer, which tends to produce spatial relevance maps that are more structured and semantically interpretable.</p> <p>Let $A^k \\in \\mathbb{R}^{H \\times W}$ denote the activation map associated with channel $k$ of a selected convolutional layer. For a class $c$, importance coefficients are computed by performing a global average pooling of the gradients over the spatial dimensions:</p> <p>$$\\alpha_k = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\frac{\\partial f_c}{\\partial A_{ij}^k}$$</p> <p>Using these coefficients, a class-specific weighted activation map is constructed as</p> <p>$$L_c^{\\text{Grad-CAM}} = \\mathrm{ReLU}\\left( \\sum_k \\alpha_k A^k \\right)$$</p> <p>The ReLU function is applied to retain only positive contributions, under the assumption that activations that increase the class score are those to be highlighted. The spatial resolution of the Grad-CAM map is limited by the size of the activation maps of the chosen layer; therefore, the resulting map is often interpolated to match the size of the original image.</p> <p>The implementation below uses hooks to capture activations and gradients at the target layer and generates the corresponding Grad-CAM map.</p> In\u00a0[5]: Copied! <pre>print(\"=\" * 70)\nprint(\"2. GRAD-CAM (Gradient-weighted Class Activation Mapping)\")\nprint(\"=\" * 70)\nprint(\n    \"\"\"Grad-CAM generates heatmaps that highlight the regions of the image\nthat are most important for a specific class, using gradients with\nrespect to an internal convolutional layer.\n\nAdvantages:\n- More interpretable maps than basic saliency maps.\n- Localizes relevant object regions.\n\nLimitations:\n- Depends on the choice of the target layer.\n- Resolution is limited by the resolution of that layer.\n\"\"\"\n)\n\n\nclass GradCAM:\n    \"\"\"Grad-CAM implementation for a target layer of a CNN.\"\"\"\n\n    def __init__(\n        self, model: nn.Module, target_layer: str, device: str = \"cuda\"\n    ) -&gt; None:\n        self.model = model.to(device)\n        self.target_layer = target_layer\n        self.device = device\n        self.gradients: torch.Tensor | None = None\n        self.activations: torch.Tensor | None = None\n        self._register_hooks()\n\n    def _register_hooks(self) -&gt; None:\n        \"\"\"\n        Registers hooks on the target layer to capture activations and gradients\n        during forward and backward passes.\n        \"\"\"\n\n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n\n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n\n        for name, module in self.model.named_modules():\n            if name == self.target_layer:\n                module.register_forward_hook(forward_hook)\n                module.register_full_backward_hook(backward_hook)\n                break\n\n    def generate_cam(self, image: torch.Tensor, target_class: int | None = None):\n        \"\"\"\n        Generates the Grad-CAM map for an image and a target class.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            target_class: Target class; if None, the model prediction is used.\n\n        Returns:\n            cam: Normalized 2D Grad-CAM map (numpy array).\n            target_class: Class used for the explanation.\n        \"\"\"\n        self.model.eval()\n        image = image.to(self.device)\n\n        output = self.model(image)\n\n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n\n        self.model.zero_grad()\n        output[0, target_class].backward()\n\n        # Weights: global average of gradients over H x W\n        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n        cam = torch.relu(cam)\n        cam = cam.squeeze().cpu().numpy()\n        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n\n        return cam, target_class\n\n    def visualize_cam(\n        self,\n        image: torch.Tensor,\n        original_image: np.ndarray,\n        target_class: int | None = None,\n    ):\n        \"\"\"\n        Visualizes the Grad-CAM map and its overlay on the original image.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            original_image: Denormalized image [H, W, 3] in [0, 1].\n            target_class: Target class; if None, the model prediction is used.\n        \"\"\"\n        cam, pred_class = self.generate_cam(image, target_class)\n\n        # Resize the map to the original image size via interpolation\n        cam_resized = zoom(\n            cam,\n            (\n                original_image.shape[0] / cam.shape[0],\n                original_image.shape[1] / cam.shape[1],\n            ),\n        )\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(original_image)\n        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n        axes[0].axis(\"off\")\n\n        axes[1].imshow(cam_resized, cmap=\"jet\")\n        axes[1].set_title(\n            f\"Grad-CAM (Class: {CIFAR10_CLASSES[pred_class]})\",\n            fontsize=12,\n            fontweight=\"bold\",\n        )\n        axes[1].axis(\"off\")\n\n        axes[2].imshow(original_image)\n        axes[2].imshow(cam_resized, cmap=\"jet\", alpha=0.5)\n        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n        axes[2].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n        return cam_resized, pred_class\n</pre> print(\"=\" * 70) print(\"2. GRAD-CAM (Gradient-weighted Class Activation Mapping)\") print(\"=\" * 70) print(     \"\"\"Grad-CAM generates heatmaps that highlight the regions of the image that are most important for a specific class, using gradients with respect to an internal convolutional layer.  Advantages: - More interpretable maps than basic saliency maps. - Localizes relevant object regions.  Limitations: - Depends on the choice of the target layer. - Resolution is limited by the resolution of that layer. \"\"\" )   class GradCAM:     \"\"\"Grad-CAM implementation for a target layer of a CNN.\"\"\"      def __init__(         self, model: nn.Module, target_layer: str, device: str = \"cuda\"     ) -&gt; None:         self.model = model.to(device)         self.target_layer = target_layer         self.device = device         self.gradients: torch.Tensor | None = None         self.activations: torch.Tensor | None = None         self._register_hooks()      def _register_hooks(self) -&gt; None:         \"\"\"         Registers hooks on the target layer to capture activations and gradients         during forward and backward passes.         \"\"\"          def forward_hook(module, input, output):             self.activations = output.detach()          def backward_hook(module, grad_input, grad_output):             self.gradients = grad_output[0].detach()          for name, module in self.model.named_modules():             if name == self.target_layer:                 module.register_forward_hook(forward_hook)                 module.register_full_backward_hook(backward_hook)                 break      def generate_cam(self, image: torch.Tensor, target_class: int | None = None):         \"\"\"         Generates the Grad-CAM map for an image and a target class.          Args:             image: Tensor [1, 3, H, W] normalized.             target_class: Target class; if None, the model prediction is used.          Returns:             cam: Normalized 2D Grad-CAM map (numpy array).             target_class: Class used for the explanation.         \"\"\"         self.model.eval()         image = image.to(self.device)          output = self.model(image)          if target_class is None:             target_class = output.argmax(dim=1).item()          self.model.zero_grad()         output[0, target_class].backward()          # Weights: global average of gradients over H x W         weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)         cam = torch.sum(weights * self.activations, dim=1, keepdim=True)         cam = torch.relu(cam)         cam = cam.squeeze().cpu().numpy()         cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)          return cam, target_class      def visualize_cam(         self,         image: torch.Tensor,         original_image: np.ndarray,         target_class: int | None = None,     ):         \"\"\"         Visualizes the Grad-CAM map and its overlay on the original image.          Args:             image: Tensor [1, 3, H, W] normalized.             original_image: Denormalized image [H, W, 3] in [0, 1].             target_class: Target class; if None, the model prediction is used.         \"\"\"         cam, pred_class = self.generate_cam(image, target_class)          # Resize the map to the original image size via interpolation         cam_resized = zoom(             cam,             (                 original_image.shape[0] / cam.shape[0],                 original_image.shape[1] / cam.shape[1],             ),         )          fig, axes = plt.subplots(1, 3, figsize=(15, 5))          axes[0].imshow(original_image)         axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")         axes[0].axis(\"off\")          axes[1].imshow(cam_resized, cmap=\"jet\")         axes[1].set_title(             f\"Grad-CAM (Class: {CIFAR10_CLASSES[pred_class]})\",             fontsize=12,             fontweight=\"bold\",         )         axes[1].axis(\"off\")          axes[2].imshow(original_image)         axes[2].imshow(cam_resized, cmap=\"jet\", alpha=0.5)         axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")         axes[2].axis(\"off\")          plt.tight_layout()         plt.show()          return cam_resized, pred_class <pre>======================================================================\n2. GRAD-CAM (Gradient-weighted Class Activation Mapping)\n======================================================================\nGrad-CAM generates heatmaps that highlight the regions of the image\nthat are most important for a specific class, using gradients with\nrespect to an internal convolutional layer.\n\nAdvantages:\n- More interpretable maps than basic saliency maps.\n- Localizes relevant object regions.\n\nLimitations:\n- Depends on the choice of the target layer.\n- Resolution is limited by the resolution of that layer.\n\n</pre> <p>Guided Backpropagation modifies the gradient flow through ReLU units by forcing to zero those gradients that are negative both in the activation and in the incoming gradient. This filtering yields sharper gradient maps that focus on features considered relevant.</p> <p>Guided Grad-CAM combines the global localization capability of Grad-CAM with the pixel-level detail of Guided Backpropagation. The usual procedure consists of three sequential steps: first, a Grad-CAM map is computed for the target class; second, guided gradients with respect to the input image are obtained; finally, the Grad-CAM map is upsampled to the input resolution and multiplied elementwise by the guided gradients. The result is a high-resolution visualization in which edges and fine details inside the Grad-CAM-relevant regions are emphasized.</p> <p>The following code implements Guided Backpropagation. This implementation integrates naturally with the <code>GradCAM</code> class to build Guided Grad-CAM by multiplying the resized Grad-CAM map by the guided gradients.</p> In\u00a0[6]: Copied! <pre>print(\"=\" * 70)\nprint(\"3. GUIDED GRAD-CAM\")\nprint(\"=\" * 70)\nprint(\n    \"\"\"Guided Grad-CAM combines Grad-CAM with Guided Backpropagation\nto obtain high-resolution visualizations that are both\nspatially precise and detailed at the pixel level.\n\nThis script implements Guided Backpropagation,\nwhich can be combined with Grad-CAM maps.\n\"\"\"\n)\n\n\nclass GuidedBackprop:\n    \"\"\"Guided Backpropagation implementation for a CNN.\"\"\"\n\n    def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:\n        self.model = model.to(device)\n        self.device = device\n        self._register_hooks()\n\n    def _register_hooks(self) -&gt; None:\n        \"\"\"\n        Registers hooks on ReLU layers to filter negative gradients\n        during the backward pass.\n        \"\"\"\n\n        def backward_hook(module, grad_input, grad_output):\n            if len(grad_input) &gt; 0 and grad_input[0] is not None:\n                return (torch.clamp(grad_input[0], min=0.0),)\n            return grad_input\n\n        for module in self.model.modules():\n            if isinstance(module, nn.ReLU):\n                module.register_full_backward_hook(backward_hook)\n\n    def generate_gradients(self, image: torch.Tensor, target_class: int | None = None):\n        \"\"\"\n        Generates guided gradients with respect to the input image.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            target_class: Target class; if None, the model prediction is used.\n\n        Returns:\n            Guided gradients as a numpy array [3, H, W].\n        \"\"\"\n        self.model.eval()\n        image = image.to(self.device)\n        image.requires_grad = True\n\n        output = self.model(image)\n\n        if target_class is None:\n            target_class = output.argmax(dim=1)\n\n        self.model.zero_grad()\n        output[0, target_class].backward()\n\n        gradients = image.grad.data.cpu().numpy()[0]\n        return gradients\n</pre> print(\"=\" * 70) print(\"3. GUIDED GRAD-CAM\") print(\"=\" * 70) print(     \"\"\"Guided Grad-CAM combines Grad-CAM with Guided Backpropagation to obtain high-resolution visualizations that are both spatially precise and detailed at the pixel level.  This script implements Guided Backpropagation, which can be combined with Grad-CAM maps. \"\"\" )   class GuidedBackprop:     \"\"\"Guided Backpropagation implementation for a CNN.\"\"\"      def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:         self.model = model.to(device)         self.device = device         self._register_hooks()      def _register_hooks(self) -&gt; None:         \"\"\"         Registers hooks on ReLU layers to filter negative gradients         during the backward pass.         \"\"\"          def backward_hook(module, grad_input, grad_output):             if len(grad_input) &gt; 0 and grad_input[0] is not None:                 return (torch.clamp(grad_input[0], min=0.0),)             return grad_input          for module in self.model.modules():             if isinstance(module, nn.ReLU):                 module.register_full_backward_hook(backward_hook)      def generate_gradients(self, image: torch.Tensor, target_class: int | None = None):         \"\"\"         Generates guided gradients with respect to the input image.          Args:             image: Tensor [1, 3, H, W] normalized.             target_class: Target class; if None, the model prediction is used.          Returns:             Guided gradients as a numpy array [3, H, W].         \"\"\"         self.model.eval()         image = image.to(self.device)         image.requires_grad = True          output = self.model(image)          if target_class is None:             target_class = output.argmax(dim=1)          self.model.zero_grad()         output[0, target_class].backward()          gradients = image.grad.data.cpu().numpy()[0]         return gradients <pre>======================================================================\n3. GUIDED GRAD-CAM\n======================================================================\nGuided Grad-CAM combines Grad-CAM with Guided Backpropagation\nto obtain high-resolution visualizations that are both\nspatially precise and detailed at the pixel level.\n\nThis script implements Guided Backpropagation,\nwhich can be combined with Grad-CAM maps.\n\n</pre> <p>Occlusion analysis adopts a complementary viewpoint to gradient-based methods. Instead of exploring the internal sensitivity of the model, it modifies the input explicitly. Small regions (patches) of the image are systematically occluded, and the effect on the probability assigned to a given class is measured. When occluding a region significantly decreases the probability, that region is interpreted as important for the prediction.</p> <p>Formally, for each position $(i,j)$ of a sliding window, an occluded version of the image $x^{(i,j)}$ is constructed, and the difference</p> <p>$$\\Delta p_c^{(i,j)} = p_c(x) - p_c\\bigl(x^{(i,j)}\\bigr)$$</p> <p>is evaluated, where $p_c(x)$ denotes the model probability assigned to class $c$. The resulting sensitivity map directly quantifies the importance of each region in terms of its impact on the model's confidence. This technique is independent of gradients and specific architectural details, although its computational cost increases with image resolution, due to the large number of model evaluations required.</p> <p>The class below implements a simple occlusion analysis, allowing the patch size and stride of the sliding window to be adjusted.</p> In\u00a0[7]: Copied! <pre>print(\"=\" * 70)\nprint(\"4. OCCLUSION ANALYSIS\")\nprint(\"=\" * 70)\nprint(\n    \"\"\"Systematically occludes regions of the image to observe\nhow the prediction changes, revealing which areas are critical.\n\nAdvantages:\n- Direct interpretation at the input level.\n- Does not require gradients or internal access to the architecture.\n\nLimitations:\n- High computational cost.\n- Sensitive to patch size and stride.\n\"\"\"\n)\n\n\nclass OcclusionAnalysis:\n    \"\"\"Occlusion analysis for obtaining sensitivity maps.\"\"\"\n\n    def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:\n        self.model = model.to(device)\n        self.model.eval()\n        self.device = device\n\n    def analyze(\n        self,\n        image: torch.Tensor,\n        target_class: int | None = None,\n        patch_size: int = 4,\n        stride: int = 2,\n    ):\n        \"\"\"\n        Computes a sensitivity map via systematic occlusion.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            target_class: Target class; if None, the model prediction is used.\n            patch_size: Side length of the square occlusion patch in pixels.\n            stride: Stride of the sliding occlusion window.\n\n        Returns:\n            2D sensitivity map (numpy array).\n        \"\"\"\n        image = image.to(self.device)\n\n        with torch.no_grad():\n            output = self.model(image)\n            if target_class is None:\n                target_class = output.argmax(dim=1).item()\n            baseline_prob = torch.softmax(output, dim=1)[0, target_class].item()\n\n        _, _, h, w = image.shape\n        sensitivity_map = np.zeros((h, w))\n\n        for i in range(0, h - patch_size + 1, stride):\n            for j in range(0, w - patch_size + 1, stride):\n                occluded_image = image.clone()\n                occluded_image[:, :, i : i + patch_size, j : j + patch_size] = 0\n\n                with torch.no_grad():\n                    output = self.model(occluded_image)\n                    prob = torch.softmax(output, dim=1)[0, target_class].item()\n\n                sensitivity = baseline_prob - prob\n                current = sensitivity_map[i : i + patch_size, j : j + patch_size].mean()\n                sensitivity_map[i : i + patch_size, j : j + patch_size] = max(\n                    current, sensitivity\n                )\n\n        return sensitivity_map\n\n    def visualize(\n        self,\n        image: torch.Tensor,\n        original_image: np.ndarray,\n        target_class: int | None = None,\n        patch_size: int = 4,\n        stride: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Visualizes the sensitivity map obtained via occlusion.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            original_image: Denormalized image [H, W, 3] in [0, 1].\n            target_class: Target class; if None, the model prediction is used.\n            patch_size: Occlusion patch size.\n            stride: Sliding window stride.\n        \"\"\"\n        print(f\"Analyzing with patch_size={patch_size}, stride={stride}...\")\n        sensitivity = self.analyze(image, target_class, patch_size, stride)\n        sensitivity = (sensitivity - sensitivity.min()) / (\n            sensitivity.max() - sensitivity.min() + 1e-8\n        )\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(original_image)\n        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n        axes[0].axis(\"off\")\n\n        axes[1].imshow(sensitivity, cmap=\"hot\")\n        axes[1].set_title(\"Sensitivity Map\", fontsize=12, fontweight=\"bold\")\n        axes[1].axis(\"off\")\n\n        axes[2].imshow(original_image)\n        axes[2].imshow(sensitivity, cmap=\"hot\", alpha=0.5)\n        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n        axes[2].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n</pre> print(\"=\" * 70) print(\"4. OCCLUSION ANALYSIS\") print(\"=\" * 70) print(     \"\"\"Systematically occludes regions of the image to observe how the prediction changes, revealing which areas are critical.  Advantages: - Direct interpretation at the input level. - Does not require gradients or internal access to the architecture.  Limitations: - High computational cost. - Sensitive to patch size and stride. \"\"\" )   class OcclusionAnalysis:     \"\"\"Occlusion analysis for obtaining sensitivity maps.\"\"\"      def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:         self.model = model.to(device)         self.model.eval()         self.device = device      def analyze(         self,         image: torch.Tensor,         target_class: int | None = None,         patch_size: int = 4,         stride: int = 2,     ):         \"\"\"         Computes a sensitivity map via systematic occlusion.          Args:             image: Tensor [1, 3, H, W] normalized.             target_class: Target class; if None, the model prediction is used.             patch_size: Side length of the square occlusion patch in pixels.             stride: Stride of the sliding occlusion window.          Returns:             2D sensitivity map (numpy array).         \"\"\"         image = image.to(self.device)          with torch.no_grad():             output = self.model(image)             if target_class is None:                 target_class = output.argmax(dim=1).item()             baseline_prob = torch.softmax(output, dim=1)[0, target_class].item()          _, _, h, w = image.shape         sensitivity_map = np.zeros((h, w))          for i in range(0, h - patch_size + 1, stride):             for j in range(0, w - patch_size + 1, stride):                 occluded_image = image.clone()                 occluded_image[:, :, i : i + patch_size, j : j + patch_size] = 0                  with torch.no_grad():                     output = self.model(occluded_image)                     prob = torch.softmax(output, dim=1)[0, target_class].item()                  sensitivity = baseline_prob - prob                 current = sensitivity_map[i : i + patch_size, j : j + patch_size].mean()                 sensitivity_map[i : i + patch_size, j : j + patch_size] = max(                     current, sensitivity                 )          return sensitivity_map      def visualize(         self,         image: torch.Tensor,         original_image: np.ndarray,         target_class: int | None = None,         patch_size: int = 4,         stride: int = 2,     ) -&gt; None:         \"\"\"         Visualizes the sensitivity map obtained via occlusion.          Args:             image: Tensor [1, 3, H, W] normalized.             original_image: Denormalized image [H, W, 3] in [0, 1].             target_class: Target class; if None, the model prediction is used.             patch_size: Occlusion patch size.             stride: Sliding window stride.         \"\"\"         print(f\"Analyzing with patch_size={patch_size}, stride={stride}...\")         sensitivity = self.analyze(image, target_class, patch_size, stride)         sensitivity = (sensitivity - sensitivity.min()) / (             sensitivity.max() - sensitivity.min() + 1e-8         )          fig, axes = plt.subplots(1, 3, figsize=(15, 5))          axes[0].imshow(original_image)         axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")         axes[0].axis(\"off\")          axes[1].imshow(sensitivity, cmap=\"hot\")         axes[1].set_title(\"Sensitivity Map\", fontsize=12, fontweight=\"bold\")         axes[1].axis(\"off\")          axes[2].imshow(original_image)         axes[2].imshow(sensitivity, cmap=\"hot\", alpha=0.5)         axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")         axes[2].axis(\"off\")          plt.tight_layout()         plt.show() <pre>======================================================================\n4. OCCLUSION ANALYSIS\n======================================================================\nSystematically occludes regions of the image to observe\nhow the prediction changes, revealing which areas are critical.\n\nAdvantages:\n- Direct interpretation at the input level.\n- Does not require gradients or internal access to the architecture.\n\nLimitations:\n- High computational cost.\n- Sensitive to patch size and stride.\n\n</pre> <p>Integrated Gradients is a theoretically grounded method to attribute a model prediction to input features. Instead of considering the gradient only at the point $x$, this method integrates gradients along a continuous path that connects a baseline $x'$ (for example, a completely black image) to the actual image $x$. This approach mitigates gradient saturation issues and satisfies desirable attribution axioms such as sensitivity and implementation invariance.</p> <p>Let $f_c$ denote the score for class $c$ (for example, the pre-softmax output). Integrated Gradients for dimension $i$ is defined as</p> <p>$$\\mathrm{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1}\\frac{\\partial f_c\\bigl(x' + \\alpha (x - x')\\bigr)}{\\partial x_i} \\, d\\alpha$$</p> <p>In practice, the integral is approximated by a discrete sum over $m$ uniformly spaced steps:</p> <p>$$\\mathrm{IG}_i(x) \\approx (x_i - x'_i) \\cdot \\frac{1}{m} \\sum_{k=1}^{m}\\frac{\\partial f_c\\bigl(x' + \\tfrac{k}{m}(x - x')\\bigr)}{\\partial x_i}$$</p> <p>Aggregating the absolute attributions over channels yields a spatial relevance map that is typically smoother and more stable than basic saliency maps, at the cost of requiring multiple model evaluations along the path between the baseline and the original image.</p> <p>The following implementation computes Integrated Gradients for a single image, allowing the baseline and the number of integration steps to be specified.</p> In\u00a0[8]: Copied! <pre>print(\"=\" * 70)\nprint(\"5. INTEGRATED GRADIENTS\")\nprint(\"=\" * 70)\nprint(\n    \"\"\"Method that attributes the prediction to input features by\nintegrating gradients along a path from a baseline\nto the actual image.\n\nAdvantages:\n- Strong theoretical foundation.\n- Mitigates gradient saturation issues.\n\nLimitations:\n- Requires multiple model evaluations.\n- Depends on the choice of baseline.\n\"\"\"\n)\n\n\nclass IntegratedGradients:\n    \"\"\"Integrated Gradients implementation for PyTorch models.\"\"\"\n\n    def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:\n        self.model = model.to(device)\n        self.model.eval()\n        self.device = device\n\n    def generate(\n        self,\n        image: torch.Tensor,\n        target_class: int | None = None,\n        baseline: torch.Tensor | None = None,\n        steps: int = 50,\n    ):\n        \"\"\"\n        Computes Integrated Gradients for an image and target class.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            target_class: Target class; if None, the model prediction is used.\n            baseline: Tensor [1, 3, H, W] used as reference. If None, a zero tensor is used.\n            steps: Number of points along the integration path.\n\n        Returns:\n            Numpy array [C, H, W] with per-channel attributions.\n        \"\"\"\n        if baseline is None:\n            baseline = torch.zeros_like(image)\n\n        baseline = baseline.to(self.device)\n        image = image.to(self.device)\n\n        with torch.no_grad():\n            output = self.model(image)\n            if target_class is None:\n                target_class = output.argmax(dim=1).item()\n\n        # Linear path between baseline and image\n        scaled_inputs = [\n            baseline + (float(i) / steps) * (image - baseline) for i in range(steps + 1)\n        ]\n        scaled_inputs = torch.cat(scaled_inputs, dim=0)\n        scaled_inputs.requires_grad = True\n\n        output = self.model(scaled_inputs)\n        self.model.zero_grad()\n\n        target_output = output[:, target_class]\n        target_output.backward(torch.ones_like(target_output))\n\n        gradients = scaled_inputs.grad\n        avg_gradients = torch.mean(gradients, dim=0, keepdim=True)\n        integrated_grads = (image - baseline) * avg_gradients\n\n        return integrated_grads.squeeze().cpu().detach().numpy()\n\n    def visualize(\n        self,\n        image: torch.Tensor,\n        original_image: np.ndarray,\n        target_class: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Visualizes spatially aggregated Integrated Gradients and its overlay.\n\n        Args:\n            image: Tensor [1, 3, H, W] normalized.\n            original_image: Denormalized image [H, W, 3] in [0, 1].\n            target_class: Target class; if None, the model prediction is used.\n        \"\"\"\n        print(\"Computing Integrated Gradients (50 steps)...\")\n        ig = self.generate(image, target_class)\n\n        ig_aggregated = np.sum(np.abs(ig), axis=0)\n        ig_aggregated = (ig_aggregated - ig_aggregated.min()) / (\n            ig_aggregated.max() - ig_aggregated.min() + 1e-8\n        )\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(original_image)\n        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n        axes[0].axis(\"off\")\n\n        axes[1].imshow(ig_aggregated, cmap=\"hot\")\n        axes[1].set_title(\"Integrated Gradients\", fontsize=12, fontweight=\"bold\")\n        axes[1].axis(\"off\")\n\n        axes[2].imshow(original_image)\n        axes[2].imshow(ig_aggregated, cmap=\"hot\", alpha=0.5)\n        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n        axes[2].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n</pre> print(\"=\" * 70) print(\"5. INTEGRATED GRADIENTS\") print(\"=\" * 70) print(     \"\"\"Method that attributes the prediction to input features by integrating gradients along a path from a baseline to the actual image.  Advantages: - Strong theoretical foundation. - Mitigates gradient saturation issues.  Limitations: - Requires multiple model evaluations. - Depends on the choice of baseline. \"\"\" )   class IntegratedGradients:     \"\"\"Integrated Gradients implementation for PyTorch models.\"\"\"      def __init__(self, model: nn.Module, device: str = \"cuda\") -&gt; None:         self.model = model.to(device)         self.model.eval()         self.device = device      def generate(         self,         image: torch.Tensor,         target_class: int | None = None,         baseline: torch.Tensor | None = None,         steps: int = 50,     ):         \"\"\"         Computes Integrated Gradients for an image and target class.          Args:             image: Tensor [1, 3, H, W] normalized.             target_class: Target class; if None, the model prediction is used.             baseline: Tensor [1, 3, H, W] used as reference. If None, a zero tensor is used.             steps: Number of points along the integration path.          Returns:             Numpy array [C, H, W] with per-channel attributions.         \"\"\"         if baseline is None:             baseline = torch.zeros_like(image)          baseline = baseline.to(self.device)         image = image.to(self.device)          with torch.no_grad():             output = self.model(image)             if target_class is None:                 target_class = output.argmax(dim=1).item()          # Linear path between baseline and image         scaled_inputs = [             baseline + (float(i) / steps) * (image - baseline) for i in range(steps + 1)         ]         scaled_inputs = torch.cat(scaled_inputs, dim=0)         scaled_inputs.requires_grad = True          output = self.model(scaled_inputs)         self.model.zero_grad()          target_output = output[:, target_class]         target_output.backward(torch.ones_like(target_output))          gradients = scaled_inputs.grad         avg_gradients = torch.mean(gradients, dim=0, keepdim=True)         integrated_grads = (image - baseline) * avg_gradients          return integrated_grads.squeeze().cpu().detach().numpy()      def visualize(         self,         image: torch.Tensor,         original_image: np.ndarray,         target_class: int | None = None,     ) -&gt; None:         \"\"\"         Visualizes spatially aggregated Integrated Gradients and its overlay.          Args:             image: Tensor [1, 3, H, W] normalized.             original_image: Denormalized image [H, W, 3] in [0, 1].             target_class: Target class; if None, the model prediction is used.         \"\"\"         print(\"Computing Integrated Gradients (50 steps)...\")         ig = self.generate(image, target_class)          ig_aggregated = np.sum(np.abs(ig), axis=0)         ig_aggregated = (ig_aggregated - ig_aggregated.min()) / (             ig_aggregated.max() - ig_aggregated.min() + 1e-8         )          fig, axes = plt.subplots(1, 3, figsize=(15, 5))          axes[0].imshow(original_image)         axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")         axes[0].axis(\"off\")          axes[1].imshow(ig_aggregated, cmap=\"hot\")         axes[1].set_title(\"Integrated Gradients\", fontsize=12, fontweight=\"bold\")         axes[1].axis(\"off\")          axes[2].imshow(original_image)         axes[2].imshow(ig_aggregated, cmap=\"hot\", alpha=0.5)         axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")         axes[2].axis(\"off\")          plt.tight_layout()         plt.show() <pre>======================================================================\n5. INTEGRATED GRADIENTS\n======================================================================\nMethod that attributes the prediction to input features by\nintegrating gradients along a path from a baseline\nto the actual image.\n\nAdvantages:\n- Strong theoretical foundation.\n- Mitigates gradient saturation issues.\n\nLimitations:\n- Requires multiple model evaluations.\n- Depends on the choice of baseline.\n\n</pre> <p>To interpret the results properly, CIFAR-10 images should be denormalized before visualization. The function below reverses the standard normalization applied during preprocessing and returns an image in a format suitable for <code>matplotlib</code>.</p> In\u00a0[9]: Copied! <pre>def denormalize_cifar10(tensor: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"\n    Denormalizes a CIFAR-10 tensor for visualization.\n\n    Args:\n        tensor: Tensor [3, H, W] normalized with CIFAR-10 mean and std.\n\n    Returns:\n        Image as numpy array [H, W, 3] with values in [0, 1].\n    \"\"\"\n    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n    std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n    denorm = tensor * std + mean\n    denorm = torch.clamp(denorm, 0, 1)\n    return denorm.permute(1, 2, 0).numpy()\n</pre> def denormalize_cifar10(tensor: torch.Tensor) -&gt; np.ndarray:     \"\"\"     Denormalizes a CIFAR-10 tensor for visualization.      Args:         tensor: Tensor [3, H, W] normalized with CIFAR-10 mean and std.      Returns:         Image as numpy array [H, W, 3] with values in [0, 1].     \"\"\"     mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)     std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)     denorm = tensor * std + mean     denorm = torch.clamp(denorm, 0, 1)     return denorm.permute(1, 2, 0).numpy() <p>Finally, all components are integrated into a coherent workflow that applies the different interpretability techniques to a test image from CIFAR-10. The pipeline includes data loading, model loading, sample selection, and the sequential execution of saliency maps, Grad-CAM, occlusion analysis, and Integrated Gradients. Guided Backpropagation is implemented and can be used to construct Guided Grad-CAM if one wishes to extend the pipeline.</p> In\u00a0[10]: Copied! <pre>def run_complete_pipeline() -&gt; None:\n    \"\"\"\n    Executes all interpretability techniques in an integrated way\n    on a single CIFAR-10 image.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"COMPLETE PIPELINE: INTERPRETABILITY IN CNNs\")\n    print(\"=\" * 70 + \"\\n\")\n\n    # Data\n    test_loader, _ = prepare_cifar10_data()\n\n    # Model\n    model = load_pretrained_model()\n\n    # Select a test image\n    print(\"Selecting test image...\")\n    images, labels = next(iter(test_loader))\n    image = images[0:1]\n    label = labels[0].item()\n    original_image = denormalize_cifar10(images[0].clone())\n    print(f\"True class: {CIFAR10_CLASSES[label]}\\n\")\n\n    # 1. Saliency Maps\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RUNNING: Saliency Maps\")\n    print(\"=\" * 70 + \"\\n\")\n    saliency_gen = SaliencyMapGenerator(model, CONFIG[\"device\"])\n    saliency_gen.visualize_saliency(image.clone(), original_image)\n\n    # 2. Grad-CAM\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RUNNING: Grad-CAM\")\n    print(\"=\" * 70 + \"\\n\")\n    grad_cam = GradCAM(model, target_layer=\"layer4\", device=CONFIG[\"device\"])\n    grad_cam.visualize_cam(image.clone(), original_image)\n\n    # 4. Occlusion Analysis\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RUNNING: Occlusion Analysis\")\n    print(\"=\" * 70 + \"\\n\")\n    occlusion = OcclusionAnalysis(model, device=CONFIG[\"device\"])\n    occlusion.visualize(image.clone(), original_image, patch_size=4, stride=2)\n\n    # 5. Integrated Gradients\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RUNNING: Integrated Gradients\")\n    print(\"=\" * 70 + \"\\n\")\n    ig = IntegratedGradients(model, device=CONFIG[\"device\"])\n    ig.visualize(image.clone(), original_image)\n\n\nif __name__ == \"__main__\":\n    run_complete_pipeline()\n</pre> def run_complete_pipeline() -&gt; None:     \"\"\"     Executes all interpretability techniques in an integrated way     on a single CIFAR-10 image.     \"\"\"     print(\"\\n\" + \"=\" * 70)     print(\"COMPLETE PIPELINE: INTERPRETABILITY IN CNNs\")     print(\"=\" * 70 + \"\\n\")      # Data     test_loader, _ = prepare_cifar10_data()      # Model     model = load_pretrained_model()      # Select a test image     print(\"Selecting test image...\")     images, labels = next(iter(test_loader))     image = images[0:1]     label = labels[0].item()     original_image = denormalize_cifar10(images[0].clone())     print(f\"True class: {CIFAR10_CLASSES[label]}\\n\")      # 1. Saliency Maps     print(\"\\n\" + \"=\" * 70)     print(\"RUNNING: Saliency Maps\")     print(\"=\" * 70 + \"\\n\")     saliency_gen = SaliencyMapGenerator(model, CONFIG[\"device\"])     saliency_gen.visualize_saliency(image.clone(), original_image)      # 2. Grad-CAM     print(\"\\n\" + \"=\" * 70)     print(\"RUNNING: Grad-CAM\")     print(\"=\" * 70 + \"\\n\")     grad_cam = GradCAM(model, target_layer=\"layer4\", device=CONFIG[\"device\"])     grad_cam.visualize_cam(image.clone(), original_image)      # 4. Occlusion Analysis     print(\"\\n\" + \"=\" * 70)     print(\"RUNNING: Occlusion Analysis\")     print(\"=\" * 70 + \"\\n\")     occlusion = OcclusionAnalysis(model, device=CONFIG[\"device\"])     occlusion.visualize(image.clone(), original_image, patch_size=4, stride=2)      # 5. Integrated Gradients     print(\"\\n\" + \"=\" * 70)     print(\"RUNNING: Integrated Gradients\")     print(\"=\" * 70 + \"\\n\")     ig = IntegratedGradients(model, device=CONFIG[\"device\"])     ig.visualize(image.clone(), original_image)   if __name__ == \"__main__\":     run_complete_pipeline() <pre>\n======================================================================\nCOMPLETE PIPELINE: INTERPRETABILITY IN CNNs\n======================================================================\n\nPreparing CIFAR-10...\n</pre> <pre>Test: 10000 images\n\nLoading pretrained model...\nModel loaded on cpu\n\nSelecting test image...\n</pre> <pre>True class: cat\n\n\n======================================================================\nRUNNING: Saliency Maps\n======================================================================\n\n</pre> <pre>\n======================================================================\nRUNNING: Grad-CAM\n======================================================================\n\n</pre> <pre>\n======================================================================\nRUNNING: Occlusion Analysis\n======================================================================\n\nAnalyzing with patch_size=4, stride=2...\n</pre> <pre>\n======================================================================\nRUNNING: Integrated Gradients\n======================================================================\n\nComputing Integrated Gradients (50 steps)...\n</pre> <p>This complete pipeline provides a practical framework for exploring interpretability in CNNs on CIFAR-10. Although the ResNet-18 model is not explicitly fine-tuned on this dataset within the script, the code structure allows the same analysis workflow to be reused with a model trained specifically on CIFAR-10 by simply replacing the model loading function with a version that retrieves weights adapted to the domain of interest.</p>"},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#interpretability-in-convolutional-neural-networks","title":"Interpretability in Convolutional Neural Networks\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#saliency-maps","title":"Saliency Maps\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#cifar-10-data-preparation","title":"CIFAR-10 Data Preparation\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#resnet-18-model-adapted-to-cifar-10","title":"ResNet-18 Model Adapted to CIFAR-10\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#saliency-maps-implementation-and-visualization","title":"Saliency Maps: Implementation and Visualization\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#grad-cam-gradient-weighted-class-activation-mapping","title":"Grad-CAM (Gradient-weighted Class Activation Mapping)\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#guided-backpropagation-and-guided-grad-cam","title":"Guided Backpropagation and Guided Grad-CAM\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#occlusion-analysis","title":"Occlusion Analysis\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#integrated-gradients","title":"Integrated Gradients\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#visualization-utilities","title":"Visualization Utilities\u00b6","text":""},{"location":"course/topic_04_computer_vision/section_10_interpretability.html#complete-interpretability-pipeline","title":"Complete Interpretability Pipeline\u00b6","text":""},{"location":"course/topic_05_sequential_models/index.html","title":"Introduction","text":"<p>This topic studies sequential models, with special emphasis on their application to text processing and time series. The starting point is the limitation of convolutional architectures when applied directly to textual data: Although convolutions capture local patterns efficiently, it is difficult for them to preserve and adequately model long dependencies in a sequence. As text lengthens, part of the relevant contextual information gradually dilutes or is lost, which limits these architectures' ability to understand long-range relationships.</p> <p>To address these limitations, models specifically designed for sequential data are introduced. The first block focuses on natural language processing (NLP), starting with tokenization techniques. Tokenization transforms raw text into a sequence of manageable units (tokens), which can be words, subwords, or characters. In this process, input text cleaning is usually applied: punctuation marks, emojis, or other symbols are removed or normalized according to the task, in order to obtain a more homogeneous representation. On these tokens, a numerical representation that models can process is subsequently built.</p> <p>Once the text is tokenized, the use of embedding layers is introduced, which allow mapping each token to a dense vector in a fixed-dimensional space. These embeddings can be learned from scratch during model training, or initialized from pretrained dictionaries and already trained tokenizers, leveraging prior knowledge accumulated in large corpora. In this context, the advantages and disadvantages of both strategies are analyzed: learning task-specific embeddings versus reuse and fine-tuning of pre-existing embeddings.</p> <p>On these vector representations, the first classical sequential architectures are studied, based on recurrent neural networks (RNN). RNNs process the sequence step by step, maintaining a hidden state that acts as memory of what has already been seen. However, they present important limitations, particularly the vanishing gradient problem: When sequences are long or the network is deep, gradients that propagate toward distant time steps tend to become very small, hindering the learning of long-term dependencies and making training unstable or inefficient.</p> <p>To mitigate these problems, LSTM (Long Short-Term Memory) are introduced, a variant of recurrent networks that incorporates memory cells and gate mechanisms. These gates explicitly control what information is stored, what is forgotten, and what is exposed at each time step, allowing information to remain relevant for longer intervals. Their internal structure, the role of input, forget, and output gates, and the advantages they provide over simple RNNs are analyzed.</p> <p>In addition to text, sequential models are naturally applied to time series. In this context, autoencoders and recurrent or convolutional variants are presented as tools for unsupervised learning of temporal patterns. These models allow detecting anomalies and unusual patterns by comparing the observed signal with the reconstruction produced by the decoder, as well as identifying samples that are out of distribution. The basic encoder-decoder configuration, the reconstruction function, and the use of error thresholds for decision-making are discussed.</p> <p>Next, the architectures that constitute the state of the art in 2025 for sequential processing are introduced: Transformers. These models are based on attention mechanisms, which allow each position in the sequence to relate directly to any other, capturing dependencies at multiple scales without resorting to explicit recurrences. A decisive advantage of Transformers is their capacity for parallel processing of tokens during the training phase, which enables very efficient exploitation of accelerated hardware (GPUs, TPUs) and overcomes one of the main limitations of RNNs and LSTMs, whose sequential processing hinders parallelization and slows convergence.</p> <p>On the basis of the standard Transformer, different derived architectures and extensions are studied, including Mixture of Experts (MoE) approaches. In these models, multiple experts are trained (for example, several Transformer-type models or related variants) and their predictions are combined through a routing module (gating network) that decides which experts to activate for each input. This scheme allows effectively increasing model capacity while maintaining controlled computational cost, by activating only a subset of experts for each input.</p> <p>Finally, the use of pretrained models in the sequential domain is explored, both for language and multimodal, with special attention to those with open weights or whose architectures have been released by the research community and industry. The Transformers library from Hugging Face is presented as a standard tool for loading, using, and adapting pretrained models, both language models (LLMs of different sizes) and vision and multimodal models. It shows how to integrate pretrained tokenizers, how to reuse embeddings, and how to perform fine-tuning or inference-only tasks with relatively low implementation effort.</p>"},{"location":"course/topic_05_sequential_models/section_01_tokenization.html","title":"Tokenization","text":"<p>Tokenization constitutes one of the fundamental processes in natural language processing and deep learning applied to text. This process consists of the systematic decomposition of textual sequences into smaller units called \"tokens\", which can correspond to words, subwords, or even individual characters, depending on the strategy employed.</p> <p>The need for tokenization arises from an inherent limitation of computational systems: they operate exclusively with numerical representations. While humans process language naturally through linguistic symbols, neural network architectures require all information to be encoded in the form of numerical vectors. Tokenization therefore acts as a bridge between the linguistic domain and the mathematical domain, allowing machine learning models to process, analyze, and generate text effectively.</p> <p>The most intuitive approach to tokenization consists of segmenting text using whitespace as natural delimiters between words. This method, although simple, allows understanding the fundamental principles of the tokenization process and establishes the foundations for more sophisticated techniques.</p> In\u00a0[1]: Copied! <pre># Basic tokenization example\ntexto = \"I like machine learning\"\n\n# Method 1: Using Python's split()\ntokens = texto.split()\nprint(\"Original text:\", texto)\nprint(\"Tokens:\", tokens)\nprint(\"Number of tokens:\", len(tokens))\n</pre> # Basic tokenization example texto = \"I like machine learning\"  # Method 1: Using Python's split() tokens = texto.split() print(\"Original text:\", texto) print(\"Tokens:\", tokens) print(\"Number of tokens:\", len(tokens)) <pre>Original text: I like machine learning\nTokens: ['I', 'like', 'machine', 'learning']\nNumber of tokens: 4\n</pre> <p>Expected output:</p> <pre><code>Original text: I like machine learning\nTokens: ['I', 'like', 'machine', 'learning']\nNumber of tokens: 4\n</code></pre> <p>To advance beyond simple text splitting, it is necessary to build a system that not only segments words, but also establishes a one-to-one correspondence between each unique word and a numerical identifier. This mapping allows representing any text as a sequence of numbers, facilitating its processing by machine learning models.</p> <p>The implementation of a basic tokenizer requires maintaining two complementary data structures: a dictionary that maps words to numbers and another that performs the inverse transformation. Additionally, a mechanism is needed to assign unique identifiers to each new word found during the training process.</p> In\u00a0[2]: Copied! <pre>class SimpleTokenizer:\n    \"\"\"\n    A basic tokenizer that splits text into words\n    and assigns them unique numbers.\n    \"\"\"\n\n    def __init__(self):\n        # Dictionary to store word -&gt; number\n        self.word_to_number = {}\n        # Inverse dictionary: number -&gt; word\n        self.number_to_word = {}\n        # Counter to assign numbers\n        self.next_number = 0\n\n    def train(self, texts):\n        \"\"\"\n        Learns which words exist in the texts.\n\n        Args:\n            texts: List of strings with training texts\n        \"\"\"\n        for text in texts:\n            # Convert to lowercase and split into words\n            words = text.lower().split()\n\n            # For each word, if we haven't seen it, assign it a number\n            for word in words:\n                if word not in self.word_to_number:\n                    self.word_to_number[word] = self.next_number\n                    self.number_to_word[self.next_number] = word\n                    self.next_number += 1\n\n        print(f\"Learned vocabulary: {len(self.word_to_number)} words\")\n\n    def encode(self, text):\n        \"\"\"\n        Converts text into a list of numbers.\n        \"\"\"\n        words = text.lower().split()\n        numbers = []\n\n        for word in words:\n            if word in self.word_to_number:\n                numbers.append(self.word_to_number[word])\n            else:\n                # If we don't know the word, use -1\n                numbers.append(-1)\n\n        return numbers\n\n    def decode(self, numbers):\n        \"\"\"\n        Converts a list of numbers back to text.\n        \"\"\"\n        words = []\n\n        for number in numbers:\n            if number in self.number_to_word:\n                words.append(self.number_to_word[number])\n            else:\n                words.append(\"[UNKNOWN]\")\n\n        return \" \".join(words)\n\n    def show_vocabulary(self):\n        \"\"\"Shows all words the tokenizer knows.\"\"\"\n        print(\"\\nComplete vocabulary:\")\n        print(\"-\" * 40)\n        for word, number in sorted(self.word_to_number.items(), key=lambda x: x[1]):\n            print(f\"{number:3d} -&gt; {word}\")\n\n\n# Usage example\nprint(\"=\" * 50)\nprint(\"EXAMPLE 1: Simple Tokenizer\")\nprint(\"=\" * 50)\n\n# Training texts\ntraining_texts = [\"i like programming\", \"i like learning\", \"programming is fun\"]\n\n# Create and train the tokenizer\ntokenizer = SimpleTokenizer()\ntokenizer.train(training_texts)\n\n# Show the learned vocabulary\ntokenizer.show_vocabulary()\n\n# Test encoding\nnew_text = \"i like learning programming\"\nprint(f\"\\nText to encode: '{new_text}'\")\n\nencoded = tokenizer.encode(new_text)\nprint(f\"Encoded text: {encoded}\")\n\ndecoded = tokenizer.decode(encoded)\nprint(f\"Decoded text: '{decoded}'\")\n\n# Test with unknown word\nunknown_text = \"i like cooking\"\nprint(f\"\\nText with new word: '{unknown_text}'\")\nencoded_unk = tokenizer.encode(unknown_text)\nprint(f\"Encoded: {encoded_unk}\")\nprint(\"Note: -1 indicates unknown word\")\n</pre> class SimpleTokenizer:     \"\"\"     A basic tokenizer that splits text into words     and assigns them unique numbers.     \"\"\"      def __init__(self):         # Dictionary to store word -&gt; number         self.word_to_number = {}         # Inverse dictionary: number -&gt; word         self.number_to_word = {}         # Counter to assign numbers         self.next_number = 0      def train(self, texts):         \"\"\"         Learns which words exist in the texts.          Args:             texts: List of strings with training texts         \"\"\"         for text in texts:             # Convert to lowercase and split into words             words = text.lower().split()              # For each word, if we haven't seen it, assign it a number             for word in words:                 if word not in self.word_to_number:                     self.word_to_number[word] = self.next_number                     self.number_to_word[self.next_number] = word                     self.next_number += 1          print(f\"Learned vocabulary: {len(self.word_to_number)} words\")      def encode(self, text):         \"\"\"         Converts text into a list of numbers.         \"\"\"         words = text.lower().split()         numbers = []          for word in words:             if word in self.word_to_number:                 numbers.append(self.word_to_number[word])             else:                 # If we don't know the word, use -1                 numbers.append(-1)          return numbers      def decode(self, numbers):         \"\"\"         Converts a list of numbers back to text.         \"\"\"         words = []          for number in numbers:             if number in self.number_to_word:                 words.append(self.number_to_word[number])             else:                 words.append(\"[UNKNOWN]\")          return \" \".join(words)      def show_vocabulary(self):         \"\"\"Shows all words the tokenizer knows.\"\"\"         print(\"\\nComplete vocabulary:\")         print(\"-\" * 40)         for word, number in sorted(self.word_to_number.items(), key=lambda x: x[1]):             print(f\"{number:3d} -&gt; {word}\")   # Usage example print(\"=\" * 50) print(\"EXAMPLE 1: Simple Tokenizer\") print(\"=\" * 50)  # Training texts training_texts = [\"i like programming\", \"i like learning\", \"programming is fun\"]  # Create and train the tokenizer tokenizer = SimpleTokenizer() tokenizer.train(training_texts)  # Show the learned vocabulary tokenizer.show_vocabulary()  # Test encoding new_text = \"i like learning programming\" print(f\"\\nText to encode: '{new_text}'\")  encoded = tokenizer.encode(new_text) print(f\"Encoded text: {encoded}\")  decoded = tokenizer.decode(encoded) print(f\"Decoded text: '{decoded}'\")  # Test with unknown word unknown_text = \"i like cooking\" print(f\"\\nText with new word: '{unknown_text}'\") encoded_unk = tokenizer.encode(unknown_text) print(f\"Encoded: {encoded_unk}\") print(\"Note: -1 indicates unknown word\") <pre>==================================================\nEXAMPLE 1: Simple Tokenizer\n==================================================\nLearned vocabulary: 6 words\n\nComplete vocabulary:\n----------------------------------------\n  0 -&gt; i\n  1 -&gt; like\n  2 -&gt; programming\n  3 -&gt; learning\n  4 -&gt; is\n  5 -&gt; fun\n\nText to encode: 'i like learning programming'\nEncoded text: [0, 1, 3, 2]\nDecoded text: 'i like learning programming'\n\nText with new word: 'i like cooking'\nEncoded: [0, 1, -1]\nNote: -1 indicates unknown word\n</pre> <p>Tokenization in real-world natural language processing applications presents challenges that go beyond simple word-to-number conversion. Situations arise that require special treatment: words that did not appear during training, sequences of different lengths that must be processed in batches, and the need to explicitly mark the beginning and end of sequences.</p> <p>To address these issues, modern tokenization systems incorporate special tokens with specific functions. The padding token allows uniformizing sequence length, facilitating parallel processing. The unknown word token provides a consistent representation for terms not seen during training. The start and end of sequence tokens allow models to explicitly identify the boundaries of each input, which is especially relevant in text generation and machine translation tasks.</p> In\u00a0[3]: Copied! <pre>class TokenizerWithSpecials:\n    \"\"\"\n    Tokenizer that handles unknown words and padding.\n    \"\"\"\n\n    def __init__(self):\n        # Special tokens\n        self.PAD = \"[PAD]\"  # For padding short sequences\n        self.UNK = \"[UNK]\"  # For unknown words\n        self.SOS = \"[SOS]\"  # Start of Sequence\n        self.EOS = \"[EOS]\"  # End of Sequence\n\n        # Initialize dictionaries with special tokens\n        self.word_to_number = {self.PAD: 0, self.UNK: 1, self.SOS: 2, self.EOS: 3}\n\n        self.number_to_word = {0: self.PAD, 1: self.UNK, 2: self.SOS, 3: self.EOS}\n\n        self.next_number = 4\n\n    def train(self, texts):\n        \"\"\"Learns the vocabulary from texts.\"\"\"\n        for text in texts:\n            words = text.lower().split()\n\n            for word in words:\n                if word not in self.word_to_number:\n                    self.word_to_number[word] = self.next_number\n                    self.number_to_word[self.next_number] = word\n                    self.next_number += 1\n\n        print(f\"Vocabulary: {len(self.word_to_number)} words\")\n        print(f\"  - Special words: 4\")\n        print(f\"  - Normal words: {len(self.word_to_number) - 4}\")\n\n    def encode(self, text, add_special=True, fixed_length=None):\n        \"\"\"\n        Encodes text with advanced options.\n\n        Args:\n            text: Text to encode\n            add_special: Whether to add [SOS] and [EOS]\n            fixed_length: If specified, adjusts to this length\n        \"\"\"\n        words = text.lower().split()\n\n        # Convert words to numbers\n        numbers = []\n        for word in words:\n            if word in self.word_to_number:\n                numbers.append(self.word_to_number[word])\n            else:\n                numbers.append(self.word_to_number[self.UNK])\n\n        # Add start and end tokens if requested\n        if add_special:\n            numbers = (\n                [self.word_to_number[self.SOS]]\n                + numbers\n                + [self.word_to_number[self.EOS]]\n            )\n\n        # Adjust to fixed length if specified\n        if fixed_length is not None:\n            if len(numbers) &lt; fixed_length:\n                # Pad with PAD\n                numbers = numbers + [self.word_to_number[self.PAD]] * (\n                    fixed_length - len(numbers)\n                )\n            else:\n                # Truncate\n                numbers = numbers[:fixed_length]\n\n        return numbers\n\n    def decode(self, numbers, remove_special=True):\n        \"\"\"Decodes numbers to text.\"\"\"\n        words = []\n\n        for number in numbers:\n            if number in self.number_to_word:\n                word = self.number_to_word[number]\n\n                # Skip special tokens if requested\n                if remove_special and word in [self.PAD, self.UNK, self.SOS, self.EOS]:\n                    continue\n\n                words.append(word)\n\n        return \" \".join(words)\n\n\n# Usage examples\nprint(\"\\n\" + \"=\" * 50)\nprint(\"EXAMPLE 2: Special Tokens and Padding\")\nprint(\"=\" * 50)\n\n# Train\ntexts = [\"hello world\", \"python is great\", \"i like learning\"]\n\ntokenizer_v2 = TokenizerWithSpecials()\ntokenizer_v2.train(texts)\n\n# Encode without fixed length\ntext1 = \"hello python\"\nprint(f\"\\nText 1: '{text1}'\")\ncod1 = tokenizer_v2.encode(text1)\nprint(f\"Encoded: {cod1}\")\nprint(f\"Length: {len(cod1)}\")\n\n# Encode with fixed length\ntext2 = \"i like\"\nprint(f\"\\nText 2: '{text2}'\")\ncod2 = tokenizer_v2.encode(text2, fixed_length=10)\nprint(f\"Encoded (fixed length=10): {cod2}\")\nprint(f\"Length: {len(cod2)}\")\n\n# Decode\nprint(f\"Decoded: '{tokenizer_v2.decode(cod2)}'\")\n</pre> class TokenizerWithSpecials:     \"\"\"     Tokenizer that handles unknown words and padding.     \"\"\"      def __init__(self):         # Special tokens         self.PAD = \"[PAD]\"  # For padding short sequences         self.UNK = \"[UNK]\"  # For unknown words         self.SOS = \"[SOS]\"  # Start of Sequence         self.EOS = \"[EOS]\"  # End of Sequence          # Initialize dictionaries with special tokens         self.word_to_number = {self.PAD: 0, self.UNK: 1, self.SOS: 2, self.EOS: 3}          self.number_to_word = {0: self.PAD, 1: self.UNK, 2: self.SOS, 3: self.EOS}          self.next_number = 4      def train(self, texts):         \"\"\"Learns the vocabulary from texts.\"\"\"         for text in texts:             words = text.lower().split()              for word in words:                 if word not in self.word_to_number:                     self.word_to_number[word] = self.next_number                     self.number_to_word[self.next_number] = word                     self.next_number += 1          print(f\"Vocabulary: {len(self.word_to_number)} words\")         print(f\"  - Special words: 4\")         print(f\"  - Normal words: {len(self.word_to_number) - 4}\")      def encode(self, text, add_special=True, fixed_length=None):         \"\"\"         Encodes text with advanced options.          Args:             text: Text to encode             add_special: Whether to add [SOS] and [EOS]             fixed_length: If specified, adjusts to this length         \"\"\"         words = text.lower().split()          # Convert words to numbers         numbers = []         for word in words:             if word in self.word_to_number:                 numbers.append(self.word_to_number[word])             else:                 numbers.append(self.word_to_number[self.UNK])          # Add start and end tokens if requested         if add_special:             numbers = (                 [self.word_to_number[self.SOS]]                 + numbers                 + [self.word_to_number[self.EOS]]             )          # Adjust to fixed length if specified         if fixed_length is not None:             if len(numbers) &lt; fixed_length:                 # Pad with PAD                 numbers = numbers + [self.word_to_number[self.PAD]] * (                     fixed_length - len(numbers)                 )             else:                 # Truncate                 numbers = numbers[:fixed_length]          return numbers      def decode(self, numbers, remove_special=True):         \"\"\"Decodes numbers to text.\"\"\"         words = []          for number in numbers:             if number in self.number_to_word:                 word = self.number_to_word[number]                  # Skip special tokens if requested                 if remove_special and word in [self.PAD, self.UNK, self.SOS, self.EOS]:                     continue                  words.append(word)          return \" \".join(words)   # Usage examples print(\"\\n\" + \"=\" * 50) print(\"EXAMPLE 2: Special Tokens and Padding\") print(\"=\" * 50)  # Train texts = [\"hello world\", \"python is great\", \"i like learning\"]  tokenizer_v2 = TokenizerWithSpecials() tokenizer_v2.train(texts)  # Encode without fixed length text1 = \"hello python\" print(f\"\\nText 1: '{text1}'\") cod1 = tokenizer_v2.encode(text1) print(f\"Encoded: {cod1}\") print(f\"Length: {len(cod1)}\")  # Encode with fixed length text2 = \"i like\" print(f\"\\nText 2: '{text2}'\") cod2 = tokenizer_v2.encode(text2, fixed_length=10) print(f\"Encoded (fixed length=10): {cod2}\") print(f\"Length: {len(cod2)}\")  # Decode print(f\"Decoded: '{tokenizer_v2.decode(cod2)}'\") <pre>\n==================================================\nEXAMPLE 2: Special Tokens and Padding\n==================================================\nVocabulary: 12 words\n  - Special words: 4\n  - Normal words: 8\n\nText 1: 'hello python'\nEncoded: [2, 4, 6, 3]\nLength: 4\n\nText 2: 'i like'\nEncoded (fixed length=10): [2, 9, 10, 3, 0, 0, 0, 0, 0, 0]\nLength: 10\nDecoded: 'i like'\n</pre> <p>Deep understanding of the tokenization process is facilitated through explicit visualization of the transformations the text undergoes at each stage. Observing how a sequence of words is converted into a sequence of numerical identifiers, and can subsequently be recovered as original text, allows identifying potential problems and understanding the system's behavior with different inputs.</p> In\u00a0[4]: Copied! <pre>def visualize_tokenization(tokenizer, texts):\n    \"\"\"\n    Visually shows how each text is tokenized.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZATION VISUALIZATION\")\n    print(\"=\" * 60)\n\n    for i, text in enumerate(texts, 1):\n        print(f\"\\n{i}. Original text:\")\n        print(f\"   '{text}'\")\n\n        # Encode\n        encoded = tokenizer.encode(text, add_special=True)\n\n        print(f\"\\n   Tokens (numbers):\")\n        print(f\"   {encoded}\")\n\n        print(f\"\\n   Visual representation:\")\n        # Show each token with its word\n        words = [\"[SOS]\"] + text.lower().split() + [\"[EOS]\"]\n        for word, number in zip(words, encoded):\n            print(f\"   {word:15} -&gt; {number:3}\")\n\n        print(f\"\\n   Decoded:\")\n        print(f\"   '{tokenizer.decode(encoded)}'\")\n        print(\"-\" * 60)\n\n\n# Visualization example\nexample_texts = [\n    \"python is great\",\n    \"i like programming\",\n    \"hello artificial intelligence\",\n]\n\nvisualize_tokenization(tokenizer_v2, example_texts)\n</pre> def visualize_tokenization(tokenizer, texts):     \"\"\"     Visually shows how each text is tokenized.     \"\"\"     print(\"\\n\" + \"=\" * 60)     print(\"TOKENIZATION VISUALIZATION\")     print(\"=\" * 60)      for i, text in enumerate(texts, 1):         print(f\"\\n{i}. Original text:\")         print(f\"   '{text}'\")          # Encode         encoded = tokenizer.encode(text, add_special=True)          print(f\"\\n   Tokens (numbers):\")         print(f\"   {encoded}\")          print(f\"\\n   Visual representation:\")         # Show each token with its word         words = [\"[SOS]\"] + text.lower().split() + [\"[EOS]\"]         for word, number in zip(words, encoded):             print(f\"   {word:15} -&gt; {number:3}\")          print(f\"\\n   Decoded:\")         print(f\"   '{tokenizer.decode(encoded)}'\")         print(\"-\" * 60)   # Visualization example example_texts = [     \"python is great\",     \"i like programming\",     \"hello artificial intelligence\", ]  visualize_tokenization(tokenizer_v2, example_texts) <pre>\n============================================================\nTOKENIZATION VISUALIZATION\n============================================================\n\n1. Original text:\n   'python is great'\n\n   Tokens (numbers):\n   [2, 6, 7, 8, 3]\n\n   Visual representation:\n   [SOS]           -&gt;   2\n   python          -&gt;   6\n   is              -&gt;   7\n   great           -&gt;   8\n   [EOS]           -&gt;   3\n\n   Decoded:\n   'python is great'\n------------------------------------------------------------\n\n2. Original text:\n   'i like programming'\n\n   Tokens (numbers):\n   [2, 9, 10, 1, 3]\n\n   Visual representation:\n   [SOS]           -&gt;   2\n   i               -&gt;   9\n   like            -&gt;  10\n   programming     -&gt;   1\n   [EOS]           -&gt;   3\n\n   Decoded:\n   'i like'\n------------------------------------------------------------\n\n3. Original text:\n   'hello artificial intelligence'\n\n   Tokens (numbers):\n   [2, 4, 1, 1, 3]\n\n   Visual representation:\n   [SOS]           -&gt;   2\n   hello           -&gt;   4\n   artificial      -&gt;   1\n   intelligence    -&gt;   1\n   [EOS]           -&gt;   3\n\n   Decoded:\n   'hello'\n------------------------------------------------------------\n</pre> <p>One of the most relevant technical aspects in textual sequence processing is the management of variable lengths. Natural texts present considerable diversity in terms of their length: from brief phrases of few words to extensive paragraphs with dozens or hundreds of tokens. However, neural network architectures, especially when processing multiple examples simultaneously in batches, require all input sequences to have uniform dimensions.</p> <p>Padding constitutes the standard solution to this problem. It consists of artificially extending shorter sequences until reaching a target length, typically determined by the longest sequence in the batch. This extension is performed through the insertion of special padding tokens that the model learns to ignore during processing. Alternatively, when a sequence exceeds the maximum allowed length, truncation is applied, preserving only the first tokens up to the established limit.</p> In\u00a0[5]: Copied! <pre>def compare_lengths(tokenizer, texts):\n    \"\"\"\n    Compares the lengths of different texts and shows\n    how padding uniformizes them.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"LENGTH COMPARISON\")\n    print(\"=\" * 60)\n\n    # Find maximum length\n    lengths = []\n    for text in texts:\n        cod = tokenizer.encode(text, add_special=True)\n        lengths.append(len(cod))\n\n    max_length = max(lengths)\n\n    print(f\"\\nMaximum length found: {max_length} tokens\")\n    print(\"\\nComparison:\")\n    print(\"-\" * 60)\n\n    for text in texts:\n        # Without padding\n        without_padding = tokenizer.encode(text, add_special=True)\n\n        # With padding\n        with_padding = tokenizer.encode(text, add_special=True, fixed_length=max_length)\n\n        print(f\"\\nText: '{text}'\")\n        print(f\"Without padding (length {len(without_padding)}): {without_padding}\")\n        print(f\"With padding (length {len(with_padding)}): {with_padding}\")\n\n        # Count how many PADs were added\n        num_pads = with_padding.count(0)\n        print(f\"PADs added: {num_pads}\")\n\n\n# Comparison example\ndifferent_texts = [\"hello\", \"python is great\", \"i like learning programming in python\"]\n\ncompare_lengths(tokenizer_v2, different_texts)\n</pre> def compare_lengths(tokenizer, texts):     \"\"\"     Compares the lengths of different texts and shows     how padding uniformizes them.     \"\"\"     print(\"\\n\" + \"=\" * 60)     print(\"LENGTH COMPARISON\")     print(\"=\" * 60)      # Find maximum length     lengths = []     for text in texts:         cod = tokenizer.encode(text, add_special=True)         lengths.append(len(cod))      max_length = max(lengths)      print(f\"\\nMaximum length found: {max_length} tokens\")     print(\"\\nComparison:\")     print(\"-\" * 60)      for text in texts:         # Without padding         without_padding = tokenizer.encode(text, add_special=True)          # With padding         with_padding = tokenizer.encode(text, add_special=True, fixed_length=max_length)          print(f\"\\nText: '{text}'\")         print(f\"Without padding (length {len(without_padding)}): {without_padding}\")         print(f\"With padding (length {len(with_padding)}): {with_padding}\")          # Count how many PADs were added         num_pads = with_padding.count(0)         print(f\"PADs added: {num_pads}\")   # Comparison example different_texts = [\"hello\", \"python is great\", \"i like learning programming in python\"]  compare_lengths(tokenizer_v2, different_texts) <pre>\n============================================================\nLENGTH COMPARISON\n============================================================\n\nMaximum length found: 8 tokens\n\nComparison:\n------------------------------------------------------------\n\nText: 'hello'\nWithout padding (length 3): [2, 4, 3]\nWith padding (length 8): [2, 4, 3, 0, 0, 0, 0, 0]\nPADs added: 5\n\nText: 'python is great'\nWithout padding (length 5): [2, 6, 7, 8, 3]\nWith padding (length 8): [2, 6, 7, 8, 3, 0, 0, 0]\nPADs added: 3\n\nText: 'i like learning programming in python'\nWithout padding (length 8): [2, 9, 10, 11, 1, 1, 6, 3]\nWith padding (length 8): [2, 9, 10, 11, 1, 1, 6, 3]\nPADs added: 0\n</pre> <p>The integration of all presented concepts materializes in the construction of a complete system capable of processing real-world text. A representative use case is product review analysis, where the objective consists of transforming opinions expressed in natural language into numerical representations that can subsequently feed sentiment classification models or other analysis tasks.</p> <p>This system integrates the tokenizer with special token management capabilities, length normalization, and vocabulary maintenance built from a training set. The resulting architecture allows processing new reviews consistently, applying the same transformations that will be used during machine learning model training.</p> In\u00a0[6]: Copied! <pre>class ReviewSystem:\n    \"\"\"\n    Complete system for processing product reviews.\n    \"\"\"\n\n    def __init__(self):\n        self.tokenizer = TokenizerWithSpecials()\n        self.reviews = []\n        self.labels = []  # 1 = positive, 0 = negative\n\n    def add_review(self, text, is_positive):\n        \"\"\"Adds a review to the system.\"\"\"\n        self.reviews.append(text)\n        self.labels.append(1 if is_positive else 0)\n\n    def train(self):\n        \"\"\"Trains the tokenizer with all reviews.\"\"\"\n        print(\"Training tokenizer with reviews...\")\n        self.tokenizer.train(self.reviews)\n\n    def process_review(self, text, length=15):\n        \"\"\"Processes a new review.\"\"\"\n        print(f\"\\nProcessing: '{text}'\")\n        print(\"-\" * 50)\n\n        # Tokenize\n        tokens = text.lower().split()\n        print(f\"1. Split into words: {tokens}\")\n\n        # Encode\n        encoded = self.tokenizer.encode(text, add_special=True, fixed_length=length)\n        print(f\"2. Convert to numbers: {encoded}\")\n\n        # Decode\n        decoded = self.tokenizer.decode(encoded)\n        print(f\"3. Decode: '{decoded}'\")\n\n        return encoded\n\n    def show_statistics(self):\n        \"\"\"Shows dataset statistics.\"\"\"\n        print(\"\\n\" + \"=\" * 60)\n        print(\"SYSTEM STATISTICS\")\n        print(\"=\" * 60)\n\n        print(f\"\\nTotal reviews: {len(self.reviews)}\")\n        print(f\"Positive reviews: {sum(self.labels)}\")\n        print(f\"Negative reviews: {len(self.labels) - sum(self.labels)}\")\n        print(f\"Vocabulary: {len(self.tokenizer.word_to_number)} words\")\n\n        # Lengths\n        lengths = [len(r.split()) for r in self.reviews]\n        print(f\"\\nAverage length: {sum(lengths)/len(lengths):.1f} words\")\n        print(f\"Minimum length: {min(lengths)} words\")\n        print(f\"Maximum length: {max(lengths)} words\")\n\n\n# Create the system\nprint(\"=\" * 60)\nprint(\"MINI PROJECT: Review System\")\nprint(\"=\" * 60)\n\nsystem = ReviewSystem()\n\n# Add training reviews\ntraining_reviews = [\n    (\"this product is excellent\", True),\n    (\"very bad quality do not recommend\", False),\n    (\"incredible i love it\", True),\n    (\"terrible experience\", False),\n    (\"perfect product arrived fast\", True),\n    (\"does not work properly\", False),\n]\n\nprint(\"\\nAdding reviews to the system...\")\nfor text, is_positive in training_reviews:\n    system.add_review(text, is_positive)\n    sentiment = \"POSITIVE\" if is_positive else \"NEGATIVE\"\n    print(f\"  - [{sentiment}] {text}\")\n\n# Train\nsystem.train()\n\n# Show statistics\nsystem.show_statistics()\n\n# Process new reviews\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PROCESSING NEW REVIEWS\")\nprint(\"=\" * 60)\n\nnew_reviews = [\n    \"excellent product very good\",\n    \"bad experience terrible\",\n    \"perfect recommend\",\n]\n\nfor review in new_reviews:\n    system.process_review(review)\n</pre> class ReviewSystem:     \"\"\"     Complete system for processing product reviews.     \"\"\"      def __init__(self):         self.tokenizer = TokenizerWithSpecials()         self.reviews = []         self.labels = []  # 1 = positive, 0 = negative      def add_review(self, text, is_positive):         \"\"\"Adds a review to the system.\"\"\"         self.reviews.append(text)         self.labels.append(1 if is_positive else 0)      def train(self):         \"\"\"Trains the tokenizer with all reviews.\"\"\"         print(\"Training tokenizer with reviews...\")         self.tokenizer.train(self.reviews)      def process_review(self, text, length=15):         \"\"\"Processes a new review.\"\"\"         print(f\"\\nProcessing: '{text}'\")         print(\"-\" * 50)          # Tokenize         tokens = text.lower().split()         print(f\"1. Split into words: {tokens}\")          # Encode         encoded = self.tokenizer.encode(text, add_special=True, fixed_length=length)         print(f\"2. Convert to numbers: {encoded}\")          # Decode         decoded = self.tokenizer.decode(encoded)         print(f\"3. Decode: '{decoded}'\")          return encoded      def show_statistics(self):         \"\"\"Shows dataset statistics.\"\"\"         print(\"\\n\" + \"=\" * 60)         print(\"SYSTEM STATISTICS\")         print(\"=\" * 60)          print(f\"\\nTotal reviews: {len(self.reviews)}\")         print(f\"Positive reviews: {sum(self.labels)}\")         print(f\"Negative reviews: {len(self.labels) - sum(self.labels)}\")         print(f\"Vocabulary: {len(self.tokenizer.word_to_number)} words\")          # Lengths         lengths = [len(r.split()) for r in self.reviews]         print(f\"\\nAverage length: {sum(lengths)/len(lengths):.1f} words\")         print(f\"Minimum length: {min(lengths)} words\")         print(f\"Maximum length: {max(lengths)} words\")   # Create the system print(\"=\" * 60) print(\"MINI PROJECT: Review System\") print(\"=\" * 60)  system = ReviewSystem()  # Add training reviews training_reviews = [     (\"this product is excellent\", True),     (\"very bad quality do not recommend\", False),     (\"incredible i love it\", True),     (\"terrible experience\", False),     (\"perfect product arrived fast\", True),     (\"does not work properly\", False), ]  print(\"\\nAdding reviews to the system...\") for text, is_positive in training_reviews:     system.add_review(text, is_positive)     sentiment = \"POSITIVE\" if is_positive else \"NEGATIVE\"     print(f\"  - [{sentiment}] {text}\")  # Train system.train()  # Show statistics system.show_statistics()  # Process new reviews print(\"\\n\" + \"=\" * 60) print(\"PROCESSING NEW REVIEWS\") print(\"=\" * 60)  new_reviews = [     \"excellent product very good\",     \"bad experience terrible\",     \"perfect recommend\", ]  for review in new_reviews:     system.process_review(review) <pre>============================================================\nMINI PROJECT: Review System\n============================================================\n\nAdding reviews to the system...\n  - [POSITIVE] this product is excellent\n  - [NEGATIVE] very bad quality do not recommend\n  - [POSITIVE] incredible i love it\n  - [NEGATIVE] terrible experience\n  - [POSITIVE] perfect product arrived fast\n  - [NEGATIVE] does not work properly\nTraining tokenizer with reviews...\nVocabulary: 26 words\n  - Special words: 4\n  - Normal words: 22\n\n============================================================\nSYSTEM STATISTICS\n============================================================\n\nTotal reviews: 6\nPositive reviews: 3\nNegative reviews: 3\nVocabulary: 26 words\n\nAverage length: 4.0 words\nMinimum length: 2 words\nMaximum length: 6 words\n\n============================================================\nPROCESSING NEW REVIEWS\n============================================================\n\nProcessing: 'excellent product very good'\n--------------------------------------------------\n1. Split into words: ['excellent', 'product', 'very', 'good']\n2. Convert to numbers: [2, 7, 5, 8, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n3. Decode: 'excellent product very'\n\nProcessing: 'bad experience terrible'\n--------------------------------------------------\n1. Split into words: ['bad', 'experience', 'terrible']\n2. Convert to numbers: [2, 9, 19, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n3. Decode: 'bad experience terrible'\n\nProcessing: 'perfect recommend'\n--------------------------------------------------\n1. Split into words: ['perfect', 'recommend']\n2. Convert to numbers: [2, 20, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n3. Decode: 'perfect recommend'\n</pre>"},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#tokenization","title":"Tokenization\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#basic-word-by-word-tokenization","title":"Basic word-by-word tokenization\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#building-a-simple-tokenizer","title":"Building a simple tokenizer\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#special-tokens-and-vocabulary-management","title":"Special tokens and vocabulary management\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#visualization-of-the-tokenization-process","title":"Visualization of the tokenization process\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#length-normalization-through-padding","title":"Length normalization through padding\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_01_tokenization.html#practical-application-review-processing-system","title":"Practical application: Review processing system\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html","title":"Recurrent Neural Networks","text":"<p>Recurrent Neural Networks (RNNs) constitute a specialized class of neural architectures designed specifically for processing data sequences. Unlike conventional feedforward neural networks, which process each input independently without retaining information from previous inputs, RNNs incorporate memory mechanisms that allow them to maintain and use information from previous time steps during the processing of new inputs.</p> <p>This memory capability is fundamental for numerous real-world applications. Consider, for example, the reading comprehension process: to correctly interpret the meaning of a word in a sentence, it is essential to consider the context provided by preceding words. Similarly, RNNs process sequential information step by step, continuously updating their internal state to reflect the accumulated knowledge of the sequence processed so far. This characteristic makes them especially suitable tools for tasks involving temporal or sequential data, such as natural language processing, time series analysis, speech recognition, and text generation.</p> <p>Before delving into the internal workings of RNNs, it is essential to understand the nature of sequential data and why it requires specialized treatment. A sequence is defined as an ordered collection of elements where the order of appearance has intrinsic meaning and directly affects the interpretation of the information. Unlike traditional tabular data, where each sample can be considered independent, in sequences there is a temporal or positional dependency between consecutive elements.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Examples of sequences in real life\nprint(\"=\" * 60)\nprint(\"WHAT ARE SEQUENCES?\")\nprint(\"=\" * 60)\n\nsequence_examples = {\n    \"Text\": [\"Hello\", \"how\", \"are\", \"you\"],\n    \"Time series\": [20, 22, 21, 23, 25, 24, 26],\n    \"Music\": [\"C\", \"D\", \"E\", \"F\", \"G\"],\n    \"Video\": [\"Frame1\", \"Frame2\", \"Frame3\", \"Frame4\"],\n}\n\nfor type_name, sequence in sequence_examples.items():\n    print(f\"\\n{type_name}:\")\n    print(f\"  {' -&gt; '.join(map(str, sequence))}\")\n    print(f\"  Length: {len(sequence)}\")\n\nprint(\"\\n\" + \"-\" * 60)\nprint(\"KEY CHARACTERISTIC: Order matters\")\nprint(\"-\" * 60)\nprint(\"'Hello how are you' makes sense\")\nprint(\"'you are how Hello' doesn't make sense\")\nprint(\"\\nRNNs preserve and use this order information.\")\n</pre> import numpy as np import matplotlib.pyplot as plt  # Examples of sequences in real life print(\"=\" * 60) print(\"WHAT ARE SEQUENCES?\") print(\"=\" * 60)  sequence_examples = {     \"Text\": [\"Hello\", \"how\", \"are\", \"you\"],     \"Time series\": [20, 22, 21, 23, 25, 24, 26],     \"Music\": [\"C\", \"D\", \"E\", \"F\", \"G\"],     \"Video\": [\"Frame1\", \"Frame2\", \"Frame3\", \"Frame4\"], }  for type_name, sequence in sequence_examples.items():     print(f\"\\n{type_name}:\")     print(f\"  {' -&gt; '.join(map(str, sequence))}\")     print(f\"  Length: {len(sequence)}\")  print(\"\\n\" + \"-\" * 60) print(\"KEY CHARACTERISTIC: Order matters\") print(\"-\" * 60) print(\"'Hello how are you' makes sense\") print(\"'you are how Hello' doesn't make sense\") print(\"\\nRNNs preserve and use this order information.\") <pre>============================================================\nWHAT ARE SEQUENCES?\n============================================================\n\nText:\n  Hello -&gt; how -&gt; are -&gt; you\n  Length: 4\n\nTime series:\n  20 -&gt; 22 -&gt; 21 -&gt; 23 -&gt; 25 -&gt; 24 -&gt; 26\n  Length: 7\n\nMusic:\n  C -&gt; D -&gt; E -&gt; F -&gt; G\n  Length: 5\n\nVideo:\n  Frame1 -&gt; Frame2 -&gt; Frame3 -&gt; Frame4\n  Length: 4\n\n------------------------------------------------------------\nKEY CHARACTERISTIC: Order matters\n------------------------------------------------------------\n'Hello how are you' makes sense\n'you are how Hello' doesn't make sense\n\nRNNs preserve and use this order information.\n</pre> <p>To deeply understand the internal workings of recurrent neural networks, it is instructive to implement a simplified version from its mathematical foundations. This pedagogical approach allows explicitly visualizing how information flows through the network and how the hidden state that acts as the system's memory is maintained.</p> <p>The basic architecture of an RNN is based on a recurrent equation that updates the hidden state at each time step. This hidden state $h_t$ at time $t$ is calculated as a nonlinear function of the current input $x_t$ and the previous hidden state $h_{t-1}$. Mathematically, this relationship is expressed as:</p> <p>$$h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)$$</p> <p>where $W_x$ represents the weight matrix associated with the input, $W_h$ the recurrent weight matrix that connects the previous state with the current one, $b$ the bias vector, and $\\tanh$ the hyperbolic tangent activation function that introduces nonlinearity and keeps the hidden state values in a bounded range.</p> In\u00a0[2]: Copied! <pre>class SimpleRNN:\n    \"\"\"\n    An extremely simple RNN for educational purposes.\n    Processes a sequence of numbers step by step.\n    \"\"\"\n\n    def __init__(self, input_size=1, hidden_size=3):\n        \"\"\"\n        Args:\n            input_size: Dimension of each input element\n            hidden_size: Size of the hidden state (the \"memory\")\n        \"\"\"\n        self.hidden_size = hidden_size\n\n        # Weights (initialized randomly small)\n        # Weights for current input\n        self.W_input = np.random.randn(hidden_size, input_size) * 0.01\n\n        # Weights for previous state (the memory)\n        self.W_hidden = np.random.randn(hidden_size, hidden_size) * 0.01\n\n        # Bias\n        self.bias = np.zeros((hidden_size, 1))\n\n        print(\"Simple RNN created\")\n        print(f\"  - Input size: {input_size}\")\n        print(f\"  - Hidden size: {hidden_size}\")\n        print(f\"  - Input weights: {self.W_input.shape}\")\n        print(f\"  - Hidden weights: {self.W_hidden.shape}\")\n\n    def step(self, x, h_prev):\n        \"\"\"\n        Processes ONE step of the sequence.\n\n        Args:\n            x: Current input (number)\n            h_prev: Previous hidden state (the memory)\n\n        Returns:\n            New hidden state\n        \"\"\"\n        # Convert input to correct shape\n        x = np.array([[x]])\n\n        # The magic RNN formula:\n        # new_state = tanh(W_input * input + W_hidden * previous_state + bias)\n        h_new = np.tanh(\n            np.dot(self.W_input, x) + np.dot(self.W_hidden, h_prev) + self.bias\n        )\n\n        return h_new\n\n    def process_sequence(self, sequence, verbose=True):\n        \"\"\"\n        Processes a complete sequence, step by step.\n\n        Args:\n            sequence: List of numbers\n            verbose: Whether to show the process\n\n        Returns:\n            List of all hidden states\n        \"\"\"\n        # Initial state (empty memory)\n        h = np.zeros((self.hidden_size, 1))\n        states = []\n\n        if verbose:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"PROCESSING SEQUENCE STEP BY STEP\")\n            print(\"=\" * 60)\n            print(f\"Input sequence: {sequence}\")\n            print(f\"Initial state (empty memory):\\n{h.flatten()}\\n\")\n\n        for t, x in enumerate(sequence):\n            if verbose:\n                print(f\" Step {t + 1} \")\n                print(f\"Input: {x}\")\n                print(f\"Previous state: {h.flatten()}\")\n\n            # Process one step\n            h = self.step(x, h)\n            states.append(h.copy())\n\n            if verbose:\n                print(f\"New state: {h.flatten()}\")\n                print()\n\n        if verbose:\n            print(\"=\" * 60)\n            print(\"SEQUENCE COMPLETED\")\n            print(\"=\" * 60)\n            print(f\"Final state (accumulated memory):\\n{h.flatten()}\")\n\n        return states\n\n\n# Usage example\nprint(\"\\n\" + \"=\" * 60)\nprint(\"EXAMPLE: Processing a simple sequence\")\nprint(\"=\" * 60)\n\n# Create RNN\nrnn = SimpleRNN(input_size=1, hidden_size=3)\n\n# Example sequence\nsequence = [1.0, 2.0, 3.0]\n\n# Process\nstates = rnn.process_sequence(sequence, verbose=True)\n</pre> class SimpleRNN:     \"\"\"     An extremely simple RNN for educational purposes.     Processes a sequence of numbers step by step.     \"\"\"      def __init__(self, input_size=1, hidden_size=3):         \"\"\"         Args:             input_size: Dimension of each input element             hidden_size: Size of the hidden state (the \"memory\")         \"\"\"         self.hidden_size = hidden_size          # Weights (initialized randomly small)         # Weights for current input         self.W_input = np.random.randn(hidden_size, input_size) * 0.01          # Weights for previous state (the memory)         self.W_hidden = np.random.randn(hidden_size, hidden_size) * 0.01          # Bias         self.bias = np.zeros((hidden_size, 1))          print(\"Simple RNN created\")         print(f\"  - Input size: {input_size}\")         print(f\"  - Hidden size: {hidden_size}\")         print(f\"  - Input weights: {self.W_input.shape}\")         print(f\"  - Hidden weights: {self.W_hidden.shape}\")      def step(self, x, h_prev):         \"\"\"         Processes ONE step of the sequence.          Args:             x: Current input (number)             h_prev: Previous hidden state (the memory)          Returns:             New hidden state         \"\"\"         # Convert input to correct shape         x = np.array([[x]])          # The magic RNN formula:         # new_state = tanh(W_input * input + W_hidden * previous_state + bias)         h_new = np.tanh(             np.dot(self.W_input, x) + np.dot(self.W_hidden, h_prev) + self.bias         )          return h_new      def process_sequence(self, sequence, verbose=True):         \"\"\"         Processes a complete sequence, step by step.          Args:             sequence: List of numbers             verbose: Whether to show the process          Returns:             List of all hidden states         \"\"\"         # Initial state (empty memory)         h = np.zeros((self.hidden_size, 1))         states = []          if verbose:             print(\"\\n\" + \"=\" * 60)             print(\"PROCESSING SEQUENCE STEP BY STEP\")             print(\"=\" * 60)             print(f\"Input sequence: {sequence}\")             print(f\"Initial state (empty memory):\\n{h.flatten()}\\n\")          for t, x in enumerate(sequence):             if verbose:                 print(f\" Step {t + 1} \")                 print(f\"Input: {x}\")                 print(f\"Previous state: {h.flatten()}\")              # Process one step             h = self.step(x, h)             states.append(h.copy())              if verbose:                 print(f\"New state: {h.flatten()}\")                 print()          if verbose:             print(\"=\" * 60)             print(\"SEQUENCE COMPLETED\")             print(\"=\" * 60)             print(f\"Final state (accumulated memory):\\n{h.flatten()}\")          return states   # Usage example print(\"\\n\" + \"=\" * 60) print(\"EXAMPLE: Processing a simple sequence\") print(\"=\" * 60)  # Create RNN rnn = SimpleRNN(input_size=1, hidden_size=3)  # Example sequence sequence = [1.0, 2.0, 3.0]  # Process states = rnn.process_sequence(sequence, verbose=True) <pre>\n============================================================\nEXAMPLE: Processing a simple sequence\n============================================================\nSimple RNN created\n  - Input size: 1\n  - Hidden size: 3\n  - Input weights: (3, 1)\n  - Hidden weights: (3, 3)\n\n============================================================\nPROCESSING SEQUENCE STEP BY STEP\n============================================================\nInput sequence: [1.0, 2.0, 3.0]\nInitial state (empty memory):\n[0. 0. 0.]\n\n Step 1 \nInput: 1.0\nPrevious state: [0. 0. 0.]\nNew state: [ 0.01283428 -0.00313019 -0.01036122]\n\n Step 2 \nInput: 2.0\nPrevious state: [ 0.01283428 -0.00313019 -0.01036122]\nNew state: [ 0.02596573 -0.00614265 -0.02087637]\n\n Step 3 \nInput: 3.0\nPrevious state: [ 0.02596573 -0.00614265 -0.02087637]\nNew state: [ 0.03909642 -0.00915192 -0.03139195]\n\n============================================================\nSEQUENCE COMPLETED\n============================================================\nFinal state (accumulated memory):\n[ 0.03909642 -0.00915192 -0.03139195]\n</pre> <p>Understanding the dynamic behavior of RNNs is facilitated by visualizing the evolution of the hidden state throughout the processing of a sequence. The hidden state acts as a feature vector that encodes relevant information from all elements processed so far. Observing how each component of this vector changes in response to new inputs allows intuiting what kind of patterns the network is learning to detect and remember.</p> In\u00a0[3]: Copied! <pre>def visualize_hidden_states(sequence, states):\n    \"\"\"\n    Visualizes how the hidden state changes at each step.\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n\n    # Prepare data\n    states_array = np.array([s.flatten() for s in states])\n    num_steps = len(states)\n    num_neurons = states_array.shape[1]\n\n    # Line plot\n    plt.subplot(1, 2, 1)\n    for i in range(num_neurons):\n        plt.plot(\n            range(1, num_steps + 1),\n            states_array[:, i],\n            marker=\"o\",\n            label=f\"Neuron {i+1}\",\n        )\n\n    plt.xlabel(\"Time step\")\n    plt.ylabel(\"Hidden state value\")\n    plt.title(\"Hidden State Evolution\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Heatmap\n    plt.subplot(1, 2, 2)\n    plt.imshow(states_array.T, aspect=\"auto\", cmap=\"coolwarm\")\n    plt.colorbar(label=\"Value\")\n    plt.xlabel(\"Time step\")\n    plt.ylabel(\"Neuron\")\n    plt.title(\"Hidden State Heatmap\")\n\n    # Add input values\n    for i, val in enumerate(sequence):\n        plt.text(i, -0.5, f\"x={val}\", ha=\"center\")\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nInterpretation:\")\n    print(\"- Each line represents a neuron in the hidden state\")\n    print(\"- Observe how values change with each new input\")\n    print(\"- The final state contains information from the ENTIRE sequence\")\n\n\n# Visualize\nvisualize_hidden_states([1.0, 2.0, 3.0], states)\n</pre> def visualize_hidden_states(sequence, states):     \"\"\"     Visualizes how the hidden state changes at each step.     \"\"\"     plt.figure(figsize=(12, 6))      # Prepare data     states_array = np.array([s.flatten() for s in states])     num_steps = len(states)     num_neurons = states_array.shape[1]      # Line plot     plt.subplot(1, 2, 1)     for i in range(num_neurons):         plt.plot(             range(1, num_steps + 1),             states_array[:, i],             marker=\"o\",             label=f\"Neuron {i+1}\",         )      plt.xlabel(\"Time step\")     plt.ylabel(\"Hidden state value\")     plt.title(\"Hidden State Evolution\")     plt.legend()     plt.grid(True, alpha=0.3)      # Heatmap     plt.subplot(1, 2, 2)     plt.imshow(states_array.T, aspect=\"auto\", cmap=\"coolwarm\")     plt.colorbar(label=\"Value\")     plt.xlabel(\"Time step\")     plt.ylabel(\"Neuron\")     plt.title(\"Hidden State Heatmap\")      # Add input values     for i, val in enumerate(sequence):         plt.text(i, -0.5, f\"x={val}\", ha=\"center\")      plt.tight_layout()     plt.show()      print(\"\\nInterpretation:\")     print(\"- Each line represents a neuron in the hidden state\")     print(\"- Observe how values change with each new input\")     print(\"- The final state contains information from the ENTIRE sequence\")   # Visualize visualize_hidden_states([1.0, 2.0, 3.0], states) <pre>\nInterpretation:\n- Each line represents a neuron in the hidden state\n- Observe how values change with each new input\n- The final state contains information from the ENTIRE sequence\n</pre> <p>Once the theoretical foundations and basic implementation are established, it is pertinent to apply RNNs to a concrete prediction task. The problem of predicting the next element of a sequence constitutes a fundamental use case that illustrates the ability of these networks to capture temporal patterns and make inferences based on historical context.</p> <p>In this scenario, the RNN processes an input sequence element by element, updating its internal state at each step. Once the complete sequence is processed, the final hidden state contains a compressed representation of all relevant information. This state is then used to generate a prediction of the next value through an additional output layer that transforms the hidden state into the desired output space.</p> In\u00a0[4]: Copied! <pre>class RNNPredictor:\n    \"\"\"\n    Simple RNN to predict the next element of a sequence.\n    \"\"\"\n\n    def __init__(self, input_size=1, hidden_size=10, output_size=1):\n        self.hidden_size = hidden_size\n\n        # Weights for recurrent layer\n        self.W_input = np.random.randn(hidden_size, input_size) * 0.1\n        self.W_hidden = np.random.randn(hidden_size, hidden_size) * 0.1\n        self.b_hidden = np.zeros((hidden_size, 1))\n\n        # Weights for output layer\n        self.W_output = np.random.randn(output_size, hidden_size) * 0.1\n        self.b_output = np.zeros((output_size, 1))\n\n        print(f\"RNN Predictor created\")\n        print(f\"  Input: {input_size}, Hidden: {hidden_size}, Output: {output_size}\")\n\n    def forward(self, sequence):\n        \"\"\"\n        Forward propagation: processes the sequence and generates predictions.\n\n        Args:\n            sequence: List of values\n\n        Returns:\n            prediction: Prediction of the next value\n            states: List of hidden states\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        states = []\n\n        # Process each element of the sequence\n        for x in sequence:\n            x = np.array([[x]])\n\n            # Update hidden state\n            h = np.tanh(\n                np.dot(self.W_input, x) + np.dot(self.W_hidden, h) + self.b_hidden\n            )\n            states.append(h.copy())\n\n        # Generate prediction based on last state\n        prediction = np.dot(self.W_output, h) + self.b_output\n\n        return prediction[0, 0], states\n\n    def simple_train(self, sequences, targets, epochs=100, learning_rate=0.01):\n        \"\"\"\n        Very simplified training (for demonstration only).\n\n        Args:\n            sequences: List of input sequences\n            targets: List of target values (next number)\n            epochs: Number of iterations\n            learning_rate: Learning speed\n        \"\"\"\n        print(\"\\nTraining RNN...\")\n        print(f\"Epochs: {epochs}, Learning rate: {learning_rate}\")\n\n        error_history = []\n\n        for epoch in range(epochs):\n            total_error = 0\n\n            for sequence, target in zip(sequences, targets):\n                # Prediction\n                prediction, _ = self.forward(sequence)\n\n                # Error\n                error = target - prediction\n                total_error += error**2\n\n                # Simple weight update (simplified gradient descent)\n                # In reality, this requires backpropagation through time (BPTT)\n                # but here we use a very simplified version\n                self.W_output += learning_rate * error * 0.01\n                self.b_output += learning_rate * error * 0.01\n\n            avg_error = total_error / len(sequences)\n            error_history.append(avg_error)\n\n            if (epoch + 1) % 20 == 0:\n                print(f\"  Epoch {epoch + 1}/{epochs} - Error: {avg_error:.4f}\")\n\n        return error_history\n\n    def predict(self, sequence):\n        \"\"\"Predicts the next value in the sequence.\"\"\"\n        prediction, states = self.forward(sequence)\n        return prediction\n\n\n# Example: Predict numbers in a sequence\nprint(\"\\n\" + \"=\" * 60)\nprint(\"EXAMPLE: Predict the next number\")\nprint(\"=\" * 60)\n\n# Training data: simple sequences\n# For example: [1, 2, 3] -&gt; 4, [2, 3, 4] -&gt; 5\ntraining_sequences = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]\n\ntraining_targets = [4, 5, 6, 7, 8]\n\nprint(\"\\nTraining data:\")\nfor seq, target in zip(training_sequences, training_targets):\n    print(f\"  {seq} -&gt; {target}\")\n\n# Create and train RNN\nrnn_predictor = RNNPredictor(input_size=1, hidden_size=10, output_size=1)\nerrors = rnn_predictor.simple_train(training_sequences, training_targets, epochs=100)\n\n# Visualize training\nplt.figure(figsize=(10, 5))\nplt.plot(errors)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Mean squared error\")\nplt.title(\"RNN Learning Curve\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Test predictions\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTING PREDICTIONS\")\nprint(\"=\" * 60)\n\ntest_sequences = [[1, 2, 3], [5, 6, 7], [10, 11, 12]]\n\nfor seq in test_sequences:\n    pred = rnn_predictor.predict(seq)\n    print(f\"Sequence: {seq}\")\n    print(f\"Prediction: {pred:.2f}\")\n    print(f\"Expected: {seq[-1] + 1}\")\n    print()\n</pre> class RNNPredictor:     \"\"\"     Simple RNN to predict the next element of a sequence.     \"\"\"      def __init__(self, input_size=1, hidden_size=10, output_size=1):         self.hidden_size = hidden_size          # Weights for recurrent layer         self.W_input = np.random.randn(hidden_size, input_size) * 0.1         self.W_hidden = np.random.randn(hidden_size, hidden_size) * 0.1         self.b_hidden = np.zeros((hidden_size, 1))          # Weights for output layer         self.W_output = np.random.randn(output_size, hidden_size) * 0.1         self.b_output = np.zeros((output_size, 1))          print(f\"RNN Predictor created\")         print(f\"  Input: {input_size}, Hidden: {hidden_size}, Output: {output_size}\")      def forward(self, sequence):         \"\"\"         Forward propagation: processes the sequence and generates predictions.          Args:             sequence: List of values          Returns:             prediction: Prediction of the next value             states: List of hidden states         \"\"\"         h = np.zeros((self.hidden_size, 1))         states = []          # Process each element of the sequence         for x in sequence:             x = np.array([[x]])              # Update hidden state             h = np.tanh(                 np.dot(self.W_input, x) + np.dot(self.W_hidden, h) + self.b_hidden             )             states.append(h.copy())          # Generate prediction based on last state         prediction = np.dot(self.W_output, h) + self.b_output          return prediction[0, 0], states      def simple_train(self, sequences, targets, epochs=100, learning_rate=0.01):         \"\"\"         Very simplified training (for demonstration only).          Args:             sequences: List of input sequences             targets: List of target values (next number)             epochs: Number of iterations             learning_rate: Learning speed         \"\"\"         print(\"\\nTraining RNN...\")         print(f\"Epochs: {epochs}, Learning rate: {learning_rate}\")          error_history = []          for epoch in range(epochs):             total_error = 0              for sequence, target in zip(sequences, targets):                 # Prediction                 prediction, _ = self.forward(sequence)                  # Error                 error = target - prediction                 total_error += error**2                  # Simple weight update (simplified gradient descent)                 # In reality, this requires backpropagation through time (BPTT)                 # but here we use a very simplified version                 self.W_output += learning_rate * error * 0.01                 self.b_output += learning_rate * error * 0.01              avg_error = total_error / len(sequences)             error_history.append(avg_error)              if (epoch + 1) % 20 == 0:                 print(f\"  Epoch {epoch + 1}/{epochs} - Error: {avg_error:.4f}\")          return error_history      def predict(self, sequence):         \"\"\"Predicts the next value in the sequence.\"\"\"         prediction, states = self.forward(sequence)         return prediction   # Example: Predict numbers in a sequence print(\"\\n\" + \"=\" * 60) print(\"EXAMPLE: Predict the next number\") print(\"=\" * 60)  # Training data: simple sequences # For example: [1, 2, 3] -&gt; 4, [2, 3, 4] -&gt; 5 training_sequences = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]  training_targets = [4, 5, 6, 7, 8]  print(\"\\nTraining data:\") for seq, target in zip(training_sequences, training_targets):     print(f\"  {seq} -&gt; {target}\")  # Create and train RNN rnn_predictor = RNNPredictor(input_size=1, hidden_size=10, output_size=1) errors = rnn_predictor.simple_train(training_sequences, training_targets, epochs=100)  # Visualize training plt.figure(figsize=(10, 5)) plt.plot(errors) plt.xlabel(\"Epoch\") plt.ylabel(\"Mean squared error\") plt.title(\"RNN Learning Curve\") plt.grid(True, alpha=0.3) plt.show()  # Test predictions print(\"\\n\" + \"=\" * 60) print(\"TESTING PREDICTIONS\") print(\"=\" * 60)  test_sequences = [[1, 2, 3], [5, 6, 7], [10, 11, 12]]  for seq in test_sequences:     pred = rnn_predictor.predict(seq)     print(f\"Sequence: {seq}\")     print(f\"Prediction: {pred:.2f}\")     print(f\"Expected: {seq[-1] + 1}\")     print() <pre>\n============================================================\nEXAMPLE: Predict the next number\n============================================================\n\nTraining data:\n  [1, 2, 3] -&gt; 4\n  [2, 3, 4] -&gt; 5\n  [3, 4, 5] -&gt; 6\n  [4, 5, 6] -&gt; 7\n  [5, 6, 7] -&gt; 8\nRNN Predictor created\n  Input: 1, Hidden: 10, Output: 1\n\nTraining RNN...\nEpochs: 100, Learning rate: 0.01\n  Epoch 20/100 - Error: 37.5041\n  Epoch 40/100 - Error: 36.3310\n  Epoch 60/100 - Error: 35.1955\n  Epoch 80/100 - Error: 34.0963\n  Epoch 100/100 - Error: 33.0323\n</pre> <pre>\n============================================================\nTESTING PREDICTIONS\n============================================================\nSequence: [1, 2, 3]\nPrediction: 0.37\nExpected: 4\n\nSequence: [5, 6, 7]\nPrediction: 0.47\nExpected: 8\n\nSequence: [10, 11, 12]\nPrediction: 0.56\nExpected: 13\n\n</pre> <p>While manual implementation of RNNs provides deep understanding of their internal mechanisms, in professional practice deep learning frameworks are used that offer optimized and efficient implementations. PyTorch, one of the most widely used frameworks in research and production, provides predefined modules for RNNs that encapsulate all the complexity of gradient calculation through backpropagation through time (BPTT) and offer GPU acceleration.</p> <p>The use of these professional implementations not only significantly reduces development time, but also guarantees numerical correctness and computational efficiency. PyTorch automatically handles complex aspects such as weight initialization, gradient calculation through time, and management of variable-length sequences.</p> In\u00a0[5]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nclass RNNPyTorch(nn.Module):\n    \"\"\"\n    Professional RNN using PyTorch.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNPyTorch, self).__init__()\n\n        self.rnn = nn.RNN(\n            input_size=input_size, hidden_size=hidden_size, batch_first=True\n        )\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        Forward propagation.\n\n        Args:\n            x: Tensor of shape (batch, sequence, features)\n\n        Returns:\n            Model output\n        \"\"\"\n        out, h_n = self.rnn(x)\n        last_output = out[:, -1, :]\n        prediction = self.fc(last_output)\n        return prediction\n\n\ndef train_rnn_pytorch(model, train_loader, epochs=50, lr=0.01):\n    \"\"\"\n    Training function for PyTorch RNN.\n    \"\"\"\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    history = []\n\n    print(\"\\nTraining with PyTorch...\")\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n\n        for sequences, targets in train_loader:\n            optimizer.zero_grad()\n            predictions = model(sequences)\n            loss = criterion(predictions, targets)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(train_loader)\n        history.append(avg_loss)\n\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n\n    return history\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RNN WITH PYTORCH\")\nprint(\"=\" * 60)\n\n\ndef create_sequence_data(num_examples=100):\n    \"\"\"Creates example data: sequence -&gt; next number\"\"\"\n    X = []\n    y = []\n\n    for i in range(num_examples):\n        start = np.random.randint(0, 50)\n        sequence = list(range(start, start + 5))\n        next_val = start + 5\n        X.append(sequence)\n        y.append(next_val)\n\n    return np.array(X), np.array(y)\n\n\nX, y = create_sequence_data(100)\n\nprint(f\"\\nData generated: {len(X)} examples\")\nprint(f\"Example sequence: {X[0]} -&gt; {y[0]}\")\n\nX_tensor = torch.FloatTensor(X).unsqueeze(-1)\ny_tensor = torch.FloatTensor(y).unsqueeze(-1)\n\nprint(f\"\\nShape of X: {X_tensor.shape}\")\nprint(f\"Shape of y: {y_tensor.shape}\")\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndataset = TensorDataset(X_tensor, y_tensor)\ntrain_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n\nmodel = RNNPyTorch(input_size=1, hidden_size=20, output_size=1)\n\nprint(f\"\\nModel created:\")\nprint(model)\n\nhistory = train_rnn_pytorch(model, train_loader, epochs=50, lr=0.01)\n\nplt.figure(figsize=(10, 5))\nplt.plot(history)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (MSE)\")\nplt.title(\"RNN Training with PyTorch\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTING TRAINED MODEL\")\nprint(\"=\" * 60)\n\nmodel.eval()\nwith torch.no_grad():\n    test_sequences = [\n        [10, 11, 12, 13, 14],\n        [25, 26, 27, 28, 29],\n        [100, 101, 102, 103, 104],\n    ]\n\n    for seq in test_sequences:\n        input_tensor = torch.FloatTensor(seq).unsqueeze(0).unsqueeze(-1)\n        prediction = model(input_tensor)\n        print(f\"\\nSequence: {seq}\")\n        print(f\"Prediction: {prediction.item():.2f}\")\n        print(f\"Expected: {seq[-1] + 1}\")\n</pre> import torch import torch.nn as nn import torch.optim as optim   class RNNPyTorch(nn.Module):     \"\"\"     Professional RNN using PyTorch.     \"\"\"      def __init__(self, input_size, hidden_size, output_size):         super(RNNPyTorch, self).__init__()          self.rnn = nn.RNN(             input_size=input_size, hidden_size=hidden_size, batch_first=True         )          self.fc = nn.Linear(hidden_size, output_size)      def forward(self, x):         \"\"\"         Forward propagation.          Args:             x: Tensor of shape (batch, sequence, features)          Returns:             Model output         \"\"\"         out, h_n = self.rnn(x)         last_output = out[:, -1, :]         prediction = self.fc(last_output)         return prediction   def train_rnn_pytorch(model, train_loader, epochs=50, lr=0.01):     \"\"\"     Training function for PyTorch RNN.     \"\"\"     criterion = nn.MSELoss()     optimizer = optim.Adam(model.parameters(), lr=lr)     history = []      print(\"\\nTraining with PyTorch...\")      for epoch in range(epochs):         epoch_loss = 0          for sequences, targets in train_loader:             optimizer.zero_grad()             predictions = model(sequences)             loss = criterion(predictions, targets)             loss.backward()             optimizer.step()             epoch_loss += loss.item()          avg_loss = epoch_loss / len(train_loader)         history.append(avg_loss)          if (epoch + 1) % 10 == 0:             print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")      return history   print(\"\\n\" + \"=\" * 60) print(\"RNN WITH PYTORCH\") print(\"=\" * 60)   def create_sequence_data(num_examples=100):     \"\"\"Creates example data: sequence -&gt; next number\"\"\"     X = []     y = []      for i in range(num_examples):         start = np.random.randint(0, 50)         sequence = list(range(start, start + 5))         next_val = start + 5         X.append(sequence)         y.append(next_val)      return np.array(X), np.array(y)   X, y = create_sequence_data(100)  print(f\"\\nData generated: {len(X)} examples\") print(f\"Example sequence: {X[0]} -&gt; {y[0]}\")  X_tensor = torch.FloatTensor(X).unsqueeze(-1) y_tensor = torch.FloatTensor(y).unsqueeze(-1)  print(f\"\\nShape of X: {X_tensor.shape}\") print(f\"Shape of y: {y_tensor.shape}\")  from torch.utils.data import TensorDataset, DataLoader  dataset = TensorDataset(X_tensor, y_tensor) train_loader = DataLoader(dataset, batch_size=10, shuffle=True)  model = RNNPyTorch(input_size=1, hidden_size=20, output_size=1)  print(f\"\\nModel created:\") print(model)  history = train_rnn_pytorch(model, train_loader, epochs=50, lr=0.01)  plt.figure(figsize=(10, 5)) plt.plot(history) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss (MSE)\") plt.title(\"RNN Training with PyTorch\") plt.grid(True, alpha=0.3) plt.show()  print(\"\\n\" + \"=\" * 60) print(\"TESTING TRAINED MODEL\") print(\"=\" * 60)  model.eval() with torch.no_grad():     test_sequences = [         [10, 11, 12, 13, 14],         [25, 26, 27, 28, 29],         [100, 101, 102, 103, 104],     ]      for seq in test_sequences:         input_tensor = torch.FloatTensor(seq).unsqueeze(0).unsqueeze(-1)         prediction = model(input_tensor)         print(f\"\\nSequence: {seq}\")         print(f\"Prediction: {prediction.item():.2f}\")         print(f\"Expected: {seq[-1] + 1}\") <pre>\n============================================================\nRNN WITH PYTORCH\n============================================================\n\nData generated: 100 examples\nExample sequence: [ 7  8  9 10 11] -&gt; 12\n\nShape of X: torch.Size([100, 5, 1])\nShape of y: torch.Size([100, 1])\n\nModel created:\nRNNPyTorch(\n  (rnn): RNN(1, 20, batch_first=True)\n  (fc): Linear(in_features=20, out_features=1, bias=True)\n)\n</pre> <pre>\nTraining with PyTorch...\nEpoch 10/50 - Loss: 317.6150\n</pre> <pre>Epoch 20/50 - Loss: 98.8682\nEpoch 30/50 - Loss: 35.6798\n</pre> <pre>Epoch 40/50 - Loss: 15.1507\nEpoch 50/50 - Loss: 7.7056\n</pre> <pre>\n============================================================\nTESTING TRAINED MODEL\n============================================================\n\nSequence: [10, 11, 12, 13, 14]\nPrediction: 15.22\nExpected: 15\n\nSequence: [25, 26, 27, 28, 29]\nPrediction: 29.95\nExpected: 30\n\nSequence: [100, 101, 102, 103, 104]\nPrediction: 43.09\nExpected: 105\n</pre>"},{"location":"course/topic_05_sequential_models/section_02_rnn.html#recurrent-neural-networks","title":"Recurrent Neural Networks\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#the-concept-of-sequences","title":"The concept of sequences\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#building-a-simple-rnn-from-scratch","title":"Building a simple RNN from scratch\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#visualization-of-the-memory-mechanism","title":"Visualization of the memory mechanism\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#sequence-prediction-with-rnns","title":"Sequence prediction with RNNs\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_02_rnn.html#implementation-with-pytorch","title":"Implementation with PyTorch\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html","title":"Long Short-Term Memory (LSTM)","text":"<p>LSTM (Long Short-Term Memory) constitutes a specialized architecture of recurrent neural networks designed specifically to address one of the most significant limitations of conventional RNNs: the inability to effectively maintain and use information across extensive sequences. This problem, known as the vanishing gradient problem in long-term dependencies, prevents simple RNNs from capturing relationships between distant elements in a sequence.</p> <p>While traditional RNNs can process sequential information and maintain an internal state, their memory capacity is limited in practice. Information tends to degrade exponentially as it propagates through multiple time steps, making it difficult to learn patterns that require remembering events that occurred many steps back. LSTMs solve this problem through a more sophisticated architecture that incorporates explicit control mechanisms over what information should be retained, updated, or discarded at each time step.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\ndef demonstrate_memory_problem():\n    \"\"\"\n    Demonstrates why we need LSTMs instead of simple RNNs.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"THE MEMORY PROBLEM IN RNNs\")\n    print(\"=\" * 60)\n\n    print(\"\\nEXAMPLE 1: Short-Term Dependency (RNN works well)\")\n    print(\"-\" * 60)\n    short_text = \"The sky is ___\"\n    print(f\"Text: {short_text}\")\n    print(\"To predict the missing word, we only need to\")\n    print(\"remember 'sky' (recent word)\")\n    print(\"\u2713 Simple RNN: WORKS WELL\")\n\n    print(\"\\n\\nEXAMPLE 2: Long-Term Dependency (RNN fails)\")\n    print(\"-\" * 60)\n    long_text = \"\"\"\n    I was born in France. I spent my childhood playing in the\n    lavender fields. Then I traveled the world for 20 years. Now\n    I live in Spain, but I speak fluent French because I grew up in ___\n    \"\"\"\n    print(f\"Text: {long_text}\")\n    print(\"\\nTo predict the missing word ('France'),\")\n    print(\"we need to remember information from the BEGINNING of the text\")\n    print(\"\u2717 Simple RNN: FAILS (forgets old information)\")\n    print(\"\u2713 LSTM: WORKS (maintains long-term memory)\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    steps = np.arange(1, 51)\n    rnn_memory = np.exp(-steps * 0.1)\n\n    axes[0].plot(steps, rnn_memory, linewidth=2, color=\"red\")\n    axes[0].axhline(y=0.2, color=\"gray\", linestyle=\"--\", alpha=0.5)\n    axes[0].fill_between(steps, 0, rnn_memory, alpha=0.3, color=\"red\")\n    axes[0].set_xlabel(\"Time Step\")\n    axes[0].set_ylabel(\"Memory Strength\")\n    axes[0].set_title(\"Simple RNN: Memory Fades\")\n    axes[0].grid(True, alpha=0.3)\n    axes[0].text(25, 0.5, \"Information is lost\", fontsize=12)\n\n    lstm_memory = np.ones(50) * 0.9 + np.random.normal(0, 0.05, 50)\n    lstm_memory = np.clip(lstm_memory, 0, 1)\n\n    axes[1].plot(steps, lstm_memory, linewidth=2, color=\"green\")\n    axes[1].axhline(y=0.8, color=\"gray\", linestyle=\"--\", alpha=0.5)\n    axes[1].fill_between(steps, 0, lstm_memory, alpha=0.3, color=\"green\")\n    axes[1].set_xlabel(\"Time Step\")\n    axes[1].set_ylabel(\"Memory Strength\")\n    axes[1].set_title(\"LSTM: Memory is Maintained\")\n    axes[1].grid(True, alpha=0.3)\n    axes[1].text(25, 0.5, \"Information is preserved\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONCLUSION: LSTM maintains information for much longer\")\n    print(\"=\" * 60)\n\n\ndemonstrate_memory_problem()\n</pre> import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim   def demonstrate_memory_problem():     \"\"\"     Demonstrates why we need LSTMs instead of simple RNNs.     \"\"\"     print(\"=\" * 60)     print(\"THE MEMORY PROBLEM IN RNNs\")     print(\"=\" * 60)      print(\"\\nEXAMPLE 1: Short-Term Dependency (RNN works well)\")     print(\"-\" * 60)     short_text = \"The sky is ___\"     print(f\"Text: {short_text}\")     print(\"To predict the missing word, we only need to\")     print(\"remember 'sky' (recent word)\")     print(\"\u2713 Simple RNN: WORKS WELL\")      print(\"\\n\\nEXAMPLE 2: Long-Term Dependency (RNN fails)\")     print(\"-\" * 60)     long_text = \"\"\"     I was born in France. I spent my childhood playing in the     lavender fields. Then I traveled the world for 20 years. Now     I live in Spain, but I speak fluent French because I grew up in ___     \"\"\"     print(f\"Text: {long_text}\")     print(\"\\nTo predict the missing word ('France'),\")     print(\"we need to remember information from the BEGINNING of the text\")     print(\"\u2717 Simple RNN: FAILS (forgets old information)\")     print(\"\u2713 LSTM: WORKS (maintains long-term memory)\")      fig, axes = plt.subplots(1, 2, figsize=(14, 5))      steps = np.arange(1, 51)     rnn_memory = np.exp(-steps * 0.1)      axes[0].plot(steps, rnn_memory, linewidth=2, color=\"red\")     axes[0].axhline(y=0.2, color=\"gray\", linestyle=\"--\", alpha=0.5)     axes[0].fill_between(steps, 0, rnn_memory, alpha=0.3, color=\"red\")     axes[0].set_xlabel(\"Time Step\")     axes[0].set_ylabel(\"Memory Strength\")     axes[0].set_title(\"Simple RNN: Memory Fades\")     axes[0].grid(True, alpha=0.3)     axes[0].text(25, 0.5, \"Information is lost\", fontsize=12)      lstm_memory = np.ones(50) * 0.9 + np.random.normal(0, 0.05, 50)     lstm_memory = np.clip(lstm_memory, 0, 1)      axes[1].plot(steps, lstm_memory, linewidth=2, color=\"green\")     axes[1].axhline(y=0.8, color=\"gray\", linestyle=\"--\", alpha=0.5)     axes[1].fill_between(steps, 0, lstm_memory, alpha=0.3, color=\"green\")     axes[1].set_xlabel(\"Time Step\")     axes[1].set_ylabel(\"Memory Strength\")     axes[1].set_title(\"LSTM: Memory is Maintained\")     axes[1].grid(True, alpha=0.3)     axes[1].text(25, 0.5, \"Information is preserved\", fontsize=12)      plt.tight_layout()     plt.show()      print(\"\\n\" + \"=\" * 60)     print(\"CONCLUSION: LSTM maintains information for much longer\")     print(\"=\" * 60)   demonstrate_memory_problem() <pre>============================================================\nTHE MEMORY PROBLEM IN RNNs\n============================================================\n\nEXAMPLE 1: Short-Term Dependency (RNN works well)\n------------------------------------------------------------\nText: The sky is ___\nTo predict the missing word, we only need to\nremember 'sky' (recent word)\n\u2713 Simple RNN: WORKS WELL\n\n\nEXAMPLE 2: Long-Term Dependency (RNN fails)\n------------------------------------------------------------\nText: \n    I was born in France. I spent my childhood playing in the\n    lavender fields. Then I traveled the world for 20 years. Now\n    I live in Spain, but I speak fluent French because I grew up in ___\n    \n\nTo predict the missing word ('France'),\nwe need to remember information from the BEGINNING of the text\n\u2717 Simple RNN: FAILS (forgets old information)\n\u2713 LSTM: WORKS (maintains long-term memory)\n</pre> <pre>\n============================================================\nCONCLUSION: LSTM maintains information for much longer\n============================================================\n</pre> <p>The LSTM architecture is based on an ingenious design that incorporates multiple interconnected components called \"gates\". These gates act as control mechanisms that regulate the flow of information through the network, determining what information should be preserved, updated, or removed from the cell's internal state. Unlike simple RNNs, where the hidden state is updated uniformly at each step, LSTMs maintain two separate states: the cell state, which acts as long-term memory, and the hidden state, which represents short-term output.</p> <p>The LSTM architecture incorporates three fundamental gates, each with a specific function. The forget gate determines what proportion of information stored in the previous cell state should be discarded. The input gate controls what new information should be incorporated into the cell state. Finally, the output gate regulates what part of the cell state should be used to generate the current hidden state. Each of these gates uses sigmoid activation functions that produce values between 0 and 1, interpretable as percentages of information that should pass through the gate.</p> In\u00a0[2]: Copied! <pre>def explain_lstm_gates():\n    \"\"\"\n    Explains the three gates that make LSTM special.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"LSTM ARCHITECTURE: THE THREE GATES\")\n    print(\"=\" * 60)\n\n    print(\n        \"\"\"\n    An LSTM has three \"gates\" that control the flow of information:\n\n    1. FORGET GATE:\n       Determines what information from the past should be discarded.\n\n       Example: \"The cat was... Yesterday I went to the park...\"\n       \u2192 Forget \"cat\" because we changed topics\n\n    2. INPUT GATE:\n       Determines what new information should be stored.\n\n       Example: \"Yesterday I went to the park...\"\n       \u2192 Remember \"park\" because it's the new important topic\n\n    3. OUTPUT GATE:\n       Determines what information should be used in the current step.\n\n       Example: \"In the park I saw ___\"\n       \u2192 Use information from \"park\" to predict next word\n    \"\"\"\n    )\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    ax1 = axes[0, 0]\n    sequence = [\"The\", \"cat\", \"sleeps\", \"Yesterday\", \"went\"]\n    importance = [0.8, 0.9, 0.7, 0.2, 0.3]\n    ax1.bar(sequence, importance, color=\"salmon\")\n    ax1.axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Forget threshold\")\n    ax1.set_ylabel(\"Keep (1) / Forget (0)\")\n    ax1.set_title(\"1. Forget Gate: What to forget?\")\n    ax1.legend()\n    ax1.set_ylim(0, 1)\n\n    ax2 = axes[0, 1]\n    new_words = [\"Yesterday\", \"went\", \"to\", \"park\"]\n    relevance = [0.6, 0.5, 0.3, 0.9]\n    ax2.bar(new_words, relevance, color=\"lightgreen\")\n    ax2.axhline(y=0.5, color=\"green\", linestyle=\"--\", label=\"Importance threshold\")\n    ax2.set_ylabel(\"Importance\")\n    ax2.set_title(\"2. Input Gate: What to remember?\")\n    ax2.legend()\n    ax2.set_ylim(0, 1)\n\n    ax3 = axes[1, 0]\n    context = [\"cat\", \"sleeps\", \"park\"]\n    use = [0.1, 0.2, 0.9]\n    ax3.bar(context, use, color=\"skyblue\")\n    ax3.axhline(y=0.5, color=\"blue\", linestyle=\"--\", label=\"Use threshold\")\n    ax3.set_ylabel(\"Use (1) / Don't use (0)\")\n    ax3.set_title(\"3. Output Gate: What to use now?\")\n    ax3.legend()\n    ax3.set_ylim(0, 1)\n\n    ax4 = axes[1, 1]\n    ax4.text(\n        0.5,\n        0.9,\n        \"INFORMATION FLOW IN LSTM\",\n        ha=\"center\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n    ax4.text(0.5, 0.7, '1. Forget: Remove \"cat\"', ha=\"center\", fontsize=11)\n    ax4.text(0.5, 0.55, \"\u2193\", ha=\"center\", fontsize=16)\n    ax4.text(0.5, 0.4, '2. Input: Add \"park\"', ha=\"center\", fontsize=11)\n    ax4.text(0.5, 0.25, \"\u2193\", ha=\"center\", fontsize=16)\n    ax4.text(0.5, 0.1, '3. Output: Use \"park\"', ha=\"center\", fontsize=11)\n    ax4.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nexplain_lstm_gates()\n</pre> def explain_lstm_gates():     \"\"\"     Explains the three gates that make LSTM special.     \"\"\"     print(\"\\n\" + \"=\" * 60)     print(\"LSTM ARCHITECTURE: THE THREE GATES\")     print(\"=\" * 60)      print(         \"\"\"     An LSTM has three \"gates\" that control the flow of information:      1. FORGET GATE:        Determines what information from the past should be discarded.         Example: \"The cat was... Yesterday I went to the park...\"        \u2192 Forget \"cat\" because we changed topics      2. INPUT GATE:        Determines what new information should be stored.         Example: \"Yesterday I went to the park...\"        \u2192 Remember \"park\" because it's the new important topic      3. OUTPUT GATE:        Determines what information should be used in the current step.         Example: \"In the park I saw ___\"        \u2192 Use information from \"park\" to predict next word     \"\"\"     )      fig, axes = plt.subplots(2, 2, figsize=(14, 10))      ax1 = axes[0, 0]     sequence = [\"The\", \"cat\", \"sleeps\", \"Yesterday\", \"went\"]     importance = [0.8, 0.9, 0.7, 0.2, 0.3]     ax1.bar(sequence, importance, color=\"salmon\")     ax1.axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Forget threshold\")     ax1.set_ylabel(\"Keep (1) / Forget (0)\")     ax1.set_title(\"1. Forget Gate: What to forget?\")     ax1.legend()     ax1.set_ylim(0, 1)      ax2 = axes[0, 1]     new_words = [\"Yesterday\", \"went\", \"to\", \"park\"]     relevance = [0.6, 0.5, 0.3, 0.9]     ax2.bar(new_words, relevance, color=\"lightgreen\")     ax2.axhline(y=0.5, color=\"green\", linestyle=\"--\", label=\"Importance threshold\")     ax2.set_ylabel(\"Importance\")     ax2.set_title(\"2. Input Gate: What to remember?\")     ax2.legend()     ax2.set_ylim(0, 1)      ax3 = axes[1, 0]     context = [\"cat\", \"sleeps\", \"park\"]     use = [0.1, 0.2, 0.9]     ax3.bar(context, use, color=\"skyblue\")     ax3.axhline(y=0.5, color=\"blue\", linestyle=\"--\", label=\"Use threshold\")     ax3.set_ylabel(\"Use (1) / Don't use (0)\")     ax3.set_title(\"3. Output Gate: What to use now?\")     ax3.legend()     ax3.set_ylim(0, 1)      ax4 = axes[1, 1]     ax4.text(         0.5,         0.9,         \"INFORMATION FLOW IN LSTM\",         ha=\"center\",         fontsize=14,         fontweight=\"bold\",     )     ax4.text(0.5, 0.7, '1. Forget: Remove \"cat\"', ha=\"center\", fontsize=11)     ax4.text(0.5, 0.55, \"\u2193\", ha=\"center\", fontsize=16)     ax4.text(0.5, 0.4, '2. Input: Add \"park\"', ha=\"center\", fontsize=11)     ax4.text(0.5, 0.25, \"\u2193\", ha=\"center\", fontsize=16)     ax4.text(0.5, 0.1, '3. Output: Use \"park\"', ha=\"center\", fontsize=11)     ax4.axis(\"off\")      plt.tight_layout()     plt.show()   explain_lstm_gates() <pre>\n============================================================\nLSTM ARCHITECTURE: THE THREE GATES\n============================================================\n\n    An LSTM has three \"gates\" that control the flow of information:\n\n    1. FORGET GATE:\n       Determines what information from the past should be discarded.\n\n       Example: \"The cat was... Yesterday I went to the park...\"\n       \u2192 Forget \"cat\" because we changed topics\n\n    2. INPUT GATE:\n       Determines what new information should be stored.\n\n       Example: \"Yesterday I went to the park...\"\n       \u2192 Remember \"park\" because it's the new important topic\n\n    3. OUTPUT GATE:\n       Determines what information should be used in the current step.\n\n       Example: \"In the park I saw ___\"\n       \u2192 Use information from \"park\" to predict next word\n    \n</pre> <p>To deeply understand the internal workings of LSTMs, it is instructive to examine a simplified implementation that explicitly exposes the underlying mathematical operations. This pedagogical approach allows visualizing how the different gates interact and how the cell and hidden states are updated at each time step.</p> <p>The fundamental equations governing the behavior of an LSTM cell can be expressed mathematically as:</p> <p>$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$ $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$ $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$ $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$ $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \\odot \\tanh(C_t)$$</p> <p>where $f_t$, $i_t$ and $o_t$ represent the forget, input and output gates respectively, $C_t$ denotes the cell state, $h_t$ the hidden state, $\\sigma$ the sigmoid function, $\\odot$ the element-wise product (Hadamard), and $W$ and $b$ the corresponding weight matrices and bias vectors.</p> In\u00a0[3]: Copied! <pre>class SimpleLSTM:\n    \"\"\"\n    Simplified LSTM for educational purposes.\n    Shows basic concepts without all the mathematical complexity.\n    \"\"\"\n\n    def __init__(self, input_size=1, hidden_size=3):\n        self.hidden_size = hidden_size\n        self.C = np.zeros((hidden_size, 1))\n        self.h = np.zeros((hidden_size, 1))\n\n        self.W_forget = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.W_input = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.W_output = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.W_candidate = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n\n        print(\"Simple LSTM created\")\n        print(f\"  - Input size: {input_size}\")\n        print(f\"  - Hidden size: {hidden_size}\")\n        print(f\"  - Cell state (memory): {self.C.shape}\")\n        print(f\"  - Hidden state (output): {self.h.shape}\")\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def tanh(self, x):\n        return np.tanh(np.clip(x, -500, 500))\n\n    def step(self, x, verbose=True):\n        \"\"\"\n        Processes ONE step with all LSTM gates.\n        \"\"\"\n        x = np.array([[x]])\n        combined = np.vstack([self.h, x])\n\n        f = self.sigmoid(np.dot(self.W_forget, combined))\n        i = self.sigmoid(np.dot(self.W_input, combined))\n        C_candidate = self.tanh(np.dot(self.W_candidate, combined))\n        self.C = f * self.C + i * C_candidate\n        o = self.sigmoid(np.dot(self.W_output, combined))\n        self.h = o * self.tanh(self.C)\n\n        if verbose:\n            print(f\"\\n  Forget Gate: {f.flatten()}\")\n            print(f\"  Input Gate: {i.flatten()}\")\n            print(f\"  Output Gate: {o.flatten()}\")\n            print(f\"  Cell State (memory): {self.C.flatten()}\")\n            print(f\"  Hidden State (output): {self.h.flatten()}\")\n\n        return self.h.copy(), self.C.copy()\n\n    def process_sequence(self, sequence):\n        \"\"\"Processes a complete sequence.\"\"\"\n        print(\"\\n\" + \"=\" * 60)\n        print(\"PROCESSING SEQUENCE WITH LSTM\")\n        print(\"=\" * 60)\n        print(f\"Sequence: {sequence}\\n\")\n\n        states_h = []\n        states_C = []\n\n        for t, x in enumerate(sequence):\n            print(f\" Step {t + 1}: Input = {x} \")\n            h, C = self.step(x, verbose=True)\n            states_h.append(h)\n            states_C.append(C)\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"SEQUENCE COMPLETED\")\n        print(\"=\" * 60)\n        print(f\"Final memory (Cell State): {self.C.flatten()}\")\n        print(f\"Final output (Hidden State): {self.h.flatten()}\")\n\n        return states_h, states_C\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"EXAMPLE: LSTM processing sequence\")\nprint(\"=\" * 60)\n\nlstm = SimpleLSTM(input_size=1, hidden_size=3)\nsequence = [1.0, 2.0, 3.0, 4.0, 5.0]\nstates_h, states_C = lstm.process_sequence(sequence)\n</pre> class SimpleLSTM:     \"\"\"     Simplified LSTM for educational purposes.     Shows basic concepts without all the mathematical complexity.     \"\"\"      def __init__(self, input_size=1, hidden_size=3):         self.hidden_size = hidden_size         self.C = np.zeros((hidden_size, 1))         self.h = np.zeros((hidden_size, 1))          self.W_forget = np.random.randn(hidden_size, input_size + hidden_size) * 0.1         self.W_input = np.random.randn(hidden_size, input_size + hidden_size) * 0.1         self.W_output = np.random.randn(hidden_size, input_size + hidden_size) * 0.1         self.W_candidate = np.random.randn(hidden_size, input_size + hidden_size) * 0.1          print(\"Simple LSTM created\")         print(f\"  - Input size: {input_size}\")         print(f\"  - Hidden size: {hidden_size}\")         print(f\"  - Cell state (memory): {self.C.shape}\")         print(f\"  - Hidden state (output): {self.h.shape}\")      def sigmoid(self, x):         return 1 / (1 + np.exp(-np.clip(x, -500, 500)))      def tanh(self, x):         return np.tanh(np.clip(x, -500, 500))      def step(self, x, verbose=True):         \"\"\"         Processes ONE step with all LSTM gates.         \"\"\"         x = np.array([[x]])         combined = np.vstack([self.h, x])          f = self.sigmoid(np.dot(self.W_forget, combined))         i = self.sigmoid(np.dot(self.W_input, combined))         C_candidate = self.tanh(np.dot(self.W_candidate, combined))         self.C = f * self.C + i * C_candidate         o = self.sigmoid(np.dot(self.W_output, combined))         self.h = o * self.tanh(self.C)          if verbose:             print(f\"\\n  Forget Gate: {f.flatten()}\")             print(f\"  Input Gate: {i.flatten()}\")             print(f\"  Output Gate: {o.flatten()}\")             print(f\"  Cell State (memory): {self.C.flatten()}\")             print(f\"  Hidden State (output): {self.h.flatten()}\")          return self.h.copy(), self.C.copy()      def process_sequence(self, sequence):         \"\"\"Processes a complete sequence.\"\"\"         print(\"\\n\" + \"=\" * 60)         print(\"PROCESSING SEQUENCE WITH LSTM\")         print(\"=\" * 60)         print(f\"Sequence: {sequence}\\n\")          states_h = []         states_C = []          for t, x in enumerate(sequence):             print(f\" Step {t + 1}: Input = {x} \")             h, C = self.step(x, verbose=True)             states_h.append(h)             states_C.append(C)          print(\"\\n\" + \"=\" * 60)         print(\"SEQUENCE COMPLETED\")         print(\"=\" * 60)         print(f\"Final memory (Cell State): {self.C.flatten()}\")         print(f\"Final output (Hidden State): {self.h.flatten()}\")          return states_h, states_C   print(\"\\n\" + \"=\" * 60) print(\"EXAMPLE: LSTM processing sequence\") print(\"=\" * 60)  lstm = SimpleLSTM(input_size=1, hidden_size=3) sequence = [1.0, 2.0, 3.0, 4.0, 5.0] states_h, states_C = lstm.process_sequence(sequence) <pre>\n============================================================\nEXAMPLE: LSTM processing sequence\n============================================================\nSimple LSTM created\n  - Input size: 1\n  - Hidden size: 3\n  - Cell state (memory): (3, 1)\n  - Hidden state (output): (3, 1)\n\n============================================================\nPROCESSING SEQUENCE WITH LSTM\n============================================================\nSequence: [1.0, 2.0, 3.0, 4.0, 5.0]\n\n Step 1: Input = 1.0 \n\n  Forget Gate: [0.4530929  0.50363832 0.51740139]\n  Input Gate: [0.48513669 0.51574368 0.46694184]\n  Output Gate: [0.45936532 0.53311855 0.51961317]\n  Cell State (memory): [-0.00533709  0.05009132  0.03903091]\n  Hidden State (output): [-0.00245165  0.0266823   0.02027068]\n Step 2: Input = 2.0 \n\n  Forget Gate: [0.40663494 0.5074331  0.53522197]\n  Input Gate: [0.47054729 0.53205951 0.43193861]\n  Output Gate: [0.41948476 0.56607898 0.53746009]\n  Cell State (memory): [-0.01400619  0.12777299  0.09303898]\n  Hidden State (output): [-0.005875    0.07193854  0.04986095]\n Step 3: Input = 3.0 \n\n  Forget Gate: [0.36162574 0.51150538 0.55331194]\n  Input Gate: [0.45608335 0.54865611 0.39625455]\n  Output Gate: [0.38057974 0.59849771 0.55419072]\n  Cell State (memory): [-0.0236261   0.22149991  0.14991432]\n  Hidden State (output): [-0.00898994  0.1304409   0.08246427]\n Step 4: Input = 4.0 \n\n  Forget Gate: [0.31888181 0.51591332 0.57152646]\n  Input Gate: [0.44167391 0.56534309 0.36085462]\n  Output Gate: [0.34296941 0.63005011 0.57019853]\n  Cell State (memory): [-0.03251603  0.32488421  0.20342246]\n  Hidden State (output): [-0.01114808  0.19778315  0.1144173 ]\n Step 5: Input = 5.0 \n\n  Forget Gate: [0.27900657 0.52064565 0.58973095]\n  Input Gate: [0.42729659 0.58197856 0.32650198]\n  Output Gate: [0.30701662 0.66047671 0.58575178]\n  Cell State (memory): [-0.03989995  0.43392857  0.25045585]\n  Hidden State (output): [-0.01224345  0.26987028  0.14371251]\n\n============================================================\nSEQUENCE COMPLETED\n============================================================\nFinal memory (Cell State): [-0.03989995  0.43392857  0.25045585]\nFinal output (Hidden State): [-0.01224345  0.26987028  0.14371251]\n</pre> <p>Understanding the dynamic behavior of LSTMs is facilitated by simultaneously visualizing the evolution of the cell state and hidden state throughout the processing of a sequence. While the hidden state tends to fluctuate rapidly in response to each new input, reflecting information relevant to the current step, the cell state exhibits more gradual changes, accumulating information persistently across multiple time steps. This fundamental difference in the dynamics of both states constitutes the key to LSTM success in handling long-term dependencies.</p> In\u00a0[4]: Copied! <pre>def visualize_gates_in_action(states_h, states_C, sequence):\n    \"\"\"\n    Visualizes how LSTM states evolve.\n    \"\"\"\n    h_array = np.array([h.flatten() for h in states_h])\n    C_array = np.array([C.flatten() for C in states_C])\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    ax1 = axes[0, 0]\n    for i in range(h_array.shape[1]):\n        ax1.plot(\n            range(1, len(sequence) + 1),\n            h_array[:, i],\n            marker=\"o\",\n            label=f\"Neuron {i+1}\",\n        )\n    ax1.set_xlabel(\"Time Step\")\n    ax1.set_ylabel(\"Value\")\n    ax1.set_title(\"Hidden State (Short-Term Output)\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    ax2 = axes[0, 1]\n    for i in range(C_array.shape[1]):\n        ax2.plot(\n            range(1, len(sequence) + 1),\n            C_array[:, i],\n            marker=\"s\",\n            label=f\"Neuron {i+1}\",\n        )\n    ax2.set_xlabel(\"Time Step\")\n    ax2.set_ylabel(\"Value\")\n    ax2.set_title(\"Cell State (Long-Term Memory)\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    ax3 = axes[1, 0]\n    im1 = ax3.imshow(h_array.T, aspect=\"auto\", cmap=\"coolwarm\", interpolation=\"nearest\")\n    ax3.set_xlabel(\"Time Step\")\n    ax3.set_ylabel(\"Neuron\")\n    ax3.set_title(\"Heatmap: Hidden State\")\n    plt.colorbar(im1, ax=ax3)\n\n    for i, val in enumerate(sequence):\n        ax3.text(i, -0.5, f\"{val}\", ha=\"center\", va=\"top\")\n\n    ax4 = axes[1, 1]\n    im2 = ax4.imshow(C_array.T, aspect=\"auto\", cmap=\"viridis\", interpolation=\"nearest\")\n    ax4.set_xlabel(\"Time Step\")\n    ax4.set_ylabel(\"Neuron\")\n    ax4.set_title(\"Heatmap: Cell State\")\n    plt.colorbar(im2, ax=ax4)\n\n    for i, val in enumerate(sequence):\n        ax4.text(i, -0.5, f\"{val}\", ha=\"center\", va=\"top\")\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nINTERPRETATION:\")\n    print(\"-\" * 60)\n    print(\"1. Hidden State (top left):\")\n    print(\"   - Changes rapidly with each new input\")\n    print(\"   - Represents immediate output\")\n    print(\"\\n2. Cell State (top right):\")\n    print(\"   - Changes more smoothly\")\n    print(\"   - Accumulates information over time\")\n    print(\"   - Is the true 'memory' of the LSTM\")\n\n\nvisualize_gates_in_action(states_h, states_C, sequence)\n</pre> def visualize_gates_in_action(states_h, states_C, sequence):     \"\"\"     Visualizes how LSTM states evolve.     \"\"\"     h_array = np.array([h.flatten() for h in states_h])     C_array = np.array([C.flatten() for C in states_C])      fig, axes = plt.subplots(2, 2, figsize=(14, 10))      ax1 = axes[0, 0]     for i in range(h_array.shape[1]):         ax1.plot(             range(1, len(sequence) + 1),             h_array[:, i],             marker=\"o\",             label=f\"Neuron {i+1}\",         )     ax1.set_xlabel(\"Time Step\")     ax1.set_ylabel(\"Value\")     ax1.set_title(\"Hidden State (Short-Term Output)\")     ax1.legend()     ax1.grid(True, alpha=0.3)      ax2 = axes[0, 1]     for i in range(C_array.shape[1]):         ax2.plot(             range(1, len(sequence) + 1),             C_array[:, i],             marker=\"s\",             label=f\"Neuron {i+1}\",         )     ax2.set_xlabel(\"Time Step\")     ax2.set_ylabel(\"Value\")     ax2.set_title(\"Cell State (Long-Term Memory)\")     ax2.legend()     ax2.grid(True, alpha=0.3)      ax3 = axes[1, 0]     im1 = ax3.imshow(h_array.T, aspect=\"auto\", cmap=\"coolwarm\", interpolation=\"nearest\")     ax3.set_xlabel(\"Time Step\")     ax3.set_ylabel(\"Neuron\")     ax3.set_title(\"Heatmap: Hidden State\")     plt.colorbar(im1, ax=ax3)      for i, val in enumerate(sequence):         ax3.text(i, -0.5, f\"{val}\", ha=\"center\", va=\"top\")      ax4 = axes[1, 1]     im2 = ax4.imshow(C_array.T, aspect=\"auto\", cmap=\"viridis\", interpolation=\"nearest\")     ax4.set_xlabel(\"Time Step\")     ax4.set_ylabel(\"Neuron\")     ax4.set_title(\"Heatmap: Cell State\")     plt.colorbar(im2, ax=ax4)      for i, val in enumerate(sequence):         ax4.text(i, -0.5, f\"{val}\", ha=\"center\", va=\"top\")      plt.tight_layout()     plt.show()      print(\"\\nINTERPRETATION:\")     print(\"-\" * 60)     print(\"1. Hidden State (top left):\")     print(\"   - Changes rapidly with each new input\")     print(\"   - Represents immediate output\")     print(\"\\n2. Cell State (top right):\")     print(\"   - Changes more smoothly\")     print(\"   - Accumulates information over time\")     print(\"   - Is the true 'memory' of the LSTM\")   visualize_gates_in_action(states_h, states_C, sequence) <pre>\nINTERPRETATION:\n------------------------------------------------------------\n1. Hidden State (top left):\n   - Changes rapidly with each new input\n   - Represents immediate output\n\n2. Cell State (top right):\n   - Changes more smoothly\n   - Accumulates information over time\n   - Is the true 'memory' of the LSTM\n</pre> <p>While manual implementations provide deep understanding of internal mechanisms, in professional practice optimized frameworks are used that encapsulate computational complexity. PyTorch offers highly efficient LSTM modules that automatically handle aspects such as gradient calculation through backpropagation through time, weight initialization, and GPU acceleration.</p> <p>To empirically demonstrate the superiority of LSTMs over simple RNNs in tasks requiring long-term memory, it is instructive to design a controlled experiment where the objective explicitly depends on information presented at the beginning of the sequence. This configuration allows directly evaluating the ability of each architecture to maintain and use information across multiple time steps.</p> In\u00a0[5]: Copied! <pre>class LSTMPyTorch(nn.Module):\n    \"\"\"\n    Professional LSTM using PyTorch.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n        super(LSTMPyTorch, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.2 if num_layers &gt; 1 else 0,\n        )\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: Tensor of shape (batch, sequence, features)\n\n        Returns:\n            prediction: Tensor of shape (batch, output_size)\n        \"\"\"\n        output, (h_n, c_n) = self.lstm(x)\n        last_output = output[:, -1, :]\n        prediction = self.fc(last_output)\n        return prediction\n\n\ndef compare_rnn_vs_lstm():\n    \"\"\"\n    Compares simple RNN vs LSTM on a real task.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"COMPARISON: RNN vs LSTM\")\n    print(\"=\" * 60)\n\n    def create_memory_sequences(num_examples=500):\n        \"\"\"\n        Creates sequences where the target depends on the FIRST element.\n        Example: [5, 2, 3, 8, 1, 9] -&gt; Predict if first number (5) is even/odd\n        \"\"\"\n        X = []\n        y = []\n\n        for _ in range(num_examples):\n            sequence = np.random.randint(0, 10, size=10)\n            first_element = sequence[0]\n            is_even = 1 if first_element % 2 == 0 else 0\n            X.append(sequence)\n            y.append(is_even)\n\n        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n\n    X, y = create_memory_sequences(500)\n\n    print(f\"\\nTask: Predict if the FIRST number is even/odd\")\n    print(f\"Example sequences:\")\n    for i in range(3):\n        result = \"EVEN\" if y[i] == 1 else \"ODD\"\n        print(f\"  {X[i]} -&gt; First element: {int(X[i][0])} -&gt; {result}\")\n\n    X_tensor = torch.FloatTensor(X).unsqueeze(-1)\n    y_tensor = torch.FloatTensor(y).unsqueeze(-1)\n\n    split = int(0.8 * len(X))\n    X_train, X_test = X_tensor[:split], X_tensor[split:]\n    y_train, y_test = y_tensor[:split], y_tensor[split:]\n\n    from torch.utils.data import TensorDataset, DataLoader\n\n    train_dataset = TensorDataset(X_train, y_train)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n    class SimpleRNN(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(SimpleRNN, self).__init__()\n            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n            output, h_n = self.rnn(x)\n            out = self.fc(output[:, -1, :])\n            return self.sigmoid(out)\n\n    class LSTMModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(LSTMModel, self).__init__()\n            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n            output, (h_n, c_n) = self.lstm(x)\n            out = self.fc(output[:, -1, :])\n            return self.sigmoid(out)\n\n    def train_model(model, train_loader, epochs=50):\n        criterion = nn.BCELoss()\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        history = []\n\n        for epoch in range(epochs):\n            epoch_loss = 0\n            for X_batch, y_batch in train_loader:\n                optimizer.zero_grad()\n                predictions = model(X_batch)\n                loss = criterion(predictions, y_batch)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            history.append(epoch_loss / len(train_loader))\n\n        return history\n\n    print(\"\\n\" + \"-\" * 60)\n    print(\"Training Simple RNN...\")\n    print(\"-\" * 60)\n    rnn = SimpleRNN(1, 20, 1)\n    hist_rnn = train_model(rnn, train_loader, epochs=50)\n\n    print(\"\\n\" + \"-\" * 60)\n    print(\"Training LSTM...\")\n    print(\"-\" * 60)\n    lstm = LSTMModel(1, 20, 1)\n    hist_lstm = train_model(lstm, train_loader, epochs=50)\n\n    def evaluate(model, X, y):\n        model.eval()\n        with torch.no_grad():\n            predictions = model(X)\n            binary_predictions = (predictions &gt; 0.5).float()\n            accuracy = (binary_predictions == y).float().mean()\n        return accuracy.item()\n\n    accuracy_rnn = evaluate(rnn, X_test, y_test)\n    accuracy_lstm = evaluate(lstm, X_test, y_test)\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    axes[0].plot(hist_rnn, label=\"Simple RNN\", linewidth=2)\n    axes[0].plot(hist_lstm, label=\"LSTM\", linewidth=2)\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Training Curves\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    models = [\"Simple RNN\", \"LSTM\"]\n    accuracies = [accuracy_rnn * 100, accuracy_lstm * 100]\n    colors = [\"salmon\", \"lightgreen\"]\n\n    bars = axes[1].bar(models, accuracies, color=colors, edgecolor=\"black\", linewidth=2)\n    axes[1].set_ylabel(\"Accuracy (%)\")\n    axes[1].set_title(\"Accuracy on Test Set\")\n    axes[1].set_ylim(0, 100)\n    axes[1].axhline(y=50, color=\"red\", linestyle=\"--\", label=\"Random (50%)\")\n    axes[1].legend()\n\n    for bar, accuracy in zip(bars, accuracies):\n        height = bar.get_height()\n        axes[1].text(\n            bar.get_x() + bar.get_width() / 2.0,\n            height,\n            f\"{accuracy:.1f}%\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=12,\n            fontweight=\"bold\",\n        )\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"RESULTS\")\n    print(\"=\" * 60)\n    print(f\"Simple RNN - Accuracy: {accuracy_rnn*100:.2f}%\")\n    print(f\"LSTM       - Accuracy: {accuracy_lstm*100:.2f}%\")\n    print(\"\\nWhy is LSTM better?\")\n    print(\"- Maintains memory of the FIRST element of the sequence\")\n    print(\"- Gates protect important information\")\n    print(\"- Simple RNN forgets early information\")\n\n\ncompare_rnn_vs_lstm()\n</pre> class LSTMPyTorch(nn.Module):     \"\"\"     Professional LSTM using PyTorch.     \"\"\"      def __init__(self, input_size, hidden_size, output_size, num_layers=1):         super(LSTMPyTorch, self).__init__()         self.hidden_size = hidden_size         self.num_layers = num_layers          self.lstm = nn.LSTM(             input_size=input_size,             hidden_size=hidden_size,             num_layers=num_layers,             batch_first=True,             dropout=0.2 if num_layers &gt; 1 else 0,         )          self.fc = nn.Linear(hidden_size, output_size)      def forward(self, x):         \"\"\"         Forward pass.          Args:             x: Tensor of shape (batch, sequence, features)          Returns:             prediction: Tensor of shape (batch, output_size)         \"\"\"         output, (h_n, c_n) = self.lstm(x)         last_output = output[:, -1, :]         prediction = self.fc(last_output)         return prediction   def compare_rnn_vs_lstm():     \"\"\"     Compares simple RNN vs LSTM on a real task.     \"\"\"     print(\"\\n\" + \"=\" * 60)     print(\"COMPARISON: RNN vs LSTM\")     print(\"=\" * 60)      def create_memory_sequences(num_examples=500):         \"\"\"         Creates sequences where the target depends on the FIRST element.         Example: [5, 2, 3, 8, 1, 9] -&gt; Predict if first number (5) is even/odd         \"\"\"         X = []         y = []          for _ in range(num_examples):             sequence = np.random.randint(0, 10, size=10)             first_element = sequence[0]             is_even = 1 if first_element % 2 == 0 else 0             X.append(sequence)             y.append(is_even)          return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)      X, y = create_memory_sequences(500)      print(f\"\\nTask: Predict if the FIRST number is even/odd\")     print(f\"Example sequences:\")     for i in range(3):         result = \"EVEN\" if y[i] == 1 else \"ODD\"         print(f\"  {X[i]} -&gt; First element: {int(X[i][0])} -&gt; {result}\")      X_tensor = torch.FloatTensor(X).unsqueeze(-1)     y_tensor = torch.FloatTensor(y).unsqueeze(-1)      split = int(0.8 * len(X))     X_train, X_test = X_tensor[:split], X_tensor[split:]     y_train, y_test = y_tensor[:split], y_tensor[split:]      from torch.utils.data import TensorDataset, DataLoader      train_dataset = TensorDataset(X_train, y_train)     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)      class SimpleRNN(nn.Module):         def __init__(self, input_size, hidden_size, output_size):             super(SimpleRNN, self).__init__()             self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)             self.fc = nn.Linear(hidden_size, output_size)             self.sigmoid = nn.Sigmoid()          def forward(self, x):             output, h_n = self.rnn(x)             out = self.fc(output[:, -1, :])             return self.sigmoid(out)      class LSTMModel(nn.Module):         def __init__(self, input_size, hidden_size, output_size):             super(LSTMModel, self).__init__()             self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)             self.fc = nn.Linear(hidden_size, output_size)             self.sigmoid = nn.Sigmoid()          def forward(self, x):             output, (h_n, c_n) = self.lstm(x)             out = self.fc(output[:, -1, :])             return self.sigmoid(out)      def train_model(model, train_loader, epochs=50):         criterion = nn.BCELoss()         optimizer = optim.Adam(model.parameters(), lr=0.001)         history = []          for epoch in range(epochs):             epoch_loss = 0             for X_batch, y_batch in train_loader:                 optimizer.zero_grad()                 predictions = model(X_batch)                 loss = criterion(predictions, y_batch)                 loss.backward()                 optimizer.step()                 epoch_loss += loss.item()              history.append(epoch_loss / len(train_loader))          return history      print(\"\\n\" + \"-\" * 60)     print(\"Training Simple RNN...\")     print(\"-\" * 60)     rnn = SimpleRNN(1, 20, 1)     hist_rnn = train_model(rnn, train_loader, epochs=50)      print(\"\\n\" + \"-\" * 60)     print(\"Training LSTM...\")     print(\"-\" * 60)     lstm = LSTMModel(1, 20, 1)     hist_lstm = train_model(lstm, train_loader, epochs=50)      def evaluate(model, X, y):         model.eval()         with torch.no_grad():             predictions = model(X)             binary_predictions = (predictions &gt; 0.5).float()             accuracy = (binary_predictions == y).float().mean()         return accuracy.item()      accuracy_rnn = evaluate(rnn, X_test, y_test)     accuracy_lstm = evaluate(lstm, X_test, y_test)      fig, axes = plt.subplots(1, 2, figsize=(14, 5))      axes[0].plot(hist_rnn, label=\"Simple RNN\", linewidth=2)     axes[0].plot(hist_lstm, label=\"LSTM\", linewidth=2)     axes[0].set_xlabel(\"Epoch\")     axes[0].set_ylabel(\"Loss\")     axes[0].set_title(\"Training Curves\")     axes[0].legend()     axes[0].grid(True, alpha=0.3)      models = [\"Simple RNN\", \"LSTM\"]     accuracies = [accuracy_rnn * 100, accuracy_lstm * 100]     colors = [\"salmon\", \"lightgreen\"]      bars = axes[1].bar(models, accuracies, color=colors, edgecolor=\"black\", linewidth=2)     axes[1].set_ylabel(\"Accuracy (%)\")     axes[1].set_title(\"Accuracy on Test Set\")     axes[1].set_ylim(0, 100)     axes[1].axhline(y=50, color=\"red\", linestyle=\"--\", label=\"Random (50%)\")     axes[1].legend()      for bar, accuracy in zip(bars, accuracies):         height = bar.get_height()         axes[1].text(             bar.get_x() + bar.get_width() / 2.0,             height,             f\"{accuracy:.1f}%\",             ha=\"center\",             va=\"bottom\",             fontsize=12,             fontweight=\"bold\",         )      plt.tight_layout()     plt.show()      print(\"\\n\" + \"=\" * 60)     print(\"RESULTS\")     print(\"=\" * 60)     print(f\"Simple RNN - Accuracy: {accuracy_rnn*100:.2f}%\")     print(f\"LSTM       - Accuracy: {accuracy_lstm*100:.2f}%\")     print(\"\\nWhy is LSTM better?\")     print(\"- Maintains memory of the FIRST element of the sequence\")     print(\"- Gates protect important information\")     print(\"- Simple RNN forgets early information\")   compare_rnn_vs_lstm() <pre>\n============================================================\nCOMPARISON: RNN vs LSTM\n============================================================\n\nTask: Predict if the FIRST number is even/odd\nExample sequences:\n  [1. 4. 9. 6. 1. 6. 9. 3. 4. 5.] -&gt; First element: 1 -&gt; ODD\n  [5. 7. 0. 9. 6. 8. 2. 7. 1. 3.] -&gt; First element: 5 -&gt; ODD\n  [8. 2. 9. 5. 1. 4. 8. 5. 3. 0.] -&gt; First element: 8 -&gt; EVEN\n\n------------------------------------------------------------\nTraining Simple RNN...\n------------------------------------------------------------\n</pre> <pre>\n------------------------------------------------------------\nTraining LSTM...\n------------------------------------------------------------\n</pre> <pre>\n============================================================\nRESULTS\n============================================================\nSimple RNN - Accuracy: 45.00%\nLSTM       - Accuracy: 49.00%\n\nWhy is LSTM better?\n- Maintains memory of the FIRST element of the sequence\n- Gates protect important information\n- Simple RNN forgets early information\n</pre> <p>Time series prediction constitutes one of the most relevant applications of LSTMs in industrial and scientific contexts. Time series, characterized by chronologically ordered sequential observations, frequently present complex patterns that include long-term trends, periodic seasonal components, and stochastic fluctuations. LSTMs are particularly suitable for this type of data due to their ability to capture temporal dependencies at multiple scales.</p> <p>In this context, the objective consists of using a window of historical observations to predict future values. The LSTM architecture processes the input sequence, extracting relevant patterns and encoding them in its internal state, to subsequently generate predictions over the desired time horizon. This approach allows performing multi-step predictions, where multiple future values are estimated simultaneously.</p> In\u00a0[6]: Copied! <pre>from torch.utils.data import TensorDataset, DataLoader\n\n\ndef time_series_lstm_project():\n    \"\"\"\n    Complete project: Time series predictor with LSTM.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PROJECT: Time Series Prediction with LSTM\")\n    print(\"=\" * 60)\n\n    def generate_time_series(n=1000):\n        \"\"\"\n        Generates a time series with multiple components:\n        - Trend\n        - Seasonality\n        - Noise\n        \"\"\"\n        t = np.arange(n)\n        trend = 0.02 * t\n        seasonal = 10 * np.sin(2 * np.pi * t / 50) + 5 * np.sin(2 * np.pi * t / 25)\n        noise = np.random.normal(0, 1, n)\n        series = trend + seasonal + noise\n        return series, t\n\n    series, time = generate_time_series(1000)\n\n    plt.figure(figsize=(14, 4))\n    plt.plot(time[:200], series[:200])\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Original Time Series (first 200 points)\")\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    print(f\"\\nSeries generated: {len(series)} points\")\n    print(f\"Minimum: {series.min():.2f}, Maximum: {series.max():.2f}\")\n\n    def create_sequences(series, sequence_length=50, horizon=1):\n        \"\"\"\n        Converts series into sequences for training.\n\n        Args:\n            series: Time series\n            sequence_length: How many steps to look back\n            horizon: How many steps to predict forward\n        \"\"\"\n        X, y = [], []\n\n        for i in range(len(series) - sequence_length - horizon + 1):\n            X.append(series[i : i + sequence_length])\n            y.append(series[i + sequence_length : i + sequence_length + horizon])\n\n        return np.array(X), np.array(y)\n\n    from sklearn.preprocessing import MinMaxScaler\n\n    scaler = MinMaxScaler()\n    series_norm = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n\n    SEQ_LENGTH = 50\n    HORIZON = 10\n\n    X, y = create_sequences(series_norm, SEQ_LENGTH, HORIZON)\n\n    print(f\"\\nSequences created:\")\n    print(f\"  X shape: {X.shape} (samples, sequence_length)\")\n    print(f\"  y shape: {y.shape} (samples, horizon)\")\n\n    split = int(0.8 * len(X))\n    X_train, X_test = X[:split], X[split:]\n    y_train, y_test = y[:split], y[split:]\n\n    X_train_t = torch.FloatTensor(X_train).unsqueeze(-1)\n    y_train_t = torch.FloatTensor(y_train)\n    X_test_t = torch.FloatTensor(X_test).unsqueeze(-1)\n    y_test_t = torch.FloatTensor(y_test)\n\n    print(f\"\\nTraining data: {len(X_train)} sequences\")\n    print(f\"Test data: {len(X_test)} sequences\")\n\n    class LSTMPredictor(nn.Module):\n        def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=10):\n            super(LSTMPredictor, self).__init__()\n            self.lstm = nn.LSTM(\n                input_size=input_size,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=0.2,\n            )\n            self.fc = nn.Linear(hidden_size, output_size)\n\n        def forward(self, x):\n            lstm_out, (h_n, c_n) = self.lstm(x)\n            last_output = lstm_out[:, -1, :]\n            prediction = self.fc(last_output)\n            return prediction\n\n    model = LSTMPredictor(\n        input_size=1, hidden_size=64, num_layers=2, output_size=HORIZON\n    )\n\n    print(f\"\\nLSTM Model:\")\n    print(model)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    train_dataset = TensorDataset(X_train_t, y_train_t)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n    print(\"\\nTraining LSTM...\")\n    train_history = []\n    test_history = []\n    EPOCHS = 50\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = criterion(predictions, y_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_train_loss = epoch_loss / len(train_loader)\n        train_history.append(avg_train_loss)\n\n        model.eval()\n        with torch.no_grad():\n            pred_test = model(X_test_t)\n            test_loss = criterion(pred_test, y_test_t).item()\n            test_history.append(test_loss)\n\n        if (epoch + 1) % 10 == 0:\n            print(\n                f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f}, Test Loss: {test_loss:.6f}\"\n            )\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_history, label=\"Train Loss\")\n    plt.plot(test_history, label=\"Test Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss (MSE)\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.yscale(\"log\")\n    plt.show()\n\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_test_t).numpy()\n\n    predictions_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(\n        predictions.shape\n    )\n    y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n\n    n_examples = 5\n    fig, axes = plt.subplots(n_examples, 1, figsize=(14, 3 * n_examples))\n\n    for i in range(n_examples):\n        idx = i * 20\n        input_data = scaler.inverse_transform(X_test[idx].reshape(-1, 1)).flatten()\n\n        axes[i].plot(\n            range(len(input_data)), input_data, \"b-\", label=\"History\", linewidth=2\n        )\n        axes[i].plot(\n            range(len(input_data), len(input_data) + HORIZON),\n            y_test_orig[idx],\n            \"g-\",\n            marker=\"o\",\n            label=\"Actual\",\n            linewidth=2,\n        )\n        axes[i].plot(\n            range(len(input_data), len(input_data) + HORIZON),\n            predictions_orig[idx],\n            \"r--\",\n            marker=\"x\",\n            label=\"Prediction\",\n            linewidth=2,\n        )\n\n        axes[i].axvline(\n            x=len(input_data) - 0.5, color=\"gray\", linestyle=\"--\", alpha=0.5\n        )\n        axes[i].set_xlabel(\"Time Step\")\n        axes[i].set_ylabel(\"Value\")\n        axes[i].set_title(f\"Example {i+1}\")\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    mae = np.mean(np.abs(y_test_orig - predictions_orig))\n    rmse = np.sqrt(np.mean((y_test_orig - predictions_orig) ** 2))\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FINAL RESULTS\")\n    print(\"=\" * 60)\n    print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n    print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n    print(f\"\\nThe model can predict {HORIZON} steps into the future\")\n    print(f\"based on the last {SEQ_LENGTH} observed values.\")\n\n\ntime_series_lstm_project()\n</pre> from torch.utils.data import TensorDataset, DataLoader   def time_series_lstm_project():     \"\"\"     Complete project: Time series predictor with LSTM.     \"\"\"     print(\"\\n\" + \"=\" * 60)     print(\"PROJECT: Time Series Prediction with LSTM\")     print(\"=\" * 60)      def generate_time_series(n=1000):         \"\"\"         Generates a time series with multiple components:         - Trend         - Seasonality         - Noise         \"\"\"         t = np.arange(n)         trend = 0.02 * t         seasonal = 10 * np.sin(2 * np.pi * t / 50) + 5 * np.sin(2 * np.pi * t / 25)         noise = np.random.normal(0, 1, n)         series = trend + seasonal + noise         return series, t      series, time = generate_time_series(1000)      plt.figure(figsize=(14, 4))     plt.plot(time[:200], series[:200])     plt.xlabel(\"Time\")     plt.ylabel(\"Value\")     plt.title(\"Original Time Series (first 200 points)\")     plt.grid(True, alpha=0.3)     plt.show()      print(f\"\\nSeries generated: {len(series)} points\")     print(f\"Minimum: {series.min():.2f}, Maximum: {series.max():.2f}\")      def create_sequences(series, sequence_length=50, horizon=1):         \"\"\"         Converts series into sequences for training.          Args:             series: Time series             sequence_length: How many steps to look back             horizon: How many steps to predict forward         \"\"\"         X, y = [], []          for i in range(len(series) - sequence_length - horizon + 1):             X.append(series[i : i + sequence_length])             y.append(series[i + sequence_length : i + sequence_length + horizon])          return np.array(X), np.array(y)      from sklearn.preprocessing import MinMaxScaler      scaler = MinMaxScaler()     series_norm = scaler.fit_transform(series.reshape(-1, 1)).flatten()      SEQ_LENGTH = 50     HORIZON = 10      X, y = create_sequences(series_norm, SEQ_LENGTH, HORIZON)      print(f\"\\nSequences created:\")     print(f\"  X shape: {X.shape} (samples, sequence_length)\")     print(f\"  y shape: {y.shape} (samples, horizon)\")      split = int(0.8 * len(X))     X_train, X_test = X[:split], X[split:]     y_train, y_test = y[:split], y[split:]      X_train_t = torch.FloatTensor(X_train).unsqueeze(-1)     y_train_t = torch.FloatTensor(y_train)     X_test_t = torch.FloatTensor(X_test).unsqueeze(-1)     y_test_t = torch.FloatTensor(y_test)      print(f\"\\nTraining data: {len(X_train)} sequences\")     print(f\"Test data: {len(X_test)} sequences\")      class LSTMPredictor(nn.Module):         def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=10):             super(LSTMPredictor, self).__init__()             self.lstm = nn.LSTM(                 input_size=input_size,                 hidden_size=hidden_size,                 num_layers=num_layers,                 batch_first=True,                 dropout=0.2,             )             self.fc = nn.Linear(hidden_size, output_size)          def forward(self, x):             lstm_out, (h_n, c_n) = self.lstm(x)             last_output = lstm_out[:, -1, :]             prediction = self.fc(last_output)             return prediction      model = LSTMPredictor(         input_size=1, hidden_size=64, num_layers=2, output_size=HORIZON     )      print(f\"\\nLSTM Model:\")     print(model)      criterion = nn.MSELoss()     optimizer = optim.Adam(model.parameters(), lr=0.001)      train_dataset = TensorDataset(X_train_t, y_train_t)     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)      print(\"\\nTraining LSTM...\")     train_history = []     test_history = []     EPOCHS = 50      for epoch in range(EPOCHS):         model.train()         epoch_loss = 0          for X_batch, y_batch in train_loader:             optimizer.zero_grad()             predictions = model(X_batch)             loss = criterion(predictions, y_batch)             loss.backward()             optimizer.step()             epoch_loss += loss.item()          avg_train_loss = epoch_loss / len(train_loader)         train_history.append(avg_train_loss)          model.eval()         with torch.no_grad():             pred_test = model(X_test_t)             test_loss = criterion(pred_test, y_test_t).item()             test_history.append(test_loss)          if (epoch + 1) % 10 == 0:             print(                 f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f}, Test Loss: {test_loss:.6f}\"             )      plt.figure(figsize=(10, 5))     plt.plot(train_history, label=\"Train Loss\")     plt.plot(test_history, label=\"Test Loss\")     plt.xlabel(\"Epoch\")     plt.ylabel(\"Loss (MSE)\")     plt.title(\"Learning Curve\")     plt.legend()     plt.grid(True, alpha=0.3)     plt.yscale(\"log\")     plt.show()      model.eval()     with torch.no_grad():         predictions = model(X_test_t).numpy()      predictions_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(         predictions.shape     )     y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)      n_examples = 5     fig, axes = plt.subplots(n_examples, 1, figsize=(14, 3 * n_examples))      for i in range(n_examples):         idx = i * 20         input_data = scaler.inverse_transform(X_test[idx].reshape(-1, 1)).flatten()          axes[i].plot(             range(len(input_data)), input_data, \"b-\", label=\"History\", linewidth=2         )         axes[i].plot(             range(len(input_data), len(input_data) + HORIZON),             y_test_orig[idx],             \"g-\",             marker=\"o\",             label=\"Actual\",             linewidth=2,         )         axes[i].plot(             range(len(input_data), len(input_data) + HORIZON),             predictions_orig[idx],             \"r--\",             marker=\"x\",             label=\"Prediction\",             linewidth=2,         )          axes[i].axvline(             x=len(input_data) - 0.5, color=\"gray\", linestyle=\"--\", alpha=0.5         )         axes[i].set_xlabel(\"Time Step\")         axes[i].set_ylabel(\"Value\")         axes[i].set_title(f\"Example {i+1}\")         axes[i].legend()         axes[i].grid(True, alpha=0.3)      plt.tight_layout()     plt.show()      mae = np.mean(np.abs(y_test_orig - predictions_orig))     rmse = np.sqrt(np.mean((y_test_orig - predictions_orig) ** 2))      print(\"\\n\" + \"=\" * 60)     print(\"FINAL RESULTS\")     print(\"=\" * 60)     print(f\"MAE (Mean Absolute Error): {mae:.4f}\")     print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")     print(f\"\\nThe model can predict {HORIZON} steps into the future\")     print(f\"based on the last {SEQ_LENGTH} observed values.\")   time_series_lstm_project() <pre>\n============================================================\nPROJECT: Time Series Prediction with LSTM\n============================================================\n</pre> <pre>\nSeries generated: 1000 points\nMinimum: -12.75, Maximum: 33.27\n</pre> <pre>\nSequences created:\n  X shape: (941, 50) (samples, sequence_length)\n  y shape: (941, 10) (samples, horizon)\n\nTraining data: 752 sequences\nTest data: 189 sequences\n\nLSTM Model:\nLSTMPredictor(\n  (lstm): LSTM(1, 64, num_layers=2, batch_first=True, dropout=0.2)\n  (fc): Linear(in_features=64, out_features=10, bias=True)\n)\n\nTraining LSTM...\n</pre> <pre>Epoch 10/50 - Train Loss: 0.015223, Test Loss: 0.016930\n</pre> <pre>Epoch 20/50 - Train Loss: 0.002486, Test Loss: 0.003815\n</pre> <pre>Epoch 30/50 - Train Loss: 0.001901, Test Loss: 0.002834\n</pre> <pre>Epoch 40/50 - Train Loss: 0.001324, Test Loss: 0.001875\n</pre> <pre>Epoch 50/50 - Train Loss: 0.001029, Test Loss: 0.001281\n</pre> <pre>\n============================================================\nFINAL RESULTS\n============================================================\nMAE (Mean Absolute Error): 1.2897\nRMSE (Root Mean Squared Error): 1.6468\n\nThe model can predict 10 steps into the future\nbased on the last 50 observed values.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"course/topic_05_sequential_models/section_03_lstm.html#long-short-term-memory-lstm","title":"Long Short-Term Memory (LSTM)\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#the-problem-of-long-term-dependencies","title":"The problem of long-term dependencies\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#lstm-architecture-the-gate-mechanism","title":"LSTM Architecture: The gate mechanism\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#conceptual-implementation-of-an-lstm","title":"Conceptual implementation of an LSTM\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#visualization-of-state-evolution","title":"Visualization of state evolution\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#implementation-with-pytorch-and-empirical-comparison","title":"Implementation with PyTorch and empirical comparison\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_03_lstm.html#application-to-time-series-prediction","title":"Application to time series prediction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html","title":"Autoencoders","text":"<p>Autoencoders constitute a class of neural architectures designed to learn compressed data representations through an encoding-decoding process. These networks are trained in an unsupervised manner to reconstruct their own inputs, forcing the network to capture the most relevant features of the data in a reduced dimensionality representation called latent space.</p> <p>The fundamental architecture of an autoencoder consists of two main components: the encoder, which transforms the input into a compressed latent representation, and the decoder, which reconstructs the original input from this representation. During training with normal data, the autoencoder learns to minimize reconstruction error, implicitly capturing the underlying distribution of the data. This property is especially useful for anomaly detection: when atypical or abnormal data is presented, the trained autoencoder produces low-quality reconstructions, manifesting significantly higher reconstruction errors that allow identifying these anomalies.</p> <p>To demonstrate the capability of autoencoders in anomaly detection, it is necessary to generate synthetic time series containing both normal patterns and controlled anomalies. Normal time series are constructed by combining deterministic components (trend and seasonality) with stochastic noise, while anomalies are subsequently injected as abrupt deviations from these established patterns.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Environment configured correctly\")\n\n\ndef generate_normal_time_series(n_points=1000):\n    \"\"\"\n    Generates a 'normal' time series with predictable patterns.\n    \"\"\"\n    t = np.arange(n_points)\n    trend = 0.01 * t\n    seasonal = 5 * np.sin(2 * np.pi * t / 50)\n    noise = np.random.normal(0, 0.5, n_points)\n    series = trend + seasonal + noise\n    return series\n\n\ndef inject_anomalies(series, n_anomalies=5, type=\"spike\"):\n    \"\"\"\n    Injects anomalies into the time series.\n\n    Args:\n        series: Original time series\n        n_anomalies: Number of anomalies to inject\n        type: 'spike' or 'dip' or 'flat'\n    \"\"\"\n    series_with_anomalies = series.copy()\n    anomaly_indices = np.random.choice(\n        range(100, len(series) - 100), size=n_anomalies, replace=False\n    )\n\n    for idx in anomaly_indices:\n        if type == \"spike\":\n            series_with_anomalies[idx : idx + 10] += np.random.uniform(15, 25)\n        elif type == \"dip\":\n            series_with_anomalies[idx : idx + 10] -= np.random.uniform(15, 25)\n        elif type == \"flat\":\n            series_with_anomalies[idx : idx + 20] = np.mean(series)\n\n    return series_with_anomalies, anomaly_indices\n\n\nprint(\"=\" * 60)\nprint(\"GENERATING DATA\")\nprint(\"=\" * 60)\n\nnormal_series = generate_normal_time_series(1000)\nseries_with_anomalies, anomaly_indices = inject_anomalies(\n    normal_series, n_anomalies=5, type=\"spike\"\n)\n\nprint(f\"\\nTime series generated: {len(normal_series)} points\")\nprint(f\"Anomalies injected: {len(anomaly_indices)}\")\nprint(f\"Anomaly positions: {sorted(anomaly_indices)}\")\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\naxes[0].plot(normal_series, label=\"Normal Series\", linewidth=1)\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Value\")\naxes[0].set_title(\"Normal Time Series\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(\n    series_with_anomalies, label=\"Series with Anomalies\", linewidth=1, color=\"orange\"\n)\nfor idx in anomaly_indices:\n    axes[1].axvspan(\n        idx,\n        idx + 10,\n        alpha=0.3,\n        color=\"red\",\n        label=\"Anomaly\" if idx == anomaly_indices[0] else \"\",\n    )\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Value\")\naxes[1].set_title(\"Time Series with Anomalies\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from sklearn.preprocessing import StandardScaler import warnings  warnings.filterwarnings(\"ignore\")  np.random.seed(42) torch.manual_seed(42)  print(\"Environment configured correctly\")   def generate_normal_time_series(n_points=1000):     \"\"\"     Generates a 'normal' time series with predictable patterns.     \"\"\"     t = np.arange(n_points)     trend = 0.01 * t     seasonal = 5 * np.sin(2 * np.pi * t / 50)     noise = np.random.normal(0, 0.5, n_points)     series = trend + seasonal + noise     return series   def inject_anomalies(series, n_anomalies=5, type=\"spike\"):     \"\"\"     Injects anomalies into the time series.      Args:         series: Original time series         n_anomalies: Number of anomalies to inject         type: 'spike' or 'dip' or 'flat'     \"\"\"     series_with_anomalies = series.copy()     anomaly_indices = np.random.choice(         range(100, len(series) - 100), size=n_anomalies, replace=False     )      for idx in anomaly_indices:         if type == \"spike\":             series_with_anomalies[idx : idx + 10] += np.random.uniform(15, 25)         elif type == \"dip\":             series_with_anomalies[idx : idx + 10] -= np.random.uniform(15, 25)         elif type == \"flat\":             series_with_anomalies[idx : idx + 20] = np.mean(series)      return series_with_anomalies, anomaly_indices   print(\"=\" * 60) print(\"GENERATING DATA\") print(\"=\" * 60)  normal_series = generate_normal_time_series(1000) series_with_anomalies, anomaly_indices = inject_anomalies(     normal_series, n_anomalies=5, type=\"spike\" )  print(f\"\\nTime series generated: {len(normal_series)} points\") print(f\"Anomalies injected: {len(anomaly_indices)}\") print(f\"Anomaly positions: {sorted(anomaly_indices)}\")  fig, axes = plt.subplots(2, 1, figsize=(14, 8))  axes[0].plot(normal_series, label=\"Normal Series\", linewidth=1) axes[0].set_xlabel(\"Time\") axes[0].set_ylabel(\"Value\") axes[0].set_title(\"Normal Time Series\") axes[0].legend() axes[0].grid(True, alpha=0.3)  axes[1].plot(     series_with_anomalies, label=\"Series with Anomalies\", linewidth=1, color=\"orange\" ) for idx in anomaly_indices:     axes[1].axvspan(         idx,         idx + 10,         alpha=0.3,         color=\"red\",         label=\"Anomaly\" if idx == anomaly_indices[0] else \"\",     ) axes[1].set_xlabel(\"Time\") axes[1].set_ylabel(\"Value\") axes[1].set_title(\"Time Series with Anomalies\") axes[1].legend() axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show() <pre>Environment configured correctly\n============================================================\nGENERATING DATA\n============================================================\n\nTime series generated: 1000 points\nAnomalies injected: 5\nAnomaly positions: [np.int64(136), np.int64(153), np.int64(156), np.int64(630), np.int64(656)]\n</pre> <p>Applying autoencoders to time series requires transforming the one-dimensional sequence into a set of fixed-length windows. This sliding window technique allows capturing local temporal patterns and provides multiple training examples from a single series. Each window represents a subsequence that the autoencoder will learn to reconstruct, and normalization of this data is essential to stabilize the training process.</p> In\u00a0[2]: Copied! <pre>def create_windows(series, window_size=50):\n    \"\"\"\n    Converts the time series into sliding windows.\n\n    Args:\n        series: Time series\n        window_size: Size of each window\n\n    Returns:\n        Array of windows\n    \"\"\"\n    windows = []\n    for i in range(len(series) - window_size + 1):\n        window = series[i : i + window_size]\n        windows.append(window)\n    return np.array(windows)\n\n\nWINDOW_SIZE = 50\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PREPARING DATA\")\nprint(\"=\" * 60)\n\nnormal_windows = create_windows(normal_series, WINDOW_SIZE)\n\nprint(f\"\\nWindows created: {normal_windows.shape}\")\nprint(f\"Each window has {WINDOW_SIZE} points\")\n\nscaler = StandardScaler()\nnormal_windows_scaled = scaler.fit_transform(normal_windows)\n\nprint(f\"Data normalized: Mean\u22480, Std\u22481\")\n\nwindows_tensor = torch.FloatTensor(normal_windows_scaled)\ntrain_dataset = TensorDataset(windows_tensor, windows_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nprint(f\"DataLoader created: {len(train_loader)} batches\")\n</pre> def create_windows(series, window_size=50):     \"\"\"     Converts the time series into sliding windows.      Args:         series: Time series         window_size: Size of each window      Returns:         Array of windows     \"\"\"     windows = []     for i in range(len(series) - window_size + 1):         window = series[i : i + window_size]         windows.append(window)     return np.array(windows)   WINDOW_SIZE = 50  print(\"\\n\" + \"=\" * 60) print(\"PREPARING DATA\") print(\"=\" * 60)  normal_windows = create_windows(normal_series, WINDOW_SIZE)  print(f\"\\nWindows created: {normal_windows.shape}\") print(f\"Each window has {WINDOW_SIZE} points\")  scaler = StandardScaler() normal_windows_scaled = scaler.fit_transform(normal_windows)  print(f\"Data normalized: Mean\u22480, Std\u22481\")  windows_tensor = torch.FloatTensor(normal_windows_scaled) train_dataset = TensorDataset(windows_tensor, windows_tensor) train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  print(f\"DataLoader created: {len(train_loader)} batches\") <pre>\n============================================================\nPREPARING DATA\n============================================================\n\nWindows created: (951, 50)\nEach window has 50 points\nData normalized: Mean\u22480, Std\u22481\nDataLoader created: 30 batches\n</pre> <p>Autoencoders based on recurrent networks are especially suitable for sequential data, as they can capture temporal dependencies in input patterns. The architecture consists of a recurrent encoder that processes the input sequence and compresses the information into a reduced dimensionality latent vector, followed by a recurrent decoder that reconstructs the original sequence from this compressed representation.</p> <p>In the context of time series, the encoder processes the input window step by step, updating its hidden state until producing a final latent representation. The decoder uses this representation to generate the output sequence, attempting to faithfully reproduce the original input. The fundamental difference between using simple RNNs or LSTMs lies in the latter's ability to maintain relevant information across longer sequences, which can result in more accurate reconstructions and better anomaly detection.</p> In\u00a0[3]: Copied! <pre>class RNNAutoencoder(nn.Module):\n    \"\"\"\n    Autoencoder using Simple RNN.\n\n    Architecture:\n    - Encoder: RNN that compresses the sequence\n    - Decoder: RNN that reconstructs the sequence\n    \"\"\"\n\n    def __init__(self, input_size=1, hidden_size=32, latent_size=16):\n        super(RNNAutoencoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n\n        self.encoder_rnn = nn.RNN(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n        )\n        self.encoder_fc = nn.Linear(hidden_size, latent_size)\n        self.decoder_fc = nn.Linear(latent_size, hidden_size)\n        self.decoder_rnn = nn.RNN(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n        )\n        self.output_fc = nn.Linear(hidden_size, input_size)\n\n    def encode(self, x):\n        \"\"\"Encodes the sequence to latent representation.\"\"\"\n        _, hidden = self.encoder_rnn(x)\n        hidden = hidden.squeeze(0)\n        latent = self.encoder_fc(hidden)\n        return latent\n\n    def decode(self, latent, seq_len):\n        \"\"\"Decodes the latent representation back to sequence.\"\"\"\n        batch_size = latent.size(0)\n        hidden = self.decoder_fc(latent)\n        hidden = hidden.unsqueeze(0)\n        decoder_input = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n        rnn_out, _ = self.decoder_rnn(decoder_input, hidden)\n        output = self.output_fc(rnn_out)\n        return output\n\n    def forward(self, x):\n        \"\"\"Complete forward pass.\"\"\"\n        seq_len = x.size(1)\n        latent = self.encode(x)\n        reconstructed = self.decode(latent, seq_len)\n        return reconstructed\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"MODEL 1: RNN AUTOENCODER\")\nprint(\"=\" * 60)\n\nrnn_model = RNNAutoencoder(input_size=1, hidden_size=32, latent_size=16)\n\nprint(rnn_model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")\n</pre> class RNNAutoencoder(nn.Module):     \"\"\"     Autoencoder using Simple RNN.      Architecture:     - Encoder: RNN that compresses the sequence     - Decoder: RNN that reconstructs the sequence     \"\"\"      def __init__(self, input_size=1, hidden_size=32, latent_size=16):         super(RNNAutoencoder, self).__init__()         self.hidden_size = hidden_size         self.latent_size = latent_size          self.encoder_rnn = nn.RNN(             input_size=input_size,             hidden_size=hidden_size,             num_layers=1,             batch_first=True,         )         self.encoder_fc = nn.Linear(hidden_size, latent_size)         self.decoder_fc = nn.Linear(latent_size, hidden_size)         self.decoder_rnn = nn.RNN(             input_size=hidden_size,             hidden_size=hidden_size,             num_layers=1,             batch_first=True,         )         self.output_fc = nn.Linear(hidden_size, input_size)      def encode(self, x):         \"\"\"Encodes the sequence to latent representation.\"\"\"         _, hidden = self.encoder_rnn(x)         hidden = hidden.squeeze(0)         latent = self.encoder_fc(hidden)         return latent      def decode(self, latent, seq_len):         \"\"\"Decodes the latent representation back to sequence.\"\"\"         batch_size = latent.size(0)         hidden = self.decoder_fc(latent)         hidden = hidden.unsqueeze(0)         decoder_input = hidden.repeat(seq_len, 1, 1).transpose(0, 1)         rnn_out, _ = self.decoder_rnn(decoder_input, hidden)         output = self.output_fc(rnn_out)         return output      def forward(self, x):         \"\"\"Complete forward pass.\"\"\"         seq_len = x.size(1)         latent = self.encode(x)         reconstructed = self.decode(latent, seq_len)         return reconstructed   print(\"\\n\" + \"=\" * 60) print(\"MODEL 1: RNN AUTOENCODER\") print(\"=\" * 60)  rnn_model = RNNAutoencoder(input_size=1, hidden_size=32, latent_size=16)  print(rnn_model) print(f\"\\nTotal parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\") <pre>\n============================================================\nMODEL 1: RNN AUTOENCODER\n============================================================\nRNNAutoencoder(\n  (encoder_rnn): RNN(1, 32, batch_first=True)\n  (encoder_fc): Linear(in_features=32, out_features=16, bias=True)\n  (decoder_fc): Linear(in_features=16, out_features=32, bias=True)\n  (decoder_rnn): RNN(32, 32, batch_first=True)\n  (output_fc): Linear(in_features=32, out_features=1, bias=True)\n)\n\nTotal parameters: 4,337\n</pre> In\u00a0[4]: Copied! <pre>class LSTMAutoencoder(nn.Module):\n    \"\"\"\n    Autoencoder using LSTM.\n\n    Similar to RNN but with better long-term memory.\n    \"\"\"\n\n    def __init__(self, input_size=1, hidden_size=32, latent_size=16):\n        super(LSTMAutoencoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n\n        self.encoder_lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n        )\n        self.encoder_fc = nn.Linear(hidden_size, latent_size)\n        self.decoder_fc = nn.Linear(latent_size, hidden_size)\n        self.decoder_lstm = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n        )\n        self.output_fc = nn.Linear(hidden_size, input_size)\n\n    def encode(self, x):\n        \"\"\"Encodes the sequence.\"\"\"\n        _, (hidden, cell) = self.encoder_lstm(x)\n        hidden = hidden.squeeze(0)\n        latent = self.encoder_fc(hidden)\n        return latent\n\n    def decode(self, latent, seq_len):\n        \"\"\"Decodes the latent representation.\"\"\"\n        batch_size = latent.size(0)\n        hidden = self.decoder_fc(latent)\n        hidden = hidden.unsqueeze(0)\n        cell = torch.zeros_like(hidden)\n        decoder_input = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n        lstm_out, _ = self.decoder_lstm(decoder_input, (hidden, cell))\n        output = self.output_fc(lstm_out)\n        return output\n\n    def forward(self, x):\n        \"\"\"Complete forward pass.\"\"\"\n        seq_len = x.size(1)\n        latent = self.encode(x)\n        reconstructed = self.decode(latent, seq_len)\n        return reconstructed\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"MODEL 2: LSTM AUTOENCODER\")\nprint(\"=\" * 60)\n\nlstm_model = LSTMAutoencoder(input_size=1, hidden_size=32, latent_size=16)\n\nprint(lstm_model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n</pre> class LSTMAutoencoder(nn.Module):     \"\"\"     Autoencoder using LSTM.      Similar to RNN but with better long-term memory.     \"\"\"      def __init__(self, input_size=1, hidden_size=32, latent_size=16):         super(LSTMAutoencoder, self).__init__()         self.hidden_size = hidden_size         self.latent_size = latent_size          self.encoder_lstm = nn.LSTM(             input_size=input_size,             hidden_size=hidden_size,             num_layers=1,             batch_first=True,         )         self.encoder_fc = nn.Linear(hidden_size, latent_size)         self.decoder_fc = nn.Linear(latent_size, hidden_size)         self.decoder_lstm = nn.LSTM(             input_size=hidden_size,             hidden_size=hidden_size,             num_layers=1,             batch_first=True,         )         self.output_fc = nn.Linear(hidden_size, input_size)      def encode(self, x):         \"\"\"Encodes the sequence.\"\"\"         _, (hidden, cell) = self.encoder_lstm(x)         hidden = hidden.squeeze(0)         latent = self.encoder_fc(hidden)         return latent      def decode(self, latent, seq_len):         \"\"\"Decodes the latent representation.\"\"\"         batch_size = latent.size(0)         hidden = self.decoder_fc(latent)         hidden = hidden.unsqueeze(0)         cell = torch.zeros_like(hidden)         decoder_input = hidden.repeat(seq_len, 1, 1).transpose(0, 1)         lstm_out, _ = self.decoder_lstm(decoder_input, (hidden, cell))         output = self.output_fc(lstm_out)         return output      def forward(self, x):         \"\"\"Complete forward pass.\"\"\"         seq_len = x.size(1)         latent = self.encode(x)         reconstructed = self.decode(latent, seq_len)         return reconstructed   print(\"\\n\" + \"=\" * 60) print(\"MODEL 2: LSTM AUTOENCODER\") print(\"=\" * 60)  lstm_model = LSTMAutoencoder(input_size=1, hidden_size=32, latent_size=16)  print(lstm_model) print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\") <pre>\n============================================================\nMODEL 2: LSTM AUTOENCODER\n============================================================\nLSTMAutoencoder(\n  (encoder_lstm): LSTM(1, 32, batch_first=True)\n  (encoder_fc): Linear(in_features=32, out_features=16, bias=True)\n  (decoder_fc): Linear(in_features=16, out_features=32, bias=True)\n  (decoder_lstm): LSTM(32, 32, batch_first=True)\n  (output_fc): Linear(in_features=32, out_features=1, bias=True)\n)\n\nTotal parameters: 14,033\n</pre> <p>The training process of autoencoders for anomaly detection is performed exclusively with normal data, allowing the network to learn to reconstruct typical patterns. The loss function used is mean squared error (MSE) between the input and its reconstruction, which measures reconstruction fidelity. During training, the optimizer adjusts the weights to minimize this error, which implicitly makes the autoencoder capture the distribution of normal data.</p> In\u00a0[5]: Copied! <pre>def train_autoencoder(model, train_loader, epochs=50, lr=0.001, name=\"Model\"):\n    \"\"\"\n    Generic function to train an autoencoder.\n    \"\"\"\n    print(f\"\\nTraining {name}...\")\n    print(\"-\" * 60)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss_history = []\n    model.train()\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n\n        for batch_x, _ in train_loader:\n            batch_x = batch_x.unsqueeze(-1)\n            optimizer.zero_grad()\n            reconstructed = model(batch_x)\n            loss = criterion(reconstructed, batch_x)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(train_loader)\n        loss_history.append(avg_loss)\n\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")\n\n    print(f\"\\n{name} trained completely\")\n    return loss_history\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING\")\nprint(\"=\" * 60)\n\nrnn_history = train_autoencoder(\n    rnn_model, train_loader, epochs=50, lr=0.001, name=\"RNN Autoencoder\"\n)\n\nlstm_history = train_autoencoder(\n    lstm_model, train_loader, epochs=50, lr=0.001, name=\"LSTM Autoencoder\"\n)\n\nplt.figure(figsize=(12, 5))\nplt.plot(rnn_history, label=\"RNN Autoencoder\", linewidth=2)\nplt.plot(lstm_history, label=\"LSTM Autoencoder\", linewidth=2)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (MSE)\")\nplt.title(\"Training Curves\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale(\"log\")\nplt.show()\n\nprint(\"\\nConvergence comparison:\")\nprint(f\"RNN  - Final loss: {rnn_history[-1]:.6f}\")\nprint(f\"LSTM - Final loss: {lstm_history[-1]:.6f}\")\n</pre> def train_autoencoder(model, train_loader, epochs=50, lr=0.001, name=\"Model\"):     \"\"\"     Generic function to train an autoencoder.     \"\"\"     print(f\"\\nTraining {name}...\")     print(\"-\" * 60)      criterion = nn.MSELoss()     optimizer = optim.Adam(model.parameters(), lr=lr)     loss_history = []     model.train()      for epoch in range(epochs):         epoch_loss = 0          for batch_x, _ in train_loader:             batch_x = batch_x.unsqueeze(-1)             optimizer.zero_grad()             reconstructed = model(batch_x)             loss = criterion(reconstructed, batch_x)             loss.backward()             optimizer.step()             epoch_loss += loss.item()          avg_loss = epoch_loss / len(train_loader)         loss_history.append(avg_loss)          if (epoch + 1) % 10 == 0:             print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")      print(f\"\\n{name} trained completely\")     return loss_history   print(\"\\n\" + \"=\" * 60) print(\"TRAINING\") print(\"=\" * 60)  rnn_history = train_autoencoder(     rnn_model, train_loader, epochs=50, lr=0.001, name=\"RNN Autoencoder\" )  lstm_history = train_autoencoder(     lstm_model, train_loader, epochs=50, lr=0.001, name=\"LSTM Autoencoder\" )  plt.figure(figsize=(12, 5)) plt.plot(rnn_history, label=\"RNN Autoencoder\", linewidth=2) plt.plot(lstm_history, label=\"LSTM Autoencoder\", linewidth=2) plt.xlabel(\"Epoch\") plt.ylabel(\"Loss (MSE)\") plt.title(\"Training Curves\") plt.legend() plt.grid(True, alpha=0.3) plt.yscale(\"log\") plt.show()  print(\"\\nConvergence comparison:\") print(f\"RNN  - Final loss: {rnn_history[-1]:.6f}\") print(f\"LSTM - Final loss: {lstm_history[-1]:.6f}\") <pre>\n============================================================\nTRAINING\n============================================================\n\nTraining RNN Autoencoder...\n------------------------------------------------------------\n</pre> <pre>Epoch 10/50 - Loss: 0.586128\n</pre> <pre>Epoch 20/50 - Loss: 0.031721\n</pre> <pre>Epoch 30/50 - Loss: 0.016982\n</pre> <pre>Epoch 40/50 - Loss: 0.015651\n</pre> <pre>Epoch 50/50 - Loss: 0.014618\n\nRNN Autoencoder trained completely\n\nTraining LSTM Autoencoder...\n------------------------------------------------------------\n</pre> <pre>Epoch 10/50 - Loss: 0.614990\n</pre> <pre>Epoch 20/50 - Loss: 0.040240\n</pre> <pre>Epoch 30/50 - Loss: 0.015739\n</pre> <pre>Epoch 40/50 - Loss: 0.013632\n</pre> <pre>Epoch 50/50 - Loss: 0.013296\n\nLSTM Autoencoder trained completely\n</pre> <pre>\nConvergence comparison:\nRNN  - Final loss: 0.014618\nLSTM - Final loss: 0.013296\n</pre> <p>Once the autoencoders are trained with normal data, anomaly detection is performed by calculating the reconstruction error for new data windows. The fundamental principle is that windows containing normal patterns will be reconstructed with high fidelity (low error), while windows with anomalies will produce low-quality reconstructions (high error). By establishing an appropriate threshold on the reconstruction error, typically based on a percentile of the error distribution, anomalous regions of the time series can be automatically identified.</p> In\u00a0[6]: Copied! <pre>def calculate_reconstruction_error(model, series, window_size, scaler):\n    \"\"\"\n    Calculates reconstruction error for each window.\n\n    High error indicates anomaly.\n    \"\"\"\n    model.eval()\n    windows = create_windows(series, window_size)\n    windows_scaled = scaler.transform(windows)\n    windows_tensor = torch.FloatTensor(windows_scaled).unsqueeze(-1)\n\n    with torch.no_grad():\n        reconstructed = model(windows_tensor)\n\n    errors = torch.mean((windows_tensor - reconstructed) ** 2, dim=(1, 2)).numpy()\n    return errors\n\n\ndef detect_anomalies(errors, threshold_percentile=95):\n    \"\"\"\n    Detects anomalies based on a threshold.\n\n    Args:\n        errors: Array of reconstruction errors\n        threshold_percentile: Percentile to define the threshold\n\n    Returns:\n        anomalies: Boolean array indicating anomalies\n        threshold: Threshold value used\n    \"\"\"\n    threshold = np.percentile(errors, threshold_percentile)\n    anomalies = errors &gt; threshold\n    return anomalies, threshold\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ANOMALY DETECTION\")\nprint(\"=\" * 60)\n\nrnn_errors = calculate_reconstruction_error(\n    rnn_model, series_with_anomalies, WINDOW_SIZE, scaler\n)\n\nlstm_errors = calculate_reconstruction_error(\n    lstm_model, series_with_anomalies, WINDOW_SIZE, scaler\n)\n\nprint(f\"\\nErrors calculated for {len(rnn_errors)} windows\")\n\nrnn_anomalies, rnn_threshold = detect_anomalies(rnn_errors, threshold_percentile=95)\nlstm_anomalies, lstm_threshold = detect_anomalies(lstm_errors, threshold_percentile=95)\n\nprint(f\"\\nRNN Threshold:  {rnn_threshold:.6f}\")\nprint(f\"LSTM Threshold: {lstm_threshold:.6f}\")\n\nprint(f\"\\nAnomalies detected:\")\nprint(f\"RNN:  {np.sum(rnn_anomalies)} windows\")\nprint(f\"LSTM: {np.sum(lstm_anomalies)} windows\")\n</pre> def calculate_reconstruction_error(model, series, window_size, scaler):     \"\"\"     Calculates reconstruction error for each window.      High error indicates anomaly.     \"\"\"     model.eval()     windows = create_windows(series, window_size)     windows_scaled = scaler.transform(windows)     windows_tensor = torch.FloatTensor(windows_scaled).unsqueeze(-1)      with torch.no_grad():         reconstructed = model(windows_tensor)      errors = torch.mean((windows_tensor - reconstructed) ** 2, dim=(1, 2)).numpy()     return errors   def detect_anomalies(errors, threshold_percentile=95):     \"\"\"     Detects anomalies based on a threshold.      Args:         errors: Array of reconstruction errors         threshold_percentile: Percentile to define the threshold      Returns:         anomalies: Boolean array indicating anomalies         threshold: Threshold value used     \"\"\"     threshold = np.percentile(errors, threshold_percentile)     anomalies = errors &gt; threshold     return anomalies, threshold   print(\"\\n\" + \"=\" * 60) print(\"ANOMALY DETECTION\") print(\"=\" * 60)  rnn_errors = calculate_reconstruction_error(     rnn_model, series_with_anomalies, WINDOW_SIZE, scaler )  lstm_errors = calculate_reconstruction_error(     lstm_model, series_with_anomalies, WINDOW_SIZE, scaler )  print(f\"\\nErrors calculated for {len(rnn_errors)} windows\")  rnn_anomalies, rnn_threshold = detect_anomalies(rnn_errors, threshold_percentile=95) lstm_anomalies, lstm_threshold = detect_anomalies(lstm_errors, threshold_percentile=95)  print(f\"\\nRNN Threshold:  {rnn_threshold:.6f}\") print(f\"LSTM Threshold: {lstm_threshold:.6f}\")  print(f\"\\nAnomalies detected:\") print(f\"RNN:  {np.sum(rnn_anomalies)} windows\") print(f\"LSTM: {np.sum(lstm_anomalies)} windows\") <pre>\n============================================================\nANOMALY DETECTION\n============================================================\n\nErrors calculated for 951 windows\n\nRNN Threshold:  10.356934\nLSTM Threshold: 10.379620\n\nAnomalies detected:\nRNN:  48 windows\nLSTM: 48 windows\n</pre> <p>Evaluating the effectiveness of autoencoders in anomaly detection requires both qualitative visualization and quantitative metrics. Visualization allows visually inspecting the correspondence between detected and actual anomalies, while metrics such as precision, recall, and F1-score provide a numerical evaluation of system performance.</p> In\u00a0[7]: Copied! <pre>def visualize_detection(series, errors, anomalies, threshold, real_indices, title):\n    \"\"\"\n    Visualizes anomaly detection.\n    \"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n\n    axes[0].plot(series, label=\"Time Series\", linewidth=1)\n    for idx in real_indices:\n        axes[0].axvspan(\n            idx,\n            idx + 10,\n            alpha=0.3,\n            color=\"red\",\n            label=\"Real Anomaly\" if idx == real_indices[0] else \"\",\n        )\n    axes[0].set_xlabel(\"Time\")\n    axes[0].set_ylabel(\"Value\")\n    axes[0].set_title(f\"{title} - Time Series\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(errors, label=\"Reconstruction Error\", linewidth=1, color=\"blue\")\n    axes[1].axhline(\n        y=threshold,\n        color=\"red\",\n        linestyle=\"--\",\n        linewidth=2,\n        label=f\"Threshold ({threshold:.4f})\",\n    )\n    axes[1].fill_between(\n        range(len(errors)),\n        0,\n        errors,\n        where=anomalies,\n        alpha=0.3,\n        color=\"red\",\n        label=\"Detected Anomaly\",\n    )\n    axes[1].set_xlabel(\"Window\")\n    axes[1].set_ylabel(\"MSE Error\")\n    axes[1].set_title(\"Reconstruction Error\")\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_yscale(\"log\")\n\n    anomaly_map = np.zeros(len(series))\n    for i, is_anomaly in enumerate(anomalies):\n        if is_anomaly:\n            anomaly_map[i : i + WINDOW_SIZE] = 1\n\n    axes[2].fill_between(\n        range(len(series)),\n        0,\n        1,\n        where=anomaly_map &gt; 0,\n        alpha=0.5,\n        color=\"red\",\n        label=\"Detected\",\n    )\n\n    real_map = np.zeros(len(series))\n    for idx in real_indices:\n        real_map[idx : idx + 10] = 1\n    axes[2].fill_between(\n        range(len(series)),\n        0,\n        0.5,\n        where=real_map &gt; 0,\n        alpha=0.5,\n        color=\"green\",\n        label=\"Real\",\n    )\n\n    axes[2].set_xlabel(\"Time\")\n    axes[2].set_ylabel(\"Anomaly\")\n    axes[2].set_title(\"Anomaly Map\")\n    axes[2].legend()\n    axes[2].set_ylim(-0.1, 1.1)\n    axes[2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"VISUALIZATION: RNN AUTOENCODER\")\nprint(\"=\" * 60)\n\nvisualize_detection(\n    series_with_anomalies,\n    rnn_errors,\n    rnn_anomalies,\n    rnn_threshold,\n    anomaly_indices,\n    \"RNN Autoencoder\",\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"VISUALIZATION: LSTM AUTOENCODER\")\nprint(\"=\" * 60)\n\nvisualize_detection(\n    series_with_anomalies,\n    lstm_errors,\n    lstm_anomalies,\n    lstm_threshold,\n    anomaly_indices,\n    \"LSTM Autoencoder\",\n)\n</pre> def visualize_detection(series, errors, anomalies, threshold, real_indices, title):     \"\"\"     Visualizes anomaly detection.     \"\"\"     fig, axes = plt.subplots(3, 1, figsize=(14, 10))      axes[0].plot(series, label=\"Time Series\", linewidth=1)     for idx in real_indices:         axes[0].axvspan(             idx,             idx + 10,             alpha=0.3,             color=\"red\",             label=\"Real Anomaly\" if idx == real_indices[0] else \"\",         )     axes[0].set_xlabel(\"Time\")     axes[0].set_ylabel(\"Value\")     axes[0].set_title(f\"{title} - Time Series\")     axes[0].legend()     axes[0].grid(True, alpha=0.3)      axes[1].plot(errors, label=\"Reconstruction Error\", linewidth=1, color=\"blue\")     axes[1].axhline(         y=threshold,         color=\"red\",         linestyle=\"--\",         linewidth=2,         label=f\"Threshold ({threshold:.4f})\",     )     axes[1].fill_between(         range(len(errors)),         0,         errors,         where=anomalies,         alpha=0.3,         color=\"red\",         label=\"Detected Anomaly\",     )     axes[1].set_xlabel(\"Window\")     axes[1].set_ylabel(\"MSE Error\")     axes[1].set_title(\"Reconstruction Error\")     axes[1].legend()     axes[1].grid(True, alpha=0.3)     axes[1].set_yscale(\"log\")      anomaly_map = np.zeros(len(series))     for i, is_anomaly in enumerate(anomalies):         if is_anomaly:             anomaly_map[i : i + WINDOW_SIZE] = 1      axes[2].fill_between(         range(len(series)),         0,         1,         where=anomaly_map &gt; 0,         alpha=0.5,         color=\"red\",         label=\"Detected\",     )      real_map = np.zeros(len(series))     for idx in real_indices:         real_map[idx : idx + 10] = 1     axes[2].fill_between(         range(len(series)),         0,         0.5,         where=real_map &gt; 0,         alpha=0.5,         color=\"green\",         label=\"Real\",     )      axes[2].set_xlabel(\"Time\")     axes[2].set_ylabel(\"Anomaly\")     axes[2].set_title(\"Anomaly Map\")     axes[2].legend()     axes[2].set_ylim(-0.1, 1.1)     axes[2].grid(True, alpha=0.3)      plt.tight_layout()     plt.show()   print(\"\\n\" + \"=\" * 60) print(\"VISUALIZATION: RNN AUTOENCODER\") print(\"=\" * 60)  visualize_detection(     series_with_anomalies,     rnn_errors,     rnn_anomalies,     rnn_threshold,     anomaly_indices,     \"RNN Autoencoder\", )  print(\"\\n\" + \"=\" * 60) print(\"VISUALIZATION: LSTM AUTOENCODER\") print(\"=\" * 60)  visualize_detection(     series_with_anomalies,     lstm_errors,     lstm_anomalies,     lstm_threshold,     anomaly_indices,     \"LSTM Autoencoder\", ) <pre>\n============================================================\nVISUALIZATION: RNN AUTOENCODER\n============================================================\n</pre> <pre>\n============================================================\nVISUALIZATION: LSTM AUTOENCODER\n============================================================\n</pre> <p>Quantitative evaluation of autoencoder performance in anomaly detection is performed using standard binary classification metrics. Precision measures the proportion of detected anomalies that are actually anomalies, recall (sensitivity) measures the proportion of actual anomalies that were detected, and the F1-score provides a harmonic mean of both metrics, offering a balanced evaluation of overall system performance.</p> In\u00a0[8]: Copied! <pre>def calculate_metrics(detected_anomalies, real_indices, window_size, series_length):\n    \"\"\"\n    Calculates detection metrics.\n    \"\"\"\n    truth = np.zeros(series_length, dtype=bool)\n    for idx in real_indices:\n        truth[idx : idx + 10] = True\n\n    predictions = np.zeros(series_length, dtype=bool)\n    for i, is_anomaly in enumerate(detected_anomalies):\n        if is_anomaly:\n            predictions[i : i + window_size] = True\n\n    true_positives = np.sum(predictions &amp; truth)\n    false_positives = np.sum(predictions &amp; ~truth)\n    false_negatives = np.sum(~predictions &amp; truth)\n    true_negatives = np.sum(~predictions &amp; ~truth)\n\n    precision = true_positives / (true_positives + false_positives + 1e-10)\n    recall = true_positives / (true_positives + false_negatives + 1e-10)\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"tp\": true_positives,\n        \"fp\": false_positives,\n        \"fn\": false_negatives,\n        \"tn\": true_negatives,\n    }\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"MODEL COMPARISON\")\nprint(\"=\" * 60)\n\nrnn_metrics = calculate_metrics(\n    rnn_anomalies, anomaly_indices, WINDOW_SIZE, len(series_with_anomalies)\n)\n\nlstm_metrics = calculate_metrics(\n    lstm_anomalies, anomaly_indices, WINDOW_SIZE, len(series_with_anomalies)\n)\n\nprint(\"\\nDETECTION METRICS:\")\nprint(\"-\" * 60)\nprint(f\"{'Metric':&lt;20} {'RNN':&gt;15} {'LSTM':&gt;15}\")\nprint(\"-\" * 60)\nprint(\n    f\"{'Precision':&lt;20} {rnn_metrics['precision']:&gt;15.4f} {lstm_metrics['precision']:&gt;15.4f}\"\n)\nprint(f\"{'Recall':&lt;20} {rnn_metrics['recall']:&gt;15.4f} {lstm_metrics['recall']:&gt;15.4f}\")\nprint(f\"{'F1-Score':&lt;20} {rnn_metrics['f1']:&gt;15.4f} {lstm_metrics['f1']:&gt;15.4f}\")\nprint(\"-\" * 60)\nprint(f\"{'TP':&lt;20} {rnn_metrics['tp']:&gt;15} {lstm_metrics['tp']:&gt;15}\")\nprint(f\"{'FP':&lt;20} {rnn_metrics['fp']:&gt;15} {lstm_metrics['fp']:&gt;15}\")\nprint(f\"{'FN':&lt;20} {rnn_metrics['fn']:&gt;15} {lstm_metrics['fn']:&gt;15}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nmetric_names = [\"Precision\", \"Recall\", \"F1-Score\"]\nrnn_values = [rnn_metrics[\"precision\"], rnn_metrics[\"recall\"], rnn_metrics[\"f1\"]]\nlstm_values = [lstm_metrics[\"precision\"], lstm_metrics[\"recall\"], lstm_metrics[\"f1\"]]\n\nx = np.arange(len(metric_names))\nwidth = 0.35\n\naxes[0].bar(x - width / 2, rnn_values, width, label=\"RNN\", color=\"skyblue\")\naxes[0].bar(x + width / 2, lstm_values, width, label=\"LSTM\", color=\"lightgreen\")\naxes[0].set_xlabel(\"Metric\")\naxes[0].set_ylabel(\"Value\")\naxes[0].set_title(\"Metrics Comparison\")\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(metric_names)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3, axis=\"y\")\naxes[0].set_ylim(0, 1)\n\nfor i, (rnn_val, lstm_val) in enumerate(zip(rnn_values, lstm_values)):\n    axes[0].text(\n        i - width / 2, rnn_val + 0.02, f\"{rnn_val:.2f}\", ha=\"center\", va=\"bottom\"\n    )\n    axes[0].text(\n        i + width / 2, lstm_val + 0.02, f\"{lstm_val:.2f}\", ha=\"center\", va=\"bottom\"\n    )\n\naxes[1].plot(rnn_errors, label=\"RNN\", alpha=0.7, linewidth=1)\naxes[1].plot(lstm_errors, label=\"LSTM\", alpha=0.7, linewidth=1)\naxes[1].axhline(\n    y=rnn_threshold, color=\"blue\", linestyle=\"--\", alpha=0.5, label=\"RNN Threshold\"\n)\naxes[1].axhline(\n    y=lstm_threshold, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"LSTM Threshold\"\n)\naxes[1].set_xlabel(\"Window\")\naxes[1].set_ylabel(\"Reconstruction Error\")\naxes[1].set_title(\"Error Comparison\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_yscale(\"log\")\n\nplt.tight_layout()\nplt.show()\n\nbest_model = \"LSTM\" if lstm_metrics[\"f1\"] &gt; rnn_metrics[\"f1\"] else \"RNN\"\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CONCLUSIONS\")\nprint(\"=\" * 60)\n\nprint(\n    f\"\"\"\nBEST MODEL: {best_model}\n\nANALYSIS:\n\n1. RNN Autoencoder:\n   - F1-Score: {rnn_metrics['f1']:.4f}\n   - Final loss: {rnn_history[-1]:.6f}\n   - Advantage: Simpler and faster to train\n   - Disadvantage: Lower long-term memory capacity\n\n2. LSTM Autoencoder:\n   - F1-Score: {lstm_metrics['f1']:.4f}\n   - Final loss: {lstm_history[-1]:.6f}\n   - Advantage: Better long-term memory, detects complex patterns\n   - Disadvantage: More parameters, slightly slower\n\"\"\"\n)\n</pre> def calculate_metrics(detected_anomalies, real_indices, window_size, series_length):     \"\"\"     Calculates detection metrics.     \"\"\"     truth = np.zeros(series_length, dtype=bool)     for idx in real_indices:         truth[idx : idx + 10] = True      predictions = np.zeros(series_length, dtype=bool)     for i, is_anomaly in enumerate(detected_anomalies):         if is_anomaly:             predictions[i : i + window_size] = True      true_positives = np.sum(predictions &amp; truth)     false_positives = np.sum(predictions &amp; ~truth)     false_negatives = np.sum(~predictions &amp; truth)     true_negatives = np.sum(~predictions &amp; ~truth)      precision = true_positives / (true_positives + false_positives + 1e-10)     recall = true_positives / (true_positives + false_negatives + 1e-10)     f1 = 2 * (precision * recall) / (precision + recall + 1e-10)      return {         \"precision\": precision,         \"recall\": recall,         \"f1\": f1,         \"tp\": true_positives,         \"fp\": false_positives,         \"fn\": false_negatives,         \"tn\": true_negatives,     }   print(\"\\n\" + \"=\" * 60) print(\"MODEL COMPARISON\") print(\"=\" * 60)  rnn_metrics = calculate_metrics(     rnn_anomalies, anomaly_indices, WINDOW_SIZE, len(series_with_anomalies) )  lstm_metrics = calculate_metrics(     lstm_anomalies, anomaly_indices, WINDOW_SIZE, len(series_with_anomalies) )  print(\"\\nDETECTION METRICS:\") print(\"-\" * 60) print(f\"{'Metric':&lt;20} {'RNN':&gt;15} {'LSTM':&gt;15}\") print(\"-\" * 60) print(     f\"{'Precision':&lt;20} {rnn_metrics['precision']:&gt;15.4f} {lstm_metrics['precision']:&gt;15.4f}\" ) print(f\"{'Recall':&lt;20} {rnn_metrics['recall']:&gt;15.4f} {lstm_metrics['recall']:&gt;15.4f}\") print(f\"{'F1-Score':&lt;20} {rnn_metrics['f1']:&gt;15.4f} {lstm_metrics['f1']:&gt;15.4f}\") print(\"-\" * 60) print(f\"{'TP':&lt;20} {rnn_metrics['tp']:&gt;15} {lstm_metrics['tp']:&gt;15}\") print(f\"{'FP':&lt;20} {rnn_metrics['fp']:&gt;15} {lstm_metrics['fp']:&gt;15}\") print(f\"{'FN':&lt;20} {rnn_metrics['fn']:&gt;15} {lstm_metrics['fn']:&gt;15}\")  fig, axes = plt.subplots(1, 2, figsize=(14, 5))  metric_names = [\"Precision\", \"Recall\", \"F1-Score\"] rnn_values = [rnn_metrics[\"precision\"], rnn_metrics[\"recall\"], rnn_metrics[\"f1\"]] lstm_values = [lstm_metrics[\"precision\"], lstm_metrics[\"recall\"], lstm_metrics[\"f1\"]]  x = np.arange(len(metric_names)) width = 0.35  axes[0].bar(x - width / 2, rnn_values, width, label=\"RNN\", color=\"skyblue\") axes[0].bar(x + width / 2, lstm_values, width, label=\"LSTM\", color=\"lightgreen\") axes[0].set_xlabel(\"Metric\") axes[0].set_ylabel(\"Value\") axes[0].set_title(\"Metrics Comparison\") axes[0].set_xticks(x) axes[0].set_xticklabels(metric_names) axes[0].legend() axes[0].grid(True, alpha=0.3, axis=\"y\") axes[0].set_ylim(0, 1)  for i, (rnn_val, lstm_val) in enumerate(zip(rnn_values, lstm_values)):     axes[0].text(         i - width / 2, rnn_val + 0.02, f\"{rnn_val:.2f}\", ha=\"center\", va=\"bottom\"     )     axes[0].text(         i + width / 2, lstm_val + 0.02, f\"{lstm_val:.2f}\", ha=\"center\", va=\"bottom\"     )  axes[1].plot(rnn_errors, label=\"RNN\", alpha=0.7, linewidth=1) axes[1].plot(lstm_errors, label=\"LSTM\", alpha=0.7, linewidth=1) axes[1].axhline(     y=rnn_threshold, color=\"blue\", linestyle=\"--\", alpha=0.5, label=\"RNN Threshold\" ) axes[1].axhline(     y=lstm_threshold, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"LSTM Threshold\" ) axes[1].set_xlabel(\"Window\") axes[1].set_ylabel(\"Reconstruction Error\") axes[1].set_title(\"Error Comparison\") axes[1].legend() axes[1].grid(True, alpha=0.3) axes[1].set_yscale(\"log\")  plt.tight_layout() plt.show()  best_model = \"LSTM\" if lstm_metrics[\"f1\"] &gt; rnn_metrics[\"f1\"] else \"RNN\"  print(\"\\n\" + \"=\" * 60) print(\"CONCLUSIONS\") print(\"=\" * 60)  print(     f\"\"\" BEST MODEL: {best_model}  ANALYSIS:  1. RNN Autoencoder:    - F1-Score: {rnn_metrics['f1']:.4f}    - Final loss: {rnn_history[-1]:.6f}    - Advantage: Simpler and faster to train    - Disadvantage: Lower long-term memory capacity  2. LSTM Autoencoder:    - F1-Score: {lstm_metrics['f1']:.4f}    - Final loss: {lstm_history[-1]:.6f}    - Advantage: Better long-term memory, detects complex patterns    - Disadvantage: More parameters, slightly slower \"\"\" ) <pre>\n============================================================\nMODEL COMPARISON\n============================================================\n\nDETECTION METRICS:\n------------------------------------------------------------\nMetric                           RNN            LSTM\n------------------------------------------------------------\nPrecision                     0.2371          0.2371\nRecall                        0.5349          0.5349\nF1-Score                      0.3286          0.3286\n------------------------------------------------------------\nTP                                23              23\nFP                                74              74\nFN                                20              20\n</pre> <pre>\n============================================================\nCONCLUSIONS\n============================================================\n\nBEST MODEL: RNN\n\nANALYSIS:\n\n1. RNN Autoencoder:\n   - F1-Score: 0.3286\n   - Final loss: 0.014618\n   - Advantage: Simpler and faster to train\n   - Disadvantage: Lower long-term memory capacity\n\n2. LSTM Autoencoder:\n   - F1-Score: 0.3286\n   - Final loss: 0.013296\n   - Advantage: Better long-term memory, detects complex patterns\n   - Disadvantage: More parameters, slightly slower\n\n</pre>"},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#autoencoders","title":"Autoencoders\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#environment-setup-and-data-generation","title":"Environment setup and data generation\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#data-preparation-using-sliding-windows","title":"Data preparation using sliding windows\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#recurrent-autoencoder-architecture","title":"Recurrent autoencoder architecture\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#training-the-autoencoders","title":"Training the autoencoders\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#anomaly-detection-through-reconstruction-error","title":"Anomaly detection through reconstruction error\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#visualization-and-evaluation-of-results","title":"Visualization and evaluation of results\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_04_autoencoders.html#quantitative-model-comparison","title":"Quantitative model comparison\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html","title":"Transformers","text":"<p>The Transformer architecture, introduced by Vaswani et al. in the paper \"Attention is All You Need\" (2017), represents a paradigm shift in sequence processing using neural networks. Unlike traditional recurrent architectures (RNNs and LSTMs), which process sequences sequentially, Transformers employ attention mechanisms that allow parallel processing of the entire sequence, capturing long-range dependencies more effectively.</p> <p>The architecture is based on the multi-head attention mechanism, which allows the model to simultaneously attend to different positions of the input sequence from multiple representation subspaces. This capability, combined with residual connections, layer normalization, and feed-forward networks, has proven to be extraordinarily effective in natural language processing tasks, computer vision, and other applications involving sequential data.</p> <p>Masking constitutes an essential component in the Transformer architecture, serving two main functions. The causal mask prevents the decoder from accessing future tokens during training, preserving the autoregressive nature of sequence generation. The padding mask allows ignoring padding tokens that are added to standardize sequence lengths in a batch.</p> In\u00a0[1]: Copied! <pre>import torch\nimport math\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef create_causal_mask(size: int) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a causal mask to prevent the decoder from attending to future tokens during training.\n\n    Args:\n        size: Length of the sequence.\n\n    Returns:\n        Causal mask of shape (size, size).\n    \"\"\"\n    return torch.tril(torch.ones(size, size))\n\n\ndef create_padding_mask(seq: torch.Tensor, pad_token: int = 0) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a mask to ignore padding tokens in a sequence.\n\n    Args:\n        seq: Sequence of tokens, shape (B, seq_len).\n        pad_token: Padding token value.\n\n    Returns:\n        Padding mask of shape (B, 1, 1, seq_len).\n    \"\"\"\n    return (seq != pad_token).unsqueeze(1).unsqueeze(1)\n</pre> import torch import math from torch import nn from torch.nn import functional as F   def create_causal_mask(size: int) -&gt; torch.Tensor:     \"\"\"     Creates a causal mask to prevent the decoder from attending to future tokens during training.      Args:         size: Length of the sequence.      Returns:         Causal mask of shape (size, size).     \"\"\"     return torch.tril(torch.ones(size, size))   def create_padding_mask(seq: torch.Tensor, pad_token: int = 0) -&gt; torch.Tensor:     \"\"\"     Creates a mask to ignore padding tokens in a sequence.      Args:         seq: Sequence of tokens, shape (B, seq_len).         pad_token: Padding token value.      Returns:         Padding mask of shape (B, 1, 1, seq_len).     \"\"\"     return (seq != pad_token).unsqueeze(1).unsqueeze(1) <p>The transformation of discrete tokens into continuous vector representations constitutes the first step in processing with Transformers. Input embeddings map each token from the vocabulary to a dense vector of dimension $d_{model}$, learned during training. Following the specification of the original paper, these embeddings are scaled by multiplying by $\\sqrt{d_{model}}$ to stabilize training.</p> <p>Since Transformers lack the intrinsic notion of sequential order present in RNNs, it is necessary to explicitly inject positional information. Positional encoding adds deterministic vectors to the embeddings, calculated using sinusoidal functions that allow the model to distinguish relative positions:</p> <p>$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$</p> <p>where $pos$ represents the position in the sequence and $i$ the embedding dimension.</p> In\u00a0[2]: Copied! <pre>class InputEmbedding(nn.Module):\n    \"\"\"Embeds input tokens into vectors of dimension d_model.\"\"\"\n\n    def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n        \"\"\"\n        Initializes input embedding layer.\n\n        Args:\n            d_model: Dimensionality of the embedding vectors.\n            vocab_size: Size of the vocabulary.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the embedding layer.\n\n        Args:\n            input_tensor: Input tensor of token indices.\n\n        Returns:\n            Tensor of embedded input scaled by sqrt(d_model).\n        \"\"\"\n        return self.embedding(input_tensor) * math.sqrt(self.d_model)\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Adds positional encoding to input embeddings.\"\"\"\n\n    def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes positional encoding layer.\n\n        Args:\n            d_model: Dimensionality of the embedding vectors.\n            sequence_length: Maximum sequence length.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.dropout = nn.Dropout(dropout_rate)\n\n        pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n        position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n\n        pe_matrix[:, 0::2] = torch.sin(position * div_term)\n        pe_matrix[:, 1::2] = torch.cos(position * div_term)\n        pe_matrix = pe_matrix.unsqueeze(0)\n\n        self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass to add positional encoding.\n\n        Args:\n            input_embedding: Tensor of input embeddings.\n\n        Returns:\n            Tensor of embeddings with added positional encoding.\n        \"\"\"\n        x = input_embedding + (\n            self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n        ).requires_grad_(False)\n        return self.dropout(x)\n</pre> class InputEmbedding(nn.Module):     \"\"\"Embeds input tokens into vectors of dimension d_model.\"\"\"      def __init__(self, d_model: int, vocab_size: int) -&gt; None:         \"\"\"         Initializes input embedding layer.          Args:             d_model: Dimensionality of the embedding vectors.             vocab_size: Size of the vocabulary.         \"\"\"         super().__init__()         self.d_model = d_model         self.vocab_size = vocab_size         self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the embedding layer.          Args:             input_tensor: Input tensor of token indices.          Returns:             Tensor of embedded input scaled by sqrt(d_model).         \"\"\"         return self.embedding(input_tensor) * math.sqrt(self.d_model)   class PositionalEncoding(nn.Module):     \"\"\"Adds positional encoding to input embeddings.\"\"\"      def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes positional encoding layer.          Args:             d_model: Dimensionality of the embedding vectors.             sequence_length: Maximum sequence length.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.d_model = d_model         self.sequence_length = sequence_length         self.dropout = nn.Dropout(dropout_rate)          pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))         position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)         div_term = torch.exp(             torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)         )          pe_matrix[:, 0::2] = torch.sin(position * div_term)         pe_matrix[:, 1::2] = torch.cos(position * div_term)         pe_matrix = pe_matrix.unsqueeze(0)          self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass to add positional encoding.          Args:             input_embedding: Tensor of input embeddings.          Returns:             Tensor of embeddings with added positional encoding.         \"\"\"         x = input_embedding + (             self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore         ).requires_grad_(False)         return self.dropout(x) <p>Layer normalization stabilizes training by normalizing activations across features for each individual example. Unlike batch normalization, which normalizes across the batch, layer normalization is more suitable for sequential data of variable length. The transformation is defined as:</p> <p>$$\\text{LayerNorm}(x) = \\alpha \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$</p> <p>where $\\mu$ and $\\sigma^2$ are the mean and variance calculated over the features, $\\alpha$ and $\\beta$ are learnable parameters, and $\\epsilon$ is a small constant for numerical stability.</p> <p>Feed-forward networks apply non-linear transformations independently to each position in the sequence. They consist of two linear transformations with an intermediate ReLU activation:</p> <p>$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$</p> In\u00a0[3]: Copied! <pre>class LayerNormalization(nn.Module):\n    \"\"\"Applies layer normalization to input embeddings.\"\"\"\n\n    def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n        \"\"\"\n        Initializes layer normalization.\n\n        Args:\n            features: Number of features in the input.\n            eps: Small constant for numerical stability.\n        \"\"\"\n        super().__init__()\n        self.features = features\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(self.features))\n        self.bias = nn.Parameter(torch.zeros(self.features))\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for layer normalization.\n\n        Args:\n            input_embedding: Tensor of input embeddings.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n        mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n        var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n        return (\n            self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n            + self.bias\n        )\n\n\nclass FeedForward(nn.Module):\n    \"\"\"Feed-forward neural network layer.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes feed-forward network.\n\n        Args:\n            d_model: Dimensionality of model embeddings.\n            d_ff: Dimensionality of feed-forward layer.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through feed-forward network.\n\n        Args:\n            input_tensor: Tensor of input embeddings.\n\n        Returns:\n            Tensor processed by feed-forward network.\n        \"\"\"\n        return self.ffn(input_tensor)\n</pre> class LayerNormalization(nn.Module):     \"\"\"Applies layer normalization to input embeddings.\"\"\"      def __init__(self, features: int, eps: float = 1e-6) -&gt; None:         \"\"\"         Initializes layer normalization.          Args:             features: Number of features in the input.             eps: Small constant for numerical stability.         \"\"\"         super().__init__()         self.features = features         self.eps = eps         self.alpha = nn.Parameter(torch.ones(self.features))         self.bias = nn.Parameter(torch.zeros(self.features))      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass for layer normalization.          Args:             input_embedding: Tensor of input embeddings.          Returns:             Normalized tensor.         \"\"\"         mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)         var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)         return (             self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))             + self.bias         )   class FeedForward(nn.Module):     \"\"\"Feed-forward neural network layer.\"\"\"      def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes feed-forward network.          Args:             d_model: Dimensionality of model embeddings.             d_ff: Dimensionality of feed-forward layer.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.d_model = d_model         self.d_ff = d_ff         self.ffn = nn.Sequential(             nn.Linear(in_features=self.d_model, out_features=self.d_ff),             nn.ReLU(),             nn.Dropout(dropout_rate),             nn.Linear(in_features=self.d_ff, out_features=self.d_model),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through feed-forward network.          Args:             input_tensor: Tensor of input embeddings.          Returns:             Tensor processed by feed-forward network.         \"\"\"         return self.ffn(input_tensor) <p>The multi-head attention mechanism constitutes the central component of the Transformer architecture. It allows the model to attend to information from different representation subspaces at different positions simultaneously. Scaled dot-product attention is calculated as:</p> <p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>where $Q$, $K$, and $V$ represent the query, key, and value matrices, and $d_k$ is the dimensionality of the keys. The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents dot products from growing excessively in magnitude.</p> <p>Multi-head attention linearly projects queries, keys, and values $h$ times with different learned projections, applies the attention function in parallel, and concatenates the results:</p> <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$</p> <p>where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$.</p> In\u00a0[4]: Copied! <pre>class MultiHeadAttention(nn.Module):\n    \"\"\"Applies multi-head attention mechanism.\"\"\"\n\n    def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes multi-head attention layer.\n\n        Args:\n            d_model: Dimensionality of model embeddings.\n            h: Number of attention heads.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n\n        if d_model % h != 0:\n            raise ValueError(\"d_model must be divisible by h\")\n\n        self.d_model = d_model\n        self.h = h\n        self.dropout = nn.Dropout(dropout_rate)\n        self.d_k = self.d_model // self.h\n        self.d_v = self.d_model // self.h\n\n        self.W_K = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_Q = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_V = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_OUTPUT_CONCAT = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n\n    @staticmethod\n    def attention(\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n        dropout: nn.Dropout | None = None,\n    ):\n        \"\"\"\n        Computes scaled dot-product attention.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Optional mask tensor.\n            dropout: Optional dropout layer.\n\n        Returns:\n            Tuple of attention output and scores.\n        \"\"\"\n        matmul_q_k = q @ k.transpose(-2, -1)\n        d_k = k.shape[-1]\n        matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n        if mask is not None:\n            matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n        attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n\n        return (attention_scores @ v), attention_scores\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through multi-head attention.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Optional mask tensor.\n\n        Returns:\n            Tensor after attention and concatenation.\n        \"\"\"\n        key_prima = self.W_K(k)\n        query_prima = self.W_Q(q)\n        value_prima = self.W_V(v)\n\n        key_prima = key_prima.view(\n            key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        query_prima = query_prima.view(\n            query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        value_prima = value_prima.view(\n            value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n\n        attention, attention_scores = MultiHeadAttention.attention(\n            k=key_prima,\n            q=query_prima,\n            v=value_prima,\n            mask=mask,\n            dropout=self.dropout,\n        )\n\n        attention = attention.transpose(1, 2)\n        b, seq_len, h, d_k = attention.size()\n        attention_concat = attention.contiguous().view(b, seq_len, h * d_k)\n\n        return self.W_OUTPUT_CONCAT(attention_concat)\n</pre> class MultiHeadAttention(nn.Module):     \"\"\"Applies multi-head attention mechanism.\"\"\"      def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes multi-head attention layer.          Args:             d_model: Dimensionality of model embeddings.             h: Number of attention heads.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()          if d_model % h != 0:             raise ValueError(\"d_model must be divisible by h\")          self.d_model = d_model         self.h = h         self.dropout = nn.Dropout(dropout_rate)         self.d_k = self.d_model // self.h         self.d_v = self.d_model // self.h          self.W_K = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_Q = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_V = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_OUTPUT_CONCAT = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )      @staticmethod     def attention(         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,         dropout: nn.Dropout | None = None,     ):         \"\"\"         Computes scaled dot-product attention.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Optional mask tensor.             dropout: Optional dropout layer.          Returns:             Tuple of attention output and scores.         \"\"\"         matmul_q_k = q @ k.transpose(-2, -1)         d_k = k.shape[-1]         matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)          if mask is not None:             matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)          attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)          if dropout is not None:             attention_scores = dropout(attention_scores)          return (attention_scores @ v), attention_scores      def forward(         self,         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Forward pass through multi-head attention.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Optional mask tensor.          Returns:             Tensor after attention and concatenation.         \"\"\"         key_prima = self.W_K(k)         query_prima = self.W_Q(q)         value_prima = self.W_V(v)          key_prima = key_prima.view(             key_prima.shape[0], key_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         query_prima = query_prima.view(             query_prima.shape[0], query_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         value_prima = value_prima.view(             value_prima.shape[0], value_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)          attention, attention_scores = MultiHeadAttention.attention(             k=key_prima,             q=query_prima,             v=value_prima,             mask=mask,             dropout=self.dropout,         )          attention = attention.transpose(1, 2)         b, seq_len, h, d_k = attention.size()         attention_concat = attention.contiguous().view(b, seq_len, h * d_k)          return self.W_OUTPUT_CONCAT(attention_concat) <p>Residual connections allow gradients to flow directly through the network, facilitating the training of deep architectures. Each sublayer in the Transformer is wrapped in a residual connection followed by layer normalization:</p> <p>$$\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$</p> <p>Encoder blocks apply self-attention followed by a feed-forward network, both with residual connections. Decoder blocks add an additional cross-attention layer that attends to the encoder output, allowing each position in the decoder to attend to all positions in the input sequence.</p> In\u00a0[5]: Copied! <pre>class ResidualConnection(nn.Module):\n    \"\"\"Applies residual connection around a sublayer.\"\"\"\n\n    def __init__(self, features: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes residual connection layer.\n\n        Args:\n            features: Number of features in the input.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNormalization(features=features)\n\n    def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using residual connection.\n\n        Args:\n            input_tensor: Input tensor to the residual layer.\n            sublayer: Sublayer to apply within the residual connection.\n\n        Returns:\n            Tensor with residual connection applied.\n        \"\"\"\n        return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n\n\nclass EncoderBlock(nn.Module):\n    \"\"\"Encoder block with attention and feed-forward layers.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes encoder block.\n\n        Args:\n            d_model: Dimensionality of model embeddings.\n            d_ff: Dimensionality of feed-forward layer.\n            h: Number of attention heads.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout_rate = dropout_rate\n\n        self.multi_head_attention_layer = MultiHeadAttention(\n            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_1 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n        self.feed_forward_layer = FeedForward(\n            d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_2 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through encoder block.\n\n        Args:\n            input_tensor: Input tensor to the encoder block.\n            mask: Optional mask tensor.\n\n        Returns:\n            Tensor after processing by the encoder block.\n        \"\"\"\n        input_tensor = self.residual_layer_1(\n            input_tensor,\n            lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n        )\n        input_tensor = self.residual_layer_2(\n            input_tensor, lambda x: self.feed_forward_layer(x)\n        )\n        return input_tensor\n\n\nclass DecoderBlock(nn.Module):\n    \"\"\"Decoder block with masked attention, cross-attention, and feed-forward layers.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initializes decoder block.\n\n        Args:\n            d_model: Dimensionality of model embeddings.\n            d_ff: Dimensionality of feed-forward layer.\n            h: Number of attention heads.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout_rate = dropout_rate\n\n        self.masked_multi_head_attention_layer = MultiHeadAttention(\n            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_1 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n        self.multi_head_attention_layer = MultiHeadAttention(\n            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_2 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n        self.feed_forward_layer = FeedForward(\n            d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_3 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n\n    def forward(\n        self,\n        decoder_input: torch.Tensor,\n        encoder_output: torch.Tensor,\n        src_mask: torch.Tensor | None = None,\n        tgt_mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through decoder block.\n\n        Args:\n            decoder_input: Input tensor to the decoder block.\n            encoder_output: Output tensor from the encoder.\n            src_mask: Optional source mask tensor.\n            tgt_mask: Optional target mask tensor.\n\n        Returns:\n            Tensor after processing by the decoder block.\n        \"\"\"\n        decoder_input = self.residual_layer_1(\n            decoder_input,\n            lambda x: self.masked_multi_head_attention_layer(\n                k=x, q=x, v=x, mask=tgt_mask\n            ),\n        )\n        decoder_input = self.residual_layer_2(\n            decoder_input,\n            lambda x: self.multi_head_attention_layer(\n                k=encoder_output, q=x, v=encoder_output, mask=src_mask\n            ),\n        )\n        decoder_output = self.residual_layer_3(\n            decoder_input, lambda x: self.feed_forward_layer(x)\n        )\n        return decoder_output\n</pre> class ResidualConnection(nn.Module):     \"\"\"Applies residual connection around a sublayer.\"\"\"      def __init__(self, features: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes residual connection layer.          Args:             features: Number of features in the input.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.dropout = nn.Dropout(dropout_rate)         self.layer_norm = LayerNormalization(features=features)      def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:         \"\"\"         Forward pass using residual connection.          Args:             input_tensor: Input tensor to the residual layer.             sublayer: Sublayer to apply within the residual connection.          Returns:             Tensor with residual connection applied.         \"\"\"         return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))   class EncoderBlock(nn.Module):     \"\"\"Encoder block with attention and feed-forward layers.\"\"\"      def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes encoder block.          Args:             d_model: Dimensionality of model embeddings.             d_ff: Dimensionality of feed-forward layer.             h: Number of attention heads.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.dropout_rate = dropout_rate          self.multi_head_attention_layer = MultiHeadAttention(             d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate         )         self.residual_layer_1 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )         self.feed_forward_layer = FeedForward(             d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate         )         self.residual_layer_2 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )      def forward(         self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None     ) -&gt; torch.Tensor:         \"\"\"         Forward pass through encoder block.          Args:             input_tensor: Input tensor to the encoder block.             mask: Optional mask tensor.          Returns:             Tensor after processing by the encoder block.         \"\"\"         input_tensor = self.residual_layer_1(             input_tensor,             lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),         )         input_tensor = self.residual_layer_2(             input_tensor, lambda x: self.feed_forward_layer(x)         )         return input_tensor   class DecoderBlock(nn.Module):     \"\"\"Decoder block with masked attention, cross-attention, and feed-forward layers.\"\"\"      def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initializes decoder block.          Args:             d_model: Dimensionality of model embeddings.             d_ff: Dimensionality of feed-forward layer.             h: Number of attention heads.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.dropout_rate = dropout_rate          self.masked_multi_head_attention_layer = MultiHeadAttention(             d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate         )         self.residual_layer_1 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )         self.multi_head_attention_layer = MultiHeadAttention(             d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate         )         self.residual_layer_2 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )         self.feed_forward_layer = FeedForward(             d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate         )         self.residual_layer_3 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )      def forward(         self,         decoder_input: torch.Tensor,         encoder_output: torch.Tensor,         src_mask: torch.Tensor | None = None,         tgt_mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Forward pass through decoder block.          Args:             decoder_input: Input tensor to the decoder block.             encoder_output: Output tensor from the encoder.             src_mask: Optional source mask tensor.             tgt_mask: Optional target mask tensor.          Returns:             Tensor after processing by the decoder block.         \"\"\"         decoder_input = self.residual_layer_1(             decoder_input,             lambda x: self.masked_multi_head_attention_layer(                 k=x, q=x, v=x, mask=tgt_mask             ),         )         decoder_input = self.residual_layer_2(             decoder_input,             lambda x: self.multi_head_attention_layer(                 k=encoder_output, q=x, v=encoder_output, mask=src_mask             ),         )         decoder_output = self.residual_layer_3(             decoder_input, lambda x: self.feed_forward_layer(x)         )         return decoder_output <p>The complete Transformer architecture integrates all the described components into an encoder-decoder structure. The encoder processes the input sequence through multiple identical layers, each applying self-attention and feed-forward transformations. The decoder generates the output sequence autoregressively, using both masked self-attention and cross-attention over the encoder output. A final projection layer transforms the decoder representations into probabilities over the output vocabulary.</p> In\u00a0[6]: Copied! <pre>class ProjectionLayer(nn.Module):\n    \"\"\"Converts d_model dimensions back to vocab_size.\"\"\"\n\n    def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n        \"\"\"\n        Initializes projection layer.\n\n        Args:\n            d_model: Dimensionality of model embeddings.\n            vocab_size: Size of the vocabulary.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.projection_layer = nn.Linear(in_features=d_model, out_features=vocab_size)\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through projection layer.\n\n        Args:\n            input_tensor: Input tensor to the projection layer.\n\n        Returns:\n            Tensor with projected dimensions.\n        \"\"\"\n        return self.projection_layer(input_tensor)\n\n\nclass Transformer(nn.Module):\n    \"\"\"Transformer model with encoder and decoder blocks.\"\"\"\n\n    def __init__(\n        self,\n        src_vocab_size: int,\n        tgt_vocab_size: int,\n        src_seq_len: int,\n        tgt_seq_len: int,\n        num_encoders: int,\n        num_decoders: int,\n        d_model: int,\n        d_ff: int,\n        h: int,\n        dropout_rate: float,\n    ) -&gt; None:\n        \"\"\"\n        Initializes transformer model.\n\n        Args:\n            src_vocab_size: Size of source vocabulary.\n            tgt_vocab_size: Size of target vocabulary.\n            src_seq_len: Maximum source sequence length.\n            tgt_seq_len: Maximum target sequence length.\n            num_encoders: Number of encoder blocks.\n            num_decoders: Number of decoder blocks.\n            d_model: Dimensionality of model embeddings.\n            d_ff: Dimensionality of feed-forward layer.\n            h: Number of attention heads.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n\n        self.src_vocab_size = src_vocab_size\n        self.tgt_vocab_size = tgt_vocab_size\n        self.src_seq_len = src_seq_len\n        self.tgt_seq_len = tgt_seq_len\n        self.num_encoders = num_encoders\n        self.num_decoders = num_decoders\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout_rate = dropout_rate\n\n        self.src_embedding = InputEmbedding(\n            d_model=self.d_model, vocab_size=self.src_vocab_size\n        )\n        self.tgt_embedding = InputEmbedding(\n            d_model=self.d_model, vocab_size=self.tgt_vocab_size\n        )\n        self.src_positional_encoding = PositionalEncoding(\n            d_model=self.d_model,\n            sequence_length=self.src_seq_len,\n            dropout_rate=self.dropout_rate,\n        )\n        self.tgt_positional_encoding = PositionalEncoding(\n            d_model=self.d_model,\n            sequence_length=self.tgt_seq_len,\n            dropout_rate=self.dropout_rate,\n        )\n\n        self.encoder_layers = nn.ModuleList(\n            [\n                EncoderBlock(\n                    d_model=self.d_model,\n                    d_ff=self.d_ff,\n                    h=self.h,\n                    dropout_rate=self.dropout_rate,\n                )\n                for _ in range(self.num_encoders)\n            ]\n        )\n\n        self.decoder_layers = nn.ModuleList(\n            [\n                DecoderBlock(\n                    d_model=self.d_model,\n                    d_ff=self.d_ff,\n                    h=self.h,\n                    dropout_rate=self.dropout_rate,\n                )\n                for _ in range(self.num_decoders)\n            ]\n        )\n\n        self.projection_layer = ProjectionLayer(\n            d_model=self.d_model, vocab_size=self.tgt_vocab_size\n        )\n\n    def encode(\n        self, encoder_input: torch.Tensor, src_mask: torch.Tensor | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Encodes source input tensor using encoder blocks.\n\n        Args:\n            encoder_input: Input tensor to the encoder.\n            src_mask: Optional source mask tensor.\n\n        Returns:\n            Encoded tensor.\n        \"\"\"\n        x = self.src_embedding(encoder_input)\n        x = self.src_positional_encoding(x)\n\n        for encoder_layer in self.encoder_layers:\n            x = encoder_layer(input_tensor=x, mask=src_mask)\n\n        return x\n\n    def decode(\n        self,\n        decoder_input: torch.Tensor,\n        encoder_output: torch.Tensor,\n        src_mask: torch.Tensor | None = None,\n        tgt_mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Decodes target input tensor using decoder blocks.\n\n        Args:\n            decoder_input: Input tensor to the decoder.\n            encoder_output: Output tensor from the encoder.\n            src_mask: Optional source mask tensor.\n            tgt_mask: Optional target mask tensor.\n\n        Returns:\n            Decoded tensor.\n        \"\"\"\n        x = self.tgt_embedding(decoder_input)\n        x = self.tgt_positional_encoding(x)\n\n        for decoder_layer in self.decoder_layers:\n            x = decoder_layer(\n                decoder_input=x,\n                encoder_output=encoder_output,\n                src_mask=src_mask,\n                tgt_mask=tgt_mask,\n            )\n\n        return x\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        tgt: torch.Tensor,\n        src_mask: torch.Tensor | None = None,\n        tgt_mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Processes input and target sequences through the encoder\n        and decoder, applying optional source and target masks.\n\n        Args:\n            src: Input sequence tensor.\n            tgt: Target sequence tensor.\n            src_mask: Optional mask for the input sequence.\n            tgt_mask: Optional mask for the target sequence.\n\n        Returns:\n            Tensor containing the final output after projection.\n        \"\"\"\n        encoder_output = self.encode(src, src_mask)\n        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n        return self.projection_layer(decoder_output)\n</pre> class ProjectionLayer(nn.Module):     \"\"\"Converts d_model dimensions back to vocab_size.\"\"\"      def __init__(self, d_model: int, vocab_size: int) -&gt; None:         \"\"\"         Initializes projection layer.          Args:             d_model: Dimensionality of model embeddings.             vocab_size: Size of the vocabulary.         \"\"\"         super().__init__()         self.d_model = d_model         self.vocab_size = vocab_size         self.projection_layer = nn.Linear(in_features=d_model, out_features=vocab_size)      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through projection layer.          Args:             input_tensor: Input tensor to the projection layer.          Returns:             Tensor with projected dimensions.         \"\"\"         return self.projection_layer(input_tensor)   class Transformer(nn.Module):     \"\"\"Transformer model with encoder and decoder blocks.\"\"\"      def __init__(         self,         src_vocab_size: int,         tgt_vocab_size: int,         src_seq_len: int,         tgt_seq_len: int,         num_encoders: int,         num_decoders: int,         d_model: int,         d_ff: int,         h: int,         dropout_rate: float,     ) -&gt; None:         \"\"\"         Initializes transformer model.          Args:             src_vocab_size: Size of source vocabulary.             tgt_vocab_size: Size of target vocabulary.             src_seq_len: Maximum source sequence length.             tgt_seq_len: Maximum target sequence length.             num_encoders: Number of encoder blocks.             num_decoders: Number of decoder blocks.             d_model: Dimensionality of model embeddings.             d_ff: Dimensionality of feed-forward layer.             h: Number of attention heads.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()          self.src_vocab_size = src_vocab_size         self.tgt_vocab_size = tgt_vocab_size         self.src_seq_len = src_seq_len         self.tgt_seq_len = tgt_seq_len         self.num_encoders = num_encoders         self.num_decoders = num_decoders         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.dropout_rate = dropout_rate          self.src_embedding = InputEmbedding(             d_model=self.d_model, vocab_size=self.src_vocab_size         )         self.tgt_embedding = InputEmbedding(             d_model=self.d_model, vocab_size=self.tgt_vocab_size         )         self.src_positional_encoding = PositionalEncoding(             d_model=self.d_model,             sequence_length=self.src_seq_len,             dropout_rate=self.dropout_rate,         )         self.tgt_positional_encoding = PositionalEncoding(             d_model=self.d_model,             sequence_length=self.tgt_seq_len,             dropout_rate=self.dropout_rate,         )          self.encoder_layers = nn.ModuleList(             [                 EncoderBlock(                     d_model=self.d_model,                     d_ff=self.d_ff,                     h=self.h,                     dropout_rate=self.dropout_rate,                 )                 for _ in range(self.num_encoders)             ]         )          self.decoder_layers = nn.ModuleList(             [                 DecoderBlock(                     d_model=self.d_model,                     d_ff=self.d_ff,                     h=self.h,                     dropout_rate=self.dropout_rate,                 )                 for _ in range(self.num_decoders)             ]         )          self.projection_layer = ProjectionLayer(             d_model=self.d_model, vocab_size=self.tgt_vocab_size         )      def encode(         self, encoder_input: torch.Tensor, src_mask: torch.Tensor | None = None     ) -&gt; torch.Tensor:         \"\"\"         Encodes source input tensor using encoder blocks.          Args:             encoder_input: Input tensor to the encoder.             src_mask: Optional source mask tensor.          Returns:             Encoded tensor.         \"\"\"         x = self.src_embedding(encoder_input)         x = self.src_positional_encoding(x)          for encoder_layer in self.encoder_layers:             x = encoder_layer(input_tensor=x, mask=src_mask)          return x      def decode(         self,         decoder_input: torch.Tensor,         encoder_output: torch.Tensor,         src_mask: torch.Tensor | None = None,         tgt_mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Decodes target input tensor using decoder blocks.          Args:             decoder_input: Input tensor to the decoder.             encoder_output: Output tensor from the encoder.             src_mask: Optional source mask tensor.             tgt_mask: Optional target mask tensor.          Returns:             Decoded tensor.         \"\"\"         x = self.tgt_embedding(decoder_input)         x = self.tgt_positional_encoding(x)          for decoder_layer in self.decoder_layers:             x = decoder_layer(                 decoder_input=x,                 encoder_output=encoder_output,                 src_mask=src_mask,                 tgt_mask=tgt_mask,             )          return x      def forward(         self,         src: torch.Tensor,         tgt: torch.Tensor,         src_mask: torch.Tensor | None = None,         tgt_mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Processes input and target sequences through the encoder         and decoder, applying optional source and target masks.          Args:             src: Input sequence tensor.             tgt: Target sequence tensor.             src_mask: Optional mask for the input sequence.             tgt_mask: Optional mask for the target sequence.          Returns:             Tensor containing the final output after projection.         \"\"\"         encoder_output = self.encode(src, src_mask)         decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)         return self.projection_layer(decoder_output)"},{"location":"course/topic_05_sequential_models/section_05_transformers.html#transformers","title":"Transformers\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#auxiliary-functions-for-masking","title":"Auxiliary functions for masking\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#input-embeddings-and-positional-encoding","title":"Input embeddings and positional encoding\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#layer-normalization-and-feed-forward-networks","title":"Layer normalization and feed-forward networks\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#multi-head-attention-mechanism","title":"Multi-head attention mechanism\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#residual-connections-and-encoder-decoder-blocks","title":"Residual connections and encoder-decoder blocks\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_05_transformers.html#complete-transformer-architecture","title":"Complete Transformer architecture\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html","title":"Mixture of Experts","text":"<p>The Mixture of Experts (MoE) architecture constitutes a deep learning paradigm that allows efficient model scaling through submodel specialization. Originally introduced by Jacobs et al. (1991) and subsequently popularized in the context of deep neural networks, this architecture is based on the \"divide and conquer\" principle: instead of training a single monolithic model for all tasks, a set of specialized models (experts) is trained along with a routing mechanism (gating network) that determines which experts should process each input.</p> <p>The computational efficiency of MoE lies in its conditional activation capability: although the model can contain a large number of parameters distributed among multiple experts, only a subset of these is activated for each specific input. This property allows building models with massive expressive capabilities while maintaining manageable computational costs during inference. The architecture has proven particularly effective in large-scale language models, where different experts can specialize in different linguistic domains, styles, or types of knowledge.</p> <p>Each expert in an MoE architecture constitutes an independent neural network designed to process a specific subset of the input space. In its simplest form, an expert can be implemented as a feed-forward network with hidden layers that transform the input into an output representation. The specialization of each expert emerges naturally during training, where the routing mechanism learns to direct different types of inputs to different experts.</p> <p>Mathematically, each expert $E_i$ can be represented as a parameterized function:</p> <p>$$E_i(x; \\theta_i) : \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_{out}}$$</p> <p>where $\\theta_i$ represents the specific parameters of expert $i$, and $d_{in}$ and $d_{out}$ are the input and output dimensionalities respectively.</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ExpertModel(nn.Module):\n    \"\"\"\n    Individual expert model for MoE\n    \"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:\n        \"\"\"\n        Initializes an expert model with a simple feed-forward network.\n\n        Args:\n            input_dim: Dimensionality of the input data.\n            output_dim: Dimensionality of the output data.\n            hidden_dim: Dimensionality of the hidden layer.\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n\n        self.model = nn.Sequential(\n            nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the expert model.\n\n        Args:\n            input_tensor: Input tensor to the model.\n\n        Returns:\n            The model's output tensor.\n        \"\"\"\n        return self.model(input_tensor)\n</pre> import torch from torch import nn from torch.nn import functional as F   class ExpertModel(nn.Module):     \"\"\"     Individual expert model for MoE     \"\"\"      def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:         \"\"\"         Initializes an expert model with a simple feed-forward network.          Args:             input_dim: Dimensionality of the input data.             output_dim: Dimensionality of the output data.             hidden_dim: Dimensionality of the hidden layer.         \"\"\"         super().__init__()         self.input_dim = input_dim         self.output_dim = output_dim         self.hidden_dim = hidden_dim          self.model = nn.Sequential(             nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim),             nn.ReLU(),             nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the expert model.          Args:             input_tensor: Input tensor to the model.          Returns:             The model's output tensor.         \"\"\"         return self.model(input_tensor) <p>The routing mechanism (gating network) constitutes the central component that determines how inputs are distributed among available experts. This network learns to assign weights to each expert based on input characteristics, producing a probability distribution over experts through a softmax function. The gating network can be interpreted as a soft classifier that determines which experts are most relevant for processing each specific input.</p> <p>The gating function $G(x; \\phi)$ produces a vector of normalized weights:</p> <p>$$G(x; \\phi) = \\text{softmax}(W_g \\cdot h(x) + b_g)$$</p> <p>where $h(x)$ represents intermediate transformations applied to the input, $W_g$ and $b_g$ are learnable gating parameters, and $\\phi$ denotes the complete set of routing network parameters. The resulting weights satisfy $\\sum_{i=1}^{N} g_i(x) = 1$, where $N$ is the total number of experts.</p> In\u00a0[2]: Copied! <pre>class Gating(nn.Module):\n    \"\"\"\n    Gating mechanism to select experts.\n    \"\"\"\n\n    def __init__(\n        self, input_dim: int, num_experts: int, dropout_rate: float = 0.2\n    ) -&gt; None:\n        \"\"\"\n        Initializes a gating network for expert selection.\n\n        Args:\n            input_dim: Dimensionality of the input data.\n            num_experts: Number of experts to select from.\n            dropout_rate: Rate of dropout for regularization.\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.num_experts = num_experts\n        self.dropout_rate = dropout_rate\n\n        self.model = nn.Sequential(\n            nn.Linear(in_features=self.input_dim, out_features=128),\n            nn.Dropout(self.dropout_rate),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=128, out_features=256),\n            nn.LeakyReLU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(in_features=256, out_features=128),\n            nn.LeakyReLU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(in_features=128, out_features=num_experts),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the gating network.\n\n        Args:\n            input_tensor: Input tensor to the network.\n\n        Returns:\n            Softmax probabilities for expert selection.\n        \"\"\"\n        return F.softmax(self.model(input_tensor), dim=-1)\n</pre> class Gating(nn.Module):     \"\"\"     Gating mechanism to select experts.     \"\"\"      def __init__(         self, input_dim: int, num_experts: int, dropout_rate: float = 0.2     ) -&gt; None:         \"\"\"         Initializes a gating network for expert selection.          Args:             input_dim: Dimensionality of the input data.             num_experts: Number of experts to select from.             dropout_rate: Rate of dropout for regularization.         \"\"\"         super().__init__()         self.input_dim = input_dim         self.num_experts = num_experts         self.dropout_rate = dropout_rate          self.model = nn.Sequential(             nn.Linear(in_features=self.input_dim, out_features=128),             nn.Dropout(self.dropout_rate),             nn.LeakyReLU(),             nn.Linear(in_features=128, out_features=256),             nn.LeakyReLU(),             nn.Dropout(self.dropout_rate),             nn.Linear(in_features=256, out_features=128),             nn.LeakyReLU(),             nn.Dropout(self.dropout_rate),             nn.Linear(in_features=128, out_features=num_experts),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the gating network.          Args:             input_tensor: Input tensor to the network.          Returns:             Softmax probabilities for expert selection.         \"\"\"         return F.softmax(self.model(input_tensor), dim=-1) <p>The complete MoE architecture integrates individual experts with the gating mechanism to produce a final output through a weighted combination of expert predictions. For an input $x$, the MoE system output is calculated as:</p> <p>$$\\text{MoE}(x) = \\sum_{i=1}^{N} g_i(x) \\cdot E_i(x)$$</p> <p>where $g_i(x)$ represents the weight assigned to expert $i$ by the gating network, and $E_i(x)$ is the output of expert $i$. This formulation allows the model to automatically learn which experts are most relevant for different regions of the input space, facilitating specialization and improving model capacity without proportionally increasing computational cost.</p> <p>During training, both experts and the gating network are jointly optimized through standard backpropagation. The gradient flows through all experts weighted by their respective gating weights, allowing the system to learn both expert specialization and optimal routing in an end-to-end manner.</p> In\u00a0[3]: Copied! <pre>class MoE(nn.Module):\n    \"\"\"\n    Mixture of Experts\n    \"\"\"\n\n    def __init__(\n        self,\n        trained_experts: list[ExpertModel],\n        input_dim: int,\n        dropout_rate: float = 0.2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a mixture of experts with gating.\n\n        Args:\n            trained_experts: List of trained expert models.\n            input_dim: Dimensionality of the input data.\n            dropout_rate: Rate of dropout in the gating network.\n        \"\"\"\n        super().__init__()\n        self.experts = nn.ModuleList(trained_experts)\n        self.num_experts = len(trained_experts)\n        self.input_dim = input_dim\n        self.dropout_rate = dropout_rate\n\n        self.gating_layer = Gating(\n            input_dim=self.input_dim,\n            num_experts=self.num_experts,\n            dropout_rate=self.dropout_rate,\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the mixture of experts.\n\n        Args:\n            input_tensor: Input tensor to the model.\n\n        Returns:\n            Weighted sum of expert outputs.\n        \"\"\"\n        expert_weights = self.gating_layer(input_tensor)\n\n        _expert_outputs: list[torch.Tensor] = []\n        for expert in self.experts:\n            _expert_outputs.append(expert(input_tensor))\n\n        expert_outputs = torch.stack(_expert_outputs, dim=-1)\n        expert_weights = expert_weights.unsqueeze(1)\n\n        return torch.sum(expert_outputs * expert_weights, dim=-1)\n</pre> class MoE(nn.Module):     \"\"\"     Mixture of Experts     \"\"\"      def __init__(         self,         trained_experts: list[ExpertModel],         input_dim: int,         dropout_rate: float = 0.2,     ) -&gt; None:         \"\"\"         Initializes a mixture of experts with gating.          Args:             trained_experts: List of trained expert models.             input_dim: Dimensionality of the input data.             dropout_rate: Rate of dropout in the gating network.         \"\"\"         super().__init__()         self.experts = nn.ModuleList(trained_experts)         self.num_experts = len(trained_experts)         self.input_dim = input_dim         self.dropout_rate = dropout_rate          self.gating_layer = Gating(             input_dim=self.input_dim,             num_experts=self.num_experts,             dropout_rate=self.dropout_rate,         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the mixture of experts.          Args:             input_tensor: Input tensor to the model.          Returns:             Weighted sum of expert outputs.         \"\"\"         expert_weights = self.gating_layer(input_tensor)          _expert_outputs: list[torch.Tensor] = []         for expert in self.experts:             _expert_outputs.append(expert(input_tensor))          expert_outputs = torch.stack(_expert_outputs, dim=-1)         expert_weights = expert_weights.unsqueeze(1)          return torch.sum(expert_outputs * expert_weights, dim=-1) <p>The MoE implementation allows direct integration into existing deep learning pipelines. The following example demonstrates basic model instantiation and usage, including verification that gating weights are correctly normalized.</p> In\u00a0[4]: Copied! <pre>if __name__ == \"__main__\":\n    input_dim = 10\n    output_dim = 5\n    num_experts = 3\n    batch_size = 32\n    hidden_dim = 128\n\n    experts = [\n        ExpertModel(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n        for _ in range(num_experts)\n    ]\n\n    moe = MoE(experts, input_dim)\n\n    x = torch.randn(batch_size, input_dim)\n    output = moe(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Expected output shape: ({batch_size}, {output_dim})\")\n\n    gating_weights = moe.gating_layer(x)\n    print(f\"Gating weights shape: {gating_weights.shape}\")\n    print(f\"Gating weights sum per sample: {gating_weights.sum(dim=1)}\")\n</pre> if __name__ == \"__main__\":     input_dim = 10     output_dim = 5     num_experts = 3     batch_size = 32     hidden_dim = 128      experts = [         ExpertModel(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)         for _ in range(num_experts)     ]      moe = MoE(experts, input_dim)      x = torch.randn(batch_size, input_dim)     output = moe(x)      print(f\"Input shape: {x.shape}\")     print(f\"Output shape: {output.shape}\")     print(f\"Expected output shape: ({batch_size}, {output_dim})\")      gating_weights = moe.gating_layer(x)     print(f\"Gating weights shape: {gating_weights.shape}\")     print(f\"Gating weights sum per sample: {gating_weights.sum(dim=1)}\") <pre>Input shape: torch.Size([32, 10])\nOutput shape: torch.Size([32, 5])\nExpected output shape: (32, 5)\nGating weights shape: torch.Size([32, 3])\nGating weights sum per sample: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n</pre>"},{"location":"course/topic_05_sequential_models/section_06_moe.html#mixture-of-experts","title":"Mixture of Experts\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html#introduction","title":"Introduction\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html#individual-expert-architecture","title":"Individual expert architecture\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html#routing-mechanism-via-gating-network","title":"Routing mechanism via gating network\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html#complete-mixture-of-experts-architecture","title":"Complete Mixture of Experts architecture\u00b6","text":""},{"location":"course/topic_05_sequential_models/section_06_moe.html#usage-example-and-verification","title":"Usage example and verification\u00b6","text":""},{"location":"course/topic_06_graphs_models/index.html","title":"Introduction","text":"<p>In this topic, we study Graph Neural Networks (GNN), whose main objective is to learn from data that can be naturally represented as graphs. A graph is a structure formed by nodes (vertices) and edges that describe relationships or interactions between entities. This form of representation is especially suitable for numerous real-world problems, in which the connections between elements are as important as the elements themselves.</p> <p>Graphs appear implicitly in many everyday domains. In social networks, for example, each user can be modeled as a node and friendship, following, or interaction relationships as edges. Furthermore, attributes or characteristics can be associated with each node, such as salary range, age, geographic location, occupation, or demographic variables. These characteristics, together with the connection structure, allow building models that better capture the dynamics and organization of the social network than if users were considered in isolation.</p> <p>Similarly, an image can be interpreted as a graph in which each pixel acts as a node connected to its neighboring pixels (in a 4, 8, or wider neighborhood), and where the intensity or color vector (for example, in RGB format) is used as node attributes. This graph perspective allows applying graph-based processing techniques to visual problems, establishing a bridge between computer vision and graph learning.</p> <p>There are also graph representations that are especially relevant in scientific and industrial fields. A paradigmatic case is that of molecules, where atoms are modeled as nodes and chemical bonds as edges. This type of representation has been key in developments such as AlphaFold, presented by Google DeepMind, for protein folding, or in multiple works on drug discovery and computational chemistry. Another example is offered by systems like Google Maps, where intersections and points of interest can be considered nodes, and roads or connections between them, edges; on this structure, problems such as traffic prediction, travel time estimation, or optimal route calculation are formulated.</p> <p>Throughout the topic, we will see how to preprocess and transform heterogeneous data to represent them as graphs suitable for training neural models. We will analyze how to explicitly define nodes, edges, and their attributes, and how to construct data structures compatible with modern deep learning libraries for graphs.</p> <p>A central objective is to understand how to implement a graph from scratch and how to apply different variants of graph neural networks to it. Special emphasis is placed on GNN based on graph convolutions (Graph Convolutional Networks, GCN), which generalize the idea of classical convolution to non-Euclidean domains. In this framework, each node updates its representation by aggregating information from its neighbors according to a message passing scheme, which allows capturing local and global patterns in the graph structure.</p> <p>In addition to graph convolutions, we also study attention mechanisms in graphs (Graph Attention Networks, GAT and variants). These methods assign differentiated weights to connections between nodes, allowing the network to learn which neighbors it should pay more attention to depending on the context and task. This approach is especially useful when the importance of relationships is not homogeneous or when a finer interpretation of interactions between nodes is desired.</p> <p>The techniques described are applied both to images modeled as graphs and to other types of data structured in networks, demonstrating the versatility of GNNs to address problems of node classification, link prediction, whole graph classification, and other related tasks.</p> <p>For practical implementation, we will use the PyTorch Geometric library, a specialized framework that extends PyTorch with data structures and optimized operations for deep learning on graphs. PyTorch Geometric facilitates the definition of graphs, the construction of graph convolution and attention layers, and the training of complex models on large collections of graphs or large-scale graphs.</p>"},{"location":"course/topic_06_graphs_models/section_01_gnns.html","title":"Graph Neural Networks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/topic_06_graphs_models/section_01_gnns.html#graph-neural-networks","title":"Graph Neural Networks\u00b6","text":""},{"location":"course/topic_06_graphs_models/section_02_pytorch_geometric.html","title":"PyTorch Geometric","text":"In\u00a0[\u00a0]: Copied!"},{"location":"course/topic_06_graphs_models/section_02_pytorch_geometric.html#pytorch-geometric","title":"PyTorch Geometric\u00b6","text":""},{"location":"other_topics/index.html","title":"Introduction","text":"<p>This section brings together a series of complementary contents that, while directly related to the central topics of the course, have not been formally included in the main structure due to time and teaching planning limitations. Despite this, they are considered matters of special theoretical and practical relevance, or areas that arouse particular interest from the professor's point of view, and that can significantly enrich the overall understanding of deep learning and its foundations.</p> <p>The purpose of this material is to offer an additional path to delve deeper into aspects that, in many cases, constitute the conceptual background of the techniques seen in the course, or represent recent development lines and current trends in the field. It is, therefore, a natural extension of the official content, aimed at those who wish to go beyond the bare minimum and explore advanced or less common topics in introductory courses.</p> <p>It is possible that, in future program revisions, part of these contents will be explicitly integrated into the main syllabus. However, at the present time they are presented as optional modules, organized by topics or subtopics, which students can consult autonomously according to their interests, learning pace, and specific needs.</p> <p>It is recommended to review the index of these additional sections and explore those topics that are of special interest, whether for their practical application, their conceptual relevance, or their connection with personal or professional projects. In case questions arise about the content, its level of currency, or its fit within the course, it is advisable to contact the professor, who can provide guidance on which materials are most appropriate in each case and how to integrate them into the individual study plan.</p>"},{"location":"other_topics/topic_01_generative_models/section_01_gans.html","title":"Generative Adversarial Networks","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n\ndef show_generated_samples(\n    generator: nn.Module, noise, device: str, num_samples: int = 16\n) -&gt; None:\n    \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        samples = generator(noise[:num_samples]).cpu()\n        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(num_samples):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n            axes[row, col].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n\n\nclass Discriminator(nn.Module):\n    def __init__(\n        self, in_channels: int, hidden_size: int = 64, dropout_rate: float = 0.2\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.in_channels = in_channels\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n\n        # Creamos el modelo que lo ajustaremos para MNIST\n        self.model = nn.Sequential(\n            # Input MNIST = (B, C, H, W) = (B, 1, 28, 28) -&gt; (B, H, 14, 14)\n            nn.Conv2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size // 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout2d(p=self.dropout_rate),\n            # (B, H, 14, 14) -&gt; (B, H, 7, 7)\n            nn.Conv2d(\n                in_channels=self.hidden_size // 2,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            nn.Dropout2d(p=self.dropout_rate),\n            # (B, H, 7, 7) -&gt; (B, 1, 7, 7)\n            nn.Conv2d(\n                in_channels=self.hidden_size,\n                out_channels=1,\n                kernel_size=1,\n                stride=1,\n                bias=False,\n            ),\n            # (B, 1, 7, 7) -&gt; (B, 1, 1, 1)\n            nn.AdaptiveAvgPool2d((1, 1)),\n            # (B, 1, 1, 1) -&gt; (B, 1),\n            nn.Flatten(),\n            nn.Dropout(p=self.dropout_rate),\n            # El discriminador ha de devolver un escalar entre 0 y 1 (falso/real)\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(input_tensor)\n\n\nclass Generator(nn.Module):\n    def __init__(\n        self, z_dim: int, data_shape: tuple[int, int, int], hidden_size: int\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.z_dim = z_dim\n        self.data_shape = data_shape\n        self.hidden_size = hidden_size\n\n        # Del ruido que es un tensor plano, vamos a crear una matriz inicial\n        # de (B, H, 7, 7) que es el tama\u00f1o antes de aplanar en el Discriminador\n        self.projection = nn.Sequential(\n            # (B, z_dim) -&gt; (B, H * 7 * 7)\n            nn.Linear(\n                in_features=self.z_dim,\n                out_features=self.hidden_size * 7 * 7,\n                bias=False,\n            ),\n        )\n\n        self.model = nn.Sequential(\n            # (B, H, 7, 7) -&gt; (B, H, 14, 14)\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            # (B, H, 14, 14) -&gt; (B, H, 28, 28)\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.hidden_size,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(num_features=self.hidden_size),\n            nn.GELU(),\n            # (B, H, 28, 28) -&gt; (B, 1, 28, 28)\n            nn.Conv2d(\n                in_channels=self.hidden_size, out_channels=1, kernel_size=1, stride=1\n            ),\n            # Normalizamos los valores entre -1 y 1\n            nn.Tanh(),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        # (B, z_dim) -&gt; (B, H * 7 * 7)\n        projection = self.projection(input_tensor)\n        # (B, H * 7 * 7) -&gt; (B, H, 7, 7)\n        projection = projection.view(projection.size(0), self.hidden_size, 7, 7)\n\n        return self.model(projection)\n\n\nclass GAN(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_size_discriminator: int,\n        dropout_rate: float,\n        z_dim: int,\n        data_shape: tuple[int, int, int],\n        hidden_size_generator: int,\n    ) -&gt; None:\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros\n        self.in_channels = in_channels\n        self.hidden_size_discriminator = hidden_size_discriminator\n        self.dropout_rate = dropout_rate\n        self.z_dim = z_dim\n        self.data_shape = data_shape\n        self.hidden_size_generator = hidden_size_generator\n\n        # Definimos los modelos\n        self.discriminator = Discriminator(\n            in_channels=self.in_channels,\n            hidden_size=self.hidden_size_discriminator,\n            dropout_rate=self.dropout_rate,\n        )\n        self.generator = Generator(\n            z_dim=self.z_dim,\n            data_shape=self.data_shape,\n            hidden_size=self.hidden_size_generator,\n        )\n\n    def forward(self, real_data: torch.Tensor, batch_size: int | None = None) -&gt; dict:\n        if batch_size is None:\n            batch_size = real_data.size(0)\n\n        # Generar ruido aleatorio\n        noise = torch.randn(batch_size, self.z_dim, device=real_data.device)\n\n        # Generar im\u00e1genes falsas\n        fake_data = self.generator(noise)\n\n        # Pasar datos reales por el discriminador\n        real_predictions = self.discriminator(real_data)\n\n        # Pasar datos falsos por el discriminador\n        fake_predictions = self.discriminator(fake_data)\n\n        return {\n            \"real_predictions\": real_predictions,\n            \"fake_predictions\": fake_predictions,\n            \"fake_data\": fake_data,\n            \"noise\": noise,\n        }\n\n    def generate_samples_inference(self, num_samples: int, device: str) -&gt; torch.Tensor:\n        self.eval()\n        with torch.no_grad():\n            # (Num_samples, z_dim)\n            noise = torch.randn(num_samples, self.z_dim, device=device)\n            # (Num_samples, z_dim) -&gt; MNIST: (Num_samples, 1, 28, 28)\n            samples = self.generator(input_tensor=noise)\n\n        return samples\n\n    def discriminator_loss(\n        self, real_predictions: torch.Tensor, fake_predictions: torch.Tensor\n    ) -&gt; torch.Tensor:\n        criterion = nn.BCELoss()\n        # Matriz de 1s\n        real_labels = torch.ones_like(real_predictions)\n        # Matriz de 0s\n        fake_labels = torch.zeros_like(fake_predictions)\n\n        real_loss = criterion(real_predictions, real_labels)\n        fake_loss = criterion(fake_predictions, fake_labels)\n\n        return (real_loss + fake_loss) / 2\n\n    def generator_loss(self, fake_predictions: torch.Tensor) -&gt; torch.Tensor:\n        criterion = nn.BCELoss()\n        fake_real_labels = torch.ones_like(fake_predictions)\n        return criterion(fake_predictions, fake_real_labels)\n\n\nif __name__ == \"__main__\":\n    # Seleccionamos el dispositivo actual\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Usando dispositivo: {device}\")\n\n    # Las GANs son muy sensibles a los hiperpar\u00e1metros\n    lr = 2e-4\n    # Dimensi\u00f3n de los datos de entrada (C, H, W)\n    data_dimension = (1, 28, 28)\n    # Esta es la dimension del ruido\n    z_dim = 100\n    batch_size = 128\n    num_epochs = 50\n\n    # Definimos el modelo\n    model = GAN(\n        in_channels=data_dimension[0],\n        hidden_size_discriminator=64,\n        dropout_rate=0.2,\n        z_dim=z_dim,\n        data_shape=data_dimension,\n        hidden_size_generator=256,\n    ).to(device)\n\n    # Transformaciones que vamos a aplicar a MNIST\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                # Normalizar a [-1, 1] para coincidir con Tanh\n                (0.5,),\n                (0.5,),\n            ),\n        ]\n    )\n\n    # Descargamos MNIST\n    dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Vamos a utilizar un optimizador para cada modelo\n    opt_disc = optim.AdamW(params=model.discriminator.parameters(), lr=lr)\n    opt_gen = optim.AdamW(params=model.generator.parameters(), lr=lr)\n\n    # Ruido fijo para generar muestras durante el entrenamiento\n    fixed_noise = torch.randn(64, z_dim, device=device)\n\n    # Listas para guardar las p\u00e9rdidas\n    disc_losses = []\n    gen_losses = []\n\n    print(\"Iniciando entrenamiento de la GAN...\")\n\n    for epoch in range(num_epochs):\n        epoch_disc_loss: int | float = 0\n        epoch_gen_loss: int | float = 0\n\n        pbar = tqdm(loader, desc=f\"\u00c9poca {epoch + 1}/{num_epochs}\")\n\n        for _, (real, _) in enumerate(pbar):\n            real = real.to(device)\n            batch_size_current = real.shape[0]\n\n            # ENTRENAR DISCRIMINADOR\n            opt_disc.zero_grad()\n\n            # Generar datos falsos\n            noise = torch.randn(batch_size_current, z_dim, device=device)\n            fake_data = model.generator(\n                noise\n            ).detach()  # Detach para no actualizar generador\n\n            # Predicciones del discriminador\n            real_preds = model.discriminator(real)\n            fake_preds = model.discriminator(fake_data)\n\n            # P\u00e9rdida del discriminador\n            lossD = model.discriminator_loss(real_preds, fake_preds)\n            lossD.backward()\n            opt_disc.step()\n\n            # ENTRENAR GENERADOR\n            opt_gen.zero_grad()\n\n            # Generar nuevos datos falsos (sin detach)\n            noise = torch.randn(batch_size_current, z_dim, device=device)\n            fake_data = model.generator(noise)\n            fake_preds_for_gen = model.discriminator(fake_data)\n\n            # P\u00e9rdida del generador\n            lossG = model.generator_loss(fake_preds_for_gen)\n            lossG.backward()\n            opt_gen.step()\n\n            # Acumular p\u00e9rdidas\n            epoch_disc_loss += lossD.item()\n            epoch_gen_loss += lossG.item()\n\n            # Actualizar barra de progreso\n            pbar.set_postfix(\n                {\"D_loss\": f\"{lossD.item():.4f}\", \"G_loss\": f\"{lossG.item():.4f}\"}\n            )\n\n        # P\u00e9rdidas promedio de la \u00e9poca\n        avg_disc_loss = epoch_disc_loss / len(loader)\n        avg_gen_loss = epoch_gen_loss / len(loader)\n        disc_losses.append(avg_disc_loss)\n        gen_losses.append(avg_gen_loss)\n\n        print(\n            f\"\u00c9poca {epoch + 1}, \"\n            f\"P\u00e9rdida D: {avg_disc_loss:.4f}, \"\n            f\"P\u00e9rdida G: {avg_gen_loss:.4f}\"\n        )\n\n        # Mostrar muestras cada 5 \u00e9pocas\n        if (epoch + 1) % 5 == 0:\n            print(f\"\\nMostrando muestras generadas en \u00e9poca {epoch + 1}...\")\n            show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n\n    print(\"\\n\u00a1Entrenamiento completado!\")\n\n    # Mostrar gr\u00e1fica de p\u00e9rdidas\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(disc_losses, label=\"Discriminador\", color=\"red\")\n    plt.plot(gen_losses, label=\"Generador\", color=\"blue\")\n    plt.xlabel(\"\u00c9poca\")\n    plt.ylabel(\"P\u00e9rdida\")\n    plt.title(\"Evoluci\u00f3n de las P\u00e9rdidas\")\n    plt.legend()\n    plt.grid(True)\n\n    # Mostrar muestras generadas finales\n    plt.subplot(1, 2, 2)\n    model.generator.eval()\n    with torch.no_grad():\n        final_samples = model.generator(fixed_noise[:16]).cpu()\n        final_samples = (final_samples + 1) / 2  # Desnormalizar\n        grid = torchvision.utils.make_grid(final_samples, nrow=4, padding=2)\n        plt.imshow(grid.permute(1, 2, 0), cmap=\"gray\")\n        plt.title(\"Muestras Finales Generadas\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nMostrando muestras generadas...\")\n    show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n</pre> # 3pps import matplotlib.pyplot as plt import torch import torchvision from torch import nn, optim from torch.utils.data import DataLoader from torchvision import datasets, transforms from tqdm import tqdm   def show_generated_samples(     generator: nn.Module, noise, device: str, num_samples: int = 16 ) -&gt; None:     \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"     generator.eval()     with torch.no_grad():         samples = generator(noise[:num_samples]).cpu()         samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]          fig, axes = plt.subplots(4, 4, figsize=(8, 8))         for i in range(num_samples):             row, col = i // 4, i % 4             axes[row, col].imshow(samples[i, 0], cmap=\"gray\")             axes[row, col].axis(\"off\")         plt.tight_layout()         plt.show()   class Discriminator(nn.Module):     def __init__(         self, in_channels: int, hidden_size: int = 64, dropout_rate: float = 0.2     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.in_channels = in_channels         self.hidden_size = hidden_size         self.dropout_rate = dropout_rate          # Creamos el modelo que lo ajustaremos para MNIST         self.model = nn.Sequential(             # Input MNIST = (B, C, H, W) = (B, 1, 28, 28) -&gt; (B, H, 14, 14)             nn.Conv2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size // 2,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size // 2),             nn.GELU(),             nn.Dropout2d(p=self.dropout_rate),             # (B, H, 14, 14) -&gt; (B, H, 7, 7)             nn.Conv2d(                 in_channels=self.hidden_size // 2,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             nn.Dropout2d(p=self.dropout_rate),             # (B, H, 7, 7) -&gt; (B, 1, 7, 7)             nn.Conv2d(                 in_channels=self.hidden_size,                 out_channels=1,                 kernel_size=1,                 stride=1,                 bias=False,             ),             # (B, 1, 7, 7) -&gt; (B, 1, 1, 1)             nn.AdaptiveAvgPool2d((1, 1)),             # (B, 1, 1, 1) -&gt; (B, 1),             nn.Flatten(),             nn.Dropout(p=self.dropout_rate),             # El discriminador ha de devolver un escalar entre 0 y 1 (falso/real)             nn.Sigmoid(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         return self.model(input_tensor)   class Generator(nn.Module):     def __init__(         self, z_dim: int, data_shape: tuple[int, int, int], hidden_size: int     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.z_dim = z_dim         self.data_shape = data_shape         self.hidden_size = hidden_size          # Del ruido que es un tensor plano, vamos a crear una matriz inicial         # de (B, H, 7, 7) que es el tama\u00f1o antes de aplanar en el Discriminador         self.projection = nn.Sequential(             # (B, z_dim) -&gt; (B, H * 7 * 7)             nn.Linear(                 in_features=self.z_dim,                 out_features=self.hidden_size * 7 * 7,                 bias=False,             ),         )          self.model = nn.Sequential(             # (B, H, 7, 7) -&gt; (B, H, 14, 14)             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             # (B, H, 14, 14) -&gt; (B, H, 28, 28)             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.hidden_size,                 kernel_size=4,                 stride=2,                 padding=1,                 bias=False,             ),             nn.BatchNorm2d(num_features=self.hidden_size),             nn.GELU(),             # (B, H, 28, 28) -&gt; (B, 1, 28, 28)             nn.Conv2d(                 in_channels=self.hidden_size, out_channels=1, kernel_size=1, stride=1             ),             # Normalizamos los valores entre -1 y 1             nn.Tanh(),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         # (B, z_dim) -&gt; (B, H * 7 * 7)         projection = self.projection(input_tensor)         # (B, H * 7 * 7) -&gt; (B, H, 7, 7)         projection = projection.view(projection.size(0), self.hidden_size, 7, 7)          return self.model(projection)   class GAN(nn.Module):     def __init__(         self,         in_channels: int,         hidden_size_discriminator: int,         dropout_rate: float,         z_dim: int,         data_shape: tuple[int, int, int],         hidden_size_generator: int,     ) -&gt; None:         # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros         self.in_channels = in_channels         self.hidden_size_discriminator = hidden_size_discriminator         self.dropout_rate = dropout_rate         self.z_dim = z_dim         self.data_shape = data_shape         self.hidden_size_generator = hidden_size_generator          # Definimos los modelos         self.discriminator = Discriminator(             in_channels=self.in_channels,             hidden_size=self.hidden_size_discriminator,             dropout_rate=self.dropout_rate,         )         self.generator = Generator(             z_dim=self.z_dim,             data_shape=self.data_shape,             hidden_size=self.hidden_size_generator,         )      def forward(self, real_data: torch.Tensor, batch_size: int | None = None) -&gt; dict:         if batch_size is None:             batch_size = real_data.size(0)          # Generar ruido aleatorio         noise = torch.randn(batch_size, self.z_dim, device=real_data.device)          # Generar im\u00e1genes falsas         fake_data = self.generator(noise)          # Pasar datos reales por el discriminador         real_predictions = self.discriminator(real_data)          # Pasar datos falsos por el discriminador         fake_predictions = self.discriminator(fake_data)          return {             \"real_predictions\": real_predictions,             \"fake_predictions\": fake_predictions,             \"fake_data\": fake_data,             \"noise\": noise,         }      def generate_samples_inference(self, num_samples: int, device: str) -&gt; torch.Tensor:         self.eval()         with torch.no_grad():             # (Num_samples, z_dim)             noise = torch.randn(num_samples, self.z_dim, device=device)             # (Num_samples, z_dim) -&gt; MNIST: (Num_samples, 1, 28, 28)             samples = self.generator(input_tensor=noise)          return samples      def discriminator_loss(         self, real_predictions: torch.Tensor, fake_predictions: torch.Tensor     ) -&gt; torch.Tensor:         criterion = nn.BCELoss()         # Matriz de 1s         real_labels = torch.ones_like(real_predictions)         # Matriz de 0s         fake_labels = torch.zeros_like(fake_predictions)          real_loss = criterion(real_predictions, real_labels)         fake_loss = criterion(fake_predictions, fake_labels)          return (real_loss + fake_loss) / 2      def generator_loss(self, fake_predictions: torch.Tensor) -&gt; torch.Tensor:         criterion = nn.BCELoss()         fake_real_labels = torch.ones_like(fake_predictions)         return criterion(fake_predictions, fake_real_labels)   if __name__ == \"__main__\":     # Seleccionamos el dispositivo actual     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     print(f\"Usando dispositivo: {device}\")      # Las GANs son muy sensibles a los hiperpar\u00e1metros     lr = 2e-4     # Dimensi\u00f3n de los datos de entrada (C, H, W)     data_dimension = (1, 28, 28)     # Esta es la dimension del ruido     z_dim = 100     batch_size = 128     num_epochs = 50      # Definimos el modelo     model = GAN(         in_channels=data_dimension[0],         hidden_size_discriminator=64,         dropout_rate=0.2,         z_dim=z_dim,         data_shape=data_dimension,         hidden_size_generator=256,     ).to(device)      # Transformaciones que vamos a aplicar a MNIST     transform = transforms.Compose(         [             transforms.ToTensor(),             transforms.Normalize(                 # Normalizar a [-1, 1] para coincidir con Tanh                 (0.5,),                 (0.5,),             ),         ]     )      # Descargamos MNIST     dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)     loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)      # Vamos a utilizar un optimizador para cada modelo     opt_disc = optim.AdamW(params=model.discriminator.parameters(), lr=lr)     opt_gen = optim.AdamW(params=model.generator.parameters(), lr=lr)      # Ruido fijo para generar muestras durante el entrenamiento     fixed_noise = torch.randn(64, z_dim, device=device)      # Listas para guardar las p\u00e9rdidas     disc_losses = []     gen_losses = []      print(\"Iniciando entrenamiento de la GAN...\")      for epoch in range(num_epochs):         epoch_disc_loss: int | float = 0         epoch_gen_loss: int | float = 0          pbar = tqdm(loader, desc=f\"\u00c9poca {epoch + 1}/{num_epochs}\")          for _, (real, _) in enumerate(pbar):             real = real.to(device)             batch_size_current = real.shape[0]              # ENTRENAR DISCRIMINADOR             opt_disc.zero_grad()              # Generar datos falsos             noise = torch.randn(batch_size_current, z_dim, device=device)             fake_data = model.generator(                 noise             ).detach()  # Detach para no actualizar generador              # Predicciones del discriminador             real_preds = model.discriminator(real)             fake_preds = model.discriminator(fake_data)              # P\u00e9rdida del discriminador             lossD = model.discriminator_loss(real_preds, fake_preds)             lossD.backward()             opt_disc.step()              # ENTRENAR GENERADOR             opt_gen.zero_grad()              # Generar nuevos datos falsos (sin detach)             noise = torch.randn(batch_size_current, z_dim, device=device)             fake_data = model.generator(noise)             fake_preds_for_gen = model.discriminator(fake_data)              # P\u00e9rdida del generador             lossG = model.generator_loss(fake_preds_for_gen)             lossG.backward()             opt_gen.step()              # Acumular p\u00e9rdidas             epoch_disc_loss += lossD.item()             epoch_gen_loss += lossG.item()              # Actualizar barra de progreso             pbar.set_postfix(                 {\"D_loss\": f\"{lossD.item():.4f}\", \"G_loss\": f\"{lossG.item():.4f}\"}             )          # P\u00e9rdidas promedio de la \u00e9poca         avg_disc_loss = epoch_disc_loss / len(loader)         avg_gen_loss = epoch_gen_loss / len(loader)         disc_losses.append(avg_disc_loss)         gen_losses.append(avg_gen_loss)          print(             f\"\u00c9poca {epoch + 1}, \"             f\"P\u00e9rdida D: {avg_disc_loss:.4f}, \"             f\"P\u00e9rdida G: {avg_gen_loss:.4f}\"         )          # Mostrar muestras cada 5 \u00e9pocas         if (epoch + 1) % 5 == 0:             print(f\"\\nMostrando muestras generadas en \u00e9poca {epoch + 1}...\")             show_generated_samples(model.generator, fixed_noise, device, num_samples=16)      print(\"\\n\u00a1Entrenamiento completado!\")      # Mostrar gr\u00e1fica de p\u00e9rdidas     plt.figure(figsize=(12, 5))      plt.subplot(1, 2, 1)     plt.plot(disc_losses, label=\"Discriminador\", color=\"red\")     plt.plot(gen_losses, label=\"Generador\", color=\"blue\")     plt.xlabel(\"\u00c9poca\")     plt.ylabel(\"P\u00e9rdida\")     plt.title(\"Evoluci\u00f3n de las P\u00e9rdidas\")     plt.legend()     plt.grid(True)      # Mostrar muestras generadas finales     plt.subplot(1, 2, 2)     model.generator.eval()     with torch.no_grad():         final_samples = model.generator(fixed_noise[:16]).cpu()         final_samples = (final_samples + 1) / 2  # Desnormalizar         grid = torchvision.utils.make_grid(final_samples, nrow=4, padding=2)         plt.imshow(grid.permute(1, 2, 0), cmap=\"gray\")         plt.title(\"Muestras Finales Generadas\")         plt.axis(\"off\")      plt.tight_layout()     plt.show()      print(\"\\nMostrando muestras generadas...\")     show_generated_samples(model.generator, fixed_noise, device, num_samples=16)"},{"location":"other_topics/topic_01_generative_models/section_01_gans.html#generative-adversarial-networks","title":"Generative Adversarial Networks\u00b6","text":""},{"location":"other_topics/topic_01_generative_models/section_02_diffusion_models.html","title":"Difussion Models","text":""},{"location":"other_topics/topic_01_generative_models/section_02_diffusion_models.html#difussion-models","title":"Difussion Models\u00b6","text":""},{"location":"other_topics/topic_02_time_series_models/section_01_other_techniques.html","title":"Time Series Techniques","text":"<p>Gramian Angular Difference Field (GADF) permite codificar series temporales a im\u00e1genes, permiten realizar una interpretaci\u00f3n en una imagen en 2D de una serie temporal univariable.</p> <p>Coordenadas polares</p> <p>Podemos convertir series temporales, representadas en un eje de coordenadas cartesianas, donde tenemos un valor en el eje y que es el valor de variable en si, en el eje x tenemos el tiempo que transcurre y como varia esa variable en el tiempo.</p> <p></p> <p>Para convertir a coordenadas polares, representado tal que (r, \u03b8), donde:</p> <ul> <li>r (radio) representa la distancia desde el origen.</li> <li>\u03b8 (\u00e1ngulo) representa la direcci\u00f3n del punto respecto a un eje de referencia (como el eje x).</li> </ul> <p>Podemos convertir a coordenadas polares de 2 formas:</p> <ul> <li>La primera ser\u00eda considerar una se\u00f1al peri\u00f3dica, como un seno o coseno, que parta desde 0 hasta 2pi, este es el periodo de la se\u00f1al y luego tendr\u00edamos que el radio es el valor en si. Visualizar ciclos o estacionalidades (en el m\u00e9todo 1).</li> <li>La segunda es medir la diferencia de tiempo que existe entre un instante t y un instante t + 1, y luego se calcula la diferencia del valor de la variable como un delta, y se calcula el angulo entre la delta de tiempo y la delta de la variable. Por tanto ser\u00eda algo como: \u03b8 = arctan(\u0394y / \u0394t), r = sqrt((\u0394t)\u00b2 + (\u0394y)\u00b2) que mide la magnitud del cambio. Detectar patrones direccionales, como si los cambios tienen una orientaci\u00f3n predominante (en el m\u00e9todo 2).</li> </ul> <p></p> <p>En este caso, como la distribuci\u00f3n del tiempo es discreta, que representa los meses, podemos convertirlo a grados haciendo:</p> <ul> <li>2 * math.pi * (t / max(t)) En el caso de tener un formato basado en datetime, con HH:MM:SS, habr\u00eda que normalizar respecto al tiempo:</li> <li>2 * math.pi * (t / 24 horas * 3600 segundos), donde t es el tiempo desde las 00:00:00 en segundos</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\n\n\n# Datos sint\u00e9ticos\n# Tiempo (por ejemplo, meses)\nmonths = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n# Valores sint\u00e9ticos (algo como una onda)\nvalues = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n\n# M\u00e9todo 1 ----------\ntheta_1 = [2 * math.pi * (month / len(months)) for month in months]\nprint(theta_1)\n\nrho_1 = values\nprint(rho_1)\n\ncoord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)]\nprint(coord_method1)\n\n# M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ----------\n# Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos\n# arrays tienen la misma cantidad de elementos\ntheta_2 = []\nrho_2 = []\nfor index, month in enumerate(months):\n    if index + 1 &lt; len(values):\n        delta_values = values[index + 1] - values[index]\n        delta_time = months[index + 1] - month\n\n        # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que\n        # saturar el valor entre 2pi y -2pi\n        if delta_time != 0:\n            theta_2.append(math.atan(delta_values / delta_time))\n        else:\n            theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)\n\n        rho_2.append(math.sqrt(delta_values**2 + delta_time**2))\nprint(\"\\n\")\nprint(theta_2)\nprint(rho_2)\ncoord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)]\nprint(coord_method2)\n</pre> # Standard libraries import math   # Datos sint\u00e9ticos # Tiempo (por ejemplo, meses) months = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Valores sint\u00e9ticos (algo como una onda) values = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]  # M\u00e9todo 1 ---------- theta_1 = [2 * math.pi * (month / len(months)) for month in months] print(theta_1)  rho_1 = values print(rho_1)  coord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)] print(coord_method1)  # M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ---------- # Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos # arrays tienen la misma cantidad de elementos theta_2 = [] rho_2 = [] for index, month in enumerate(months):     if index + 1 &lt; len(values):         delta_values = values[index + 1] - values[index]         delta_time = months[index + 1] - month          # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que         # saturar el valor entre 2pi y -2pi         if delta_time != 0:             theta_2.append(math.atan(delta_values / delta_time))         else:             theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)          rho_2.append(math.sqrt(delta_values**2 + delta_time**2)) print(\"\\n\") print(theta_2) print(rho_2) coord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)] print(coord_method2) In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\n\n\n# M\u00e9todo 1\nplt.subplot(1, 2, 1, projection=\"polar\")\nplt.plot(theta_1, rho_1, marker=\"o\")\nplt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")\n\n# M\u00e9todo 2\nplt.subplot(1, 2, 2, projection=\"polar\")\nplt.plot(theta_2, rho_2, marker=\"o\")\nplt.title(\"M\u00e9todo 2: Cambio como vector\")\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt   # M\u00e9todo 1 plt.subplot(1, 2, 1, projection=\"polar\") plt.plot(theta_1, rho_1, marker=\"o\") plt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")  # M\u00e9todo 2 plt.subplot(1, 2, 2, projection=\"polar\") plt.plot(theta_2, rho_2, marker=\"o\") plt.title(\"M\u00e9todo 2: Cambio como vector\")  plt.tight_layout() plt.show() <p>Continuando con Gramian, tenemos que la matriz Gramian es una matriz que consiste en realizar el producto vectorial entre cada pareja de vectores.</p> <p></p> <p>La matriz de Gram preserva la dependencia temporal, pues el tiempo incrementa del mismo modo que lo hace la posici\u00f3n de la matriz 2D de arriba a la izquierda y de arriba a la derecha, por lo que el tiempo se codifica en la geometr\u00eda de la matriz. Es decir, la matriz mantiene las relaciones angulares entre todos los puntos de la serie. Para ello se siguen los pasos siguientes:</p> <ul> <li><p>Paso 1: Normaliza la serie: Primero necesitas normalizar tu serie a un rango de $[-1, 1]$ (porque luego aplicaremos el arccos):</p> <p>$$   \\tilde{x}_i = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)} \\times 2 - 1   $$</p> </li> <li><p>Paso 2: Convierte los valores a \u00e1ngulos. Para cada valor de la serie normalizada:</p> <p>$$   \\phi_i = \\arccos(\\tilde{x}_i)   $$</p> <p>Esto convierte cada valor en un \u00e1ngulo entre $0$ y $\\pi$, que representa su posici\u00f3n relativa dentro del ciclo.</p> </li> <li><p>Paso 3: Construye la matriz GADF</p> <p>La idea es comparar cada par de puntos $(\\phi_i, \\phi_j)$ de la serie y calcular:</p> <p>$$   \\text{GADF}[i,j] = \\sin(\\phi_i - \\phi_j)   $$</p> <ul> <li>Esto mide la diferencia angular entre dos puntos.</li> <li>El resultado es una matriz cuadrada $N \\times N$ que puedes tratar como una imagen.</li> </ul> </li> </ul> <p>Tambi\u00e9n, existe el Gramian Angular Summation Field (GASF) usa la suma en vez de la diferencia:</p> <p>$$ \\text{GASF}[i,j] = \\cos(\\phi_i + \\phi_j) $$</p> In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\nimport random\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef normalization(samples: list) -&gt; list:\n\n    min_val = min(samples)\n    max_val = max(samples)\n\n    return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]\n\n\n@njit\ndef degrees(samples: list) -&gt; list:\n\n    return [math.acos(sample) for sample in samples]\n\n\n@njit\ndef gadf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.sin(sample_i - sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n@njit\ndef gasf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.cos(sample_i + sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n# Datos temporales (por ejemplo, 16 puntos en el tiempo)\nx = random.sample(range(1, 4000), 2400)\nx_gadf_matrix = gadf_matrix(samples=x)\nx_gasf_matrix = gasf_matrix(samples=x)\n\nprint(x_gadf_matrix.shape)\nprint(x_gasf_matrix.shape)\n\nplt.imshow(x_gadf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n\nplt.imshow(x_gasf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n</pre> # Standard libraries import math import random  # 3pps import matplotlib.pyplot as plt import numpy as np from numba import njit   @njit def normalization(samples: list) -&gt; list:      min_val = min(samples)     max_val = max(samples)      return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]   @njit def degrees(samples: list) -&gt; list:      return [math.acos(sample) for sample in samples]   @njit def gadf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.sin(sample_i - sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   @njit def gasf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.cos(sample_i + sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   # Datos temporales (por ejemplo, 16 puntos en el tiempo) x = random.sample(range(1, 4000), 2400) x_gadf_matrix = gadf_matrix(samples=x) x_gasf_matrix = gasf_matrix(samples=x)  print(x_gadf_matrix.shape) print(x_gasf_matrix.shape)  plt.imshow(x_gadf_matrix, origin=\"upper\") plt.colorbar() plt.show()  plt.imshow(x_gasf_matrix, origin=\"upper\") plt.colorbar() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"other_topics/topic_02_time_series_models/section_01_other_techniques.html#time-series-techniques","title":"Time Series Techniques\u00b6","text":""},{"location":"other_topics/topic_02_time_series_models/section_01_other_techniques.html#gramian-angular-field","title":"Gramian Angular Field\u00b6","text":""},{"location":"other_topics/topic_03_others/section_01_knowledge_distillation.html","title":"Knowledge Distillation","text":"In\u00a0[\u00a0]: Copied!"},{"location":"other_topics/topic_03_others/section_01_knowledge_distillation.html#knowledge-distillation","title":"Knowledge Distillation\u00b6","text":""},{"location":"other_topics/topic_03_others/section_04_shift_invariance_cnns.html","title":"Shift Invariance in Convolutions","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEste clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214\n\"\"\"\n\n# Standard libraries\nfrom typing import Literal\n\n# 3pps\nimport torch\nfrom torch import nn\n\n\nclass AdaptivePolyphaseSampling(nn.Module):\n    def __init__(\n        self,\n        norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the class with normalization option.\n\n        Args:\n            norm: Normalization type or value, defaults to 2.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self._stride = 2\n        self.norm = norm\n\n    def forward(\n        self, input_tensor: torch.Tensor, return_index: bool = False\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Processes input tensor to extract dominant polyphase component.\n\n        Args:\n            input_tensor: Tensor with shape (B, C, H, W).\n            return_index: If True, returns index of dominant component.\n\n        Returns:\n            Output tensor, optionally with index if return_index is True.\n        \"\"\"\n\n        # Tenemos a la entrada un tensor de (B, C, H, W)\n        # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n        # de paso elevado al cuadrado, porque nos vemos tanto en la\n        # altura como en la anchura , en total 4\n        poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n        poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n        poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n        poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n        # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n        polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n        # Extraemos las dimensiones\n        b, p, _, _, _ = polyphase_combined.size()\n\n        # Combinamos los valores de los canales, altura y anchura del tensor\n        polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))\n\n        # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n        polyphase_norms = torch.linalg.vector_norm(\n            input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)\n        )\n\n        # Seleccionamos el componente polif\u00e1sico de mayor orden\n        polyphase_max_norm = torch.argmax(polyphase_norms)\n\n        # Obtenemos el componente polif\u00e1sico de mayor orden\n        output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n        # En el paper existe la opci\u00f3n de devolver el \u00edndice\n        if return_index:\n            return output_tensor, polyphase_max_norm\n\n        # En caso contrario solo devolvemos el tensor\n        return output_tensor\n\n\nif __name__ == \"__main__\":\n    model = AdaptivePolyphaseSampling()\n\n    x = torch.randn(1, 3, 4, 4)\n    output_model = model(x)\n\n    print(output_model)\n</pre> \"\"\" Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214 \"\"\"  # Standard libraries from typing import Literal  # 3pps import torch from torch import nn   class AdaptivePolyphaseSampling(nn.Module):     def __init__(         self,         norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,     ) -&gt; None:         \"\"\"         Initializes the class with normalization option.          Args:             norm: Normalization type or value, defaults to 2.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self._stride = 2         self.norm = norm      def forward(         self, input_tensor: torch.Tensor, return_index: bool = False     ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:         \"\"\"         Processes input tensor to extract dominant polyphase component.          Args:             input_tensor: Tensor with shape (B, C, H, W).             return_index: If True, returns index of dominant component.          Returns:             Output tensor, optionally with index if return_index is True.         \"\"\"          # Tenemos a la entrada un tensor de (B, C, H, W)         # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o         # de paso elevado al cuadrado, porque nos vemos tanto en la         # altura como en la anchura , en total 4         poly_a = input_tensor[:, :, :: self._stride, :: self._stride]         poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]         poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]         poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]          # Combinamos las componentes en un solo tensor (B, P, C, H, W)         polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)          # Extraemos las dimensiones         b, p, _, _, _ = polyphase_combined.size()          # Combinamos los valores de los canales, altura y anchura del tensor         polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))          # Aplicamos la norma a la \u00faltima dimensi\u00f3n         polyphase_norms = torch.linalg.vector_norm(             input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)         )          # Seleccionamos el componente polif\u00e1sico de mayor orden         polyphase_max_norm = torch.argmax(polyphase_norms)          # Obtenemos el componente polif\u00e1sico de mayor orden         output_tensor = polyphase_combined[:, polyphase_max_norm, ...]          # En el paper existe la opci\u00f3n de devolver el \u00edndice         if return_index:             return output_tensor, polyphase_max_norm          # En caso contrario solo devolvemos el tensor         return output_tensor   if __name__ == \"__main__\":     model = AdaptivePolyphaseSampling()      x = torch.randn(1, 3, 4, 4)     output_model = model(x)      print(output_model) In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEste clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001\n\"\"\"\n\n# 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass LearnablePolyphaseSampling(nn.Module):\n    def __init__(self, channel_size: int, hidden_size: int) -&gt; None:\n        \"\"\"\n        Initializes the model with specified channel and hidden sizes.\n\n        Args:\n            channel_size: Number of input channels for the Conv2D layer.\n            hidden_size: Number of hidden units for the Conv2D layer.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self._stride = 2\n\n        # Definimos el modelo \u00fanico para cada componente\n        self.conv_model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channel_size,\n                out_channels=hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=hidden_size,\n                out_channels=hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.Flatten(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor, return_index: bool = False\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Processes input to extract dominant polyphase component.\n\n        Args:\n            input_tensor: Tensor with shape (B, C, H, W).\n            return_index: If True, returns index of dominant component.\n\n        Returns:\n            Tensor of dominant component, optionally with index.\n        \"\"\"\n\n        # Tenemos a la entrada un tensor de (B, C, H, W)\n        # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n        # de paso elevado al cuadrado, porque nos vemos tanto en la\n        # altura como en la anchura , en total 4\n        poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n        poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n        poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n        poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n        # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n        polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n        # Utilizamos el modelo basado en convoluciones por cada componente\n        _logits = []\n        for polyphase in range(polyphase_combined.size()[1]):\n            _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))\n        logits = torch.squeeze(torch.stack(_logits))\n\n        # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n        polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)\n\n        # Seleccionamos el componente polif\u00e1sico de mayor orden\n        polyphase_max_norm = torch.argmax(polyphase_norms)\n\n        # Obtenemos el componente polif\u00e1sico de mayor orden\n        output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n        # En el paper existe la opci\u00f3n de devolver el \u00edndice\n        if return_index:\n            return output_tensor, polyphase_max_norm\n\n        # En caso contrario solo devolvemos el tensor\n        return output_tensor\n\n\nif __name__ == \"__main__\":\n    model = LearnablePolyphaseSampling(channel_size=3, hidden_size=64)\n\n    x = torch.randn(1, 3, 4, 4)\n    output_model = model(x)\n\n    print(output_model)\n</pre> \"\"\" Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001 \"\"\"  # 3pps import torch from torch import nn from torch.nn import functional as F   class LearnablePolyphaseSampling(nn.Module):     def __init__(self, channel_size: int, hidden_size: int) -&gt; None:         \"\"\"         Initializes the model with specified channel and hidden sizes.          Args:             channel_size: Number of input channels for the Conv2D layer.             hidden_size: Number of hidden units for the Conv2D layer.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self._stride = 2          # Definimos el modelo \u00fanico para cada componente         self.conv_model = nn.Sequential(             nn.Conv2d(                 in_channels=channel_size,                 out_channels=hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,             ),             nn.ReLU(),             nn.Conv2d(                 in_channels=hidden_size,                 out_channels=hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,             ),             nn.Flatten(),             nn.AdaptiveAvgPool2d(1),         )      def forward(         self, input_tensor: torch.Tensor, return_index: bool = False     ) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:         \"\"\"         Processes input to extract dominant polyphase component.          Args:             input_tensor: Tensor with shape (B, C, H, W).             return_index: If True, returns index of dominant component.          Returns:             Tensor of dominant component, optionally with index.         \"\"\"          # Tenemos a la entrada un tensor de (B, C, H, W)         # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o         # de paso elevado al cuadrado, porque nos vemos tanto en la         # altura como en la anchura , en total 4         poly_a = input_tensor[:, :, :: self._stride, :: self._stride]         poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]         poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]         poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]          # Combinamos las componentes en un solo tensor (B, P, C, H, W)         polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)          # Utilizamos el modelo basado en convoluciones por cada componente         _logits = []         for polyphase in range(polyphase_combined.size()[1]):             _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))         logits = torch.squeeze(torch.stack(_logits))          # Aplicamos la norma a la \u00faltima dimensi\u00f3n         polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)          # Seleccionamos el componente polif\u00e1sico de mayor orden         polyphase_max_norm = torch.argmax(polyphase_norms)          # Obtenemos el componente polif\u00e1sico de mayor orden         output_tensor = polyphase_combined[:, polyphase_max_norm, ...]          # En el paper existe la opci\u00f3n de devolver el \u00edndice         if return_index:             return output_tensor, polyphase_max_norm          # En caso contrario solo devolvemos el tensor         return output_tensor   if __name__ == \"__main__\":     model = LearnablePolyphaseSampling(channel_size=3, hidden_size=64)      x = torch.randn(1, 3, 4, 4)     output_model = model(x)      print(output_model)"},{"location":"other_topics/topic_03_others/section_04_shift_invariance_cnns.html#shift-invariance-in-convolutions","title":"Shift Invariance in Convolutions\u00b6","text":""},{"location":"other_topics/topic_03_others/section_04_shift_invariance_cnns.html#aps","title":"APS\u00b6","text":""},{"location":"other_topics/topic_03_others/section_04_shift_invariance_cnns.html#lps","title":"LPS\u00b6","text":""},{"location":"other_topics/topic_03_others/section_05_scale_invariance_cnns.html","title":"Scale Invariance in Convolutions","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n\nclass RieszLayer(Layer):\n    def __init__(self, output_channels, **kwargs):\n        super().__init__(**kwargs)\n        self.output_channels = output_channels\n\n    def build(self, input_shape):\n        _, _, _, input_channels = input_shape\n        self.riesz_weights = self.add_weight(\n            shape=(5 * input_channels, self.output_channels),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"riesz_weights\",\n        )\n        super().build(input_shape)\n\n    @tf.function\n    def riesz_transform(self, input_image, H, W):\n        # Recompute frequency grids based on current input dimensions\n        n1 = tf.cast(\n            tf.signal.fftshift(tf.linspace(-H // 2, H // 2 - 1, H)), tf.float32\n        )\n        n2 = tf.cast(\n            tf.signal.fftshift(tf.linspace(-W // 2, W // 2 - 1, W)), tf.float32\n        )\n\n        n1 = tf.reshape(n1, (-1, 1))  # Column vector\n        n2 = tf.reshape(n2, (1, -1))  # Row vector\n        norm = tf.sqrt(n1**2 + n2**2 + 1e-8)\n\n        real_part_R1 = n1 / norm\n        imag_part_R1 = -tf.sqrt(1 - real_part_R1**2)\n        real_part_R2 = n2 / norm\n        imag_part_R2 = -tf.sqrt(1 - real_part_R2**2)\n\n        # Fourier transform of the input\n        I_hat = tf.signal.fft2d(tf.cast(input_image, tf.complex64))\n\n        # First-order Riesz transforms\n        I1 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R1, imag_part_R1))\n        )\n        I2 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R2, imag_part_R2))\n        )\n\n        # Second-order Riesz transforms\n        I_20 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R1**2, imag_part_R1**2))\n        )\n        I_02 = tf.math.real(\n            tf.signal.ifft2d(I_hat * tf.complex(real_part_R2**2, imag_part_R2**2))\n        )\n        I_11 = tf.math.real(\n            tf.signal.ifft2d(\n                I_hat\n                * tf.complex(real_part_R1 * real_part_R2, imag_part_R1 * imag_part_R2)\n            )\n        )\n\n        return tf.stack([I1, I2, I_20, I_11, I_02], axis=-1)\n\n    def call(self, inputs):\n        batch_size, H, W, input_channels = (\n            tf.shape(inputs)[0],\n            tf.shape(inputs)[1],\n            tf.shape(inputs)[2],\n            tf.shape(inputs)[3],\n        )\n\n        # Reshape for channel-wise processing\n        inputs_reshaped = tf.reshape(inputs, (-1, H, W))  # Combine batch and channels\n\n        # Apply Riesz transform to each input slice using vectorization\n        riesz_transformed = tf.map_fn(\n            lambda x: self.riesz_transform(x, H, W),\n            inputs_reshaped,\n            fn_output_signature=tf.float32,\n        )\n\n        # Reshape back to original batch format with Riesz feature dimension\n        riesz_features = tf.reshape(\n            riesz_transformed, (batch_size, H, W, 5 * input_channels)\n        )\n\n        # Linear combination using weights\n        riesz_features_flat = tf.reshape(\n            riesz_features, (batch_size * H * W, 5 * input_channels)\n        )\n        combined_features_flat = tf.matmul(riesz_features_flat, self.riesz_weights)\n        combined_features = tf.reshape(\n            combined_features_flat, (batch_size, H, W, self.output_channels)\n        )\n\n        return combined_features\n\n\n# Verify functionality on a sample input with variable sizes\nsample_input_small = np.random.rand(1, 14, 14, 1).astype(np.float32)\nsample_input_large = np.random.rand(1, 56, 56, 1).astype(np.float32)\n\nprint(\"Small input shape:\", sample_input_small.shape)\ntransformed_output_small = RieszLayer(output_channels=5)(sample_input_small)\nprint(\"Transformed output shape (small input):\", transformed_output_small.shape)\n\nprint(\"Large input shape:\", sample_input_large.shape)\ntransformed_output_large = RieszLayer(output_channels=5)(sample_input_large)\nprint(\"Transformed output shape (large input):\", transformed_output_large.shape)\n\n\n# Helper function to resize the dataset\ndef resize_dataset(images, target_size):\n    resized_images = np.array(\n        [cv2.resize(img, target_size, interpolation=cv2.INTER_AREA) for img in images]\n    )\n    return np.expand_dims(resized_images, axis=-1)\n\n\n# Load and preprocess MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n\n# Add channel dimension (grayscale)\nx_train = np.expand_dims(x_train, axis=-1)\nx_test = np.expand_dims(x_test, axis=-1)\n\n# Resize images for evaluation at different scales\nx_test_small = resize_dataset(x_test[..., 0], (14, 14))\nx_test_large = resize_dataset(x_test[..., 0], (56, 56))\n\n\n# Define the neural network model using the functional API\ndef create_riesz_cnn(input_shape=(None, None, 1)):\n    input_layer = tf.keras.Input(shape=input_shape)\n\n    x = RieszLayer(16, name=\"riesz_1\")(input_layer)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(32, name=\"riesz_2\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(40, name=\"riesz_3\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = RieszLayer(48, name=\"riesz_4\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    output = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    return tf.keras.Model(inputs=input_layer, outputs=output)\n\n\n# Create the Riesz-based CNN model\nmodel = create_riesz_cnn(input_shape=(None, None, 1))\n\n# Compile the model with AdamW optimizer\nmodel.compile(\n    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()  # Corrected typo from 'suummary' to 'summary'\n\n# Train the model with the learning rate scheduler\nhistory = model.fit(\n    x_train,\n    y_train,\n    shuffle=True,\n    validation_split=0.2,\n    batch_size=64,\n    epochs=10,\n)\n\n# Evaluate the model on different scales\noriginal_acc = model.evaluate(x_test, y_test, verbose=0)\nsmall_acc = model.evaluate(x_test_small, y_test, verbose=0)\nlarge_acc = model.evaluate(x_test_large, y_test, verbose=0)\n\n# Print results\nprint(f\"Accuracy on original scale (28x28): {original_acc[1]:.4f}\")\nprint(f\"Accuracy on smaller scale (14x14): {small_acc[1]:.4f}\")\nprint(f\"Accuracy on larger scale (56x56): {large_acc[1]:.4f}\")\n</pre> # 3pps import cv2 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Layer   class RieszLayer(Layer):     def __init__(self, output_channels, **kwargs):         super().__init__(**kwargs)         self.output_channels = output_channels      def build(self, input_shape):         _, _, _, input_channels = input_shape         self.riesz_weights = self.add_weight(             shape=(5 * input_channels, self.output_channels),             initializer=\"glorot_uniform\",             trainable=True,             name=\"riesz_weights\",         )         super().build(input_shape)      @tf.function     def riesz_transform(self, input_image, H, W):         # Recompute frequency grids based on current input dimensions         n1 = tf.cast(             tf.signal.fftshift(tf.linspace(-H // 2, H // 2 - 1, H)), tf.float32         )         n2 = tf.cast(             tf.signal.fftshift(tf.linspace(-W // 2, W // 2 - 1, W)), tf.float32         )          n1 = tf.reshape(n1, (-1, 1))  # Column vector         n2 = tf.reshape(n2, (1, -1))  # Row vector         norm = tf.sqrt(n1**2 + n2**2 + 1e-8)          real_part_R1 = n1 / norm         imag_part_R1 = -tf.sqrt(1 - real_part_R1**2)         real_part_R2 = n2 / norm         imag_part_R2 = -tf.sqrt(1 - real_part_R2**2)          # Fourier transform of the input         I_hat = tf.signal.fft2d(tf.cast(input_image, tf.complex64))          # First-order Riesz transforms         I1 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R1, imag_part_R1))         )         I2 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R2, imag_part_R2))         )          # Second-order Riesz transforms         I_20 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R1**2, imag_part_R1**2))         )         I_02 = tf.math.real(             tf.signal.ifft2d(I_hat * tf.complex(real_part_R2**2, imag_part_R2**2))         )         I_11 = tf.math.real(             tf.signal.ifft2d(                 I_hat                 * tf.complex(real_part_R1 * real_part_R2, imag_part_R1 * imag_part_R2)             )         )          return tf.stack([I1, I2, I_20, I_11, I_02], axis=-1)      def call(self, inputs):         batch_size, H, W, input_channels = (             tf.shape(inputs)[0],             tf.shape(inputs)[1],             tf.shape(inputs)[2],             tf.shape(inputs)[3],         )          # Reshape for channel-wise processing         inputs_reshaped = tf.reshape(inputs, (-1, H, W))  # Combine batch and channels          # Apply Riesz transform to each input slice using vectorization         riesz_transformed = tf.map_fn(             lambda x: self.riesz_transform(x, H, W),             inputs_reshaped,             fn_output_signature=tf.float32,         )          # Reshape back to original batch format with Riesz feature dimension         riesz_features = tf.reshape(             riesz_transformed, (batch_size, H, W, 5 * input_channels)         )          # Linear combination using weights         riesz_features_flat = tf.reshape(             riesz_features, (batch_size * H * W, 5 * input_channels)         )         combined_features_flat = tf.matmul(riesz_features_flat, self.riesz_weights)         combined_features = tf.reshape(             combined_features_flat, (batch_size, H, W, self.output_channels)         )          return combined_features   # Verify functionality on a sample input with variable sizes sample_input_small = np.random.rand(1, 14, 14, 1).astype(np.float32) sample_input_large = np.random.rand(1, 56, 56, 1).astype(np.float32)  print(\"Small input shape:\", sample_input_small.shape) transformed_output_small = RieszLayer(output_channels=5)(sample_input_small) print(\"Transformed output shape (small input):\", transformed_output_small.shape)  print(\"Large input shape:\", sample_input_large.shape) transformed_output_large = RieszLayer(output_channels=5)(sample_input_large) print(\"Transformed output shape (large input):\", transformed_output_large.shape)   # Helper function to resize the dataset def resize_dataset(images, target_size):     resized_images = np.array(         [cv2.resize(img, target_size, interpolation=cv2.INTER_AREA) for img in images]     )     return np.expand_dims(resized_images, axis=-1)   # Load and preprocess MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]  # Add channel dimension (grayscale) x_train = np.expand_dims(x_train, axis=-1) x_test = np.expand_dims(x_test, axis=-1)  # Resize images for evaluation at different scales x_test_small = resize_dataset(x_test[..., 0], (14, 14)) x_test_large = resize_dataset(x_test[..., 0], (56, 56))   # Define the neural network model using the functional API def create_riesz_cnn(input_shape=(None, None, 1)):     input_layer = tf.keras.Input(shape=input_shape)      x = RieszLayer(16, name=\"riesz_1\")(input_layer)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(32, name=\"riesz_2\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(40, name=\"riesz_3\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = RieszLayer(48, name=\"riesz_4\")(x)     x = tf.keras.layers.BatchNormalization()(x)     x = tf.keras.layers.Activation(\"relu\")(x)      x = tf.keras.layers.GlobalAveragePooling2D()(x)     x = tf.keras.layers.Dense(128, activation=\"relu\")(x)     output = tf.keras.layers.Dense(10, activation=\"softmax\")(x)      return tf.keras.Model(inputs=input_layer, outputs=output)   # Create the Riesz-based CNN model model = create_riesz_cnn(input_shape=(None, None, 1))  # Compile the model with AdamW optimizer model.compile(     optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3),     loss=\"sparse_categorical_crossentropy\",     metrics=[\"accuracy\"], ) model.summary()  # Corrected typo from 'suummary' to 'summary'  # Train the model with the learning rate scheduler history = model.fit(     x_train,     y_train,     shuffle=True,     validation_split=0.2,     batch_size=64,     epochs=10, )  # Evaluate the model on different scales original_acc = model.evaluate(x_test, y_test, verbose=0) small_acc = model.evaluate(x_test_small, y_test, verbose=0) large_acc = model.evaluate(x_test_large, y_test, verbose=0)  # Print results print(f\"Accuracy on original scale (28x28): {original_acc[1]:.4f}\") print(f\"Accuracy on smaller scale (14x14): {small_acc[1]:.4f}\") print(f\"Accuracy on larger scale (56x56): {large_acc[1]:.4f}\")"},{"location":"other_topics/topic_03_others/section_05_scale_invariance_cnns.html#scale-invariance-in-convolutions","title":"Scale Invariance in Convolutions\u00b6","text":""},{"location":"other_topics/topic_03_others/section_05_scale_invariance_cnns.html#riesz","title":"Riesz\u00b6","text":""},{"location":"other_topics/topic_03_others/section_10_vision_transformers.html","title":"Transformers in Computer Vision","text":"<p>Convolutional Autoencoders and Attention-Based Architectures</p> <p>Convolutional autoencoders are specialized networks designed for self-supervised feature extraction, dimensionality reduction, and data reconstruction. They consist of encoder-decoder structures where the encoder compresses input data into a latent representation, and the decoder reconstructs the original input.</p> <p>Attention-based architectures further enhance CNNs by enabling the network to focus selectively on relevant spatial or channel features. These mechanisms improve model interpretability and performance, particularly in complex tasks requiring contextual understanding.</p> In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Patches(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        img_height: int,\n        img_width: int,\n    ) -&gt; None:\n        \"\"\"\n        Initialize patch extraction module.\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            img_height: Height of the input image.\n            img_width: Width of the input image.\n\n        Raises:\n            ValueError: If img_height not divisible by patch height.\n            ValueError: If img_width not divisible by patch width.\n        \"\"\"\n\n        super().__init__()\n\n        if img_height % patch_size_height != 0:\n            raise ValueError(\n                \"img_height tiene que se divisible entre el patch_size_height\"\n            )\n\n        if img_width % patch_size_width != 0:\n            raise ValueError(\n                \"img_width tiene que se divisible entre el patch_size_width\"\n            )\n\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.unfold = nn.Unfold(\n            kernel_size=(self.patch_size_height, self.patch_size_width),\n            stride=(self.patch_size_height, self.patch_size_width),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Extract patches from input tensor.\n\n        Args:\n            input_tensor: Batch of images as a tensor.\n\n        Returns:\n            Tensor with patches from input images.\n        \"\"\"\n\n        # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n        patches = self.unfold(input_tensor)\n        # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n        return patches.transpose(2, 1)\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        in_channels: int,\n        d_model: int,\n    ) -&gt; None:\n        \"\"\"\n        Initialize patch embedding module.\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            in_channels: Number of input channels.\n            d_model: Dimension of the model.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.in_channels = in_channels\n        self.d_model = d_model\n\n        # Esta es una de las diferencias con usar transformers en el texto\n        # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n\n        # de los pixeles\n        self.embedding = nn.Linear(\n            in_features=self.in_channels\n            * self.patch_size_height\n            * self.patch_size_width,\n            out_features=self.d_model,\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Apply linear projection to input tensor.\n\n        Args:\n            input_tensor: Batch of image patches as a tensor.\n\n        Returns:\n            Tensor after linear projection of patches.\n        \"\"\"\n\n        return self.embedding(input_tensor)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize positional encoding module.\n\n        Args:\n            d_model: Dimension of the model.\n            sequence_length: Max length of input sequences.\n            dropout_rate: Dropout rate applied on outputs.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.d_model = d_model\n\n        # Cuando le damos una secuencia de tokens, tenemos que saber\n        # la longitud m\u00e1xima de la secuencia\n        self.sequence_length = sequence_length\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Creamos una matriz del positional embedding\n        # (sequence_length, d_model)\n        pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n        # Crear vector de posiciones\n        position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n        # Crear vector de divisores\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n\n        # Aplicar sin y cos\n        pe_matrix[:, 0::2] = torch.sin(position * div_term)\n        pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n        # Tenemos que convertirlo a (1, sequence_length, d_model) para\n        # procesarlo por lotes\n        pe_matrix = pe_matrix.unsqueeze(0)\n\n        # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n        self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Add positional encoding to input embeddings.\n\n        Args:\n            input_embedding: Batch of input embeddings.\n\n        Returns:\n            Embeddings with added positional encoding.\n        \"\"\"\n\n        # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n        # Seleccionamos\n        x = input_embedding + (\n            self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n        ).requires_grad_(False)\n        return self.dropout(x)\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n        \"\"\"\n        Initialize layer normalization module.\n\n        Args:\n            features: Number of features in input.\n            eps: Small value to avoid division by zero.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.features = features\n        self.eps = eps\n\n        # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n        self.alpha = nn.Parameter(torch.ones(self.features))\n        # Utilizamos un factor del sesgo para sumar\n        self.bias = nn.Parameter(torch.zeros(self.features))\n\n    def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Apply layer normalization to input embeddings.\n\n        Args:\n            input_embedding: Batch of input embeddings.\n\n        Returns:\n            Normalized embeddings.\n        \"\"\"\n\n        # (B, sequence_length, d_model)\n        mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n        var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n        return (\n            self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n            + self.bias\n        )\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize feed-forward neural network.\n\n        Args:\n            d_model: Input and output feature dimensions.\n            d_ff: Hidden layer feature dimensions.\n            dropout_rate: Dropout rate applied on layers.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # Definimos los par\u00e1metros de la clase\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        # Creamos el modelo secuencial\n        self.ffn = nn.Sequential(\n            nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through feed-forward layers.\n\n        Args:\n            input_tensor: Batch of input tensors.\n\n        Returns:\n            Output tensor after feed-forward processing.\n        \"\"\"\n\n        # (B, sequence_length, d_model)\n        return self.ffn(input_tensor)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize multi-head attention module.\n\n        Args:\n            d_model: Number of features in input.\n            h: Number of attention heads.\n            dropout_rate: Dropout rate applied on scores.\n        \"\"\"\n\n        # Constructor de la clase\n        super().__init__()\n\n        # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n        # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n        if d_model % h != 0:\n            raise ValueError(\"d_model ha de ser divisible entre h\")\n\n        self.d_model = d_model\n        self.h = h\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Valore establecidos en el paper\n        self.d_k = self.d_model // self.h\n        self.d_v = self.d_model // self.h\n\n        # Par\u00e1metros\n        self.W_K = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_Q = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_V = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n        self.W_OUTPUT_CONCAT = nn.Linear(\n            in_features=self.d_model, out_features=self.d_model, bias=False\n        )\n\n    @staticmethod\n    def attention(\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n        dropout: nn.Dropout | None = None,\n    ):\n        \"\"\"\n        Compute attention scores and output.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Mask tensor, optional.\n            dropout: Dropout layer, optional.\n\n        Returns:\n            Tuple of attention output and scores.\n        \"\"\"\n\n        # Primero realizamos el producto matricial con la transpuesta\n        # q = (Batch, h, seq_len, d_k)\n        # k.T = (Batch, h, d_k, seq_len)\n        # matmul_q_k = (Batch, h, seq_len, seq_len)\n        matmul_q_k = q @ k.transpose(-2, -1)\n\n        # Luego realizamos el escalado\n        d_k = k.shape[-1]\n        matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n        # El enmascarado es para el decoder, relleno de infinitos\n        if mask is not None:\n            matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n        # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n        attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n        # Aplicamos dropout\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n\n        # Multiplicamos por el valor\n        # attention_scores = (Batch, h, seq_len, seq_len)\n        # v = (Batch, h, seq_len, d_k)\n        # Output = (Batch, h, seq_len, d_k)\n        return (attention_scores @ v), attention_scores\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        q: torch.Tensor,\n        v: torch.Tensor,\n        mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensors through multi-head attention.\n\n        Args:\n            k: Key tensor.\n            q: Query tensor.\n            v: Value tensor.\n            mask: Mask tensor, optional.\n\n        Returns:\n            Output tensor after attention processing.\n        \"\"\"\n\n        # k -&gt; (Batch, seq_len, d_model) igual para el resto\n        key_prima = self.W_K(k)\n        query_prima = self.W_Q(q)\n        value_prima = self.W_V(v)\n\n        # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n        # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n        # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n        key_prima = key_prima.view(\n            key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        query_prima = query_prima.view(\n            query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n        value_prima = value_prima.view(\n            value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n        ).transpose(1, 2)\n\n        # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n        # attention = (Batch, h, seq_len, d_k)\n        # attention_scores = (Batch, h, seq_len, seq_len)\n        attention, attention_scores = MultiHeadAttention.attention(\n            k=key_prima,\n            q=query_prima,\n            v=value_prima,\n            mask=mask,\n            dropout=self.dropout,\n        )\n\n        # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n        # Queremos (Batch, seq_len, d_model)\n        # self.d_k = self.d_model // self.h; d_model = d_k * h\n        attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n        b, seq_len, h, d_k = attention.size()\n        # Al parecer, contiguous permite evitar errores de memoria\n        attention_concat = attention.contiguous().view(\n            b, seq_len, h * d_k\n        )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n        return self.W_OUTPUT_CONCAT(attention_concat)\n\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, features: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize residual connection module.\n\n        Args:\n            features: Number of features in input.\n            dropout_rate: Dropout rate for sublayer output.\n        \"\"\"\n\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNormalization(features=features)\n\n    def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n        \"\"\"\n        Apply residual connection to sublayer output.\n\n        Args:\n            input_tensor: Original input tensor.\n            sublayer: Sublayer module to apply.\n\n        Returns:\n            Tensor with residual connection applied.\n        \"\"\"\n\n        return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n        \"\"\"\n        Initialize encoder block module.\n\n        Args:\n            d_model: Number of features in input.\n            d_ff: Hidden layer feature dimensions.\n            h: Number of attention heads.\n            dropout_rate: Dropout rate for layers.\n        \"\"\"\n\n        super().__init__()\n\n        # Parametros\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout_rate = dropout_rate\n\n        # Definicion de las capas\n        self.multi_head_attention_layer = MultiHeadAttention(\n            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_1 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n        self.feed_forward_layer = FeedForward(\n            d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n        )\n        self.residual_layer_2 = ResidualConnection(\n            features=d_model, dropout_rate=self.dropout_rate\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through encoder block.\n\n        Args:\n            input_tensor: Batch of input tensors.\n            mask: Mask tensor, optional.\n\n        Returns:\n            Output tensor after encoder block processing.\n        \"\"\"\n\n        # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n        input_tensor = self.residual_layer_1(\n            input_tensor,\n            lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n        )\n\n        # Segunda conexi\u00f3n residual con feed-forward\n        input_tensor = self.residual_layer_2(\n            input_tensor, lambda x: self.feed_forward_layer(x)\n        )\n\n        return input_tensor\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        patch_size_height: int,\n        patch_size_width: int,\n        img_height: int,\n        img_width: int,\n        in_channels: int,\n        num_encoders: int,\n        d_model: int,\n        d_ff: int,\n        h: int,\n        num_classes: int,\n        dropout_rate: float,\n    ) -&gt; None:\n        \"\"\"\n        Initialize Vision Transformer (VIT).\n\n        Args:\n            patch_size_height: Height of each patch.\n            patch_size_width: Width of each patch.\n            img_height: Height of input images.\n            img_width: Width of input images.\n            in_channels: Number of input channels.\n            num_encoders: Number of encoder blocks.\n            d_model: Dimension of the model.\n            d_ff: Dimension of feed-forward layers.\n            h: Number of attention heads.\n            num_classes: Number of output classes.\n            dropout_rate: Dropout rate for layers.\n        \"\"\"\n\n        super().__init__()\n\n        self.patch_size_height = patch_size_height\n        self.patch_size_width = patch_size_width\n        self.img_height = img_height\n        self.img_width = img_width\n        self.in_channels = in_channels\n        self.num_encoders = num_encoders\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n\n        # N\u00famero de patches\n        self.num_patches = (img_height // patch_size_height) * (\n            img_width // patch_size_width\n        )\n\n        # CLS token permite tener una representaci\u00f3n global de todos los inputs\n        # de la imagen (de los diferentes embeddings de cada patch)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n\n        self.patch_layer = Patches(\n            patch_size_height=self.patch_size_height,\n            patch_size_width=self.patch_size_width,\n            img_height=self.img_height,\n            img_width=self.img_width,\n        )\n\n        self.embeddings = PatchEmbedding(\n            patch_size_height=self.patch_size_height,\n            patch_size_width=self.patch_size_width,\n            in_channels=self.in_channels,\n            d_model=self.d_model,\n        )\n\n        # Entiendo que la longitud de la secuencia coincide con el numero de patches\n        # y un embedding m\u00e1s de la clase,\n        self.positional_encoding = PositionalEncoding(\n            d_model=self.d_model,\n            sequence_length=self.num_patches + 1,\n            dropout_rate=self.dropout_rate,\n        )\n\n        # Capas del Encoder\n        self.encoder_layers = nn.ModuleList(\n            [\n                EncoderBlock(\n                    d_model=self.d_model,\n                    d_ff=self.d_ff,\n                    h=self.h,\n                    dropout_rate=self.dropout_rate,\n                )\n                for _ in range(self.num_encoders)\n            ]\n        )\n\n        self.layer_norm = LayerNormalization(features=self.d_model)\n\n        self.mlp_classifier = nn.Sequential(\n            nn.Linear(in_features=self.d_model, out_features=self.d_model),\n            nn.GELU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(in_features=self.d_model, out_features=num_classes),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process input tensor through VIT model.\n\n        Args:\n            input_tensor: Batch of input images.\n\n        Returns:\n            Classification output tensor.\n        \"\"\"\n\n        # Extraemos los patches\n        input_patches = self.patch_layer(input_tensor)\n\n        # Convertimso a embeddings los patches\n        patch_embeddings = self.embeddings(input_patches)\n\n        # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia\n        # (B, 1, d_model)\n        cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n        # (B, num_patches+1, d_model)\n        embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n\n        # A\u00f1adir positional encoding\n        embeddings = self.positional_encoding(embeddings)\n\n        # Encoders del transformer\n        encoder_output = embeddings\n        for encoder_layer in self.encoder_layers:\n            encoder_output = encoder_layer(encoder_output)\n\n        # Usar solo el CLS token para clasificaci\u00f3n\n        encoder_output = self.layer_norm(encoder_output)\n        cls_output = encoder_output[:, 0]\n\n        # Clasificaci\u00f3n final\n        return self.mlp_classifier(cls_output)\n\n\nif __name__ == \"__main__\":\n    model = VisionTransformer(\n        patch_size_height=16,\n        patch_size_width=16,\n        img_height=224,\n        img_width=224,\n        in_channels=3,\n        num_encoders=12,\n        d_model=768,\n        d_ff=3072,\n        h=12,\n        num_classes=1000,\n        dropout_rate=0.1,\n    )\n\n    x = torch.randn(2, 3, 224, 224)\n    output = model(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n</pre> # Standard libraries import math  # 3pps import torch from torch import nn from torch.nn import functional as F   class Patches(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         img_height: int,         img_width: int,     ) -&gt; None:         \"\"\"         Initialize patch extraction module.          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             img_height: Height of the input image.             img_width: Width of the input image.          Raises:             ValueError: If img_height not divisible by patch height.             ValueError: If img_width not divisible by patch width.         \"\"\"          super().__init__()          if img_height % patch_size_height != 0:             raise ValueError(                 \"img_height tiene que se divisible entre el patch_size_height\"             )          if img_width % patch_size_width != 0:             raise ValueError(                 \"img_width tiene que se divisible entre el patch_size_width\"             )          self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.unfold = nn.Unfold(             kernel_size=(self.patch_size_height, self.patch_size_width),             stride=(self.patch_size_height, self.patch_size_width),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Extract patches from input tensor.          Args:             input_tensor: Batch of images as a tensor.          Returns:             Tensor with patches from input images.         \"\"\"          # unfold devuelve (b, c * patch_height * patch_width, num_patches)         patches = self.unfold(input_tensor)         # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)         return patches.transpose(2, 1)   class PatchEmbedding(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         in_channels: int,         d_model: int,     ) -&gt; None:         \"\"\"         Initialize patch embedding module.          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             in_channels: Number of input channels.             d_model: Dimension of the model.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.in_channels = in_channels         self.d_model = d_model          # Esta es una de las diferencias con usar transformers en el texto         # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n         # de los pixeles         self.embedding = nn.Linear(             in_features=self.in_channels             * self.patch_size_height             * self.patch_size_width,             out_features=self.d_model,         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Apply linear projection to input tensor.          Args:             input_tensor: Batch of image patches as a tensor.          Returns:             Tensor after linear projection of patches.         \"\"\"          return self.embedding(input_tensor)   class PositionalEncoding(nn.Module):     def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize positional encoding module.          Args:             d_model: Dimension of the model.             sequence_length: Max length of input sequences.             dropout_rate: Dropout rate applied on outputs.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.d_model = d_model          # Cuando le damos una secuencia de tokens, tenemos que saber         # la longitud m\u00e1xima de la secuencia         self.sequence_length = sequence_length         self.dropout = nn.Dropout(dropout_rate)          # Creamos una matriz del positional embedding         # (sequence_length, d_model)         pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))          # Crear vector de posiciones         position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)          # Crear vector de divisores         div_term = torch.exp(             torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)         )          # Aplicar sin y cos         pe_matrix[:, 0::2] = torch.sin(position * div_term)         pe_matrix[:, 1::2] = torch.cos(position * div_term)          # Tenemos que convertirlo a (1, sequence_length, d_model) para         # procesarlo por lotes         pe_matrix = pe_matrix.unsqueeze(0)          # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo         self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Add positional encoding to input embeddings.          Args:             input_embedding: Batch of input embeddings.          Returns:             Embeddings with added positional encoding.         \"\"\"          # (B, ..., d_model) -&gt; (B, sequence_length, d_model)         # Seleccionamos         x = input_embedding + (             self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore         ).requires_grad_(False)         return self.dropout(x)   class LayerNormalization(nn.Module):     def __init__(self, features: int, eps: float = 1e-6) -&gt; None:         \"\"\"         Initialize layer normalization module.          Args:             features: Number of features in input.             eps: Small value to avoid division by zero.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.features = features         self.eps = eps          # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n         self.alpha = nn.Parameter(torch.ones(self.features))         # Utilizamos un factor del sesgo para sumar         self.bias = nn.Parameter(torch.zeros(self.features))      def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Apply layer normalization to input embeddings.          Args:             input_embedding: Batch of input embeddings.          Returns:             Normalized embeddings.         \"\"\"          # (B, sequence_length, d_model)         mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)         var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)         return (             self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))             + self.bias         )   class FeedForward(nn.Module):     def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize feed-forward neural network.          Args:             d_model: Input and output feature dimensions.             d_ff: Hidden layer feature dimensions.             dropout_rate: Dropout rate applied on layers.         \"\"\"          # Constructor de la clase         super().__init__()          # Definimos los par\u00e1metros de la clase         self.d_model = d_model         self.d_ff = d_ff          # Creamos el modelo secuencial         self.ffn = nn.Sequential(             nn.Linear(in_features=self.d_model, out_features=self.d_ff),             nn.GELU(),             nn.Dropout(dropout_rate),             nn.Linear(in_features=self.d_ff, out_features=self.d_model),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Process input tensor through feed-forward layers.          Args:             input_tensor: Batch of input tensors.          Returns:             Output tensor after feed-forward processing.         \"\"\"          # (B, sequence_length, d_model)         return self.ffn(input_tensor)   class MultiHeadAttention(nn.Module):     def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize multi-head attention module.          Args:             d_model: Number of features in input.             h: Number of attention heads.             dropout_rate: Dropout rate applied on scores.         \"\"\"          # Constructor de la clase         super().__init__()          # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas         # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0         if d_model % h != 0:             raise ValueError(\"d_model ha de ser divisible entre h\")          self.d_model = d_model         self.h = h         self.dropout = nn.Dropout(dropout_rate)          # Valore establecidos en el paper         self.d_k = self.d_model // self.h         self.d_v = self.d_model // self.h          # Par\u00e1metros         self.W_K = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_Q = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_V = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )         self.W_OUTPUT_CONCAT = nn.Linear(             in_features=self.d_model, out_features=self.d_model, bias=False         )      @staticmethod     def attention(         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,         dropout: nn.Dropout | None = None,     ):         \"\"\"         Compute attention scores and output.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Mask tensor, optional.             dropout: Dropout layer, optional.          Returns:             Tuple of attention output and scores.         \"\"\"          # Primero realizamos el producto matricial con la transpuesta         # q = (Batch, h, seq_len, d_k)         # k.T = (Batch, h, d_k, seq_len)         # matmul_q_k = (Batch, h, seq_len, seq_len)         matmul_q_k = q @ k.transpose(-2, -1)          # Luego realizamos el escalado         d_k = k.shape[-1]         matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)          # El enmascarado es para el decoder, relleno de infinitos         if mask is not None:             matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)          # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n         attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)          # Aplicamos dropout         if dropout is not None:             attention_scores = dropout(attention_scores)          # Multiplicamos por el valor         # attention_scores = (Batch, h, seq_len, seq_len)         # v = (Batch, h, seq_len, d_k)         # Output = (Batch, h, seq_len, d_k)         return (attention_scores @ v), attention_scores      def forward(         self,         k: torch.Tensor,         q: torch.Tensor,         v: torch.Tensor,         mask: torch.Tensor | None = None,     ) -&gt; torch.Tensor:         \"\"\"         Process input tensors through multi-head attention.          Args:             k: Key tensor.             q: Query tensor.             v: Value tensor.             mask: Mask tensor, optional.          Returns:             Output tensor after attention processing.         \"\"\"          # k -&gt; (Batch, seq_len, d_model) igual para el resto         key_prima = self.W_K(k)         query_prima = self.W_Q(q)         value_prima = self.W_V(v)          # Cambiamos las dimensiones y hacemos el split de los embedding para cada head         # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)         # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)         key_prima = key_prima.view(             key_prima.shape[0], key_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         query_prima = query_prima.view(             query_prima.shape[0], query_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)         value_prima = value_prima.view(             value_prima.shape[0], value_prima.shape[1], self.h, self.d_k         ).transpose(1, 2)          # Obtenemos la matriz de atencion y la puntuaci\u00f3n         # attention = (Batch, h, seq_len, d_k)         # attention_scores = (Batch, h, seq_len, seq_len)         attention, attention_scores = MultiHeadAttention.attention(             k=key_prima,             q=query_prima,             v=value_prima,             mask=mask,             dropout=self.dropout,         )          # Tenemos que concatenar la informaci\u00f3n de todas las cabezas         # Queremos (Batch, seq_len, d_model)         # self.d_k = self.d_model // self.h; d_model = d_k * h         attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)         b, seq_len, h, d_k = attention.size()         # Al parecer, contiguous permite evitar errores de memoria         attention_concat = attention.contiguous().view(             b, seq_len, h * d_k         )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)          return self.W_OUTPUT_CONCAT(attention_concat)   class ResidualConnection(nn.Module):     def __init__(self, features: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize residual connection module.          Args:             features: Number of features in input.             dropout_rate: Dropout rate for sublayer output.         \"\"\"          super().__init__()          self.dropout = nn.Dropout(dropout_rate)         self.layer_norm = LayerNormalization(features=features)      def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:         \"\"\"         Apply residual connection to sublayer output.          Args:             input_tensor: Original input tensor.             sublayer: Sublayer module to apply.          Returns:             Tensor with residual connection applied.         \"\"\"          return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))   class EncoderBlock(nn.Module):     def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:         \"\"\"         Initialize encoder block module.          Args:             d_model: Number of features in input.             d_ff: Hidden layer feature dimensions.             h: Number of attention heads.             dropout_rate: Dropout rate for layers.         \"\"\"          super().__init__()          # Parametros         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.dropout_rate = dropout_rate          # Definicion de las capas         self.multi_head_attention_layer = MultiHeadAttention(             d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate         )         self.residual_layer_1 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )         self.feed_forward_layer = FeedForward(             d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate         )         self.residual_layer_2 = ResidualConnection(             features=d_model, dropout_rate=self.dropout_rate         )      def forward(         self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None     ) -&gt; torch.Tensor:         \"\"\"         Process input tensor through encoder block.          Args:             input_tensor: Batch of input tensors.             mask: Mask tensor, optional.          Returns:             Output tensor after encoder block processing.         \"\"\"          # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada         input_tensor = self.residual_layer_1(             input_tensor,             lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),         )          # Segunda conexi\u00f3n residual con feed-forward         input_tensor = self.residual_layer_2(             input_tensor, lambda x: self.feed_forward_layer(x)         )          return input_tensor   class VisionTransformer(nn.Module):     def __init__(         self,         patch_size_height: int,         patch_size_width: int,         img_height: int,         img_width: int,         in_channels: int,         num_encoders: int,         d_model: int,         d_ff: int,         h: int,         num_classes: int,         dropout_rate: float,     ) -&gt; None:         \"\"\"         Initialize Vision Transformer (VIT).          Args:             patch_size_height: Height of each patch.             patch_size_width: Width of each patch.             img_height: Height of input images.             img_width: Width of input images.             in_channels: Number of input channels.             num_encoders: Number of encoder blocks.             d_model: Dimension of the model.             d_ff: Dimension of feed-forward layers.             h: Number of attention heads.             num_classes: Number of output classes.             dropout_rate: Dropout rate for layers.         \"\"\"          super().__init__()          self.patch_size_height = patch_size_height         self.patch_size_width = patch_size_width         self.img_height = img_height         self.img_width = img_width         self.in_channels = in_channels         self.num_encoders = num_encoders         self.d_model = d_model         self.d_ff = d_ff         self.h = h         self.num_classes = num_classes         self.dropout_rate = dropout_rate          # N\u00famero de patches         self.num_patches = (img_height // patch_size_height) * (             img_width // patch_size_width         )          # CLS token permite tener una representaci\u00f3n global de todos los inputs         # de la imagen (de los diferentes embeddings de cada patch)         self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))          self.patch_layer = Patches(             patch_size_height=self.patch_size_height,             patch_size_width=self.patch_size_width,             img_height=self.img_height,             img_width=self.img_width,         )          self.embeddings = PatchEmbedding(             patch_size_height=self.patch_size_height,             patch_size_width=self.patch_size_width,             in_channels=self.in_channels,             d_model=self.d_model,         )          # Entiendo que la longitud de la secuencia coincide con el numero de patches         # y un embedding m\u00e1s de la clase,         self.positional_encoding = PositionalEncoding(             d_model=self.d_model,             sequence_length=self.num_patches + 1,             dropout_rate=self.dropout_rate,         )          # Capas del Encoder         self.encoder_layers = nn.ModuleList(             [                 EncoderBlock(                     d_model=self.d_model,                     d_ff=self.d_ff,                     h=self.h,                     dropout_rate=self.dropout_rate,                 )                 for _ in range(self.num_encoders)             ]         )          self.layer_norm = LayerNormalization(features=self.d_model)          self.mlp_classifier = nn.Sequential(             nn.Linear(in_features=self.d_model, out_features=self.d_model),             nn.GELU(),             nn.Dropout(self.dropout_rate),             nn.Linear(in_features=self.d_model, out_features=num_classes),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Process input tensor through VIT model.          Args:             input_tensor: Batch of input images.          Returns:             Classification output tensor.         \"\"\"          # Extraemos los patches         input_patches = self.patch_layer(input_tensor)          # Convertimso a embeddings los patches         patch_embeddings = self.embeddings(input_patches)          # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia         # (B, 1, d_model)         cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)         # (B, num_patches+1, d_model)         embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)          # A\u00f1adir positional encoding         embeddings = self.positional_encoding(embeddings)          # Encoders del transformer         encoder_output = embeddings         for encoder_layer in self.encoder_layers:             encoder_output = encoder_layer(encoder_output)          # Usar solo el CLS token para clasificaci\u00f3n         encoder_output = self.layer_norm(encoder_output)         cls_output = encoder_output[:, 0]          # Clasificaci\u00f3n final         return self.mlp_classifier(cls_output)   if __name__ == \"__main__\":     model = VisionTransformer(         patch_size_height=16,         patch_size_width=16,         img_height=224,         img_width=224,         in_channels=3,         num_encoders=12,         d_model=768,         d_ff=3072,         h=12,         num_classes=1000,         dropout_rate=0.1,     )      x = torch.randn(2, 3, 224, 224)     output = model(x)      print(f\"Input shape: {x.shape}\")     print(f\"Output shape: {output.shape}\")"},{"location":"other_topics/topic_03_others/section_10_vision_transformers.html#transformers-in-computer-vision","title":"Transformers in Computer Vision\u00b6","text":""},{"location":"other_topics/topic_03_others/section_10_vision_transformers.html#vit","title":"ViT\u00b6","text":""},{"location":"other_topics/topic_03_others/vq_vae.html","title":"Vq vae","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:\n        \"\"\"\n        Initializes a residual block that applies two convolutional\n        layers and ReLU activations.\n\n        Args:\n            in_channels: Number of input channels for the block.\n            hidden_size: Number of channels in the hidden layer.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.hidden_size = hidden_size\n\n        self.res_block = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False,\n            ),\n            nn.ReLU(),\n            nn.Conv2d(\n                in_channels=self.hidden_size,\n                out_channels=self.in_channels,\n                kernel_size=1,\n                stride=1,\n                bias=False,\n            ),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the residual block.\n\n        Args:\n            input_tensor: The input tensor to the block.\n\n        Returns:\n            A tensor that is the sum of the input tensor and the\n            block's output.\n        \"\"\"\n\n        return input_tensor + self.res_block(input_tensor)\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_residuals: int,\n        hidden_size: int = 256,\n        kernel_size: int = 4,\n        stride: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes an encoder with convolutional layers and residual\n        blocks.\n\n        Args:\n            in_channels: Number of input channels to the encoder.\n            num_residuals: Number of residual blocks in the encoder.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of the convolutional kernels.\n            stride: Stride of the convolutional kernels.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.num_residuals = num_residuals\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        self.model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=hidden_size,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=1,\n            ),\n            nn.Conv2d(\n                in_channels=hidden_size,\n                out_channels=hidden_size,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=1,\n            ),\n        )\n\n        self.residual_blocks = nn.ModuleList(\n            [\n                ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n                for _ in range(self.num_residuals)\n            ]\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the encoder.\n\n        Args:\n            input_tensor: The input tensor to the encoder.\n\n        Returns:\n            A tensor processed by convolutional layers and residual\n            blocks.\n        \"\"\"\n\n        encoder_output = self.model(input_tensor)\n        for res_block in self.residual_blocks:\n            encoder_output = res_block(encoder_output)\n        return encoder_output\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_residuals: int,\n        out_channels: int = 3,  # Channel output (RGB)\n        hidden_size: int = 256,\n        kernel_size: int = 4,\n        stride: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a decoder with residual blocks and transpose\n        convolutional layers.\n\n        Args:\n            in_channels: Number of input channels to the decoder.\n            num_residuals: Number of residual blocks in the decoder.\n            out_channels: Number of output channels, e.g., RGB.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of the convolutional kernels.\n            stride: Stride of the convolutional kernels.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.num_residuals = num_residuals\n        self.out_channels = out_channels\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        self.residual_blocks = nn.ModuleList(\n            [\n                ResidualBlock(\n                    in_channels=self.in_channels, hidden_size=self.hidden_size\n                )\n                for _ in range(self.num_residuals)\n            ]\n        )\n\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=self.in_channels,\n                out_channels=self.hidden_size,\n                kernel_size=self.kernel_size,\n                stride=self.stride,\n                padding=1,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_size,\n                out_channels=self.out_channels,\n                kernel_size=self.kernel_size,\n                stride=self.stride,\n                padding=1,\n            ),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the decoder.\n\n        Args:\n            input_tensor: The input tensor to the decoder.\n\n        Returns:\n            A tensor processed by residual blocks and transpose\n            convolutional layers.\n        \"\"\"\n\n        decoder_output = input_tensor\n        for res_block in self.residual_blocks:\n            decoder_output = res_block(decoder_output)\n\n        return self.model(decoder_output)\n\n\nclass VectorQuantizer(nn.Module):\n    def __init__(\n        self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n    ) -&gt; None:\n        \"\"\"\n        Initializes a vector quantizer with a learnable codebook.\n\n        Args:\n            size_discrete_space: Number of discrete embeddings.\n            size_embeddings: Size of each embedding vector.\n            beta: Weighting factor for the commitment loss.\n        \"\"\"\n\n        super().__init__()\n\n        self.size_discrete_space = size_discrete_space\n        self.size_embeddings = size_embeddings\n        self.beta = beta\n\n        # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings\n        # Ha de ser una matriz aprendible\n        self.codebook = nn.Embedding(\n            num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n        )\n        # Initialize weights uniformly\n        self.codebook.weight.data.uniform_(\n            -1 / self.size_discrete_space, 1 / self.size_discrete_space\n        )\n\n    def forward(\n        self, encoder_output: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Quantizes the encoder output using the codebook.\n\n        Args:\n            encoder_output: Tensor of encoder outputs.\n\n        Returns:\n            A tuple containing VQ loss, quantized tensor, perplexity,\n            and encodings.\n        \"\"\"\n\n        # Comentario de otras implementaciones: The channels are used as the space\n        # in which to quantize.\n        # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)\n        encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n        b, h, w, c = encoder_output.size()\n        encoder_output_flat = encoder_output.reshape(-1, c)\n\n        # Calculamos la distancia entre ambos vectores\n        distances = (\n            torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n            + torch.sum(self.codebook.weight**2, dim=1)\n            - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n        )\n\n        # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n        # Matriz de ceros de (indices, size_discrete_space)\n        encodings = torch.zeros(\n            encoding_indices.shape[0],\n            self.size_discrete_space,\n            device=encoder_output.device,\n        )\n        # Colocamos un 1 en los indices de los encodings con el\n        # valor m\u00ednimo de distancia creando un vector one-hot\n        encodings.scatter_(1, encoding_indices, 1)\n\n        # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n        # del codebook y le damos formato de nuevo al tensor\n        quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n\n        # VQ-VAE loss terms\n        # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2\n        # FIX: Corrected variable names and loss calculation\n        commitment_loss = F.mse_loss(\n            quantized.detach(), encoder_output\n        )  # ||sg[z_e] - e||^2\n        embedding_loss = F.mse_loss(\n            quantized, encoder_output.detach()\n        )  # ||z_e - sg[e]||^2\n        vq_loss = commitment_loss + self.beta * embedding_loss\n\n        # Straight-through estimator\n        quantized = encoder_output + (quantized - encoder_output).detach()\n\n        # Calculate perplexity\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        # convert quantized from BHWC -&gt; BCHW\n        return (\n            vq_loss,\n            quantized.permute(0, 3, 1, 2).contiguous(),\n            perplexity,\n            encodings,\n        )\n\n\nclass VQVAE(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        size_discrete_space: int,\n        size_embeddings: int,\n        num_residuals: int,\n        hidden_size: int,\n        kernel_size: int,\n        stride: int,\n        beta: float = 0.25,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n\n        Args:\n            in_channels: Number of input channels for the model.\n            size_discrete_space: Number of discrete embeddings.\n            size_embeddings: Size of each embedding vector.\n            num_residuals: Number of residual blocks in encoder/decoder.\n            hidden_size: Number of channels in hidden layers.\n            kernel_size: Size of convolutional kernels.\n            stride: Stride of convolutional kernels.\n            beta: Weighting factor for the commitment loss.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.size_discrete_space = size_discrete_space\n        self.size_embeddings = size_embeddings\n        self.num_residuals = num_residuals\n        self.hidden_size = hidden_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.beta = beta\n\n        self.encoder = Encoder(\n            in_channels=self.in_channels,\n            num_residuals=self.num_residuals,\n            hidden_size=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n        )\n        self.decoder = Decoder(\n            in_channels=self.hidden_size,\n            num_residuals=self.num_residuals,\n            out_channels=self.in_channels,\n            hidden_size=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n        )\n\n        self.vector_quantizer = VectorQuantizer(\n            size_discrete_space=self.size_discrete_space,\n            size_embeddings=self.hidden_size,\n            beta=self.beta,\n        )\n\n    def forward(\n        self, input_tensor: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass through VQ-VAE model.\n\n        Args:\n            input_tensor: Input tensor to the model.\n\n        Returns:\n            A tuple containing VQ loss, reconstructed tensor,\n            and perplexity.\n        \"\"\"\n\n        encoder_output = self.encoder(input_tensor)\n        vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n        decoder_output = self.decoder(quantized)\n        return vq_loss, decoder_output, perplexity\n\n\nif __name__ == \"__main__\":\n    model = VQVAE(\n        in_channels=3,\n        size_discrete_space=512,\n        size_embeddings=64,\n        num_residuals=2,\n        hidden_size=128,\n        kernel_size=4,\n        stride=2,\n        beta=0.25,\n    )\n\n    x = torch.randn(4, 3, 64, 64)\n    vq_loss, reconstruction, perplexity = model(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Reconstruction shape: {reconstruction.shape}\")\n    print(f\"VQ Loss: {vq_loss.item():.4f}\")\n    print(f\"Perplexity: {perplexity.item():.4f}\")\n</pre> # 3pps import torch from torch import nn from torch.nn import functional as F   class ResidualBlock(nn.Module):     def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:         \"\"\"         Initializes a residual block that applies two convolutional         layers and ReLU activations.          Args:             in_channels: Number of input channels for the block.             hidden_size: Number of channels in the hidden layer.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.hidden_size = hidden_size          self.res_block = nn.Sequential(             nn.ReLU(),             nn.Conv2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size,                 kernel_size=3,                 stride=1,                 padding=1,                 bias=False,             ),             nn.ReLU(),             nn.Conv2d(                 in_channels=self.hidden_size,                 out_channels=self.in_channels,                 kernel_size=1,                 stride=1,                 bias=False,             ),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the residual block.          Args:             input_tensor: The input tensor to the block.          Returns:             A tensor that is the sum of the input tensor and the             block's output.         \"\"\"          return input_tensor + self.res_block(input_tensor)   class Encoder(nn.Module):     def __init__(         self,         in_channels: int,         num_residuals: int,         hidden_size: int = 256,         kernel_size: int = 4,         stride: int = 2,     ) -&gt; None:         \"\"\"         Initializes an encoder with convolutional layers and residual         blocks.          Args:             in_channels: Number of input channels to the encoder.             num_residuals: Number of residual blocks in the encoder.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of the convolutional kernels.             stride: Stride of the convolutional kernels.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.num_residuals = num_residuals         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride          self.model = nn.Sequential(             nn.Conv2d(                 in_channels=in_channels,                 out_channels=hidden_size,                 kernel_size=kernel_size,                 stride=stride,                 padding=1,             ),             nn.Conv2d(                 in_channels=hidden_size,                 out_channels=hidden_size,                 kernel_size=kernel_size,                 stride=stride,                 padding=1,             ),         )          self.residual_blocks = nn.ModuleList(             [                 ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)                 for _ in range(self.num_residuals)             ]         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the encoder.          Args:             input_tensor: The input tensor to the encoder.          Returns:             A tensor processed by convolutional layers and residual             blocks.         \"\"\"          encoder_output = self.model(input_tensor)         for res_block in self.residual_blocks:             encoder_output = res_block(encoder_output)         return encoder_output   class Decoder(nn.Module):     def __init__(         self,         in_channels: int,         num_residuals: int,         out_channels: int = 3,  # Channel output (RGB)         hidden_size: int = 256,         kernel_size: int = 4,         stride: int = 2,     ) -&gt; None:         \"\"\"         Initializes a decoder with residual blocks and transpose         convolutional layers.          Args:             in_channels: Number of input channels to the decoder.             num_residuals: Number of residual blocks in the decoder.             out_channels: Number of output channels, e.g., RGB.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of the convolutional kernels.             stride: Stride of the convolutional kernels.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.num_residuals = num_residuals         self.out_channels = out_channels         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride          self.residual_blocks = nn.ModuleList(             [                 ResidualBlock(                     in_channels=self.in_channels, hidden_size=self.hidden_size                 )                 for _ in range(self.num_residuals)             ]         )          self.model = nn.Sequential(             nn.ConvTranspose2d(                 in_channels=self.in_channels,                 out_channels=self.hidden_size,                 kernel_size=self.kernel_size,                 stride=self.stride,                 padding=1,             ),             nn.ConvTranspose2d(                 in_channels=self.hidden_size,                 out_channels=self.out_channels,                 kernel_size=self.kernel_size,                 stride=self.stride,                 padding=1,             ),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         Forward pass through the decoder.          Args:             input_tensor: The input tensor to the decoder.          Returns:             A tensor processed by residual blocks and transpose             convolutional layers.         \"\"\"          decoder_output = input_tensor         for res_block in self.residual_blocks:             decoder_output = res_block(decoder_output)          return self.model(decoder_output)   class VectorQuantizer(nn.Module):     def __init__(         self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25     ) -&gt; None:         \"\"\"         Initializes a vector quantizer with a learnable codebook.          Args:             size_discrete_space: Number of discrete embeddings.             size_embeddings: Size of each embedding vector.             beta: Weighting factor for the commitment loss.         \"\"\"          super().__init__()          self.size_discrete_space = size_discrete_space         self.size_embeddings = size_embeddings         self.beta = beta          # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings         # Ha de ser una matriz aprendible         self.codebook = nn.Embedding(             num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings         )         # Initialize weights uniformly         self.codebook.weight.data.uniform_(             -1 / self.size_discrete_space, 1 / self.size_discrete_space         )      def forward(         self, encoder_output: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"         Quantizes the encoder output using the codebook.          Args:             encoder_output: Tensor of encoder outputs.          Returns:             A tuple containing VQ loss, quantized tensor, perplexity,             and encodings.         \"\"\"          # Comentario de otras implementaciones: The channels are used as the space         # in which to quantize.         # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)         encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()         b, h, w, c = encoder_output.size()         encoder_output_flat = encoder_output.reshape(-1, c)          # Calculamos la distancia entre ambos vectores         distances = (             torch.sum(encoder_output_flat**2, dim=1, keepdim=True)             + torch.sum(self.codebook.weight**2, dim=1)             - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())         )          # Realizamos el encoding y extendemos una dimension (B*H*W, 1)         encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)          # Matriz de ceros de (indices, size_discrete_space)         encodings = torch.zeros(             encoding_indices.shape[0],             self.size_discrete_space,             device=encoder_output.device,         )         # Colocamos un 1 en los indices de los encodings con el         # valor m\u00ednimo de distancia creando un vector one-hot         encodings.scatter_(1, encoding_indices, 1)          # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)         # del codebook y le damos formato de nuevo al tensor         quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)          # VQ-VAE loss terms         # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2         # FIX: Corrected variable names and loss calculation         commitment_loss = F.mse_loss(             quantized.detach(), encoder_output         )  # ||sg[z_e] - e||^2         embedding_loss = F.mse_loss(             quantized, encoder_output.detach()         )  # ||z_e - sg[e]||^2         vq_loss = commitment_loss + self.beta * embedding_loss          # Straight-through estimator         quantized = encoder_output + (quantized - encoder_output).detach()          # Calculate perplexity         avg_probs = torch.mean(encodings, dim=0)         perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))          # convert quantized from BHWC -&gt; BCHW         return (             vq_loss,             quantized.permute(0, 3, 1, 2).contiguous(),             perplexity,             encodings,         )   class VQVAE(nn.Module):     def __init__(         self,         in_channels: int,         size_discrete_space: int,         size_embeddings: int,         num_residuals: int,         hidden_size: int,         kernel_size: int,         stride: int,         beta: float = 0.25,     ) -&gt; None:         \"\"\"         Initializes a VQ-VAE model with encoder, decoder, and quantizer.          Args:             in_channels: Number of input channels for the model.             size_discrete_space: Number of discrete embeddings.             size_embeddings: Size of each embedding vector.             num_residuals: Number of residual blocks in encoder/decoder.             hidden_size: Number of channels in hidden layers.             kernel_size: Size of convolutional kernels.             stride: Stride of convolutional kernels.             beta: Weighting factor for the commitment loss.         \"\"\"          super().__init__()          self.in_channels = in_channels         self.size_discrete_space = size_discrete_space         self.size_embeddings = size_embeddings         self.num_residuals = num_residuals         self.hidden_size = hidden_size         self.kernel_size = kernel_size         self.stride = stride         self.beta = beta          self.encoder = Encoder(             in_channels=self.in_channels,             num_residuals=self.num_residuals,             hidden_size=self.hidden_size,             kernel_size=self.kernel_size,             stride=self.stride,         )         self.decoder = Decoder(             in_channels=self.hidden_size,             num_residuals=self.num_residuals,             out_channels=self.in_channels,             hidden_size=self.hidden_size,             kernel_size=self.kernel_size,             stride=self.stride,         )          self.vector_quantizer = VectorQuantizer(             size_discrete_space=self.size_discrete_space,             size_embeddings=self.hidden_size,             beta=self.beta,         )      def forward(         self, input_tensor: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"         Forward pass through VQ-VAE model.          Args:             input_tensor: Input tensor to the model.          Returns:             A tuple containing VQ loss, reconstructed tensor,             and perplexity.         \"\"\"          encoder_output = self.encoder(input_tensor)         vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)         decoder_output = self.decoder(quantized)         return vq_loss, decoder_output, perplexity   if __name__ == \"__main__\":     model = VQVAE(         in_channels=3,         size_discrete_space=512,         size_embeddings=64,         num_residuals=2,         hidden_size=128,         kernel_size=4,         stride=2,         beta=0.25,     )      x = torch.randn(4, 3, 64, 64)     vq_loss, reconstruction, perplexity = model(x)      print(f\"Input shape: {x.shape}\")     print(f\"Reconstruction shape: {reconstruction.shape}\")     print(f\"VQ Loss: {vq_loss.item():.4f}\")     print(f\"Perplexity: {perplexity.item():.4f}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"other_topics/topic_03_others/vq_vae.html#vector-quantization-variational-auto-encoders-vq-vae","title":"Vector Quantization Variational Auto Encoders (VQ-VAE)\u00b6","text":""}]}