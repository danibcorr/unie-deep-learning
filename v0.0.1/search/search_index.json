{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#deep-learning-course","title":"Deep Learning Course","text":"<p>Deep learning course repository.</p>"},{"location":"notebooks/Computer%20Vision/step_1_image_processing.html","title":"Step 1 image processing","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_2_convolutional_layer.html","title":"Step 2 convolutional layer","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_3_lenet.html","title":"LeNet","text":"<p>We begin by importing the necessary Python modules and libraries for building and training our neural network with the MNIST dataset.</p> In\u00a0[1]: Copied! <pre># Standard libraries\nfrom typing import Any\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.manifold import TSNE\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n</pre> # Standard libraries from typing import Any  # 3pps import matplotlib.pyplot as plt import torch from sklearn.manifold import TSNE from torch import nn from torch.utils.data import DataLoader from torchinfo import summary from torchvision import datasets, transforms from tqdm import tqdm In\u00a0[2]: Copied! <pre>def show_images(images, labels):\n    fig, axes = plt.subplots(1, len(images), figsize=(10, 2))\n    for img, label, ax in zip(images, labels, axes):\n        ax.imshow(img.squeeze(), cmap='gray')\n        ax.set_title(f'Label: {label}')\n        ax.axis('off')\n    plt.show()\n</pre> def show_images(images, labels):     fig, axes = plt.subplots(1, len(images), figsize=(10, 2))     for img, label, ax in zip(images, labels, axes):         ax.imshow(img.squeeze(), cmap='gray')         ax.set_title(f'Label: {label}')         ax.axis('off')     plt.show() <p>We start by loading the MNIST dataset, including both the training and test sets. While loading the data, we apply two important transformations. First, each image is converted into a PyTorch tensor, which allows the model to process the data efficiently. Second, we normalize the images using <code>transforms.Normalize((0.1307,), (0.3081,))</code>. These numbers represent the mean (<code>0.1307</code>) and standard deviation (<code>0.3081</code>) of the MNIST dataset, and normalization ensures that the data has a consistent scale. This step is important because it helps the model train more effectively and converge faster. By combining these transformations, we prepare the dataset in a way that is both suitable for the model and optimized for learning.</p> In\u00a0[3]: Copied! <pre>transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=True,\n    download=True,\n    transform=transform\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\",\n    train=False,\n    download=True,\n    transform=transform\n)\n</pre> transform = transforms.Compose([     transforms.ToTensor(),     transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST(     root=\"./data\",     train=True,     download=True,     transform=transform ) test_dataset = datasets.MNIST(     root=\"./data\",     train=False,     download=True,     transform=transform ) In\u00a0[4]: Copied! <pre>train_dataset\n</pre> train_dataset Out[4]: <pre>Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> In\u00a0[5]: Copied! <pre>test_dataset\n</pre> test_dataset Out[5]: <pre>Dataset MNIST\n    Number of datapoints: 10000\n    Root location: ./data\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )</pre> <p>We can use the <code>DataLoader</code> class to divide the dataset into batches and shuffle the data efficiently using PyTorch\u2019s built-in functionality. To get started, we first define some global variables or constants that will be used throughout the data loading and training process.</p> In\u00a0[6]: Copied! <pre>BATCH_SIZE: int = 32\n</pre> BATCH_SIZE: int = 32 In\u00a0[7]: Copied! <pre>train_dataloader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n)\n</pre> train_dataloader = DataLoader(     dataset=train_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) test_dataloader = DataLoader(     dataset=test_dataset,     batch_size=BATCH_SIZE,     shuffle=True, ) <p>We can visualize some examples of sample-label pairs from the dataset. If we take one batch from the <code>DataLoader</code>, we will get as many samples as the chosen batch size. Each MNIST sample is a grayscale image of size 28 \u00d7 28, meaning it has a single channel. By inspecting these batches, we can better understand the structure and format of the data before feeding it into a neural network.</p> In\u00a0[8]: Copied! <pre>data_iter = iter(train_dataloader)\ntrain_images, train_labels = next(data_iter)\ntrain_images.shape, train_labels.shape\n</pre> data_iter = iter(train_dataloader) train_images, train_labels = next(data_iter) train_images.shape, train_labels.shape Out[8]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>We will display the first 10 samples from the dataset. This allows us to quickly inspect the images and their corresponding labels to ensure that the data has been loaded and preprocessed correctly.</p> In\u00a0[9]: Copied! <pre>show_images(train_images[:10], train_labels[:10])\n</pre> show_images(train_images[:10], train_labels[:10]) <p>Next, we will create a convolutional neural network (CNN) model, inspired by Yann LeCun\u2019s LeNet architecture, and adapt it for the MNIST dataset. This model will use convolutional layers to automatically extract features from the images, followed by fully connected layers to perform classification.</p> In\u00a0[10]: Copied! <pre>class LeNet(nn.Module):\n\n    def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:\n\n        super().__init__(**kwargs)\n\n        self.input_tensor_shape = input_tensor_shape\n\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=16),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n            nn.Flatten(),\n            nn.Linear(32, 10),\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.model(input_tensor)\n</pre> class LeNet(nn.Module):      def __init__(self, input_tensor_shape: tuple[int, ...], **kwargs: Any) -&gt; None:          super().__init__(**kwargs)          self.input_tensor_shape = input_tensor_shape          self.model = nn.Sequential(             nn.Conv2d(in_channels=self.input_tensor_shape[0], out_channels=16, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=16),             nn.ReLU(),             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=\"valid\"),             nn.BatchNorm2d(num_features=32),             nn.ReLU(),             nn.AdaptiveAvgPool2d(output_size=(1,1)),             nn.Flatten(),             nn.Linear(32, 10),         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.model(input_tensor) <p>We define the optimizer as AdamW and use cross-entropy as the loss function. AdamW is an adaptive optimizer that combines the benefits of Adam with correct weight decay, helping the model converge efficiently. Cross-entropy loss is well-suited for multi-class classification tasks like MNIST, as it measures the difference between the predicted probabilities and the true class labels.</p> In\u00a0[11]: Copied! <pre>model = LeNet(input_tensor_shape=(1,28,28))\nsummary(model, input_size=(BATCH_SIZE, 1,28,28))\n</pre> model = LeNet(input_tensor_shape=(1,28,28)) summary(model, input_size=(BATCH_SIZE, 1,28,28)) Out[11]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLeNet                                    [32, 10]                  --\n\u251c\u2500Sequential: 1-1                        [32, 10]                  --\n\u2502    \u2514\u2500Conv2d: 2-1                       [32, 16, 13, 13]          272\n\u2502    \u2514\u2500BatchNorm2d: 2-2                  [32, 16, 13, 13]          32\n\u2502    \u2514\u2500ReLU: 2-3                         [32, 16, 13, 13]          --\n\u2502    \u2514\u2500Conv2d: 2-4                       [32, 32, 5, 5]            8,224\n\u2502    \u2514\u2500BatchNorm2d: 2-5                  [32, 32, 5, 5]            64\n\u2502    \u2514\u2500ReLU: 2-6                         [32, 32, 5, 5]            --\n\u2502    \u2514\u2500AdaptiveAvgPool2d: 2-7            [32, 32, 1, 1]            --\n\u2502    \u2514\u2500Flatten: 2-8                      [32, 32]                  --\n\u2502    \u2514\u2500Linear: 2-9                       [32, 10]                  330\n==========================================================================================\nTotal params: 8,922\nTrainable params: 8,922\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 8.06\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 1.80\nParams size (MB): 0.04\nEstimated Total Size (MB): 1.93\n==========================================================================================</pre> In\u00a0[12]: Copied! <pre>optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4)\nloss_function = torch.nn.CrossEntropyLoss()\n</pre> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4) loss_function = torch.nn.CrossEntropyLoss() <p>Now, we need to create the training loop. This loop will iterate over the dataset for a number of epochs, feeding batches of data through the model, computing the loss, performing backpropagation, and updating the model\u2019s parameters. A well-structured training loop is essential for effectively training the network and monitoring its performance over time.</p> In\u00a0[13]: Copied! <pre>NUM_EPOCHS: int = 5\n</pre> NUM_EPOCHS: int = 5 In\u00a0[14]: Copied! <pre>train_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n    for batch_image, batch_label in train_loop:\n\n        optimizer.zero_grad()\n        outputs = model(batch_image)\n        loss = loss_function(outputs, batch_label)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += batch_label.size(0)\n        correct += (predicted == batch_label).sum().item()\n\n    train_losses.append(running_loss / len(train_dataloader))\n    train_accuracies.append(100 * correct / total)\n\n    model.eval()\n    test_loss, correct_test, total_test = 0.0, 0, 0\n\n    test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)\n\n    with torch.no_grad():\n        for images, labels in test_loop:\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n    test_losses.append(test_loss / len(test_dataloader))\n    test_accuracies.append(100 * correct_test / total_test)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"\n          f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")\n\nepochs = range(1, NUM_EPOCHS+1)\n\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Test Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\nplt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Training vs Test Accuracy\")\nplt.legend()\nplt.show()\n</pre> train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], []  for epoch in range(NUM_EPOCHS):     model.train()     running_loss, correct, total = 0.0, 0, 0      train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)      for batch_image, batch_label in train_loop:          optimizer.zero_grad()         outputs = model(batch_image)         loss = loss_function(outputs, batch_label)          loss.backward()         optimizer.step()          running_loss += loss.item()         _, predicted = torch.max(outputs, 1)         total += batch_label.size(0)         correct += (predicted == batch_label).sum().item()      train_losses.append(running_loss / len(train_dataloader))     train_accuracies.append(100 * correct / total)      model.eval()     test_loss, correct_test, total_test = 0.0, 0, 0      test_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)      with torch.no_grad():         for images, labels in test_loop:             outputs = model(images)             loss = loss_function(outputs, labels)              test_loss += loss.item()             _, predicted = torch.max(outputs, 1)             total_test += labels.size(0)             correct_test += (predicted == labels).sum().item()      test_losses.append(test_loss / len(test_dataloader))     test_accuracies.append(100 * correct_test / total_test)      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"           f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% \"           f\"| Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")  epochs = range(1, NUM_EPOCHS+1)  plt.plot(epochs, train_losses, label=\"Train Loss\") plt.plot(epochs, test_losses, label=\"Test Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Training vs Test Loss\") plt.legend() plt.show()  plt.plot(epochs, train_accuracies, label=\"Train Accuracy\") plt.plot(epochs, test_accuracies, label=\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy (%)\") plt.title(\"Training vs Test Accuracy\") plt.legend() plt.show() <pre>                                                                      \r</pre> <pre>Epoch [1/5] Train Loss: 0.9334, Train Acc: 76.77% | Test Loss: 0.4189, Test Acc: 90.49%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [2/5] Train Loss: 0.3321, Train Acc: 92.07% | Test Loss: 0.2243, Test Acc: 94.45%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [3/5] Train Loss: 0.2210, Train Acc: 94.38% | Test Loss: 0.1833, Test Acc: 95.19%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [4/5] Train Loss: 0.1745, Train Acc: 95.46% | Test Loss: 0.1389, Test Acc: 96.30%\n</pre> <pre>                                                                      \r</pre> <pre>Epoch [5/5] Train Loss: 0.1469, Train Acc: 96.10% | Test Loss: 0.1318, Test Acc: 96.39%\n</pre> <p>Now, we can visualize the data in a lower-dimensional space using t-SNE. This technique allows us to project high-dimensional representations\u2014such as the feature outputs from our model\u2014into two or three dimensions, making it easier to observe patterns, clusters, or separations between different classes. Visualizing the data in this way can provide valuable insights into how well the model is learning to distinguish between digits.</p> In\u00a0[15]: Copied! <pre>all_labels = []\nembeddings = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_image, batch_label in train_dataloader:\n        output = model(batch_image)\n        embeddings.append(output.cpu())\n        all_labels.append(batch_label) \n\nembeddings = torch.cat(embeddings, dim=0)\nall_labels = torch.cat(all_labels, dim=0)\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',\n                  init='random', perplexity=30).fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15)\nplt.colorbar(scatter, ticks=range(10), label=\"Classes\")\nplt.title(\"t-SNE Training Embeddings MNIST\")\nplt.show()\n</pre> all_labels = [] embeddings = []  model.eval() with torch.no_grad():     for batch_image, batch_label in train_dataloader:         output = model(batch_image)         embeddings.append(output.cpu())         all_labels.append(batch_label)   embeddings = torch.cat(embeddings, dim=0) all_labels = torch.cat(all_labels, dim=0)  X_embedded = TSNE(n_components=2, learning_rate='auto',                   init='random', perplexity=30).fit_transform(embeddings)  plt.figure(figsize=(10, 8)) scatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap=\"tab10\", alpha=0.7, s=15) plt.colorbar(scatter, ticks=range(10), label=\"Classes\") plt.title(\"t-SNE Training Embeddings MNIST\") plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Computer%20Vision/step_3_lenet.html#lenet","title":"LeNet\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_3_lenet.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_3_lenet.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_3_lenet.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_4_vgg.html","title":"Step 4 vgg","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_5_resnet.html","title":"Step 5 resnet","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_6_autoencoders.html","title":"Step 6 autoencoders","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_7_transfer_learning.html","title":"Step 7 transfer learning","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_8_knowledge_distillation.html","title":"Step 8 knowledge distillation","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_9_intepretability.html","title":"Step 9 intepretability","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Computer%20Vision/step_x_normalization.html","title":"Local Reponse Normalization","text":"<p>La Normalizaci\u00f3n de Respuesta Local (LRN, por sus siglas en ingl\u00e9s) es una t\u00e9cnica introducida en las primeras arquitecturas de redes neuronales convolucionales (CNNs), destacando especialmente en AlexNet (2012). Su prop\u00f3sito principal es mejorar la capacidad de generalizaci\u00f3n del modelo y promover la competencia entre neuronas dentro de una misma capa convolucional.</p> <p>La LRN se inspira en los mecanismos biol\u00f3gicos de inhibici\u00f3n lateral observados en el sistema visual humano, particularmente en la retina. En este proceso biol\u00f3gico, la activaci\u00f3n de una c\u00e9lula nerviosa inhibe la respuesta de las neuronas vecinas, lo que incrementa el contraste y mejora la percepci\u00f3n de bordes y detalles. De manera an\u00e1loga, la LRN permite que una neurona con una activaci\u00f3n alta reduzca la magnitud de las activaciones de las neuronas cercanas, resaltando as\u00ed aquellas respuestas m\u00e1s relevantes y disminuyendo la redundancia entre filtros.</p> <p>El procedimiento de la LRN puede describirse del siguiente modo: para cada neurona activada, se considera un conjunto reducido de canales adyacentes (por ejemplo, los cinco canales circundantes). La activaci\u00f3n de la neurona se normaliza dividi\u00e9ndola por un factor dependiente de la energ\u00eda local, es decir, de la suma de los cuadrados de las activaciones dentro de esa vecindad. En consecuencia, las neuronas con activaciones significativamente superiores a las de sus vecinas mantienen su valor elevado, mientras que aquellas con activaciones m\u00e1s bajas son atenuadas. Esta din\u00e1mica fomenta la especializaci\u00f3n de los filtros y contribuye a una representaci\u00f3n m\u00e1s discriminativa de las caracter\u00edsticas.</p> <p>A pesar de su utilidad inicial, la LRN fue gradualmente reemplazada por m\u00e9todos de normalizaci\u00f3n m\u00e1s eficientes y estables, tales como Batch Normalization (BN), Layer Normalization (LN) e Instance Normalization (IN). Estas t\u00e9cnicas ofrecen mayor estabilidad num\u00e9rica, aceleran el entrenamiento y mejoran el rendimiento general de las redes profundas. En la pr\u00e1ctica moderna, el uso de LRN es escaso, dado que las nuevas estrategias de normalizaci\u00f3n resultan m\u00e1s simples, robustas y efectivas en una amplia variedad de arquitecturas y contextos de aprendizaje profundo.</p> <p>Paper: https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</p> In\u00a0[1]: Copied! <pre># 3pps\nimport torch\nfrom torch import nn\n</pre> # 3pps import torch from torch import nn In\u00a0[2]: Copied! <pre># 3pps\nimport torch\nimport torch.nn as nn\n\n\nclass LocalResponseNormalization(nn.Module):\n\n    def __init__(self, k: float = 2.0, n: int = 5, alpha: float = 1e-4, beta: float = 0.75) -&gt; None:\n        super().__init__()\n        self.k = k\n        self.n = n\n        self.alpha = alpha\n        self.beta = beta\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        batch, channels, height, width = input_tensor.shape\n        response_normalization = input_tensor.clone()  \n\n        for channel in range(channels):\n            for x in range(height):\n                for y in range(width):\n                    end_iterator = min(channels - 1, (channel + self.n)//2)\n                    start_iterator = max(0, (channel - self.n)//2)\n                    numerator = input_tensor[:, channel, x, y]\n                    denominator = (\n                        self.k\n                        + self.alpha * sum(\n                            (input_tensor[:, i, x, y] ** 2) for i in range(start_iterator, end_iterator + 1)\n                        )\n                    ) ** self.beta\n\n                    response_normalization[:, channel, x, y] = numerator / denominator\n\n        return response_normalization\n</pre> # 3pps import torch import torch.nn as nn   class LocalResponseNormalization(nn.Module):      def __init__(self, k: float = 2.0, n: int = 5, alpha: float = 1e-4, beta: float = 0.75) -&gt; None:         super().__init__()         self.k = k         self.n = n         self.alpha = alpha         self.beta = beta      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:         batch, channels, height, width = input_tensor.shape         response_normalization = input_tensor.clone()            for channel in range(channels):             for x in range(height):                 for y in range(width):                     end_iterator = min(channels - 1, (channel + self.n)//2)                     start_iterator = max(0, (channel - self.n)//2)                     numerator = input_tensor[:, channel, x, y]                     denominator = (                         self.k                         + self.alpha * sum(                             (input_tensor[:, i, x, y] ** 2) for i in range(start_iterator, end_iterator + 1)                         )                     ) ** self.beta                      response_normalization[:, channel, x, y] = numerator / denominator          return response_normalization In\u00a0[3]: Copied! <pre>lrn = LocalResponseNormalization()\nx = torch.randn(1, 10, 32, 32)\ny = lrn(x)\n</pre> lrn = LocalResponseNormalization() x = torch.randn(1, 10, 32, 32) y = lrn(x) <p>La Global Response Normalization (GRN) es una t\u00e9cnica reciente en el \u00e1mbito de la visi\u00f3n por computadora, introducida en el trabajo \u201cConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\u201d.</p> <p>Esta t\u00e9cnica se incorpora como una capa de normalizaci\u00f3n global cuyo prop\u00f3sito principal es fomentar la competencia entre canales dentro de los mapas de caracter\u00edsticas de las redes convolucionales. Su implementaci\u00f3n busca mitigar el fen\u00f3meno conocido como feature collapse, frecuente en autoencoders enmascarados completamente convolucionales. El feature collapse ocurre cuando varios canales de una red neuronal presentan redundancia o p\u00e9rdida de diversidad. En tales casos, algunos canales pueden generar activaciones constantes o saturarse, reduciendo la variabilidad de las representaciones internas y, en consecuencia, la calidad de las caracter\u00edsticas aprendidas. GRN aborda este problema mediante un proceso de normalizaci\u00f3n y recalibraci\u00f3n que equilibra las contribuciones de los distintos canales.</p> <p>El mecanismo de GRN se estructura en tres etapas fundamentales. Considerando un tensor de activaciones X con dimensiones (N, C, H, W), correspondientes a tama\u00f1o de lote, n\u00famero de canales, altura y anchura, el proceso se desarrolla de la siguiente manera:</p> <ol> <li><p>Agregaci\u00f3n global de caracter\u00edsticas: Para cada canal i, se calcula una norma global (usualmente la norma $L_2$) a partir de todos los valores espaciales del mapa de caracter\u00edsticas. $$ G_i = \\sqrt{\\sum_{h,w} X_{i,h,w}^2} $$ Este c\u00e1lculo produce un vector $G(X) = [G\u2081, G\u2082, \u2026, G_C]$, que representa la magnitud global de activaci\u00f3n de cada canal.</p> </li> <li><p>Normalizaci\u00f3n intercanal: Posteriormente, los valores de norma se normalizan entre canales, dividi\u00e9ndose cada uno por la media global de las normas o por otra estad\u00edstica equivalente. $$ N_i = \\frac{G_i}{\\mathrm{mean}(G(X)) + \\epsilon} $$ Este paso genera un factor de ponderaci\u00f3n relativo que indica la intensidad de activaci\u00f3n de cada canal respecto al resto.</p> </li> <li><p>Recalibraci\u00f3n de caracter\u00edsticas y conexi\u00f3n residual: Cada canal del mapa de entrada se reescala multiplic\u00e1ndolo por su correspondiente factor normalizado (N\u1d62), aplicando adem\u00e1s un escalado (\u03b3) y un sesgo (\u03b2) aprendibles, junto con una conexi\u00f3n residual hacia la entrada original: $$ \\text{Output}_i = \\gamma \\cdot (X_i \\cdot N_i) + \\beta + X_i $$ Los par\u00e1metros \u03b3 y \u03b2 se ajustan durante el entrenamiento y son espec\u00edficos de cada canal.</p> </li> </ol> In\u00a0[4]: Copied! <pre>class GlobalResponseNormalization(nn.Module):\n\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n        \n        self.num_channels = num_channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))\n        self.beta = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \n        gx = torch.norm(input_tensor, p=2, dim=(2,3), keepdim=True)\n        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n\n        return self.gamma * (input_tensor * nx) + self.beta + input_tensor\n</pre> class GlobalResponseNormalization(nn.Module):      def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()                  self.num_channels = num_channels         self.eps = eps          self.gamma = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))         self.beta = nn.Parameter(data=torch.zeros(size=(1, self.num_channels, 1, 1)))      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:                  gx = torch.norm(input_tensor, p=2, dim=(2,3), keepdim=True)         nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)          return self.gamma * (input_tensor * nx) + self.beta + input_tensor In\u00a0[5]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\ngrn = GlobalResponseNormalization(x.shape[1])\ny = grn(x)\nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) grn = GlobalResponseNormalization(x.shape[1]) y = grn(x) print(y.shape) <pre>torch.Size([8, 128, 32, 32])\n</pre> In\u00a0[6]: Copied! <pre>class BatchNormalization2D(nn.Module):\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n\n        self.num_channels = num_channels\n        self.eps = eps\n        \n        # For inference\n        self.register_buffer('running_mean', torch.zeros(1, num_channels, 1, 1))\n        self.register_buffer('running_std', torch.ones(1, num_channels, 1, 1))\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        # Input tensor -&gt; (B, C, H, W) -&gt; Batch norm is applied for each batch in H X W\n        mean = x.mean(dim=(0, 2, 3), keepdim=True)\n        std = x.std(dim=(0, 2, 3), keepdim=True)\n        \n        self.running_mean = mean.detach()\n        self.running_std = std.detach()\n        \n        return (x - mean) / (std + self.eps)\n</pre> class BatchNormalization2D(nn.Module):     def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()          self.num_channels = num_channels         self.eps = eps                  # For inference         self.register_buffer('running_mean', torch.zeros(1, num_channels, 1, 1))         self.register_buffer('running_std', torch.ones(1, num_channels, 1, 1))      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          # Input tensor -&gt; (B, C, H, W) -&gt; Batch norm is applied for each batch in H X W         mean = x.mean(dim=(0, 2, 3), keepdim=True)         std = x.std(dim=(0, 2, 3), keepdim=True)                  self.running_mean = mean.detach()         self.running_std = std.detach()                  return (x - mean) / (std + self.eps) In\u00a0[7]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\nbn = BatchNormalization2D(num_channels=x.shape[1]).train()\ny = bn(x)\nprint(y.shape)\n\nbn.eval()           \nwith torch.no_grad():  \n    y = bn(x)       \nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) bn = BatchNormalization2D(num_channels=x.shape[1]).train() y = bn(x) print(y.shape)  bn.eval()            with torch.no_grad():       y = bn(x)        print(y.shape) <pre>torch.Size([8, 128, 32, 32])\ntorch.Size([8, 128, 32, 32])\n</pre> In\u00a0[8]: Copied! <pre>class LayerNormalization2D(nn.Module):\n\n    def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\n\n        super().__init__()\n\n        self.num_channels = num_channels\n        self.eps = eps\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        mean = input_tensor.mean(dim=(1,2,3), keepdim=True) \n        std = input_tensor.mean(dim=(1,2,3), keepdim=True)\n\n        return (x - mean) / (std + self.eps)\n</pre> class LayerNormalization2D(nn.Module):      def __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:          super().__init__()          self.num_channels = num_channels         self.eps = eps      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          mean = input_tensor.mean(dim=(1,2,3), keepdim=True)          std = input_tensor.mean(dim=(1,2,3), keepdim=True)          return (x - mean) / (std + self.eps) In\u00a0[9]: Copied! <pre>x = torch.randn(8, 128, 32, 32)\nln = LayerNormalization2D(num_channels=x.shape[1]).train()\ny = ln(x)\nprint(y.shape)\n</pre> x = torch.randn(8, 128, 32, 32) ln = LayerNormalization2D(num_channels=x.shape[1]).train() y = ln(x) print(y.shape) <pre>torch.Size([8, 128, 32, 32])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Computer%20Vision/step_x_normalization.html#local-reponse-normalization","title":"Local Reponse Normalization\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_x_normalization.html#global-response-normalization","title":"Global Response Normalization\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_x_normalization.html#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"notebooks/Computer%20Vision/step_x_normalization.html#layer-normalization","title":"Layer Normalization\u00b6","text":""},{"location":"notebooks/Graphs/step_1_gnns.html","title":"Step 1 gnns","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Graphs/step_2_pytorch_geometric.html","title":"Step 2 pytorch geometric","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Mathematics/step_1_data_structures.html","title":"Comprobar disponibilidad de la GPU","text":"In\u00a0[1]: Copied! <pre># 3pps\nimport torch\n\nprint(f\"Version de Pytorch: {torch.__version__}\")\n\n# Comprobar si la GPU est\u00e1 disponible\nif torch.cuda.is_available():\n    print(\"GPU disponible\")\n    # Obtener el nombre de la GPU\n    print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")\n    # Obtener el n\u00famero de GPUs\n    print(f\"N\u00famero de GPUs: {torch.cuda.device_count()}\")\nelse:\n    print(\"No se detect\u00f3 GPU, se usar\u00e1 la CPU\")\n</pre> # 3pps import torch  print(f\"Version de Pytorch: {torch.__version__}\")  # Comprobar si la GPU est\u00e1 disponible if torch.cuda.is_available():     print(\"GPU disponible\")     # Obtener el nombre de la GPU     print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")     # Obtener el n\u00famero de GPUs     print(f\"N\u00famero de GPUs: {torch.cuda.device_count()}\") else:     print(\"No se detect\u00f3 GPU, se usar\u00e1 la CPU\") <pre>\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/dani/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n    self._run_once()\n  File \"/Users/dani/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n    handle._run()\n  File \"/Users/dani/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n    result = self._run_cell(\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n    result = runner(coro)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/var/folders/qv/4cwcqgqn0yg_rfr41_mcxgt80000gn/T/ipykernel_7581/1446002316.py\", line 1, in &lt;module&gt;\n    import torch\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in &lt;module&gt;\n    from .functional import *  # noqa: F403\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in &lt;module&gt;\n    import torch.nn.functional as F\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in &lt;module&gt;\n    from .modules import *  # noqa: F403\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in &lt;module&gt;\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\n  File \"/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in &lt;module&gt;\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n/Users/dani/Documents/Repositorios publico/curso-pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n</pre> <pre>Version de Pytorch: 2.2.2\nNo se detect\u00f3 GPU, se usar\u00e1 la CPU\n</pre> In\u00a0[2]: Copied! <pre># Tensor escalar\nescalar = torch.tensor(7)\nescalar\n</pre> # Tensor escalar escalar = torch.tensor(7) escalar Out[2]: <pre>tensor(7)</pre> In\u00a0[3]: Copied! <pre># Un valor escalar no tiene dimensiones, no tiene rango\nescalar.ndim\n</pre> # Un valor escalar no tiene dimensiones, no tiene rango escalar.ndim Out[3]: <pre>0</pre> In\u00a0[4]: Copied! <pre># Obtener el valores del escalar\nescalar.item()\n</pre> # Obtener el valores del escalar escalar.item() Out[4]: <pre>7</pre> In\u00a0[5]: Copied! <pre># Creacion de un vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Creacion de un vector vector = torch.tensor([7, 7]) vector Out[5]: <pre>tensor([7, 7])</pre> In\u00a0[6]: Copied! <pre>vector.ndim\n</pre> vector.ndim Out[6]: <pre>1</pre> In\u00a0[7]: Copied! <pre>vector.shape\n</pre> vector.shape Out[7]: <pre>torch.Size([2])</pre> In\u00a0[8]: Copied! <pre># Creacion de una matriz \nmatriz = torch.tensor([[1,2,3], [4,5,6]])\nmatriz\n</pre> # Creacion de una matriz  matriz = torch.tensor([[1,2,3], [4,5,6]]) matriz Out[8]: <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])</pre> In\u00a0[9]: Copied! <pre>matriz.ndim\n</pre> matriz.ndim Out[9]: <pre>2</pre> In\u00a0[10]: Copied! <pre>matriz.shape\n</pre> matriz.shape Out[10]: <pre>torch.Size([2, 3])</pre> In\u00a0[11]: Copied! <pre># Creacion de un tensor\ntensor = torch.tensor([[\n    [[1,2,3],\n    [4,5,6]],\n    [[7,8,9],\n    [6,4,3]]\n]])\ntensor\n</pre> # Creacion de un tensor tensor = torch.tensor([[     [[1,2,3],     [4,5,6]],     [[7,8,9],     [6,4,3]] ]]) tensor Out[11]: <pre>tensor([[[[1, 2, 3],\n          [4, 5, 6]],\n\n         [[7, 8, 9],\n          [6, 4, 3]]]])</pre> In\u00a0[12]: Copied! <pre>tensor.ndim\n</pre> tensor.ndim Out[12]: <pre>4</pre> In\u00a0[13]: Copied! <pre>tensor.shape\n</pre> tensor.shape Out[13]: <pre>torch.Size([1, 2, 2, 3])</pre> In\u00a0[14]: Copied! <pre># Tensores aleatorios\ntensor_aleatorio = torch.rand((2,3,4))\ntensor_aleatorio\n</pre> # Tensores aleatorios tensor_aleatorio = torch.rand((2,3,4)) tensor_aleatorio Out[14]: <pre>tensor([[[0.7315, 0.7036, 0.4462, 0.1433],\n         [0.6909, 0.6339, 0.0940, 0.1992],\n         [0.3216, 0.2674, 0.4474, 0.3813]],\n\n        [[0.2377, 0.2528, 0.8061, 0.7399],\n         [0.2578, 0.1669, 0.8909, 0.7263],\n         [0.4010, 0.2280, 0.4493, 0.5748]]])</pre> In\u00a0[15]: Copied! <pre>tensor_aleatorio.ndim, tensor_aleatorio.shape\n</pre> tensor_aleatorio.ndim, tensor_aleatorio.shape Out[15]: <pre>(3, torch.Size([2, 3, 4]))</pre> In\u00a0[16]: Copied! <pre># Tensores con ceros y unos\ntensores_ceros = torch.zeros((3,4))\ntensores_ceros\n</pre> # Tensores con ceros y unos tensores_ceros = torch.zeros((3,4)) tensores_ceros Out[16]: <pre>tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])</pre> In\u00a0[17]: Copied! <pre>tensores_unos = torch.ones((3,4))\ntensores_unos\n</pre> tensores_unos = torch.ones((3,4)) tensores_unos Out[17]: <pre>tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])</pre> In\u00a0[18]: Copied! <pre># Crear un tensor con un rango de valores\nrango = torch.arange(start=0, end=100, step=2)\nrango\n</pre> # Crear un tensor con un rango de valores rango = torch.arange(start=0, end=100, step=2) rango Out[18]: <pre>tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n        72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98])</pre> In\u00a0[19]: Copied! <pre># Crear tensores con la misma dimension de otro tensor\nprint(f\"Shape de 'rango': {rango.shape}\")\ncopia_rango = torch.zeros_like(input=rango)\nprint(f\"Shape de 'copia_rango': {copia_rango.shape}\")\ncopia_rango\n</pre> # Crear tensores con la misma dimension de otro tensor print(f\"Shape de 'rango': {rango.shape}\") copia_rango = torch.zeros_like(input=rango) print(f\"Shape de 'copia_rango': {copia_rango.shape}\") copia_rango <pre>Shape de 'rango': torch.Size([50])\nShape de 'copia_rango': torch.Size([50])\n</pre> Out[19]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])</pre> <p>En el caso de realizar operaciones entre tensores que no tengan el mismo tipo de dato (data type), o tengan las dimensiones adecuadas para operar, o no se encuentren en el mismo espacio de memoria de la CPU o GPU, pueden generar conflictos y errores</p> In\u00a0[20]: Copied! <pre>tensor = torch.rand(size=(2,2,3))\ntensor\n</pre> tensor = torch.rand(size=(2,2,3)) tensor Out[20]: <pre>tensor([[[0.6785, 0.3092, 0.5808],\n         [0.1312, 0.6005, 0.6055]],\n\n        [[0.4862, 0.9419, 0.3044],\n         [0.6141, 0.1724, 0.7371]]])</pre> In\u00a0[21]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") <pre>Data type: torch.float32\nShape: torch.Size([2, 2, 3])\nDevice: cpu\n</pre> In\u00a0[22]: Copied! <pre>tensor = torch.rand(size=(2,2,3), dtype=torch.float16)\ntensor\n</pre> tensor = torch.rand(size=(2,2,3), dtype=torch.float16) tensor Out[22]: <pre>tensor([[[0.6182, 0.3872, 0.4619],\n         [0.9092, 0.6328, 0.1250]],\n\n        [[0.4951, 0.7109, 0.2578],\n         [0.6694, 0.7402, 0.0249]]], dtype=torch.float16)</pre> In\u00a0[23]: Copied! <pre>print(f\"Data type: {tensor.dtype}\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Device: {tensor.device}\")\n</pre> print(f\"Data type: {tensor.dtype}\") print(f\"Shape: {tensor.shape}\") print(f\"Device: {tensor.device}\") <pre>Data type: torch.float16\nShape: torch.Size([2, 2, 3])\nDevice: cpu\n</pre> In\u00a0[24]: Copied! <pre>tensor = torch.rand(size=(2,3))\ntensor\n</pre> tensor = torch.rand(size=(2,3)) tensor Out[24]: <pre>tensor([[0.0704, 0.3993, 0.8154],\n        [0.3288, 0.6869, 0.8930]])</pre> In\u00a0[25]: Copied! <pre>tensor.max(dim=0)\n</pre> tensor.max(dim=0) Out[25]: <pre>torch.return_types.max(\nvalues=tensor([0.3288, 0.6869, 0.8930]),\nindices=tensor([1, 1, 1]))</pre> In\u00a0[26]: Copied! <pre>tensor.max(dim=1)\n</pre> tensor.max(dim=1) Out[26]: <pre>torch.return_types.max(\nvalues=tensor([0.8154, 0.8930]),\nindices=tensor([2, 2]))</pre> <p>Tenemos que las columnas est\u00e1n representadas por el eje/dim 0, y las filas por el eje/dim 1</p> In\u00a0[27]: Copied! <pre>tensor.mean(dim=0), tensor.mean(dim=1)\n</pre> tensor.mean(dim=0), tensor.mean(dim=1) Out[27]: <pre>(tensor([0.1996, 0.5431, 0.8542]), tensor([0.4283, 0.6362]))</pre> In\u00a0[28]: Copied! <pre>torch.mean(tensor, dim=0), torch.mean(tensor, dim=1)\n</pre> torch.mean(tensor, dim=0), torch.mean(tensor, dim=1) Out[28]: <pre>(tensor([0.1996, 0.5431, 0.8542]), tensor([0.4283, 0.6362]))</pre> In\u00a0[29]: Copied! <pre>tensor\n</pre> tensor Out[29]: <pre>tensor([[0.0704, 0.3993, 0.8154],\n        [0.3288, 0.6869, 0.8930]])</pre> In\u00a0[30]: Copied! <pre>torch.argmax(tensor, dim=0), tensor.argmax(dim=0)\n</pre> torch.argmax(tensor, dim=0), tensor.argmax(dim=0) Out[30]: <pre>(tensor([1, 1, 1]), tensor([1, 1, 1]))</pre> In\u00a0[31]: Copied! <pre>tensor.argmax(dim=1)\n</pre> tensor.argmax(dim=1) Out[31]: <pre>tensor([2, 2])</pre> In\u00a0[32]: Copied! <pre>tensor\n</pre> tensor Out[32]: <pre>tensor([[0.0704, 0.3993, 0.8154],\n        [0.3288, 0.6869, 0.8930]])</pre> In\u00a0[33]: Copied! <pre>tensor[:, tensor.argmax(dim=1)[0]]\n</pre> tensor[:, tensor.argmax(dim=1)[0]] Out[33]: <pre>tensor([0.8154, 0.8930])</pre> In\u00a0[34]: Copied! <pre>matriz = torch.rand((4,4))\nmatriz\n</pre> matriz = torch.rand((4,4)) matriz Out[34]: <pre>tensor([[0.0380, 0.9873, 0.3822, 0.0607],\n        [0.1410, 0.3361, 0.7345, 0.8650],\n        [0.8603, 0.4539, 0.2303, 0.4235],\n        [0.6302, 0.5351, 0.6732, 0.6050]])</pre> In\u00a0[35]: Copied! <pre>submatrix_1 = matriz[0::2, 0::2]\nsubmatrix_2 = matriz[0::2, 1::2]\nsubmatrix_3 = matriz[1::2, 0::2]\nsubmatrix_4 = matriz[1::2, 1::2]\nsubmatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4])\nsubmatrices\n</pre> submatrix_1 = matriz[0::2, 0::2] submatrix_2 = matriz[0::2, 1::2] submatrix_3 = matriz[1::2, 0::2] submatrix_4 = matriz[1::2, 1::2] submatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4]) submatrices Out[35]: <pre>tensor([[[0.0380, 0.3822],\n         [0.8603, 0.2303]],\n\n        [[0.9873, 0.0607],\n         [0.4539, 0.4235]],\n\n        [[0.1410, 0.7345],\n         [0.6302, 0.6732]],\n\n        [[0.3361, 0.8650],\n         [0.5351, 0.6050]]])</pre> In\u00a0[36]: Copied! <pre>submatrices.shape\n</pre> submatrices.shape Out[36]: <pre>torch.Size([4, 2, 2])</pre> In\u00a0[37]: Copied! <pre>submatrix_1, submatrix_2, submatrix_3, submatrix_4\n</pre> submatrix_1, submatrix_2, submatrix_3, submatrix_4 Out[37]: <pre>(tensor([[0.0380, 0.3822],\n         [0.8603, 0.2303]]),\n tensor([[0.9873, 0.0607],\n         [0.4539, 0.4235]]),\n tensor([[0.1410, 0.7345],\n         [0.6302, 0.6732]]),\n tensor([[0.3361, 0.8650],\n         [0.5351, 0.6050]]))</pre> In\u00a0[38]: Copied! <pre>print(submatrices.shape)\nsubmatrices = submatrices.unsqueeze(dim=0)\nsubmatrices.shape\n</pre> print(submatrices.shape) submatrices = submatrices.unsqueeze(dim=0) submatrices.shape <pre>torch.Size([4, 2, 2])\n</pre> Out[38]: <pre>torch.Size([1, 4, 2, 2])</pre> In\u00a0[39]: Copied! <pre>norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1))\nnorm\n</pre> norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1)) norm Out[39]: <pre>tensor([[0.9698, 1.1678, 1.1873, 1.2302]])</pre> In\u00a0[40]: Copied! <pre>(0.5262 ** 2 + 0.0480 ** 2 + 0.0578 ** 2 + 0.1393 ** 2) ** (1/2)\n</pre> (0.5262 ** 2 + 0.0480 ** 2 + 0.0578 ** 2 + 0.1393 ** 2) ** (1/2) Out[40]: <pre>0.5494868242278426</pre> In\u00a0[41]: Copied! <pre>(0.2742 ** 2 + 0.6927 ** 2 + 0.9618 ** 2 + 0.0417 ** 2) ** (1/2)\n</pre> (0.2742 ** 2 + 0.6927 ** 2 + 0.9618 ** 2 + 0.0417 ** 2) ** (1/2) Out[41]: <pre>1.217299084038101</pre> In\u00a0[42]: Copied! <pre>submatrices[:, torch.argmax(norm), :, :]\n</pre> submatrices[:, torch.argmax(norm), :, :] Out[42]: <pre>tensor([[[0.3361, 0.8650],\n         [0.5351, 0.6050]]])</pre> In\u00a0[55]: Copied! <pre>torch.manual_seed(42)\n</pre> torch.manual_seed(42) Out[55]: <pre>&lt;torch._C.Generator at 0x10f5796d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Mathematics/step_1_data_structures.html#comprobar-disponibilidad-de-la-gpu","title":"Comprobar disponibilidad de la GPU\u00b6","text":""},{"location":"notebooks/Mathematics/step_1_data_structures.html#introduccion-a-los-tensores","title":"Introducci\u00f3n a los tensores\u00b6","text":""},{"location":"notebooks/Mathematics/step_1_data_structures.html#crear-un-tensor","title":"Crear un tensor\u00b6","text":""},{"location":"notebooks/Mathematics/step_1_data_structures.html#problemas-entre-tensores","title":"Problemas entre tensores\u00b6","text":""},{"location":"notebooks/Mathematics/step_1_data_structures.html#obtencion-de-datos-de-tensores","title":"Obtencion de datos de tensores\u00b6","text":""},{"location":"notebooks/Mathematics/step_1_data_structures.html#agregacion-de-tensores","title":"Agregacion de tensores\u00b6","text":""},{"location":"notebooks/Mathematics/step_2_linear_algebra_calculus.html","title":"Step 2 linear algebra calculus","text":"In\u00a0[6]: Copied! <pre># Libraries to import\u200b\n\nimport torch\u200b\n\nimport sympy as sp\u200b\n\n\u200b\n\n# Create input tensor with gradient tracking\u200b\n\nx = torch.tensor([2.0, 3.0], requires_grad=True)\u200b\n\n\u200b\n\n# Define a differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2\u200b\n\ny = x[0]**2 + 3*x[0]*x[1] + x[1]**2\u200b\n\n\u200b\n\n# Compute gradients\u200b\n\ny.backward()\u200b\n\n\u200b\n\n# Gradients each input\u200b\n\ngrad_x1 = x.grad[0]  # \u2202f/\u2202x1\u200b\n\ngrad_x2 = x.grad[1]  # \u2202f/\u2202x2\u200b\n\n\u200b\n\n# Print results\u200b\n\nprint(\"PyTorch gradients:\")\u200b\n\nprint(\"Gradient \u2202f/\u2202x1:\", grad_x1)\u200b\n\nprint(\"Gradient \u2202f/\u2202x2:\", grad_x2)\u200b\n\n\u200b\n\n# Define symbolic variables\u200b\n\nx1, x2 = sp.symbols('x1 x2')\u200b\n\n\u200b\n\n# Define the same function symbolically\u200b\n\nf = x1**2 + 3*x1*x2 + x2**2\u200b\n\n\u200b\n\n# Compute symbolic derivatives\u200b\n\ndf_dx1 = sp.diff(f, x1)\u200b\n\ndf_dx2 = sp.diff(f, x2)\u200b\n\n\u200b\n\n# Show the derivative formulas\u200b\n\nprint(\"\nSymPy derivative formulas:\")\u200b\n\nprint(\"\u2202f/\u2202x1 =\", df_dx1)\u200b\n\nprint(\"\u2202f/\u2202x2 =\", df_dx2)\u200b\n\n\u200b\n\n# Evaluate derivatives at a specific point (x1=2, x2=3)\u200b\n\ngrad_x1_sym = df_dx1.evalf(subs={x1:2, x2:3})\u200b\n\ngrad_x2_sym = df_dx2.evalf(subs={x1:2, x2:3})\u200b\n\n\u200b\n\n# Print numerical results\u200b\n\nprint(\"\nSymPy symbolic gradients evaluated at (x1=2, x2=3):\")\u200b\n\nprint(\"Gradient x1:\", grad_x1_sym)\u200b\n\nprint(\"Gradient x2:\", grad_x2_sym)\u200b\n</pre> # Libraries to import\u200b  import torch\u200b  import sympy as sp\u200b  \u200b  # Create input tensor with gradient tracking\u200b  x = torch.tensor([2.0, 3.0], requires_grad=True)\u200b  \u200b  # Define a differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2\u200b  y = x[0]**2 + 3*x[0]*x[1] + x[1]**2\u200b  \u200b  # Compute gradients\u200b  y.backward()\u200b  \u200b  # Gradients each input\u200b  grad_x1 = x.grad[0]  # \u2202f/\u2202x1\u200b  grad_x2 = x.grad[1]  # \u2202f/\u2202x2\u200b  \u200b  # Print results\u200b  print(\"PyTorch gradients:\")\u200b  print(\"Gradient \u2202f/\u2202x1:\", grad_x1)\u200b  print(\"Gradient \u2202f/\u2202x2:\", grad_x2)\u200b  \u200b  # Define symbolic variables\u200b  x1, x2 = sp.symbols('x1 x2')\u200b  \u200b  # Define the same function symbolically\u200b  f = x1**2 + 3*x1*x2 + x2**2\u200b  \u200b  # Compute symbolic derivatives\u200b  df_dx1 = sp.diff(f, x1)\u200b  df_dx2 = sp.diff(f, x2)\u200b  \u200b  # Show the derivative formulas\u200b  print(\" SymPy derivative formulas:\")\u200b  print(\"\u2202f/\u2202x1 =\", df_dx1)\u200b  print(\"\u2202f/\u2202x2 =\", df_dx2)\u200b  \u200b  # Evaluate derivatives at a specific point (x1=2, x2=3)\u200b  grad_x1_sym = df_dx1.evalf(subs={x1:2, x2:3})\u200b  grad_x2_sym = df_dx2.evalf(subs={x1:2, x2:3})\u200b  \u200b  # Print numerical results\u200b  print(\" SymPy symbolic gradients evaluated at (x1=2, x2=3):\")\u200b  print(\"Gradient x1:\", grad_x1_sym)\u200b  print(\"Gradient x2:\", grad_x2_sym)\u200b <pre>\n  Cell In[6], line 3\n    import torch\u200b\n                ^\nSyntaxError: invalid non-printable character U+200B\n</pre> In\u00a0[11]: Copied! <pre># 3pps\nimport torch\n\n# Example 1: Quadratic Function\n# y = x\u00b2, dy/dx = 2x\nx = torch.tensor(3.0, requires_grad=True)\ny = x**2\ny.backward()\nprint(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n\n# Example 2: Multiple Variables\n# z = 2a + 3b, dz/da = 2, dz/db = 3\na = torch.tensor(4.0, requires_grad=True)\nb = torch.tensor(5.0, requires_grad=True)\nz = 2*a + 3*b\nz.backward()\nprint(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")\n\n# Example 3: Chain Rule\n# y = (2x + 1)\u00b2, dy/dx = 4(2x + 1)\nx = torch.tensor(3.0, requires_grad=True)\ny = (2*x + 1)**2\ny.backward()\nprint(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")\n</pre> # 3pps import torch  # Example 1: Quadratic Function # y = x\u00b2, dy/dx = 2x x = torch.tensor(3.0, requires_grad=True) y = x**2 y.backward() print(f\"y = x\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\")  # Example 2: Multiple Variables # z = 2a + 3b, dz/da = 2, dz/db = 3 a = torch.tensor(4.0, requires_grad=True) b = torch.tensor(5.0, requires_grad=True) z = 2*a + 3*b z.backward() print(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")  # Example 3: Chain Rule # y = (2x + 1)\u00b2, dy/dx = 4(2x + 1) x = torch.tensor(3.0, requires_grad=True) y = (2*x + 1)**2 y.backward() print(f\"y = (2x+1)\u00b2 | x={x.item()}, dy/dx={x.grad.item()}\") <pre>y = x\u00b2 | x=3.0, dy/dx=6.0\nz = 2a + 3b | dz/da=2.0, dz/db=3.0\ny = (2x+1)\u00b2 | x=3.0, dy/dx=28.0\n</pre> In\u00a0[\u00a0]: Copied! <pre># Example 4: Linear Regression\n# y = w\u00b7x + b, dy/dx = w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nw = torch.tensor([0.5, -1.0])\nb = 2.0\ny = w[0]*x[0] + w[1]*x[1] + b\ny.backward()\nprint(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")\n\n# Example 5: Logistic Regression\n# y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nz = w[0]*x[0] + w[1]*x[1] + b\ny = torch.sigmoid(z)\ny.backward()\nprint(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\")\n</pre> # Example 4: Linear Regression # y = w\u00b7x + b, dy/dx = w x = torch.tensor([2.0, 3.0], requires_grad=True) w = torch.tensor([0.5, -1.0]) b = 2.0 y = w[0]*x[0] + w[1]*x[1] + b y.backward() print(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")  # Example 5: Logistic Regression # y = \u03c3(w\u00b7x + b), dy/dx = \u03c3'(z)\u00b7w x = torch.tensor([2.0, 3.0], requires_grad=True) z = w[0]*x[0] + w[1]*x[1] + b y = torch.sigmoid(z) y.backward() print(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\") <pre>Linear | dy/dx1=0.5, dy/dx2=-1.0\nLogistic | dy/dx1=0.1250, dy/dx2=-0.2500\ntensor(0.5000, grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> In\u00a0[16]: Copied! <pre># 3pps\n# Libraries to import\nimport torch\nimport torch.nn.functional as F\n\n# Input features\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Weights and bias for 3 classes\nW = torch.tensor([[0.2, -0.5, 0.3],   # weights for x[0]\n                  [0.4, 0.1, -0.2],   # weights for x[1]\n                  [0.1, 0.3, 0.2]], requires_grad=False)\nb = torch.tensor([0.0, 0.0, 0.0])\n\n# Linear scores for each class: s = W^T x + b\nlogits = torch.matmul(x, W) + b  # shape [3]\n\n# Apply Softmax to get probabilities\nprobs = F.softmax(logits, dim=0)\n\n# Pick the predicted class probability\npred_class_idx = probs.argmax()\ntop_prob = probs[pred_class_idx]\n\n# Compute gradients w.r.t input\ntop_prob.backward()\n\n# Print results\nprint(\"Multiclass Classification | Probabilities:\", probs.detach().numpy())\nprint(\"Predicted class index:\", pred_class_idx.item())\nprint(\"Gradients inputs:\", x.grad.detach().numpy())\n</pre> # 3pps # Libraries to import import torch import torch.nn.functional as F  # Input features x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # Weights and bias for 3 classes W = torch.tensor([[0.2, -0.5, 0.3],   # weights for x[0]                   [0.4, 0.1, -0.2],   # weights for x[1]                   [0.1, 0.3, 0.2]], requires_grad=False) b = torch.tensor([0.0, 0.0, 0.0])  # Linear scores for each class: s = W^T x + b logits = torch.matmul(x, W) + b  # shape [3]  # Apply Softmax to get probabilities probs = F.softmax(logits, dim=0)  # Pick the predicted class probability pred_class_idx = probs.argmax() top_prob = probs[pred_class_idx]  # Compute gradients w.r.t input top_prob.backward()  # Print results print(\"Multiclass Classification | Probabilities:\", probs.detach().numpy()) print(\"Predicted class index:\", pred_class_idx.item()) print(\"Gradients inputs:\", x.grad.detach().numpy()) <pre>Multiclass Classification | Probabilities: [0.51389724 0.25519383 0.23090893]\nPredicted class index: 0\nGradients inputs: [ 0.07993404  0.1105411  -0.03809503]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Mathematics/step_3_mathematics_concepts.html","title":"Cosine Similarity","text":"In\u00a0[1]: Copied! <pre># 3pps\nimport numpy as np\n</pre> # 3pps import numpy as np In\u00a0[2]: Copied! <pre>def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)\n\n\ndef cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:\n    return matriz @ matriz.T\n</pre> def normalizar_matriz(matriz: np.ndarray) -&gt; np.ndarray:     return matriz/np.expand_dims((np.sqrt(np.sum(np.power(matriz, 2), axis=1))), axis=-1)   def cosine_similarity(matriz: np.ndarray) -&gt; np.ndarray:     return matriz @ matriz.T In\u00a0[3]: Copied! <pre>X = np.array([\n    [1, 2, 3],  \n    [4, 5, 6],   \n    [1, 0, 0],  \n    [0, 1, 0]   \n], dtype=float)\n\nprint(\"Embeddings originales:\\n\", X)\n\nX_normalized = normalizar_matriz(matriz=X)\nprint(\"\\nEmbeddings normalizados:\\n\", X_normalized)\n\nsimilarity_matrix = cosine_similarity(matriz=X_normalized)\nprint(\"\\nMatriz de similitud:\\n\", similarity_matrix)\n</pre> X = np.array([     [1, 2, 3],       [4, 5, 6],        [1, 0, 0],       [0, 1, 0]    ], dtype=float)  print(\"Embeddings originales:\\n\", X)  X_normalized = normalizar_matriz(matriz=X) print(\"\\nEmbeddings normalizados:\\n\", X_normalized)  similarity_matrix = cosine_similarity(matriz=X_normalized) print(\"\\nMatriz de similitud:\\n\", similarity_matrix) <pre>Embeddings originales:\n [[1. 2. 3.]\n [4. 5. 6.]\n [1. 0. 0.]\n [0. 1. 0.]]\n\nEmbeddings normalizados:\n [[0.26726124 0.53452248 0.80178373]\n [0.45584231 0.56980288 0.68376346]\n [1.         0.         0.        ]\n [0.         1.         0.        ]]\n\nMatriz de similitud:\n [[1.         0.97463185 0.26726124 0.53452248]\n [0.97463185 1.         0.45584231 0.56980288]\n [0.26726124 0.45584231 1.         0.        ]\n [0.53452248 0.56980288 0.         1.        ]]\n</pre>"},{"location":"notebooks/Mathematics/step_3_mathematics_concepts.html#cosine-similarity","title":"Cosine Similarity\u00b6","text":""},{"location":"notebooks/Mathematics/step_3_mathematics_concepts.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/step_3_mathematics_concepts.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Mathematics/step_3_mathematics_concepts.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html","title":"Artificial Neuron","text":"In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\nfrom typing import Callable\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> # Standard libraries import math from typing import Callable  # 3pps import matplotlib.pyplot as plt import numpy as np In\u00a0[\u00a0]: Copied! <pre>def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:\n    plt.title(f\"{name_function} function\")\n    plt.plot(steps, function(steps))\n    plt.grid()\n    plt.show()\n</pre> def plot_function(name_function: str, steps: np.ndarray, function: Callable) -&gt; None:     plt.title(f\"{name_function} function\")     plt.plot(steps, function(steps))     plt.grid()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def sigmoid(input: np.ndarray) -&gt; np.ndarray:\n    return 1/(1+np.exp((-1) * input))\n\n\ndef tanh(input: np.ndarray) -&gt; np.ndarray:\n    return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))\n\n\ndef relu(input: np.ndarray) -&gt; np.ndarray:\n    return [max(0, elem) for elem in input]\n\n\ndef leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    return [max(0.1 * elem, elem) for elem in input]\n\n\ndef elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:\n    return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]\n\n\ndef swish(input: np.ndarray) -&gt; np.ndarray:\n    return input * sigmoid(input)\n\n\ndef gelu(input: np.ndarray) -&gt; np.ndarray:\n    return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3)))\n</pre> def sigmoid(input: np.ndarray) -&gt; np.ndarray:     return 1/(1+np.exp((-1) * input))   def tanh(input: np.ndarray) -&gt; np.ndarray:     return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))   def relu(input: np.ndarray) -&gt; np.ndarray:     return [max(0, elem) for elem in input]   def leaky_relu(input: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:     return [max(0.1 * elem, elem) for elem in input]   def elu(input: np.ndarray, alpha: float = 0.5) -&gt; np.ndarray:     return [alpha * (np.exp(elem) - 1) if elem &lt; 0 else elem for elem in input]   def swish(input: np.ndarray) -&gt; np.ndarray:     return input * sigmoid(input)   def gelu(input: np.ndarray) -&gt; np.ndarray:     return 0.5 * input * (1 + tanh(math.sqrt(2/math.pi) * (input + 0.044715 * input ** 3))) In\u00a0[\u00a0]: Copied! <pre>steps = np.arange(-10, 10, 0.1)\n\nplot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid)\nplot_function(name_function=\"Tanh\", steps=steps, function=tanh)\nplot_function(name_function=\"ReLU\", steps=steps, function=relu)\nplot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu)\nplot_function(name_function=\"ELU\", steps=steps, function=elu)\nplot_function(name_function=\"Swish\", steps=steps, function=swish)\nplot_function(name_function=\"GELU\", steps=steps, function=gelu)\n</pre> steps = np.arange(-10, 10, 0.1)  plot_function(name_function=\"Sigmoid\", steps=steps, function=sigmoid) plot_function(name_function=\"Tanh\", steps=steps, function=tanh) plot_function(name_function=\"ReLU\", steps=steps, function=relu) plot_function(name_function=\"LeakyReLU\", steps=steps, function=leaky_relu) plot_function(name_function=\"ELU\", steps=steps, function=elu) plot_function(name_function=\"Swish\", steps=steps, function=swish) plot_function(name_function=\"GELU\", steps=steps, function=gelu) In\u00a0[\u00a0]: Copied! <pre># Standard libraries\nimport math\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n</pre> # Standard libraries import math  # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_circles from sklearn.model_selection import train_test_split from torch import nn In\u00a0[\u00a0]: Copied! <pre>class BinaryClassifier(nn.Module):\n\n    def __init__(self, num_classes: int) -&gt; None:\n\n        super().__init__()\n\n        self.num_classes = num_classes\n\n        self.model = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.GELU(),           \n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.model(input_tensor)\n</pre> class BinaryClassifier(nn.Module):      def __init__(self, num_classes: int) -&gt; None:          super().__init__()          self.num_classes = num_classes          self.model = nn.Sequential(             nn.Linear(2, 16),             nn.GELU(),                        nn.Linear(16, 1),             nn.Sigmoid()         )      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.model(input_tensor) In\u00a0[\u00a0]: Copied! <pre>n_samples = 1000\nX, y = make_circles(\n    n_samples, noise=0.03, random_state=42\n)\nX.shape, y.shape\n</pre> n_samples = 1000 X, y = make_circles(     n_samples, noise=0.03, random_state=42 ) X.shape, y.shape In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X[:, 0], X[:, 1], c=y)\n</pre> plt.scatter(X[:, 0], X[:, 1], c=y) In\u00a0[\u00a0]: Copied! <pre>model = BinaryClassifier(num_classes=2)\nloss_function = nn.BCELoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2)\n</pre> model = BinaryClassifier(num_classes=2) loss_function = nn.BCELoss() optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2) In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32)) In\u00a0[\u00a0]: Copied! <pre>print(y_train.min(), y_train.max(), y_train.dtype)\nprint(y_test.min(), y_test.max(), y_test.dtype)\n</pre> print(y_train.min(), y_train.max(), y_train.dtype) print(y_test.min(), y_test.max(), y_test.dtype) In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.show()\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\n</pre> plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train) plt.show() plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show() In\u00a0[\u00a0]: Copied! <pre>num_epochs = 20\nbatch_size = 32\nnum_batches = math.ceil(len(X_train) / batch_size)\nnum_batches_test = math.ceil(len(X_test) / batch_size)\n\nplot_loss_train = []\nplot_loss_test = []\nplot_acc_train = []\nplot_acc_test = []\n\nfor epoch in range(num_epochs): \n    loss_epoch_train = []\n    loss_epoch_test = []\n    accuracy_train = []\n    accuracy_test = []\n    \n    model.train()\n    for i in range(num_batches):\n        X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n        y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n        optimizer.zero_grad()\n        predictions = model(X_batch)\n        loss = loss_function(predictions, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        loss_epoch_train.append(loss.item())\n        pred_labels = (predictions &gt;= 0.5).float()\n        acc = (pred_labels == y_batch).float().mean().item() * 100\n        accuracy_train.append(acc)\n\n    model.eval()\n    with torch.inference_mode():\n        for i in range(num_batches_test):\n            X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]\n            y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n\n            predictions_inference = model(X_test_batch)\n            loss_test = loss_function(predictions_inference, y_test_batch)\n            loss_epoch_test.append(loss_test.item())\n\n            pred_labels_test = (predictions_inference &gt;= 0.5).float()\n            acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100\n            accuracy_test.append(acc_test)\n\n    # Promedios de la \u00e9poca\n    train_loss_mean = np.mean(loss_epoch_train)\n    test_loss_mean = np.mean(loss_epoch_test)\n    train_acc_mean = np.mean(accuracy_train)\n    test_acc_mean = np.mean(accuracy_test)\n\n    print(\n        f\"Epoch: {epoch+1}, \"\n        f\"Train Loss: {train_loss_mean:.4f}, \"\n        f\"Test Loss: {test_loss_mean:.4f}, \"\n        f\"Train Acc: {train_acc_mean:.2f}%, \"\n        f\"Test Acc: {test_acc_mean:.2f}%\"\n    )\n\n    # Guardar para graficar\n    plot_loss_train.append(train_loss_mean)\n    plot_loss_test.append(test_loss_mean)\n    plot_acc_train.append(train_acc_mean)\n    plot_acc_test.append(test_acc_mean)\n</pre> num_epochs = 20 batch_size = 32 num_batches = math.ceil(len(X_train) / batch_size) num_batches_test = math.ceil(len(X_test) / batch_size)  plot_loss_train = [] plot_loss_test = [] plot_acc_train = [] plot_acc_test = []  for epoch in range(num_epochs):      loss_epoch_train = []     loss_epoch_test = []     accuracy_train = []     accuracy_test = []          model.train()     for i in range(num_batches):         X_batch = X_train[i * batch_size : (i + 1) * batch_size]         y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)          optimizer.zero_grad()         predictions = model(X_batch)         loss = loss_function(predictions, y_batch)         loss.backward()         optimizer.step()          loss_epoch_train.append(loss.item())         pred_labels = (predictions &gt;= 0.5).float()         acc = (pred_labels == y_batch).float().mean().item() * 100         accuracy_train.append(acc)      model.eval()     with torch.inference_mode():         for i in range(num_batches_test):             X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]             y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)              predictions_inference = model(X_test_batch)             loss_test = loss_function(predictions_inference, y_test_batch)             loss_epoch_test.append(loss_test.item())              pred_labels_test = (predictions_inference &gt;= 0.5).float()             acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100             accuracy_test.append(acc_test)      # Promedios de la \u00e9poca     train_loss_mean = np.mean(loss_epoch_train)     test_loss_mean = np.mean(loss_epoch_test)     train_acc_mean = np.mean(accuracy_train)     test_acc_mean = np.mean(accuracy_test)      print(         f\"Epoch: {epoch+1}, \"         f\"Train Loss: {train_loss_mean:.4f}, \"         f\"Test Loss: {test_loss_mean:.4f}, \"         f\"Train Acc: {train_acc_mean:.2f}%, \"         f\"Test Acc: {test_acc_mean:.2f}%\"     )      # Guardar para graficar     plot_loss_train.append(train_loss_mean)     plot_loss_test.append(test_loss_mean)     plot_acc_train.append(train_acc_mean)     plot_acc_test.append(test_acc_mean) In\u00a0[\u00a0]: Copied! <pre>plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\")\nplt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\")\nplt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\")\nplt.legend()\nplt.show()\n</pre> plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\") plt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\") plt.legend() plt.show()  plt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\") plt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.show()\nwith torch.inference_mode():\n    predictions=model(X_test)\npredictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\nplt.show()\n</pre> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test) plt.show() with torch.inference_mode():     predictions=model(X_test) predictions = np.where(predictions.numpy() &gt;= 1e-1, 1, 0) plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions) plt.show()"},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#activation-functions","title":"Activation Functions\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#functions","title":"Functions\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#classification-toy-example","title":"Classification Toy Example\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#functionsclasses","title":"Functions/Classes\u00b6","text":""},{"location":"notebooks/Mathematics/step_4_artificial_neuron.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html","title":"Example 1","text":"In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Definici\u00f3n de la funci\u00f3n\n\n\ndef function(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n\n# C\u00e1lculo del gradiente (derivadas parciales)\n\n\ndef gradiente(input: np.ndarray) -&gt; np.ndarray:\n    assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"\n    \n    df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])\n    df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])\n\n    return np.stack([df_x1, df_x2], axis=1)\n\n# Algoritmo de descenso del gradiente\n\n\ndef descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):\n    dim = 2\n    X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]\n    trayectorias = [X.copy()]\n\n    for _ in range(num_iteraciones):\n        X = X - learning_rate * gradiente(input=X)\n        trayectorias.append(X.copy())\n        \n    return np.array(trayectorias)\n\n\n# Ejecuci\u00f3n del descenso del gradiente\ntrayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)\n\n# Visualizaci\u00f3n de trayectorias en el espacio 2D\nfor i in range(trayectoria.shape[1]):\n    plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Trayectorias del descenso del gradiente\")\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np  # Definici\u00f3n de la funci\u00f3n   def function(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"     return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])  # C\u00e1lculo del gradiente (derivadas parciales)   def gradiente(input: np.ndarray) -&gt; np.ndarray:     assert input.shape[-1] == 2, \"La entrada debe contener 2 elementos\"          df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(0.5 * input[:, 0]) * np.cos(0.5 * input[:, 1])     df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(0.5 * input[:, 0]) * np.sin(0.5 * input[:, 1])      return np.stack([df_x1, df_x2], axis=1)  # Algoritmo de descenso del gradiente   def descenso_gradiente(num_puntos: int = 10, num_iteraciones: int = 30, learning_rate: float = 1e-3):     dim = 2     X = np.random.rand(num_puntos, dim) * 10  # Inicializaci\u00f3n en el dominio [0,10]     trayectorias = [X.copy()]      for _ in range(num_iteraciones):         X = X - learning_rate * gradiente(input=X)         trayectorias.append(X.copy())              return np.array(trayectorias)   # Ejecuci\u00f3n del descenso del gradiente trayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)  # Visualizaci\u00f3n de trayectorias en el espacio 2D for i in range(trayectoria.shape[1]):     plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")  plt.xlabel(\"x1\") plt.ylabel(\"x2\") plt.title(\"Trayectorias del descenso del gradiente\") plt.show() In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport torch\n</pre> # 3pps import matplotlib.pyplot as plt import torch In\u00a0[\u00a0]: Copied! <pre>tiempo = torch.arange(0, 20).float()\ntiempo\n</pre> tiempo = torch.arange(0, 20).float() tiempo In\u00a0[\u00a0]: Copied! <pre>velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1\nplt.scatter(tiempo, velocidad)\n</pre> velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1 plt.scatter(tiempo, velocidad) In\u00a0[\u00a0]: Copied! <pre>velocidad.shape, tiempo.shape\n</pre> velocidad.shape, tiempo.shape In\u00a0[\u00a0]: Copied! <pre>def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:\n    a, b, c = parametros\n    return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c\n</pre> def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -&gt; float:     a, b, c = parametros     return a * (instante_tiempo ** 2) + (b * instante_tiempo) + c In\u00a0[\u00a0]: Copied! <pre>def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:\n    return (real - predicted).square().mean()\n</pre> def loss_function(predicted: torch.Tensor, real: torch.Tensor) -&gt; torch.Tensor:     return (real - predicted).square().mean() In\u00a0[\u00a0]: Copied! <pre>parametros = torch.randn(3).requires_grad_()\nparametros\n</pre> parametros = torch.randn(3).requires_grad_() parametros In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones In\u00a0[\u00a0]: Copied! <pre>def show_preds(tiempo, real, preds: torch.Tensor):\n    plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")\n    plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")\n    plt.legend()\n    plt.show()\n\n\nshow_preds(tiempo, velocidad, predicciones)\n</pre> def show_preds(tiempo, real, preds: torch.Tensor):     plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")     plt.scatter(tiempo, preds.detach().cpu().numpy(), color=\"red\", label=\"Predicho\")     plt.legend()     plt.show()   show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>perdida = loss_function(predicciones, velocidad)\nperdida\n</pre> perdida = loss_function(predicciones, velocidad) perdida <p>Aplicamos backward y comprobamos los gradientes</p> In\u00a0[\u00a0]: Copied! <pre>perdida.backward()\nparametros.grad\n</pre> perdida.backward() parametros.grad <p>Podemos utilizar un ratio de aprendizaje, actualizar el gradiente a partir de ese ratio y volver a colocar 0 en los gradientes para realizar una nueva evaluaci\u00f3n</p> In\u00a0[\u00a0]: Copied! <pre>lr = 1e-5\nparametros.data = parametros.data - lr * parametros.grad.data\nparametros.grad = None\n</pre> lr = 1e-5 parametros.data = parametros.data - lr * parametros.grad.data parametros.grad = None In\u00a0[\u00a0]: Copied! <pre>predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\npredicciones\n</pre> predicciones = funcion(instante_tiempo=tiempo, parametros=parametros) predicciones In\u00a0[\u00a0]: Copied! <pre>show_preds(tiempo, velocidad, predicciones)\n</pre> show_preds(tiempo, velocidad, predicciones) In\u00a0[\u00a0]: Copied! <pre>def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):\n    predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)\n    perdida = loss_function(predicted=predicciones, real=datos_a_predecir)\n    perdida.backward()\n\n    # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles\n    with torch.no_grad():\n        parametros_aprendibles -= lr * parametros_aprendibles.grad\n    \n    # Otra forma de resetear los gradientes\n    parametros_aprendibles.grad.zero_()\n\n    show_preds(tiempo, datos_a_predecir, predicciones)\n    return predicciones, parametros_aprendibles, perdida\n</pre> def apply_step_training(tiempo, parametros_aprendibles, datos_a_predecir, lr=1e-5):     predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)     perdida = loss_function(predicted=predicciones, real=datos_a_predecir)     perdida.backward()      # Hacerlo as\u00ed es m\u00e1s seguro para actualizar los par\u00e1metros aprendibles     with torch.no_grad():         parametros_aprendibles -= lr * parametros_aprendibles.grad          # Otra forma de resetear los gradientes     parametros_aprendibles.grad.zero_()      show_preds(tiempo, datos_a_predecir, predicciones)     return predicciones, parametros_aprendibles, perdida In\u00a0[\u00a0]: Copied! <pre># 3pps\nfrom tqdm import tqdm\n</pre> # 3pps from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>num_epochs = 20\nparametros_aprendibles = torch.randn(3, requires_grad=True)\n\nfor epoch in tqdm(range(num_epochs)):\n    predicciones, parametros_aprendibles, perdida = apply_step_training(\n        tiempo=tiempo, \n        parametros_aprendibles=parametros_aprendibles, \n        datos_a_predecir=velocidad\n    )\n    print(f\"Epoch {epoch+1}, perdida: {perdida}\")\n</pre> num_epochs = 20 parametros_aprendibles = torch.randn(3, requires_grad=True)  for epoch in tqdm(range(num_epochs)):     predicciones, parametros_aprendibles, perdida = apply_step_training(         tiempo=tiempo,          parametros_aprendibles=parametros_aprendibles,          datos_a_predecir=velocidad     )     print(f\"Epoch {epoch+1}, perdida: {perdida}\") In\u00a0[\u00a0]: Copied! <pre>def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:\n\n    # (tensor_entrada) -&gt; (B, N)\n    # peso -&gt; (B, N, 1)\n    # (N) \n    return tensor_entrada @ w + b\n\n\nclass CapaLineal:\n\n    def __init__(self, shape_entrada: int) -&gt; None:\n\n        self.w = torch.randn()\n</pre> def linear_layer(tensor_entrada: torch.Tensor) -&gt; torch.Tensor:      # (tensor_entrada) -&gt; (B, N)     # peso -&gt; (B, N, 1)     # (N)      return tensor_entrada @ w + b   class CapaLineal:      def __init__(self, shape_entrada: int) -&gt; None:          self.w = torch.randn() In\u00a0[\u00a0]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\n</pre> # 3pps import matplotlib.pyplot as plt import numpy as np import torch from sklearn.model_selection import train_test_split from torch import nn In\u00a0[\u00a0]: Copied! <pre>class Linear(nn.Module):\n\n    def __init__(self, ) -&gt; None:\n\n        super().__init__()\n\n        self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)\n        self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)\n\n    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n\n        return self.weight * input_tensor + self.bias\n</pre> class Linear(nn.Module):      def __init__(self, ) -&gt; None:          super().__init__()          self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)         self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)      def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:          return self.weight * input_tensor + self.bias In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre>start = 0\nend = 1\nsteps = 0.02\nX = np.arange(start, end, steps)\nX\n</pre> start = 0 end = 1 steps = 0.02 X = np.arange(start, end, steps) X In\u00a0[\u00a0]: Copied! <pre>bias = 0.3\nweight = 0.7\ny = weight * X + bias\n</pre> bias = 0.3 weight = 0.7 y = weight * X + bias In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\nX_train.dtype\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape X_train = torch.from_numpy(X_train.astype(np.float32)) X_test = torch.from_numpy(X_test.astype(np.float32)) y_train = torch.from_numpy(y_train.astype(np.float32)) y_test = torch.from_numpy(y_test.astype(np.float32)) X_train.dtype In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\")\nplt.show()\nplt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\")\nplt.show()\n</pre> plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\") plt.show() plt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\") plt.show() In\u00a0[\u00a0]: Copied! <pre>linear_model = Linear()\nlist(linear_model.parameters())\nlinear_model.state_dict()\n</pre> linear_model = Linear() list(linear_model.parameters()) linear_model.state_dict() In\u00a0[\u00a0]: Copied! <pre>linear_model.eval()\nwith torch.no_grad():\n    predictions = linear_model(X_test)\npredictions\n</pre> linear_model.eval() with torch.no_grad():     predictions = linear_model(X_test) predictions <p>De la documentacion: InferenceMode is analogous to no_grad and should be used when you are certain your operations will not interact with autograd (e.g., during data loading or model evaluation). Compared to no_grad, it removes additional overhead by disabling view tracking and version counter bumps. It is also more restrictive, in that tensors created in this mode cannot be used in computations recorded by autograd. Vamos que no tiene en cuenta el trackeo de los gradientes y lo hace m\u00e1s seguro para evitar la actualizaci\u00f3n de par\u00e1metros del modelo. A parte hace m\u00e1s r\u00e1pida la ejecuci\u00f3n de c\u00f3digo en inferencia</p> In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_2 = linear_model(X_test)\npredictions_2\n</pre> with torch.inference_mode():     predictions_2 = linear_model(X_test) predictions_2 In\u00a0[\u00a0]: Copied! <pre>X_test.shape, predictions.shape\n</pre> X_test.shape, predictions.shape In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>loss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(linear_model.parameters())\n</pre> loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(linear_model.parameters()) In\u00a0[\u00a0]: Copied! <pre>num_epochs: int = 50\n\nfor epoch in range(num_epochs):\n    epoch_losses_train = []\n    epoch_losses_test = []\n\n    for x, y in zip(X_train, y_train):\n        optimizer.zero_grad()\n\n        output_model = linear_model(x)\n        loss = loss_fn(output_model, y)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_losses_train.append(loss.item())\n\n    with torch.inference_mode():\n        for x, y in zip(X_test, y_test):\n            output_model = linear_model(x)\n            loss = loss_fn(output_model, y)\n            epoch_losses_test.append(loss.item())\n\n    print(f\"Epoch: {epoch+1}, \"\n          f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"\n          f\"Test Loss: {np.mean(epoch_losses_test):.4f}\")\n</pre> num_epochs: int = 50  for epoch in range(num_epochs):     epoch_losses_train = []     epoch_losses_test = []      for x, y in zip(X_train, y_train):         optimizer.zero_grad()          output_model = linear_model(x)         loss = loss_fn(output_model, y)          loss.backward()         optimizer.step()          epoch_losses_train.append(loss.item())      with torch.inference_mode():         for x, y in zip(X_test, y_test):             output_model = linear_model(x)             loss = loss_fn(output_model, y)             epoch_losses_test.append(loss.item())      print(f\"Epoch: {epoch+1}, \"           f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"           f\"Test Loss: {np.mean(epoch_losses_test):.4f}\") In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_trained = linear_model(X_test)\nplt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_trained = linear_model(X_test) plt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>torch.save(linear_model, \"linear_model.pth\")\nlinear_model_loaded = torch.load(\"linear_model.pth\")\nlinear_model_loaded.state_dict()\n</pre> torch.save(linear_model, \"linear_model.pth\") linear_model_loaded = torch.load(\"linear_model.pth\") linear_model_loaded.state_dict() In\u00a0[\u00a0]: Copied! <pre>with torch.inference_mode():\n    predictions_loaded = linear_model_loaded(X_test)\nplt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions\")\nplt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\nplt.legend()\nplt.show()\n</pre> with torch.inference_mode():     predictions_loaded = linear_model_loaded(X_test) plt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions\") plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\") plt.legend() plt.show()"},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#example-1","title":"Example 1\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#example-2","title":"Example 2\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#another-example","title":"Another example\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#libraries","title":"Libraries\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#functionclasses","title":"Function/Classes\u00b6","text":""},{"location":"notebooks/Mathematics/step_5_gradient_descent.html#main","title":"Main\u00b6","text":""},{"location":"notebooks/Mathematics/step_6_deep_neural_networks.html","title":"Step 6 deep neural networks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Mathematics/step_7_regularization_techniques.html","title":"Step 7 regularization techniques","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Mathematics/step_8_optimizers.html","title":"Step 8 optimizers","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Mathematics/step_9_metrics.html","title":"Step 9 metrics","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_1_rnns.html","title":"Step 1 rnns","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_2_lstm.html","title":"Step 2 lstm","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_3_autoencoders.html","title":"Step 3 autoencoders","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_4_transformers.html","title":"Step 4 transformers","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_5_moe.html","title":"Step 5 moe","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Sequential%20Models/step_6_open_models.html","title":"Step 6 open models","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/Time%20Series/step_x_other_techniques.html","title":"Gramian Angular Field","text":"<p>Gramian Angular Difference Field (GADF) permite codificar series temporales a im\u00e1genes, permiten realizar una interpretaci\u00f3n en una imagen en 2D de una serie temporal univariable.</p> <p>Podemos convertir series temporales, representadas en un eje de coordenadas cartesianas, donde tenemos un valor en el eje y que es el valor de variable en si, en el eje x tenemos el tiempo que transcurre y como varia esa variable en el tiempo.</p> <p></p> <p>Para convertir a coordenadas polares, representado tal que (r, \u03b8), donde:</p> <ul> <li>r (radio) representa la distancia desde el origen.</li> <li>\u03b8 (\u00e1ngulo) representa la direcci\u00f3n del punto respecto a un eje de referencia (como el eje x).</li> </ul> <p>Podemos convertir a coordenadas polares de 2 formas:</p> <ul> <li>La primera ser\u00eda considerar una se\u00f1al peri\u00f3dica, como un seno o coseno, que parta desde 0 hasta 2pi, este es el periodo de la se\u00f1al y luego tendr\u00edamos que el radio es el valor en si. Visualizar ciclos o estacionalidades (en el m\u00e9todo 1).</li> <li>La segunda es medir la diferencia de tiempo que existe entre un instante t y un instante t + 1, y luego se calcula la diferencia del valor de la variable como un delta, y se calcula el angulo entre la delta de tiempo y la delta de la variable. Por tanto ser\u00eda algo como: \u03b8 = arctan(\u0394y / \u0394t), r = sqrt((\u0394t)\u00b2 + (\u0394y)\u00b2) que mide la magnitud del cambio. Detectar patrones direccionales, como si los cambios tienen una orientaci\u00f3n predominante (en el m\u00e9todo 2).</li> </ul> <p></p> <p>En este caso, como la distribuci\u00f3n del tiempo es discreta, que representa los meses, podemos convertirlo a grados haciendo:</p> <ul> <li>2 * math.pi * (t / max(t)) En el caso de tener un formato basado en datetime, con HH:MM:SS, habr\u00eda que normalizar respecto al tiempo:</li> <li>2 * math.pi * (t / 24 horas * 3600 segundos), donde t es el tiempo desde las 00:00:00 en segundos</li> </ul> In\u00a0[1]: Copied! <pre># Standard libraries\nimport math\n\n# Datos sint\u00e9ticos\n# Tiempo (por ejemplo, meses)\nmonths = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n# Valores sint\u00e9ticos (algo como una onda)\nvalues = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n\n# M\u00e9todo 1 ----------\ntheta_1 = [2 * math.pi * (month / len(months)) for month in months]\nprint(theta_1)\n\nrho_1 = values\nprint(rho_1)\n\ncoord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)]\nprint(coord_method1)\n\n# M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ----------\n# Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos\n# arrays tienen la misma cantidad de elementos\ntheta_2 = []\nrho_2 = []\nfor index, month in enumerate(months):\n    if index + 1 &lt; len(values):\n        delta_values = values[index + 1] - values[index]\n        delta_time = months[index + 1] - month\n\n        # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que\n        # saturar el valor entre 2pi y -2pi\n        if delta_time != 0:\n            theta_2.append(math.atan(delta_values / delta_time))\n        else:\n            theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)\n\n        rho_2.append(math.sqrt(delta_values**2 + delta_time**2))\nprint(\"\\n\")\nprint(theta_2)\nprint(rho_2)\ncoord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)]\nprint(coord_method2)\n</pre> # Standard libraries import math  # Datos sint\u00e9ticos # Tiempo (por ejemplo, meses) months = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Valores sint\u00e9ticos (algo como una onda) values = [5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]  # M\u00e9todo 1 ---------- theta_1 = [2 * math.pi * (month / len(months)) for month in months] print(theta_1)  rho_1 = values print(rho_1)  coord_method1 = [tuple(pair_month_value) for pair_month_value in zip(theta_1, rho_1)] print(coord_method1)  # M\u00e9todo 2 (valor de la pendiente entre puntos, no del punto en si) ---------- # Lo mejor ser\u00eda rellanar con 0 los meses en los que no hay datos, as\u00ed ambos # arrays tienen la misma cantidad de elementos theta_2 = [] rho_2 = [] for index, month in enumerate(months):     if index + 1 &lt; len(values):         delta_values = values[index + 1] - values[index]         delta_time = months[index + 1] - month          # Podr\u00eda ser que el valor de delta_time fuese cero en ese caso, habr\u00eda que         # saturar el valor entre 2pi y -2pi         if delta_time != 0:             theta_2.append(math.atan(delta_values / delta_time))         else:             theta_2.append(math.pi / 2 if delta_values &gt; 0 else -math.pi / 2)          rho_2.append(math.sqrt(delta_values**2 + delta_time**2)) print(\"\\n\") print(theta_2) print(rho_2) coord_method2 = [tuple(pair_month_value) for pair_month_value in zip(theta_2, rho_2)] print(coord_method2) <pre>[0.0, 0.5235987755982988, 1.0471975511965976, 1.5707963267948966, 2.0943951023931953, 2.6179938779914944, 3.141592653589793, 3.6651914291880923, 4.1887902047863905, 4.71238898038469, 5.235987755982989, 5.759586531581287]\n[5, 6, 8, 9, 10, 9, 8, 6, 4, 3, 4, 5]\n[(0.0, 5), (0.5235987755982988, 6), (1.0471975511965976, 8), (1.5707963267948966, 9), (2.0943951023931953, 10), (2.6179938779914944, 9), (3.141592653589793, 8), (3.6651914291880923, 6), (4.1887902047863905, 4), (4.71238898038469, 3), (5.235987755982989, 4), (5.759586531581287, 5)]\n\n\n[0.7853981633974483, 1.1071487177940906, 0.7853981633974483, 0.7853981633974483, -0.7853981633974483, -0.7853981633974483, -1.1071487177940906, -1.1071487177940906, -0.7853981633974483, 0.7853981633974483, 0.7853981633974483]\n[1.4142135623730951, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951, 2.23606797749979, 2.23606797749979, 1.4142135623730951, 1.4142135623730951, 1.4142135623730951]\n[(0.7853981633974483, 1.4142135623730951), (1.1071487177940906, 2.23606797749979), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-0.7853981633974483, 1.4142135623730951), (-1.1071487177940906, 2.23606797749979), (-1.1071487177940906, 2.23606797749979), (-0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951), (0.7853981633974483, 1.4142135623730951)]\n</pre> In\u00a0[2]: Copied! <pre># 3pps\nimport matplotlib.pyplot as plt\n\n# M\u00e9todo 1\nplt.subplot(1, 2, 1, projection=\"polar\")\nplt.plot(theta_1, rho_1, marker=\"o\")\nplt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")\n\n# M\u00e9todo 2\nplt.subplot(1, 2, 2, projection=\"polar\")\nplt.plot(theta_2, rho_2, marker=\"o\")\nplt.title(\"M\u00e9todo 2: Cambio como vector\")\n\nplt.tight_layout()\nplt.show()\n</pre> # 3pps import matplotlib.pyplot as plt  # M\u00e9todo 1 plt.subplot(1, 2, 1, projection=\"polar\") plt.plot(theta_1, rho_1, marker=\"o\") plt.title(\"M\u00e9todo 1: Tiempo como \u00e1ngulo\")  # M\u00e9todo 2 plt.subplot(1, 2, 2, projection=\"polar\") plt.plot(theta_2, rho_2, marker=\"o\") plt.title(\"M\u00e9todo 2: Cambio como vector\")  plt.tight_layout() plt.show() <p>Continuando con Gramian, tenemos que la matriz Gramian es una matriz que consiste en realizar el producto vectorial entre cada pareja de vectores.</p> <p></p> <p>La matriz de Gram preserva la dependencia temporal, pues el tiempo incrementa del mismo modo que lo hace la posici\u00f3n de la matriz 2D de arriba a la izquierda y de arriba a la derecha, por lo que el tiempo se codifica en la geometr\u00eda de la matriz. Es decir, la matriz mantiene las relaciones angulares entre todos los puntos de la serie. Para ello se siguen los pasos siguientes:</p> <ul> <li><p>Paso 1: Normaliza la serie: Primero necesitas normalizar tu serie a un rango de $[-1, 1]$ (porque luego aplicaremos el arccos):</p> <p>$$   \\tilde{x}_i = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)} \\times 2 - 1   $$</p> </li> <li><p>Paso 2: Convierte los valores a \u00e1ngulos. Para cada valor de la serie normalizada:</p> <p>$$   \\phi_i = \\arccos(\\tilde{x}_i)   $$</p> <p>Esto convierte cada valor en un \u00e1ngulo entre $0$ y $\\pi$, que representa su posici\u00f3n relativa dentro del ciclo.</p> </li> <li><p>Paso 3: Construye la matriz GADF</p> <p>La idea es comparar cada par de puntos $(\\phi_i, \\phi_j)$ de la serie y calcular:</p> <p>$$   \\text{GADF}[i,j] = \\sin(\\phi_i - \\phi_j)   $$</p> <ul> <li>Esto mide la diferencia angular entre dos puntos.</li> <li>El resultado es una matriz cuadrada $N \\times N$ que puedes tratar como una imagen.</li> </ul> </li> </ul> <p>Tambi\u00e9n, existe el Gramian Angular Summation Field (GASF) usa la suma en vez de la diferencia:</p> <p>$$ \\text{GASF}[i,j] = \\cos(\\phi_i + \\phi_j) $$</p> In\u00a0[5]: Copied! <pre># Standard libraries\nimport math\nimport random\n\n# 3pps\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef normalization(samples: list) -&gt; list:\n\n    min_val = min(samples)\n    max_val = max(samples)\n\n    return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]\n\n\n@njit\ndef degrees(samples: list) -&gt; list:\n\n    return [math.acos(sample) for sample in samples]\n\n\n@njit\ndef gadf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.sin(sample_i - sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n@njit\ndef gasf_matrix(samples: list) -&gt; list:\n\n    norm_samples = normalization(samples)\n    theta_samples = degrees(norm_samples)\n\n    n = len(samples)\n    gadf_matrix_list = []\n\n    for sample_i in theta_samples:\n        for sample_j in theta_samples:\n            gadf_matrix_list.append(math.cos(sample_i + sample_j))\n\n    return np.array(gadf_matrix_list).reshape(n, n)\n\n\n# Datos temporales (por ejemplo, 16 puntos en el tiempo)\nx = random.sample(range(1, 4000), 2400)\nx_gadf_matrix = gadf_matrix(samples=x)\nx_gasf_matrix = gasf_matrix(samples=x)\n\nprint(x_gadf_matrix.shape)\nprint(x_gasf_matrix.shape)\n\nplt.imshow(x_gadf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n\nplt.imshow(x_gasf_matrix, origin=\"upper\")\nplt.colorbar()\nplt.show()\n</pre> # Standard libraries import math import random  # 3pps import matplotlib.pyplot as plt import numpy as np from numba import njit   @njit def normalization(samples: list) -&gt; list:      min_val = min(samples)     max_val = max(samples)      return [((sample - min_val) / (max_val - min_val)) * 2 - 1 for sample in samples]   @njit def degrees(samples: list) -&gt; list:      return [math.acos(sample) for sample in samples]   @njit def gadf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.sin(sample_i - sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   @njit def gasf_matrix(samples: list) -&gt; list:      norm_samples = normalization(samples)     theta_samples = degrees(norm_samples)      n = len(samples)     gadf_matrix_list = []      for sample_i in theta_samples:         for sample_j in theta_samples:             gadf_matrix_list.append(math.cos(sample_i + sample_j))      return np.array(gadf_matrix_list).reshape(n, n)   # Datos temporales (por ejemplo, 16 puntos en el tiempo) x = random.sample(range(1, 4000), 2400) x_gadf_matrix = gadf_matrix(samples=x) x_gasf_matrix = gasf_matrix(samples=x)  print(x_gadf_matrix.shape) print(x_gasf_matrix.shape)  plt.imshow(x_gadf_matrix, origin=\"upper\") plt.colorbar() plt.show()  plt.imshow(x_gasf_matrix, origin=\"upper\") plt.colorbar() plt.show() <pre>(2400, 2400)\n(2400, 2400)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Time%20Series/step_x_other_techniques.html#gramian-angular-field","title":"Gramian Angular Field\u00b6","text":""},{"location":"notebooks/Time%20Series/step_x_other_techniques.html#coordenadas-polares","title":"Coordenadas polares\u00b6","text":""},{"location":"repo/cv/layers.html","title":"Layers","text":""},{"location":"repo/cv/layers.html#src.cv.layers.aps","title":"<code>aps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2011.14214</p>"},{"location":"repo/cv/layers.html#src.cv.layers.aps.AdaptivePolyphaseSampling","title":"<code>AdaptivePolyphaseSampling(norm=2)</code>","text":"<p>Initializes the class with normalization option.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>int | float | Literal['fro', 'nuc', 'inf', '-inf'] | None</code> <p>Normalization type or value, defaults to 2.</p> <code>2</code> Source code in <code>src/cv/layers/aps.py</code> <pre><code>def __init__(\n    self,\n    norm: int | float | Literal[\"fro\", \"nuc\", \"inf\", \"-inf\"] | None = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes the class with normalization option.\n\n    Args:\n        norm: Normalization type or value, defaults to 2.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n    self.norm = norm\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.aps.AdaptivePolyphaseSampling.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input tensor to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Output tensor, optionally with index if return_index is True.</p> Source code in <code>src/cv/layers/aps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input tensor to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Output tensor, optionally with index if return_index is True.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Extraemos las dimensiones\n    b, p, _, _, _ = polyphase_combined.size()\n\n    # Combinamos los valores de los canales, altura y anchura del tensor\n    polyphase_combined_reshaped = torch.reshape(polyphase_combined, (b, p, -1))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = torch.linalg.vector_norm(\n        input=polyphase_combined_reshaped, ord=self.norm, dim=(-1)\n    )\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.lps","title":"<code>lps</code>","text":"<p>Este clase implementa la capa APS de este paper: https://arxiv.org/abs/2210.08001</p>"},{"location":"repo/cv/layers.html#src.cv.layers.lps.LearnablePolyphaseSampling","title":"<code>LearnablePolyphaseSampling(channel_size, hidden_size)</code>","text":"<p>Initializes the model with specified channel and hidden sizes.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of input channels for the Conv2D layer.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden units for the Conv2D layer.</p> required Source code in <code>src/cv/layers/lps.py</code> <pre><code>def __init__(self, channel_size: int, hidden_size: int) -&gt; None:\n    \"\"\"\n    Initializes the model with specified channel and hidden sizes.\n\n    Args:\n        channel_size: Number of input channels for the Conv2D layer.\n        hidden_size: Number of hidden units for the Conv2D layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self._stride = 2\n\n    # Definimos el modelo \u00fanico para cada componente\n    self.conv_model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=channel_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        ),\n        nn.Flatten(),\n        nn.AdaptiveAvgPool2d(1),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.lps.LearnablePolyphaseSampling.forward","title":"<code>forward(input_tensor, return_index=False)</code>","text":"<p>Processes input to extract dominant polyphase component.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with shape (B, C, H, W).</p> required <code>return_index</code> <code>bool</code> <p>If True, returns index of dominant component.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, Tensor]</code> <p>Tensor of dominant component, optionally with index.</p> Source code in <code>src/cv/layers/lps.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, return_index: bool = False\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Processes input to extract dominant polyphase component.\n\n    Args:\n        input_tensor: Tensor with shape (B, C, H, W).\n        return_index: If True, returns index of dominant component.\n\n    Returns:\n        Tensor of dominant component, optionally with index.\n    \"\"\"\n\n    # Tenemos a la entrada un tensor de (B, C, H, W)\n    # El n\u00famero de componentes polif\u00e1sicas coincide con el tama\u00f1o\n    # de paso elevado al cuadrado, porque nos vemos tanto en la\n    # altura como en la anchura , en total 4\n    poly_a = input_tensor[:, :, :: self._stride, :: self._stride]\n    poly_b = input_tensor[:, :, :: self._stride, 1 :: self._stride]\n    poly_c = input_tensor[:, :, 1 :: self._stride, :: self._stride]\n    poly_d = input_tensor[:, :, 1 :: self._stride, 1 :: self._stride]\n\n    # Combinamos las componentes en un solo tensor (B, P, C, H, W)\n    polyphase_combined = torch.stack((poly_a, poly_b, poly_c, poly_d), dim=1)\n\n    # Utilizamos el modelo basado en convoluciones por cada componente\n    _logits = []\n    for polyphase in range(polyphase_combined.size()[1]):\n        _logits.append(self.conv_model(polyphase_combined[:, polyphase, ...]))\n    logits = torch.squeeze(torch.stack(_logits))\n\n    # Aplicamos la norma a la \u00faltima dimensi\u00f3n\n    polyphase_norms = F.gumbel_softmax(logits, tau=1, hard=False)\n\n    # Seleccionamos el componente polif\u00e1sico de mayor orden\n    polyphase_max_norm = torch.argmax(polyphase_norms)\n\n    # Obtenemos el componente polif\u00e1sico de mayor orden\n    output_tensor = polyphase_combined[:, polyphase_max_norm, ...]\n\n    # En el paper existe la opci\u00f3n de devolver el \u00edndice\n    if return_index:\n        return output_tensor, polyphase_max_norm\n\n    # En caso contrario solo devolvemos el tensor\n    return output_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.se","title":"<code>se</code>","text":"<p>Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507</p>"},{"location":"repo/cv/layers.html#src.cv.layers.se.SqueezeExcitation","title":"<code>SqueezeExcitation(channel_size, ratio)</code>","text":"<p>Implements Squeeze-and-Excitation (SE) block.</p> <p>Parameters:</p> Name Type Description Default <code>channel_size</code> <code>int</code> <p>Number of channels in the input tensor.</p> required <code>ratio</code> <code>int</code> <p>Reduction factor for the compression layer.</p> required Source code in <code>src/cv/layers/se.py</code> <pre><code>def __init__(self, channel_size: int, ratio: int) -&gt; None:\n    \"\"\"\n    Implements Squeeze-and-Excitation (SE) block.\n\n    Args:\n        channel_size: Number of channels in the input tensor.\n        ratio: Reduction factor for the compression layer.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Vamos a crear un modelo Sequential\n    self.se_block = nn.Sequential(\n        nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)\n        nn.Flatten(),  # (B, C)\n        nn.Linear(\n            in_features=channel_size, out_features=channel_size // ratio\n        ),  # (B, C//ratio)\n        nn.ReLU(),  # (B, C//ratio)\n        nn.Linear(\n            in_features=channel_size // ratio, out_features=channel_size\n        ),  # (B, C)\n        nn.Sigmoid(),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.se.SqueezeExcitation.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Applies attention mechanism to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor with shape (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with attention applied, same shape as input.</p> Source code in <code>src/cv/layers/se.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies attention mechanism to input tensor.\n\n    Args:\n        input_tensor: Input tensor with shape (B, C, H, W).\n\n    Returns:\n        Tensor with attention applied, same shape as input.\n    \"\"\"\n\n    # Primero podemos obtener el tama\u00f1o del tensor de entrada\n    b, c, _, _ = input_tensor.size()\n\n    # Obtenemos el tensor de aplicar SE\n    x = self.se_block(input_tensor)\n\n    # Modificamos el shape del tensor para ajustarlo al input\n    x = x.view(b, c, 1, 1)\n\n    # Aplicamos el producto como mecanismo de atenci\u00f3n\n    return x * input_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit","title":"<code>vit</code>","text":""},{"location":"repo/cv/layers.html#src.cv.layers.vit.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Initialize encoder block module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize encoder block module.\n\n    Args:\n        d_model: Number of features in input.\n        d_ff: Hidden layer feature dimensions.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Process input tensor through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after encoder block processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through encoder block.\n\n    Args:\n        input_tensor: Batch of input tensors.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after encoder block processing.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Initialize feed-forward neural network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Input and output feature dimensions.</p> required <code>d_ff</code> <code>int</code> <p>Hidden layer feature dimensions.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize feed-forward neural network.\n\n    Args:\n        d_model: Input and output feature dimensions.\n        d_ff: Hidden layer feature dimensions.\n        dropout_rate: Dropout rate applied on layers.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.GELU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through feed-forward layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after feed-forward processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through feed-forward layers.\n\n    Args:\n        input_tensor: Batch of input tensors.\n\n    Returns:\n        Output tensor after feed-forward processing.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Initialize layer normalization module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-06</code> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initialize layer normalization module.\n\n    Args:\n        features: Number of features in input.\n        eps: Small value to avoid division by zero.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Apply layer normalization to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized embeddings.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply layer normalization to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Normalized embeddings.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Initialize multi-head attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Number of features in input.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on scores.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize multi-head attention module.\n\n    Args:\n        d_model: Number of features in input.\n        h: Number of attention heads.\n        dropout_rate: Dropout rate applied on scores.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Compute attention scores and output.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Dropout layer, optional.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Compute attention scores and output.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n        dropout: Dropout layer, optional.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Process input tensors through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Mask tensor, optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after attention processing.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensors through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Mask tensor, optional.\n\n    Returns:\n        Output tensor after attention processing.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PatchEmbedding","title":"<code>PatchEmbedding(patch_size_height, patch_size_width, in_channels, d_model)</code>","text":"<p>Initialize patch embedding module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    in_channels: int,\n    d_model: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch embedding module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        in_channels: Number of input channels.\n        d_model: Dimension of the model.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.in_channels = in_channels\n    self.d_model = d_model\n\n    # Esta es una de las diferencias con usar transformers en el texto\n    # Aqu\u00ed usamos FFN en vez de Embedding layer, es una proyecci\u00f3n\n    # de los pixeles\n    self.embedding = nn.Linear(\n        in_features=self.in_channels\n        * self.patch_size_height\n        * self.patch_size_width,\n        out_features=self.d_model,\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PatchEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Apply linear projection to input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of image patches as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after linear projection of patches.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Apply linear projection to input tensor.\n\n    Args:\n        input_tensor: Batch of image patches as a tensor.\n\n    Returns:\n        Tensor after linear projection of patches.\n    \"\"\"\n\n    return self.embedding(input_tensor)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.Patches","title":"<code>Patches(patch_size_height, patch_size_width, img_height, img_width)</code>","text":"<p>Initialize patch extraction module.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of the input image.</p> required <code>img_width</code> <code>int</code> <p>Width of the input image.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If img_height not divisible by patch height.</p> <code>ValueError</code> <p>If img_width not divisible by patch width.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n) -&gt; None:\n    \"\"\"\n    Initialize patch extraction module.\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of the input image.\n        img_width: Width of the input image.\n\n    Raises:\n        ValueError: If img_height not divisible by patch height.\n        ValueError: If img_width not divisible by patch width.\n    \"\"\"\n\n    super().__init__()\n\n    if img_height % patch_size_height != 0:\n        raise ValueError(\n            \"img_height tiene que se divisible entre el patch_size_height\"\n        )\n\n    if img_width % patch_size_width != 0:\n        raise ValueError(\n            \"img_width tiene que se divisible entre el patch_size_width\"\n        )\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.unfold = nn.Unfold(\n        kernel_size=(self.patch_size_height, self.patch_size_width),\n        stride=(self.patch_size_height, self.patch_size_width),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.Patches.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Extract patches from input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of images as a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with patches from input images.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Extract patches from input tensor.\n\n    Args:\n        input_tensor: Batch of images as a tensor.\n\n    Returns:\n        Tensor with patches from input images.\n    \"\"\"\n\n    # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n    patches = self.unfold(input_tensor)\n    # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n    return patches.transpose(2, 1)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Initialize positional encoding module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>sequence_length</code> <code>int</code> <p>Max length of input sequences.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied on outputs.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize positional encoding module.\n\n    Args:\n        d_model: Dimension of the model.\n        sequence_length: Max length of input sequences.\n        dropout_rate: Dropout rate applied on outputs.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Add positional encoding to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Batch of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Embeddings with added positional encoding.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Add positional encoding to input embeddings.\n\n    Args:\n        input_embedding: Batch of input embeddings.\n\n    Returns:\n        Embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Initialize residual connection module.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in input.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for sublayer output.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initialize residual connection module.\n\n    Args:\n        features: Number of features in input.\n        dropout_rate: Dropout rate for sublayer output.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Apply residual connection to sublayer output.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Original input tensor.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer module to apply.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Apply residual connection to sublayer output.\n\n    Args:\n        input_tensor: Original input tensor.\n        sublayer: Sublayer module to apply.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.VisionTransformer","title":"<code>VisionTransformer(patch_size_height, patch_size_width, img_height, img_width, in_channels, num_encoders, d_model, d_ff, h, num_classes, dropout_rate)</code>","text":"<p>Initialize Vision Transformer (VIT).</p> <p>Parameters:</p> Name Type Description Default <code>patch_size_height</code> <code>int</code> <p>Height of each patch.</p> required <code>patch_size_width</code> <code>int</code> <p>Width of each patch.</p> required <code>img_height</code> <code>int</code> <p>Height of input images.</p> required <code>img_width</code> <code>int</code> <p>Width of input images.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>d_ff</code> <code>int</code> <p>Dimension of feed-forward layers.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for layers.</p> required Source code in <code>src/cv/layers/vit.py</code> <pre><code>def __init__(\n    self,\n    patch_size_height: int,\n    patch_size_width: int,\n    img_height: int,\n    img_width: int,\n    in_channels: int,\n    num_encoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    num_classes: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initialize Vision Transformer (VIT).\n\n    Args:\n        patch_size_height: Height of each patch.\n        patch_size_width: Width of each patch.\n        img_height: Height of input images.\n        img_width: Width of input images.\n        in_channels: Number of input channels.\n        num_encoders: Number of encoder blocks.\n        d_model: Dimension of the model.\n        d_ff: Dimension of feed-forward layers.\n        h: Number of attention heads.\n        num_classes: Number of output classes.\n        dropout_rate: Dropout rate for layers.\n    \"\"\"\n\n    super().__init__()\n\n    self.patch_size_height = patch_size_height\n    self.patch_size_width = patch_size_width\n    self.img_height = img_height\n    self.img_width = img_width\n    self.in_channels = in_channels\n    self.num_encoders = num_encoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.num_classes = num_classes\n    self.dropout_rate = dropout_rate\n\n    # N\u00famero de patches\n    self.num_patches = (img_height // patch_size_height) * (\n        img_width // patch_size_width\n    )\n\n    # CLS token permite tener una representaci\u00f3n global de todos los inputs\n    # de la imagen (de los diferentes embeddings de cada patch)\n    self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n\n    self.patch_layer = Patches(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        img_height=self.img_height,\n        img_width=self.img_width,\n    )\n\n    self.embeddings = PatchEmbedding(\n        patch_size_height=self.patch_size_height,\n        patch_size_width=self.patch_size_width,\n        in_channels=self.in_channels,\n        d_model=self.d_model,\n    )\n\n    # Entiendo que la longitud de la secuencia coincide con el numero de patches\n    # y un embedding m\u00e1s de la clase,\n    self.positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.num_patches + 1,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    self.layer_norm = LayerNormalization(features=self.d_model)\n\n    self.mlp_classifier = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_model),\n        nn.GELU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=self.d_model, out_features=num_classes),\n    )\n</code></pre>"},{"location":"repo/cv/layers.html#src.cv.layers.vit.VisionTransformer.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Process input tensor through VIT model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Classification output tensor.</p> Source code in <code>src/cv/layers/vit.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input tensor through VIT model.\n\n    Args:\n        input_tensor: Batch of input images.\n\n    Returns:\n        Classification output tensor.\n    \"\"\"\n\n    # Extraemos los patches\n    input_patches = self.patch_layer(input_tensor)\n\n    # Convertimso a embeddings los patches\n    patch_embeddings = self.embeddings(input_patches)\n\n    # Tenemos que a\u00f1adir el token de la clase al inicio de la secuencia\n    # (B, 1, d_model)\n    cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n    # (B, num_patches+1, d_model)\n    embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n\n    # A\u00f1adir positional encoding\n    embeddings = self.positional_encoding(embeddings)\n\n    # Encoders del transformer\n    encoder_output = embeddings\n    for encoder_layer in self.encoder_layers:\n        encoder_output = encoder_layer(encoder_output)\n\n    # Usar solo el CLS token para clasificaci\u00f3n\n    encoder_output = self.layer_norm(encoder_output)\n    cls_output = encoder_output[:, 0]\n\n    # Clasificaci\u00f3n final\n    return self.mlp_classifier(cls_output)\n</code></pre>"},{"location":"repo/cv/models.html","title":"Models","text":""},{"location":"repo/cv/models.html#src.cv.models.vq_vae","title":"<code>vq_vae</code>","text":""},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Decoder","title":"<code>Decoder(in_channels, num_residuals, out_channels=3, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes a decoder with residual blocks and transpose convolutional layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the decoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the decoder.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels, e.g., RGB.</p> <code>3</code> <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    out_channels: int = 3,  # Channel output (RGB)\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes a decoder with residual blocks and transpose\n    convolutional layers.\n\n    Args:\n        in_channels: Number of input channels to the decoder.\n        num_residuals: Number of residual blocks in the decoder.\n        out_channels: Number of output channels, e.g., RGB.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.out_channels = out_channels\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(\n                in_channels=self.in_channels, hidden_size=self.hidden_size\n            )\n            for _ in range(self.num_residuals)\n        ]\n    )\n\n    self.model = nn.Sequential(\n        nn.ConvTranspose2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=self.hidden_size,\n            out_channels=self.out_channels,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=1,\n        ),\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Decoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the decoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by residual blocks and transpose</p> <code>Tensor</code> <p>convolutional layers.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the decoder.\n\n    Args:\n        input_tensor: The input tensor to the decoder.\n\n    Returns:\n        A tensor processed by residual blocks and transpose\n        convolutional layers.\n    \"\"\"\n\n    decoder_output = input_tensor\n    for res_block in self.residual_blocks:\n        decoder_output = res_block(decoder_output)\n\n    return self.model(decoder_output)\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Encoder","title":"<code>Encoder(in_channels, num_residuals, hidden_size=256, kernel_size=4, stride=2)</code>","text":"<p>Initializes an encoder with convolutional layers and residual blocks.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels to the encoder.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in the encoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> <code>256</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride of the convolutional kernels.</p> <code>2</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_residuals: int,\n    hidden_size: int = 256,\n    kernel_size: int = 4,\n    stride: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initializes an encoder with convolutional layers and residual\n    blocks.\n\n    Args:\n        in_channels: Number of input channels to the encoder.\n        num_residuals: Number of residual blocks in the encoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of the convolutional kernels.\n        stride: Stride of the convolutional kernels.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n\n    self.model = nn.Sequential(\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n        nn.Conv2d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1,\n        ),\n    )\n\n    self.residual_blocks = nn.ModuleList(\n        [\n            ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n            for _ in range(self.num_residuals)\n        ]\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.Encoder.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the encoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor processed by convolutional layers and residual</p> <code>Tensor</code> <p>blocks.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the encoder.\n\n    Args:\n        input_tensor: The input tensor to the encoder.\n\n    Returns:\n        A tensor processed by convolutional layers and residual\n        blocks.\n    \"\"\"\n\n    encoder_output = self.model(input_tensor)\n    for res_block in self.residual_blocks:\n        encoder_output = res_block(encoder_output)\n    return encoder_output\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.ResidualBlock","title":"<code>ResidualBlock(in_channels, hidden_size=256)</code>","text":"<p>Initializes a residual block that applies two convolutional layers and ReLU activations.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the block.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in the hidden layer.</p> <code>256</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(self, in_channels: int, hidden_size: int = 256) -&gt; None:\n    \"\"\"\n    Initializes a residual block that applies two convolutional\n    layers and ReLU activations.\n\n    Args:\n        in_channels: Number of input channels for the block.\n        hidden_size: Number of channels in the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.hidden_size = hidden_size\n\n    self.res_block = nn.Sequential(\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.in_channels,\n            out_channels=self.hidden_size,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        ),\n        nn.ReLU(),\n        nn.Conv2d(\n            in_channels=self.hidden_size,\n            out_channels=self.in_channels,\n            kernel_size=1,\n            stride=1,\n            bias=False,\n        ),\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.ResidualBlock.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the residual block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor to the block.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the sum of the input tensor and the</p> <code>Tensor</code> <p>block's output.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the residual block.\n\n    Args:\n        input_tensor: The input tensor to the block.\n\n    Returns:\n        A tensor that is the sum of the input tensor and the\n        block's output.\n    \"\"\"\n\n    return input_tensor + self.res_block(input_tensor)\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VQVAE","title":"<code>VQVAE(in_channels, size_discrete_space, size_embeddings, num_residuals, hidden_size, kernel_size, stride, beta=0.25)</code>","text":"<p>Initializes a VQ-VAE model with encoder, decoder, and quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels for the model.</p> required <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>num_residuals</code> <code>int</code> <p>Number of residual blocks in encoder/decoder.</p> required <code>hidden_size</code> <code>int</code> <p>Number of channels in hidden layers.</p> required <code>kernel_size</code> <code>int</code> <p>Size of convolutional kernels.</p> required <code>stride</code> <code>int</code> <p>Stride of convolutional kernels.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    size_discrete_space: int,\n    size_embeddings: int,\n    num_residuals: int,\n    hidden_size: int,\n    kernel_size: int,\n    stride: int,\n    beta: float = 0.25,\n) -&gt; None:\n    \"\"\"\n    Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n\n    Args:\n        in_channels: Number of input channels for the model.\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        num_residuals: Number of residual blocks in encoder/decoder.\n        hidden_size: Number of channels in hidden layers.\n        kernel_size: Size of convolutional kernels.\n        stride: Stride of convolutional kernels.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.num_residuals = num_residuals\n    self.hidden_size = hidden_size\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.beta = beta\n\n    self.encoder = Encoder(\n        in_channels=self.in_channels,\n        num_residuals=self.num_residuals,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n    self.decoder = Decoder(\n        in_channels=self.hidden_size,\n        num_residuals=self.num_residuals,\n        out_channels=self.in_channels,\n        hidden_size=self.hidden_size,\n        kernel_size=self.kernel_size,\n        stride=self.stride,\n    )\n\n    self.vector_quantizer = VectorQuantizer(\n        size_discrete_space=self.size_discrete_space,\n        size_embeddings=self.hidden_size,\n        beta=self.beta,\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VQVAE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through VQ-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, reconstructed tensor,</p> <code>Tensor</code> <p>and perplexity.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass through VQ-VAE model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        A tuple containing VQ loss, reconstructed tensor,\n        and perplexity.\n    \"\"\"\n\n    encoder_output = self.encoder(input_tensor)\n    vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n    decoder_output = self.decoder(quantized)\n    return vq_loss, decoder_output, perplexity\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VectorQuantizer","title":"<code>VectorQuantizer(size_discrete_space, size_embeddings, beta=0.25)</code>","text":"<p>Initializes a vector quantizer with a learnable codebook.</p> <p>Parameters:</p> Name Type Description Default <code>size_discrete_space</code> <code>int</code> <p>Number of discrete embeddings.</p> required <code>size_embeddings</code> <code>int</code> <p>Size of each embedding vector.</p> required <code>beta</code> <code>float</code> <p>Weighting factor for the commitment loss.</p> <code>0.25</code> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def __init__(\n    self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n) -&gt; None:\n    \"\"\"\n    Initializes a vector quantizer with a learnable codebook.\n\n    Args:\n        size_discrete_space: Number of discrete embeddings.\n        size_embeddings: Size of each embedding vector.\n        beta: Weighting factor for the commitment loss.\n    \"\"\"\n\n    super().__init__()\n\n    self.size_discrete_space = size_discrete_space\n    self.size_embeddings = size_embeddings\n    self.beta = beta\n\n    # Definimos el codebook como una matriz de K embeddings x D tama\u00f1o de embeddings\n    # Ha de ser una matriz aprendible\n    self.codebook = nn.Embedding(\n        num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n    )\n    # Initialize weights uniformly\n    self.codebook.weight.data.uniform_(\n        -1 / self.size_discrete_space, 1 / self.size_discrete_space\n    )\n</code></pre>"},{"location":"repo/cv/models.html#src.cv.models.vq_vae.VectorQuantizer.forward","title":"<code>forward(encoder_output)</code>","text":"<p>Quantizes the encoder output using the codebook.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_output</code> <code>Tensor</code> <p>Tensor of encoder outputs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing VQ loss, quantized tensor, perplexity,</p> <code>Tensor</code> <p>and encodings.</p> Source code in <code>src/cv/models/vq_vae.py</code> <pre><code>def forward(\n    self, encoder_output: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the encoder output using the codebook.\n\n    Args:\n        encoder_output: Tensor of encoder outputs.\n\n    Returns:\n        A tuple containing VQ loss, quantized tensor, perplexity,\n        and encodings.\n    \"\"\"\n\n    # Comentario de otras implementaciones: The channels are used as the space\n    # in which to quantize.\n    # Encoder output -&gt;  (B, C, H, W) -&gt; (0, 1, 2, 3) -&gt; (0, 2, 3, 1) -&gt; (0*2*3, 1)\n    encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n    b, h, w, c = encoder_output.size()\n    encoder_output_flat = encoder_output.reshape(-1, c)\n\n    # Calculamos la distancia entre ambos vectores\n    distances = (\n        torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n        + torch.sum(self.codebook.weight**2, dim=1)\n        - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n    )\n\n    # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n    encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n\n    # Matriz de ceros de (indices, size_discrete_space)\n    encodings = torch.zeros(\n        encoding_indices.shape[0],\n        self.size_discrete_space,\n        device=encoder_output.device,\n    )\n    # Colocamos un 1 en los indices de los encodings con el\n    # valor m\u00ednimo de distancia creando un vector one-hot\n    encodings.scatter_(1, encoding_indices, 1)\n\n    # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n    # del codebook y le damos formato de nuevo al tensor\n    quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n\n    # VQ-VAE loss terms\n    # L = ||sg[z_e] - e||^2 + \u03b2||z_e - sg[e]||^2\n    # FIX: Corrected variable names and loss calculation\n    commitment_loss = F.mse_loss(\n        quantized.detach(), encoder_output\n    )  # ||sg[z_e] - e||^2\n    embedding_loss = F.mse_loss(\n        quantized, encoder_output.detach()\n    )  # ||z_e - sg[e]||^2\n    vq_loss = commitment_loss + self.beta * embedding_loss\n\n    # Straight-through estimator\n    quantized = encoder_output + (quantized - encoder_output).detach()\n\n    # Calculate perplexity\n    avg_probs = torch.mean(encodings, dim=0)\n    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n    # convert quantized from BHWC -&gt; BCHW\n    return (\n        vq_loss,\n        quantized.permute(0, 3, 1, 2).contiguous(),\n        perplexity,\n        encodings,\n    )\n</code></pre>"},{"location":"repo/generative/models.html","title":"Models","text":""},{"location":"repo/generative/models.html#src.generative.models.gan","title":"<code>gan</code>","text":""},{"location":"repo/generative/models.html#src.generative.models.gan.show_generated_samples","title":"<code>show_generated_samples(generator, noise, device, num_samples=16)</code>","text":"<p>Funci\u00f3n auxiliar para mostrar muestras generadas</p> Source code in <code>src/generative/models/gan.py</code> <pre><code>def show_generated_samples(\n    generator: nn.Module, noise, device: str, num_samples: int = 16\n) -&gt; None:\n    \"\"\"Funci\u00f3n auxiliar para mostrar muestras generadas\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        samples = generator(noise[:num_samples]).cpu()\n        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(num_samples):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n            axes[row, col].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"repo/nlp/layers.html","title":"Layers","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.moe","title":"<code>moe</code>","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.ExpertModel","title":"<code>ExpertModel(input_dim, output_dim, hidden_dim)</code>","text":"<p>Modelo experto individual para MoE</p> <p>Initializes an expert model with a simple feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output data.</p> required <code>hidden_dim</code> <code>int</code> <p>Dimensionality of the hidden layer.</p> required Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:\n    \"\"\"\n    Initializes an expert model with a simple feed-forward network.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        output_dim: Dimensionality of the output data.\n        hidden_dim: Dimensionality of the hidden layer.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.ExpertModel.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the expert model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's output tensor.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the expert model.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        The model's output tensor.\n    \"\"\"\n\n    return self.model(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.Gating","title":"<code>Gating(input_dim, num_experts, dropout_rate=0.2)</code>","text":"<p>Gating mechanism to select experts.</p> <p>Initializes a gating network for expert selection.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts to select from.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> <code>0.2</code> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(\n    self, input_dim: int, num_experts: int, dropout_rate: float = 0.2\n) -&gt; None:\n    \"\"\"\n    Initializes a gating network for expert selection.\n\n    Args:\n        input_dim: Dimensionality of the input data.\n        num_experts: Number of experts to select from.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.num_experts = num_experts\n    self.dropout_rate = dropout_rate\n\n    self.model = nn.Sequential(\n        nn.Linear(in_features=self.input_dim, out_features=128),\n        nn.Dropout(self.dropout_rate),\n        nn.LeakyReLU(),\n        nn.Linear(in_features=128, out_features=256),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=256, out_features=128),\n        nn.LeakyReLU(),\n        nn.Dropout(self.dropout_rate),\n        nn.Linear(in_features=128, out_features=num_experts),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.Gating.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the gating network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the network.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Softmax probabilities for expert selection.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the gating network.\n\n    Args:\n        input_tensor: Input tensor to the network.\n\n    Returns:\n        Softmax probabilities for expert selection.\n    \"\"\"\n\n    return F.softmax(self.model(input_tensor), dim=-1)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.MoE","title":"<code>MoE(trained_experts, input_dim, dropout_rate=0.2)</code>","text":"<p>Mixture of Experts</p> <p>Initializes a mixture of experts with gating.</p> <p>Parameters:</p> Name Type Description Default <code>trained_experts</code> <code>list[ExpertModel]</code> <p>List of trained expert models.</p> required <code>input_dim</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout in the gating network.</p> <code>0.2</code> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def __init__(\n    self,\n    trained_experts: list[ExpertModel],\n    input_dim: int,\n    dropout_rate: float = 0.2,\n) -&gt; None:\n    \"\"\"\n    Initializes a mixture of experts with gating.\n\n    Args:\n        trained_experts: List of trained expert models.\n        input_dim: Dimensionality of the input data.\n        dropout_rate: Rate of dropout in the gating network.\n    \"\"\"\n\n    super().__init__()\n\n    self.experts = nn.ModuleList(trained_experts)\n    self.num_experts = len(trained_experts)\n    self.input_dim = input_dim\n    self.dropout_rate = dropout_rate\n\n    self.gating_layer = Gating(\n        input_dim=self.input_dim,\n        num_experts=self.num_experts,\n        dropout_rate=self.dropout_rate,\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.moe.MoE.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the mixture of experts.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Weighted sum of expert outputs.</p> Source code in <code>src/nlp/layers/moe.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the mixture of experts.\n\n    Args:\n        input_tensor: Input tensor to the model.\n\n    Returns:\n        Weighted sum of expert outputs.\n    \"\"\"\n\n    # Obtenemos los pesos del selector\n    expert_weights = self.gating_layer(input_tensor)\n\n    # Obtenemos la salida de todos los expertos\n    _expert_outputs: list[torch.Tensor] = []\n    for expert in self.experts:\n        _expert_outputs.append(expert(input_tensor))\n\n    # Stack de salidas [b, output_dim, num_experts]\n    expert_outputs = torch.stack(_expert_outputs, dim=-1)\n\n    # [b, num_experts] -&gt; [b, 1, num_experts]\n    expert_weights = expert_weights.unsqueeze(1)\n\n    # Suma ponderada de la selecci\u00f3n de expertos\n    # [b, output_dim, num_experts] * [b, 1, num_experts]\n    return torch.sum(expert_outputs * expert_weights, dim=-1)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer","title":"<code>transformer</code>","text":""},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.DecoderBlock","title":"<code>DecoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Decoder block with masked attention, cross-attention, and feed-forward layers.</p> <p>Initializes decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes decoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    self.masked_multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_3 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.DecoderBlock.forward","title":"<code>forward(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Forward pass through decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder block.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the decoder block.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,  # M\u00e1scara para el encoder (padding)\n    tgt_mask: torch.Tensor | None = None,  # M\u00e1scara causal para el decoder\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through decoder block.\n\n    Args:\n        decoder_input: Input tensor to the decoder block.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Tensor after processing by the decoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    decoder_input = self.residual_layer_1(\n        decoder_input,\n        lambda x: self.masked_multi_head_attention_layer(\n            k=x, q=x, v=x, mask=tgt_mask\n        ),\n    )\n\n    # Aqu\u00ed tenemos que hacer cross-attention, usamos como K, V los encoder\n    # y Q del decoder\n    decoder_input = self.residual_layer_2(\n        decoder_input,\n        lambda x: self.multi_head_attention_layer(\n            k=encoder_output, q=x, v=encoder_output, mask=src_mask\n        ),\n    )\n\n    decoder_output = self.residual_layer_3(\n        decoder_input, lambda x: self.feed_forward_layer(x)\n    )\n\n    return decoder_output\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.EncoderBlock","title":"<code>EncoderBlock(d_model, d_ff, h, dropout_rate)</code>","text":"<p>Encoder block with attention and feed-forward layers.</p> <p>Initializes encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes encoder block.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Parametros\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Definicion de las capas\n    self.multi_head_attention_layer = MultiHeadAttention(\n        d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_1 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n    self.feed_forward_layer = FeedForward(\n        d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n    )\n    self.residual_layer_2 = ResidualConnection(\n        features=d_model, dropout_rate=self.dropout_rate\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.EncoderBlock.forward","title":"<code>forward(input_tensor, mask=None)</code>","text":"<p>Forward pass through encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the encoder block.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after processing by the encoder block.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through encoder block.\n\n    Args:\n        input_tensor: Input tensor to the encoder block.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after processing by the encoder block.\n    \"\"\"\n\n    # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n    input_tensor = self.residual_layer_1(\n        input_tensor,\n        lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n    )\n\n    # Segunda conexi\u00f3n residual con feed-forward\n    input_tensor = self.residual_layer_2(\n        input_tensor, lambda x: self.feed_forward_layer(x)\n    )\n\n    return input_tensor\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.FeedForward","title":"<code>FeedForward(d_model, d_ff, dropout_rate)</code>","text":"<p>Feed-forward neural network layer.</p> <p>Initializes feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes feed-forward network.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.d_ff = d_ff\n\n    # Creamos el modelo secuencial\n    self.ffn = nn.Sequential(\n        nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n        nn.ReLU(),\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.FeedForward.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor processed by feed-forward network.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through feed-forward network.\n\n    Args:\n        input_tensor: Tensor of input embeddings.\n\n    Returns:\n        Tensor processed by feed-forward network.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    return self.ffn(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.InputEmbedding","title":"<code>InputEmbedding(d_model, vocab_size)</code>","text":"<p>Embeds input tokens into vectors of dimension d_model.</p> <p>Initializes input embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes input embedding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    # Utilizamos la capa Embedding de PyTorch que funciona como\n    # una tabal lookup that stores embeddings of a fixed dictionary and size.\n    # Osea que es un diccionario que tiene por cada token, hasta un total de\n    # vocab_size, un vector de tama\u00f1o d_model. En el paper: we use learned\n    # embeddings to convert the input tokens and output tokens to vectors\n    # of dimension dmodel\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.InputEmbedding.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor of token indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embedded input scaled by sqrt(d_model).</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the embedding layer.\n\n    Args:\n        input_tensor: Input tensor of token indices.\n\n    Returns:\n        Tensor of embedded input scaled by sqrt(d_model).\n    \"\"\"\n\n    # Paper: In the embedding layers, we multiply those weights by sqrt(d_model)\n    # Input_tensor (B, ...) -&gt; (B, ..., d_model)\n    return self.embedding(input_tensor) * math.sqrt(self.d_model)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.LayerNormalization","title":"<code>LayerNormalization(features, eps=1e-06)</code>","text":"<p>Applies layer normalization to input embeddings.</p> <p>Initializes layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, features: int, eps: float = 1e-6) -&gt; None:\n    \"\"\"\n    Initializes layer normalization.\n\n    Args:\n        features: Number of features in the input.\n        eps: Small constant for numerical stability.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.features = features\n    self.eps = eps\n\n    # Utilizamos un factor alpha para multiplicar el valor de la normalizaci\u00f3n\n    self.alpha = nn.Parameter(torch.ones(self.features))\n    # Utilizamos un factor del sesgo para sumar\n    self.bias = nn.Parameter(torch.zeros(self.features))\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.LayerNormalization.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass for layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for layer normalization.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n\n    # (B, sequence_length, d_model)\n    mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n    var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n    return (\n        self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n        + self.bias\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention","title":"<code>MultiHeadAttention(d_model, h, dropout_rate)</code>","text":"<p>Applies multi-head attention mechanism.</p> <p>Initializes multi-head attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, h: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes multi-head attention layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # el tamalo de los embeddings debe ser proporcional al n\u00famero de cabezas\n    # para realizar la divisi\u00f3n, por lo que es el resto ha de ser 0\n    if d_model % h != 0:\n        raise ValueError(\"d_model ha de ser divisible entre h\")\n\n    self.d_model = d_model\n    self.h = h\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Valore establecidos en el paper\n    self.d_k = self.d_model // self.h\n    self.d_v = self.d_model // self.h\n\n    # Par\u00e1metros\n    self.W_K = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_Q = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_V = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n    self.W_OUTPUT_CONCAT = nn.Linear(\n        in_features=self.d_model, out_features=self.d_model, bias=False\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention.attention","title":"<code>attention(k, q, v, mask=None, dropout=None)</code>  <code>staticmethod</code>","text":"<p>Computes scaled dot-product attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <code>dropout</code> <code>Dropout | None</code> <p>Optional dropout layer.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of attention output and scores.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>@staticmethod\ndef attention(\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: nn.Dropout | None = None,\n):\n    \"\"\"\n    Computes scaled dot-product attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n        dropout: Optional dropout layer.\n\n    Returns:\n        Tuple of attention output and scores.\n    \"\"\"\n\n    # Primero realizamos el producto matricial con la transpuesta\n    # q = (Batch, h, seq_len, d_k)\n    # k.T = (Batch, h, d_k, seq_len)\n    # matmul_q_k = (Batch, h, seq_len, seq_len)\n    matmul_q_k = q @ k.transpose(-2, -1)\n\n    # Luego realizamos el escalado\n    d_k = k.shape[-1]\n    matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n\n    # El enmascarado es para el decoder, relleno de infinitos\n    if mask is not None:\n        matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n\n    # Obtenemos los scores/puntuaci\u00f3n de la atenci\u00f3n\n    attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n\n    # Aplicamos dropout\n    if dropout is not None:\n        attention_scores = dropout(attention_scores)\n\n    # Multiplicamos por el valor\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    # v = (Batch, h, seq_len, d_k)\n    # Output = (Batch, h, seq_len, d_k)\n    return (attention_scores @ v), attention_scores\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.MultiHeadAttention.forward","title":"<code>forward(k, q, v, mask=None)</code>","text":"<p>Forward pass through multi-head attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>Key tensor.</p> required <code>q</code> <code>Tensor</code> <p>Query tensor.</p> required <code>v</code> <code>Tensor</code> <p>Value tensor.</p> required <code>mask</code> <code>Tensor | None</code> <p>Optional mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after attention and concatenation.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    q: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through multi-head attention.\n\n    Args:\n        k: Key tensor.\n        q: Query tensor.\n        v: Value tensor.\n        mask: Optional mask tensor.\n\n    Returns:\n        Tensor after attention and concatenation.\n    \"\"\"\n\n    # k -&gt; (Batch, seq_len, d_model) igual para el resto\n    key_prima = self.W_K(k)\n    query_prima = self.W_Q(q)\n    value_prima = self.W_V(v)\n\n    # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n    # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n    # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n    key_prima = key_prima.view(\n        key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    query_prima = query_prima.view(\n        query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n    value_prima = value_prima.view(\n        value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n    ).transpose(1, 2)\n\n    # Obtenemos la matriz de atencion y la puntuaci\u00f3n\n    # attention = (Batch, h, seq_len, d_k)\n    # attention_scores = (Batch, h, seq_len, seq_len)\n    attention, attention_scores = MultiHeadAttention.attention(\n        k=key_prima,\n        q=query_prima,\n        v=value_prima,\n        mask=mask,\n        dropout=self.dropout,\n    )\n\n    # Tenemos que concatenar la informaci\u00f3n de todas las cabezas\n    # Queremos (Batch, seq_len, d_model)\n    # self.d_k = self.d_model // self.h; d_model = d_k * h\n    attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n    b, seq_len, h, d_k = attention.size()\n    # Al parecer, contiguous permite evitar errores de memoria\n    attention_concat = attention.contiguous().view(\n        b, seq_len, h * d_k\n    )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n\n    return self.W_OUTPUT_CONCAT(attention_concat)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.PositionalEncoding","title":"<code>PositionalEncoding(d_model, sequence_length, dropout_rate)</code>","text":"<p>Adds positional encoding to input embeddings.</p> <p>Initializes positional encoding layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of the embedding vectors.</p> required <code>sequence_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes positional encoding layer.\n\n    Args:\n        d_model: Dimensionality of the embedding vectors.\n        sequence_length: Maximum sequence length.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    # Constructor de la clase\n    super().__init__()\n\n    # Definimos los par\u00e1metros de la clase\n    self.d_model = d_model\n\n    # Cuando le damos una secuencia de tokens, tenemos que saber\n    # la longitud m\u00e1xima de la secuencia\n    self.sequence_length = sequence_length\n    self.dropout = nn.Dropout(dropout_rate)\n\n    # Creamos una matriz del positional embedding\n    # (sequence_length, d_model)\n    pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n\n    # # Ahora rellenamos la matriz de posiciones\n    # # La posici\u00f3n va hasta el m\u00e1ximo de la longitud de la secuencia\n    # for pos in range(self.sequence_length):\n    # \tfor i in range(0, d_model, 2):\n    # \t\t# Para las posiciones pares usamos el seno\n    # \t\tpe_matrix[pos, i] = torch.sin(pos / (10000 ** ((2 * i) / d_model)))\n    # \t\t# Para las posiciones impares usamos el coseno\n    # \t\tpe_matrix[pos, i + 1] = torch.cos(\n    # \t\t\tpos / (10000 ** ((2 * (i + 1)) / d_model))\n    # \t\t)\n\n    # Crear vector de posiciones\n    position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n\n    # Crear vector de divisores\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n\n    # Aplicar sin y cos\n    pe_matrix[:, 0::2] = torch.sin(position * div_term)\n    pe_matrix[:, 1::2] = torch.cos(position * div_term)\n\n    # Tenemos que convertirlo a (1, sequence_length, d_model) para\n    # procesarlo por lotes\n    pe_matrix = pe_matrix.unsqueeze(0)\n\n    # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n    self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.PositionalEncoding.forward","title":"<code>forward(input_embedding)</code>","text":"<p>Forward pass to add positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>input_embedding</code> <code>Tensor</code> <p>Tensor of input embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embeddings with added positional encoding.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_embedding: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass to add positional encoding.\n\n    Args:\n        input_embedding: Tensor of input embeddings.\n\n    Returns:\n        Tensor of embeddings with added positional encoding.\n    \"\"\"\n\n    # (B, ..., d_model) -&gt; (B, sequence_length, d_model)\n    # Seleccionamos\n    x = input_embedding + (\n        self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n    ).requires_grad_(False)\n    return self.dropout(x)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ProjectionLayer","title":"<code>ProjectionLayer(d_model, vocab_size)</code>","text":"<p>Converts d_model dimensions back to vocab_size.</p> <p>Initializes projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, d_model: int, vocab_size: int) -&gt; None:\n    \"\"\"\n    Initializes projection layer.\n\n    Args:\n        d_model: Dimensionality of model embeddings.\n        vocab_size: Size of the vocabulary.\n    \"\"\"\n\n    super().__init__()\n\n    self.d_model = d_model\n    self.vocab_size = vocab_size\n\n    self.projection_layer = nn.Linear(in_features=d_model, out_features=vocab_size)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ProjectionLayer.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Forward pass through projection layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the projection layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with projected dimensions.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through projection layer.\n\n    Args:\n        input_tensor: Input tensor to the projection layer.\n\n    Returns:\n        Tensor with projected dimensions.\n    \"\"\"\n\n    return self.projection_layer(input_tensor)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ResidualConnection","title":"<code>ResidualConnection(features, dropout_rate)</code>","text":"<p>Applies residual connection around a sublayer.</p> <p>Initializes residual connection layer.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features in the input.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(self, features: int, dropout_rate: float) -&gt; None:\n    \"\"\"\n    Initializes residual connection layer.\n\n    Args:\n        features: Number of features in the input.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_rate)\n    self.layer_norm = LayerNormalization(features=features)\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.ResidualConnection.forward","title":"<code>forward(input_tensor, sublayer)</code>","text":"<p>Forward pass using residual connection.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to the residual layer.</p> required <code>sublayer</code> <code>Module</code> <p>Sublayer to apply within the residual connection.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with residual connection applied.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using residual connection.\n\n    Args:\n        input_tensor: Input tensor to the residual layer.\n        sublayer: Sublayer to apply within the residual connection.\n\n    Returns:\n        Tensor with residual connection applied.\n    \"\"\"\n\n    return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer","title":"<code>Transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, num_encoders, num_decoders, d_model, d_ff, h, dropout_rate)</code>","text":"<p>Transformer model with encoder and decoder blocks.</p> <p>Initializes transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>src_vocab_size</code> <code>int</code> <p>Size of source vocabulary.</p> required <code>tgt_vocab_size</code> <code>int</code> <p>Size of target vocabulary.</p> required <code>src_seq_len</code> <code>int</code> <p>Maximum source sequence length.</p> required <code>tgt_seq_len</code> <code>int</code> <p>Maximum target sequence length.</p> required <code>num_encoders</code> <code>int</code> <p>Number of encoder blocks.</p> required <code>num_decoders</code> <code>int</code> <p>Number of decoder blocks.</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of model embeddings.</p> required <code>d_ff</code> <code>int</code> <p>Dimensionality of feed-forward layer.</p> required <code>h</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Rate of dropout for regularization.</p> required Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def __init__(\n    self,\n    src_vocab_size: int,\n    tgt_vocab_size: int,\n    src_seq_len: int,\n    tgt_seq_len: int,\n    num_encoders: int,\n    num_decoders: int,\n    d_model: int,\n    d_ff: int,\n    h: int,\n    dropout_rate: float,\n) -&gt; None:\n    \"\"\"\n    Initializes transformer model.\n\n    Args:\n        src_vocab_size: Size of source vocabulary.\n        tgt_vocab_size: Size of target vocabulary.\n        src_seq_len: Maximum source sequence length.\n        tgt_seq_len: Maximum target sequence length.\n        num_encoders: Number of encoder blocks.\n        num_decoders: Number of decoder blocks.\n        d_model: Dimensionality of model embeddings.\n        d_ff: Dimensionality of feed-forward layer.\n        h: Number of attention heads.\n        dropout_rate: Rate of dropout for regularization.\n    \"\"\"\n\n    super().__init__()\n\n    # Par\u00e1metros\n    self.src_vocab_size = src_vocab_size\n    self.tgt_vocab_size = tgt_vocab_size\n    self.src_seq_len = src_seq_len\n    self.tgt_seq_len = tgt_seq_len\n    self.num_encoders = num_encoders\n    self.num_decoders = num_decoders\n    self.d_model = d_model\n    self.d_ff = d_ff\n    self.h = h\n    self.dropout_rate = dropout_rate\n\n    # Embeddings y Positional Encoding\n    self.src_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.src_vocab_size\n    )\n    self.tgt_embedding = InputEmbedding(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n    self.src_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.src_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n    self.tgt_positional_encoding = PositionalEncoding(\n        d_model=self.d_model,\n        sequence_length=self.tgt_seq_len,\n        dropout_rate=self.dropout_rate,\n    )\n\n    # Capas del Encoder\n    self.encoder_layers = nn.ModuleList(\n        [\n            EncoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_encoders)\n        ]\n    )\n\n    # Capas del Decoder\n    self.decoder_layers = nn.ModuleList(\n        [\n            DecoderBlock(\n                d_model=self.d_model,\n                d_ff=self.d_ff,\n                h=self.h,\n                dropout_rate=self.dropout_rate,\n            )\n            for _ in range(self.num_decoders)\n        ]\n    )\n\n    # Capa de proyecci\u00f3n final\n    self.projection_layer = ProjectionLayer(\n        d_model=self.d_model, vocab_size=self.tgt_vocab_size\n    )\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.decode","title":"<code>decode(decoder_input, encoder_output, src_mask=None, tgt_mask=None)</code>","text":"<p>Decodes target input tensor using decoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_input</code> <code>Tensor</code> <p>Input tensor to the decoder.</p> required <code>encoder_output</code> <code>Tensor</code> <p>Output tensor from the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional target mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def decode(\n    self,\n    decoder_input: torch.Tensor,\n    encoder_output: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Decodes target input tensor using decoder blocks.\n\n    Args:\n        decoder_input: Input tensor to the decoder.\n        encoder_output: Output tensor from the encoder.\n        src_mask: Optional source mask tensor.\n        tgt_mask: Optional target mask tensor.\n\n    Returns:\n        Decoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.tgt_embedding(decoder_input)\n    x = self.tgt_positional_encoding(x)\n\n    # Pasar por todas las capas del decoder\n    for decoder_layer in self.decoder_layers:\n        x = decoder_layer(\n            decoder_input=x,\n            encoder_output=encoder_output,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n        )\n\n    return x\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.encode","title":"<code>encode(encoder_input, src_mask=None)</code>","text":"<p>Encodes source input tensor using encoder blocks.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_input</code> <code>Tensor</code> <p>Input tensor to the encoder.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional source mask tensor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded tensor.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def encode(\n    self, encoder_input: torch.Tensor, src_mask: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Encodes source input tensor using encoder blocks.\n\n    Args:\n        encoder_input: Input tensor to the encoder.\n        src_mask: Optional source mask tensor.\n\n    Returns:\n        Encoded tensor.\n    \"\"\"\n\n    # Aplicar embedding y positional encoding\n    x = self.src_embedding(encoder_input)\n    x = self.src_positional_encoding(x)\n\n    # Pasar por todas las capas del encoder\n    for encoder_layer in self.encoder_layers:\n        x = encoder_layer(input_tensor=x, mask=src_mask)\n\n    return x\n</code></pre>"},{"location":"repo/nlp/layers.html#src.nlp.layers.transformer.Transformer.forward","title":"<code>forward(src, tgt, src_mask=None, tgt_mask=None)</code>","text":"<p>Processes input and target sequences through the encoder and decoder, applying optional source and target masks.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input sequence tensor.</p> required <code>tgt</code> <code>Tensor</code> <p>Target sequence tensor.</p> required <code>src_mask</code> <code>Tensor | None</code> <p>Optional mask for the input sequence.</p> <code>None</code> <code>tgt_mask</code> <code>Tensor | None</code> <p>Optional mask for the target sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing the final output after projection.</p> Source code in <code>src/nlp/layers/transformer.py</code> <pre><code>def forward(\n    self,\n    src: torch.Tensor,\n    tgt: torch.Tensor,\n    src_mask: torch.Tensor | None = None,\n    tgt_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Processes input and target sequences through the encoder\n    and decoder, applying optional source and target masks.\n\n    Args:\n        src: Input sequence tensor.\n        tgt: Target sequence tensor.\n        src_mask: Optional mask for the input sequence.\n        tgt_mask: Optional mask for the target sequence.\n\n    Returns:\n        Tensor containing the final output after projection.\n    \"\"\"\n\n    # Encoder\n    encoder_output = self.encode(src, src_mask)\n\n    # Decoder\n    decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n\n    # Projection\n    return self.projection_layer(decoder_output)\n</code></pre>"},{"location":"repo/nlp/utils.html","title":"Utilities","text":""},{"location":"repo/nlp/utils.html#src.nlp.utils","title":"<code>utils</code>","text":""},{"location":"repo/nlp/utils.html#src.nlp.utils.create_causal_mask","title":"<code>create_causal_mask(size)</code>","text":"<p>Creates a causal mask to prevent the decoder from attending to future tokens during training.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Length of the sequence.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Causal mask of shape (size, size).</p> Source code in <code>src/nlp/utils.py</code> <pre><code>def create_causal_mask(size: int) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a causal mask to prevent the decoder from attending\n    to future tokens during training.\n\n    Args:\n        size: Length of the sequence.\n\n    Returns:\n        Causal mask of shape (size, size).\n    \"\"\"\n\n    return torch.tril(torch.ones(size, size))\n</code></pre>"},{"location":"repo/nlp/utils.html#src.nlp.utils.create_padding_mask","title":"<code>create_padding_mask(seq, pad_token=0)</code>","text":"<p>Creates a mask to ignore padding tokens in a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Tensor</code> <p>Sequence of tokens, shape (B, seq_len).</p> required <code>pad_token</code> <code>int</code> <p>Padding token value.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Padding mask of shape (B, 1, 1, seq_len).</p> Source code in <code>src/nlp/utils.py</code> <pre><code>def create_padding_mask(seq: torch.Tensor, pad_token: int = 0) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a mask to ignore padding tokens in a sequence.\n\n    Args:\n        seq: Sequence of tokens, shape (B, seq_len).\n        pad_token: Padding token value.\n\n    Returns:\n        Padding mask of shape (B, 1, 1, seq_len).\n    \"\"\"\n\n    return (seq != pad_token).unsqueeze(1).unsqueeze(1)\n</code></pre>"}]}