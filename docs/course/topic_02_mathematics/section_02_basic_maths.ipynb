{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Mathematics and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, some fundamental concepts of differential calculus applied to machine\n",
    "learning are introduced, illustrating how PyTorch allows calculating gradients\n",
    "automatically through its _autograd_ system. The objective is to connect the traditional\n",
    "mathematical formulation (symbolic calculus) with practical implementation in code, and\n",
    "show how these gradients are used in typical tasks such as linear regression, logistic\n",
    "regression, or multiclass classification.\n",
    "\n",
    "The central idea is as follows: A differentiable function is defined that depends on one\n",
    "or more tensors with `requires_grad=True`, a scalar value is calculated from them, and\n",
    "`backward()` is invoked. From that moment, PyTorch traverses the computational graph it\n",
    "has built internally and calculates the partial derivatives of the scalar output with\n",
    "respect to each of the differentiable inputs, storing them in the `.grad` attribute of\n",
    "the corresponding tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Calculation: PyTorch versus SymPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the parallelism between symbolic calculus and automatic differentiation,\n",
    "consider the scalar function of two variables:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 3 x_1 x_2 + x_2^2.\n",
    "$$\n",
    "\n",
    "In PyTorch, a tensor `x` with two components is defined and gradient tracking is\n",
    "activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import sympy as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "# Create input tensor with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Define the differentiable function: f(x1, x2) = x1^2 + 3*x1*x2 + x2^2\n",
    "y = x[0] ** 2 + 3 * x[0] * x[1] + x[1] ** 2\n",
    "\n",
    "# Calculate gradients\n",
    "y.backward()\n",
    "\n",
    "# Gradients with respect to each input\n",
    "grad_x1 = x.grad[0]  # ∂f/∂x1\n",
    "grad_x2 = x.grad[1]  # ∂f/∂x2\n",
    "\n",
    "print(\"PyTorch gradients:\")\n",
    "print(\"Gradient ∂f/∂x1:\", grad_x1)\n",
    "print(\"Gradient ∂f/∂x2:\", grad_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch automatically constructs the operation graph that leads from `x` to `y` and, when\n",
    "invoking `y.backward()`, calculates the partial derivatives ∂f/∂x₁ and ∂f/∂x₂ at the\n",
    "specific point `x = [2, 3]`. These derivatives are stored in `x.grad`.\n",
    "\n",
    "In parallel, the same function can be represented symbolically with SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbolic variables\n",
    "x1, x2 = sp.symbols(\"x1 x2\")\n",
    "\n",
    "# Define the same function symbolically\n",
    "f = x1**2 + 3 * x1 * x2 + x2**2\n",
    "\n",
    "# Calculate symbolic derivatives\n",
    "df_dx1 = sp.diff(f, x1)\n",
    "df_dx2 = sp.diff(f, x2)\n",
    "\n",
    "print(\"SymPy derivative formulas:\")\n",
    "print(\"∂f/∂x1 =\", df_dx1)\n",
    "print(\"∂f/∂x2 =\", df_dx2)\n",
    "\n",
    "# Evaluate derivatives at point (x1=2, x2=3)\n",
    "grad_x1_sym = df_dx1.evalf(subs={x1: 2, x2: 3})\n",
    "grad_x2_sym = df_dx2.evalf(subs={x1: 2, x2: 3})\n",
    "\n",
    "print(\"SymPy symbolic gradients evaluated at (x1=2, x2=3):\")\n",
    "print(\"Gradient x1:\", grad_x1_sym)\n",
    "print(\"Gradient x2:\", grad_x2_sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SymPy provides closed-form symbolic expressions for derivatives and allows evaluating\n",
    "them at specific points. The comparison between SymPy's results and PyTorch's shows how\n",
    "PyTorch's automatic differentiation matches analytical derivatives, which helps validate\n",
    "the implementation and understand the relationship between theory and practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are several simple examples that illustrate how PyTorch calculates derivatives in\n",
    "different contexts: single-variable functions, multi-variable functions, chain rule\n",
    "application, and simple linear and logistic models. These examples allow intuitively\n",
    "understanding how the _autograd_ system tracks operations and applies the rules of\n",
    "differential calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "\n",
    "\n",
    "# Example 1: Quadratic function\n",
    "# y = x², dy/dx = 2x\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"y = x² | x={x.item()}, dy/dx={x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first case, the function is one-dimensional and simple. PyTorch automatically\n",
    "applies the power rule for derivatives and obtains dy/dx = 2x evaluated at x = 3.\n",
    "\n",
    "In a scenario with multiple variables, PyTorch calculates partial gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Multiple variables\n",
    "# z = 2a + 3b, dz/da = 2, dz/db = 3\n",
    "a = torch.tensor(4.0, requires_grad=True)\n",
    "b = torch.tensor(5.0, requires_grad=True)\n",
    "z = 2 * a + 3 * b\n",
    "z.backward()\n",
    "print(f\"z = 2a + 3b | dz/da={a.grad.item()}, dz/db={b.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `a.grad` contains ∂z/∂a and `b.grad` contains ∂z/∂b, as expected from a linear\n",
    "function in two variables.\n",
    "\n",
    "The chain rule is applied implicitly when the function is composed of several\n",
    "intermediate operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Chain rule\n",
    "# y = (2x + 1)², dy/dx = 4(2x + 1)\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = (2 * x + 1) ** 2\n",
    "y.backward()\n",
    "print(f\"y = (2x+1)² | x={x.item()}, dy/dx={x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, PyTorch internally decomposes the function into elementary steps\n",
    "(multiplication, addition, power) and combines their derivatives following the chain\n",
    "rule, without the user needing to do it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives acquire a central role when working with linear and logistic models, as they\n",
    "allow quantifying how the model output changes with small variations in the inputs or\n",
    "parameters. The following examples show how PyTorch calculates gradients with respect to\n",
    "inputs in simple configurations.\n",
    "\n",
    "In a linear model with two features, with weights `w` and bias `b`, the output is:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + b,\n",
    "$$\n",
    "\n",
    "so the derivatives with respect to the inputs are ∂y/∂x₁ = w₁ and ∂y/∂x₂ = w₂:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Linear regression\n",
    "# y = w·x + b, dy/dx = w\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([0.5, -1.0])\n",
    "b = 2.0\n",
    "\n",
    "y = w[0] * x[0] + w[1] * x[1] + b\n",
    "y.backward()\n",
    "\n",
    "print(f\"Linear | dy/dx1={x.grad[0].item()}, dy/dx2={x.grad[1].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch exactly reproduces these derivatives: the gradient of the output with respect to\n",
    "each component of `x` matches the corresponding weight. This behavior is what generalizes\n",
    "when calculating gradients with respect to model parameters during training.\n",
    "\n",
    "In the case of logistic regression, a sigmoid function is applied over the linear\n",
    "combination:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b,\\\\\n",
    "y = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "The derivative with respect to the inputs is given by the chain rule:\n",
    "$\\frac{\\partial y}{\\partial x_i} = \\sigma'(z)\\, w_i$, where\n",
    "$\\sigma'(z) = \\sigma(z)\\,(1 - \\sigma(z))$. PyTorch handles this composition\n",
    "automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Logistic regression\n",
    "# y = σ(w·x + b), dy/dx = σ'(z)·w\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "z = w[0] * x[0] + w[1] * x[1] + b\n",
    "y = torch.sigmoid(z)\n",
    "\n",
    "y.backward()\n",
    "print(f\"Logistic | dy/dx1={x.grad[0].item():.4f}, dy/dx2={x.grad[1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values contained in `x.grad` reflect the sensitivity of the predicted probability\n",
    "with respect to each of the input features, and illustrate how the nonlinear activation\n",
    "function (the sigmoid) affects the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiclass classification tasks, it is common to use a linear layer followed by a\n",
    "softmax function. The linear layer calculates a score or _logit_ for each class, and\n",
    "softmax transforms these scores into probabilities that sum to 1. Below is a simple\n",
    "example with three input features and three output classes.\n",
    "\n",
    "Consider an input vector `x` and a weight matrix `W`, where each column of `W` can be\n",
    "interpreted as the weight vector associated with a class. From them, the logits are\n",
    "obtained as a matrix product and softmax is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Input features\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Weight matrix for 3 classes\n",
    "W = torch.tensor(\n",
    "    [\n",
    "        [0.2, -0.5, 0.3],\n",
    "        [0.4, 0.1, -0.2],\n",
    "        [0.1, 0.3, 0.2],\n",
    "    ],\n",
    "    requires_grad=False,\n",
    ")\n",
    "b = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# Linear scores for each class: logits = W^T x + b\n",
    "logits = torch.matmul(x, W) + b  # shape [3]\n",
    "\n",
    "# Apply Softmax to obtain probabilities\n",
    "probs = F.softmax(logits, dim=0)\n",
    "\n",
    "# Select the probability of the predicted class (the highest)\n",
    "pred_class_idx = probs.argmax()\n",
    "top_prob = probs[pred_class_idx]\n",
    "\n",
    "# Calculate gradients with respect to the input\n",
    "top_prob.backward()\n",
    "\n",
    "print(\"Multiclass Classification | Probabilities:\", probs.detach().numpy())\n",
    "print(\"Predicted class index:\", pred_class_idx.item())\n",
    "print(\"Gradients inputs:\", x.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `logits` is a one-dimensional tensor of size 3 containing the linear\n",
    "score of each class. The `F.softmax` function transforms these logits into a probability\n",
    "vector. Next, the probability of the class with the highest value (`top_prob`) is\n",
    "selected and `backward()` is called to calculate the gradient of that probability with\n",
    "respect to the input vector `x`.\n",
    "\n",
    "The resulting values in `x.grad` indicate how the probability of the predicted class\n",
    "would vary if each component of the input were slightly perturbed. This information can\n",
    "be used, for example, to analyze the model's sensitivity to input features or as the\n",
    "basis for explanation techniques and adversarial example generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
