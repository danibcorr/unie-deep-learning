{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent constitutes the core of training algorithms in machine learning and deep\n",
    "learning. In essence, it is an iterative procedure that adjusts model parameters in the\n",
    "direction opposite to the gradient of the cost function, with the objective of minimizing\n",
    "said function. This section first presents a purely numerical example in two dimensions,\n",
    "to visualize descent trajectories, and then several practical examples in PyTorch that\n",
    "show how the gradient is used to learn the parameters of simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Gradient Descent in a Two-Dimensional Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, a nonlinear function of two variables is defined and its gradients\n",
    "are calculated analytically. From several random initial points, gradient descent is\n",
    "applied and the trajectories are visualized in the parameter plane, which provides a\n",
    "geometric idea of the optimization process.\n",
    "\n",
    "The function considered is:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = \\sin(x_1)\\cos(x_2) + \\sin(0.5\\, x_1)\\cos(0.5\\, x_2),\n",
    "$$\n",
    "\n",
    "implemented in NumPy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function definition\n",
    "def function(input: np.ndarray) -> np.ndarray:\n",
    "    assert input.shape[-1] == 2, \"The input must contain 2 elements\"\n",
    "    return np.sin(input[:, 0]) * np.cos(input[:, 1]) + np.sin(\n",
    "        0.5 * input[:, 0]\n",
    "    ) * np.cos(0.5 * input[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the partial derivatives are defined analytically, that is, the gradient\n",
    "$\\nabla\n",
    "f(x_1, x_2) = (\\partial f/\\partial x_1, \\partial f/\\partial x_2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient calculation (partial derivatives)\n",
    "def gradiente(input: np.ndarray) -> np.ndarray:\n",
    "    assert input.shape[-1] == 2, \"The input must contain 2 elements\"\n",
    "\n",
    "    df_x1 = np.cos(input[:, 0]) * np.cos(input[:, 1]) + 0.5 * np.cos(\n",
    "        0.5 * input[:, 0]\n",
    "    ) * np.cos(0.5 * input[:, 1])\n",
    "    df_x2 = -np.sin(input[:, 0]) * np.sin(input[:, 1]) - 0.5 * np.sin(\n",
    "        0.5 * input[:, 0]\n",
    "    ) * np.sin(0.5 * input[:, 1])\n",
    "\n",
    "    return np.stack([df_x1, df_x2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm is implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent algorithm\n",
    "def descenso_gradiente(\n",
    "    num_puntos: int = 10,\n",
    "    num_iteraciones: int = 30,\n",
    "    learning_rate: float = 1e-3,\n",
    "):\n",
    "    dim = 2\n",
    "    # Random initialization in the domain [0, 10] x [0, 10]\n",
    "    X = np.random.rand(num_puntos, dim) * 10\n",
    "    trayectorias = [X.copy()]\n",
    "\n",
    "    for _ in range(num_iteraciones):\n",
    "        X = X - learning_rate * gradiente(input=X)\n",
    "        trayectorias.append(X.copy())\n",
    "\n",
    "    return np.array(trayectorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is executed for several initial points and their trajectories are plotted\n",
    "in the $(x_1, x_2)$ plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute gradient descent\n",
    "trayectoria = descenso_gradiente(num_puntos=5, num_iteraciones=30)\n",
    "\n",
    "# Visualize trajectories in 2D plane\n",
    "for i in range(trayectoria.shape[1]):\n",
    "    plt.plot(trayectoria[:, i, 0], trayectoria[:, i, 1], marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Gradient Descent Trajectories\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each curve shows how a point moves iteratively in the descent direction of $f$. This\n",
    "example visually illustrates the fundamental idea: the gradient indicates the direction\n",
    "of maximum increase, and the algorithm moves in the opposite direction to approach\n",
    "function minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Fitting a Quadratic Function in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second example, it is shown how to apply gradient descent in PyTorch to fit a\n",
    "quadratic function to synthetically generated data. A relationship between time and\n",
    "velocity is simulated that approximately follows a parabola, with added noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Synthetic data\n",
    "tiempo = torch.arange(0, 20).float()\n",
    "velocidad = torch.randn(20) * 3 + 0.75 * (tiempo - 9.5) ** 2 + 1\n",
    "\n",
    "plt.scatter(tiempo, velocidad)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Synthetic data (time vs. velocity)\")\n",
    "plt.show()\n",
    "\n",
    "velocidad.shape, tiempo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumed model is a quadratic function of the form\n",
    "\n",
    "$$\\hat{v}(t) = a t^2 + b t + c, $$\n",
    "\n",
    "where $(a, b, c)$ are learnable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion(instante_tiempo: torch.Tensor, parametros: torch.Tensor) -> torch.Tensor:\n",
    "    a, b, c = parametros\n",
    "    return a * (instante_tiempo**2) + b * instante_tiempo + c\n",
    "\n",
    "\n",
    "def loss_function(predicted: torch.Tensor, real: torch.Tensor) -> torch.Tensor:\n",
    "    return (real - predicted).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are initialized randomly and the initial prediction is observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = torch.randn(3, requires_grad=True)\n",
    "parametros\n",
    "\n",
    "predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\n",
    "predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the fit, an auxiliary function is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_preds(tiempo, real, preds: torch.Tensor):\n",
    "    plt.scatter(tiempo, real, color=\"blue\", label=\"Real\")\n",
    "    plt.scatter(\n",
    "        tiempo,\n",
    "        preds.detach().cpu().numpy(),\n",
    "        color=\"red\",\n",
    "        label=\"Predicted\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_preds(tiempo, velocidad, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial loss is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdida = loss_function(predicciones, velocidad)\n",
    "perdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a manual gradient descent step is applied: the gradient is calculated using\n",
    "`backward()`, parameters are updated, and gradients are reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradients\n",
    "perdida.backward()\n",
    "parametros.grad\n",
    "\n",
    "# Gradient descent step\n",
    "lr = 1e-5\n",
    "parametros.data = parametros.data - lr * parametros.grad.data\n",
    "parametros.grad = None\n",
    "\n",
    "# New prediction after update\n",
    "predicciones = funcion(instante_tiempo=tiempo, parametros=parametros)\n",
    "show_preds(tiempo, velocidad, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To repeat this process systematically, it is encapsulated in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_step_training(\n",
    "    tiempo,\n",
    "    parametros_aprendibles,\n",
    "    datos_a_predecir,\n",
    "    lr: float = 1e-5,\n",
    "):\n",
    "    predicciones = funcion(instante_tiempo=tiempo, parametros=parametros_aprendibles)\n",
    "    perdida = loss_function(predicted=predicciones, real=datos_a_predecir)\n",
    "    perdida.backward()\n",
    "\n",
    "    # Update parameters without gradient tracking\n",
    "    with torch.no_grad():\n",
    "        parametros_aprendibles -= lr * parametros_aprendibles.grad\n",
    "\n",
    "    # Reset gradients\n",
    "    parametros_aprendibles.grad.zero_()\n",
    "\n",
    "    show_preds(tiempo, datos_a_predecir, predicciones)\n",
    "    return predicciones, parametros_aprendibles, perdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is executed for several epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 20\n",
    "parametros_aprendibles = torch.randn(3, requires_grad=True)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    predicciones, parametros_aprendibles, perdida = apply_step_training(\n",
    "        tiempo=tiempo,\n",
    "        parametros_aprendibles=parametros_aprendibles,\n",
    "        datos_a_predecir=velocidad,\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1}, loss: {perdida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flow illustrates the key training components in PyTorch:\n",
    "\n",
    "- Definition of a differentiable function.\n",
    "- Loss calculation.\n",
    "- Call to `backward()` to obtain gradients.\n",
    "- Manual parameter update within a `torch.no_grad()` context.\n",
    "- Gradient reset before the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Manually Implemented Linear Layer and Simple Linear Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, two complementary ideas are introduced: the abstraction of a linear layer\n",
    "and the implementation of a linear model in PyTorch as a subclass of `nn.Module`.\n",
    "\n",
    "First, a function that would represent a linear layer applied to an input is sketched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(tensor_entrada: torch.Tensor) -> torch.Tensor:\n",
    "    # tensor_entrada: (B, N)\n",
    "    # w: (N,)\n",
    "    # b: scalar\n",
    "    return tensor_entrada @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a minimalist class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaLineal:\n",
    "    def __init__(self, shape_entrada: int) -> None:\n",
    "        self.w = torch.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is just a sketch, it serves to connect with PyTorch's standard\n",
    "implementation using `nn.Module`. Next, a fully functional linear model is proposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
    "        self.bias = nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weight * input_tensor + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available device is checked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data following a linear relationship is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 1\n",
    "steps = 0.02\n",
    "X = np.arange(start, end, steps)\n",
    "\n",
    "bias = 0.3\n",
    "weight = 0.7\n",
    "y = weight * X + bias\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=4, label=\"Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_test, y_test, c=\"g\", s=4, label=\"Testing\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialized and its parameters are inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Linear()\n",
    "list(linear_model.parameters())\n",
    "linear_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, the model is evaluated on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = linear_model(X_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an important distinction is introduced: `torch.no_grad()` and\n",
    "`torch.inference_mode()`. From PyTorch's documentation:\n",
    "\n",
    "- `no_grad` disables gradient tracking during the block, which avoids storing information\n",
    "  for autograd.\n",
    "- `inference_mode` is analogous to `no_grad` but more strict and efficient: it also\n",
    "  disables view tracking and version counting, and ensures that tensors created in this\n",
    "  context are not subsequently used in computations with autograd.\n",
    "\n",
    "In practice, `inference_mode` is recommended for inference code, where it is known that\n",
    "the model will not be trained or updated. This reduces overhead and increases safety\n",
    "against accidental parameter modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    predictions_2 = linear_model(X_test)\n",
    "\n",
    "predictions_2\n",
    "\n",
    "plt.scatter(X_test, predictions, c=\"r\", s=4, label=\"Predictions (no_grad)\")\n",
    "plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function and optimizer based on PyTorch are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()  # Mean absolute error\n",
    "optimizer = torch.optim.SGD(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model is trained for several epochs, iterating over training data and\n",
    "evaluating on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses_train = []\n",
    "    epoch_losses_test = []\n",
    "\n",
    "    # Training phase\n",
    "    linear_model.train()\n",
    "    for x, y_true in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_model = linear_model(x)\n",
    "        loss = loss_fn(output_model, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    # Evaluation phase\n",
    "    linear_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x, y_true in zip(X_test, y_test):\n",
    "            output_model = linear_model(x)\n",
    "            loss = loss_fn(output_model, y_true)\n",
    "            epoch_losses_test.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}, \"\n",
    "        f\"Train Loss: {np.mean(epoch_losses_train):.4f}, \"\n",
    "        f\"Test Loss: {np.mean(epoch_losses_test):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, final predictions are compared with real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    predictions_trained = linear_model(X_test)\n",
    "\n",
    "plt.scatter(X_test, predictions_trained, c=\"r\", s=4, label=\"Predictions\")\n",
    "plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is illustrated how to save and load the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the state dict\n",
    "torch.save(linear_model.state_dict(), \"linear_model_state.pth\")\n",
    "\n",
    "# Load the state dict\n",
    "linear_model_loaded = Linear()  # Create a new instance\n",
    "linear_model_loaded.load_state_dict(\n",
    "    torch.load(\"linear_model_state.pth\", weights_only=True)\n",
    ")\n",
    "linear_model_loaded.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    predictions_loaded = linear_model_loaded(X_test)\n",
    "\n",
    "plt.scatter(X_test, predictions_loaded, c=\"r\", s=4, label=\"Predictions (loaded)\")\n",
    "plt.scatter(X_test, y_test, c=\"b\", s=4, label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
