{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures and Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the fundamental data structures of PyTorch, with particular emphasis on the tensor, which constitutes the core abstraction of the framework. The objective is to provide a clear and rigorous understanding of what a tensor is, how it is constructed, which types are available, and why it plays a central role throughout the entire deep learning workflow, from model design and experimentation to training and inference.\n",
    "\n",
    "In PyTorch, tensors represent the primary means of storing and manipulating data. Conceptually, a tensor can be understood as a generalization of familiar mathematical objects such as scalars, vectors, and matrices to an arbitrary number of dimensions. Each tensor contains numerical values arranged according to a specific shape, which defines both its dimensionality and the size along each dimension. Depending on computational requirements, tensors may reside either in main memory on the CPU or in device memory on a GPU, enabling efficient execution of large-scale numerical operations.\n",
    "\n",
    "From a practical perspective, PyTorch tensors are closely analogous to NumPy’s `ndarray`, sharing similar semantics and many common operations. However, tensors are specifically designed to support high-performance computation and seamless acceleration through specialized hardware, particularly graphics processing units. This capability makes tensors especially suitable for the intensive linear algebra operations that underpin modern deep learning models.\n",
    "\n",
    "Within this context, the section explains how tensors can be created from predefined Python lists or matrix-like structures, and how their data types can be explicitly specified. This includes choosing between integer and floating-point representations, as well as selecting different levels of numerical precision. Such choices have direct implications for memory consumption, computational efficiency, and numerical stability.\n",
    "\n",
    "In addition, the fundamental properties of tensors are examined in detail. These include the number of dimensions, accessible through the `ndim` attribute, the tensor’s shape, which describes its dimensional structure, and the device on which the tensor is stored, indicated by the `device` attribute. Understanding how to inspect and modify these properties is essential, as many tensor operations require compatible shapes, data types, and devices in order to execute correctly and efficiently. Collectively, these concepts form the foundation for effective and reliable use of PyTorch in deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of machine learning, the use of GPUs allows significantly accelerating\n",
    "tensor processing and the execution of large-scale neural models. PyTorch provides\n",
    "utilities to verify if the system has a compatible GPU and to obtain information about\n",
    "available devices. The following code snippet illustrates how to perform this check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "No GPU detected, CPU will be used\n"
     ]
    }
   ],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"No GPU detected, CPU will be used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of check is especially useful at the beginning of a Jupyter notebook or\n",
    "training script, as it allows dynamically adapting the code to available hardware, moving\n",
    "tensors and models to the appropriate device through operations like `tensor.to(\"cuda\")`\n",
    "or `model.to(\"cuda\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: CPU vs GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the computational advantages of using GPUs, consider the following example\n",
    "that compares the execution time of matrix multiplication operations on both devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "\n",
    "\n",
    "# Define matrix size\n",
    "size = (5000, 5000)\n",
    "\n",
    "# Create tensors on CPU and GPU\n",
    "cpu_tensor = torch.rand(size)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = torch.rand(size, device=\"cuda\")\n",
    "\n",
    "    # Measure CPU time\n",
    "    start = time.time()\n",
    "    cpu_result = cpu_tensor @ cpu_tensor\n",
    "    cpu_time = time.time() - start\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "    # Measure GPU time\n",
    "    start = time.time()\n",
    "    gpu_result = gpu_tensor @ gpu_tensor\n",
    "    torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.cuda.synchronize()` call is essential to ensure accurate timing measurements,\n",
    "as GPU operations are asynchronous by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensors"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "A tensor is the fundamental data structure in PyTorch. From an intuitive point of view, a\n",
    "scalar is a 0-dimensional tensor, a vector is a 1-dimensional tensor, a matrix is a\n",
    "2-dimensional tensor, and from there on, we speak of higher-order tensors (3D, 4D, etc.).\n",
    "This generalization allows representing very diverse data, such as images, tokenized text\n",
    "sequences, or multivariate time series, in a unified way.\n",
    "\n",
    "Tensors allow storing data efficiently, both in CPU and GPU, and support a wide variety\n",
    "of mathematical operations: additions, products, reductions, linear algebra operations,\n",
    "and many others. Most deep learning algorithms are implemented as compositions of\n",
    "operations on tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensor Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports various data types optimized for different use cases. The following\n",
    "table summarizes the most common types:\n",
    "\n",
    "| dtype                               | Description                   | Typical Use Case                                   | Memory per Element |\n",
    "| ----------------------------------- | ----------------------------- | -------------------------------------------------- | ------------------ |\n",
    "| `torch.float32` (or `torch.float`)  | 32-bit floating point         | Default type, general purpose training             | 4 bytes            |\n",
    "| `torch.float64` (or `torch.double`) | 64-bit floating point         | High precision scientific computing                | 8 bytes            |\n",
    "| `torch.float16` (or `torch.half`)   | 16-bit floating point         | Mixed precision training, inference acceleration   | 2 bytes            |\n",
    "| `torch.bfloat16`                    | Brain floating point (16-bit) | Modern TPU/GPU training, better range than float16 | 2 bytes            |\n",
    "| `torch.int64` (or `torch.long`)     | 64-bit integer                | Indices, labels, sizes                             | 8 bytes            |\n",
    "| `torch.int32` (or `torch.int`)      | 32-bit integer                | Integer computations                               | 4 bytes            |\n",
    "| `torch.int16` (or `torch.short`)    | 16-bit integer                | Memory-constrained integer storage                 | 2 bytes            |\n",
    "| `torch.int8`                        | 8-bit integer                 | Quantized models, extreme memory savings           | 1 byte             |\n",
    "| `torch.uint8`                       | 8-bit unsigned integer        | Image data (0-255 range)                           | 1 byte             |\n",
    "| `torch.bool`                        | Boolean                       | Masks, logical conditions                          | 1 byte             |\n",
    "\n",
    "The choice of data type significantly impacts memory consumption, computational speed,\n",
    "and numerical stability. For instance, using `float16` can reduce memory usage by 50%\n",
    "compared to `float32`, enabling training of larger models, but may require careful\n",
    "handling of numerical underflow/overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors: Scalars, Vectors, Matrices, and Higher-Order Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch facilitates the creation of tensors of different dimensions. The following code\n",
    "snippet illustrates how to construct a scalar, a vector, a matrix, and a higher-order\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar tensor\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar has no additional dimensions, so its number of dimensions is 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the Python numerical value associated with the scalar, the `.item()` method is\n",
    "used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, vectors (1-dimensional tensors) can be defined by providing a list of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 7])\n",
      "1\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Creating a vector\n",
    "vector = torch.tensor([7, 7])\n",
    "print(vector)\n",
    "print(vector.ndim)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `vector.ndim` returns `1`, and `vector.shape` indicates the vector's\n",
    "length. Similarly, a matrix is represented as a list of lists, generating a 2-dimensional\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "2\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a matrix\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(matrix)\n",
    "print(matrix.ndim)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix has two rows and three columns, so its shape is `(2, 3)`.\n",
    "\n",
    "Higher-order tensors are constructed by nesting additional lists. For example, the\n",
    "following tensor has three dimensions, organized hierarchically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[7, 8, 9],\n",
      "         [6, 4, 3]]])\n",
      "3\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a three-dimensional tensor\n",
    "tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [6, 4, 3]]])\n",
    "print(tensor)\n",
    "print(tensor.ndim)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` attribute describes the size of each dimension. Understanding this structure\n",
    "is key to designing and interpreting neural network architectures, as the inputs and\n",
    "outputs of each layer are represented as tensors with specific shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors with Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is very common to create test tensors or initialize model parameters\n",
    "using random values. PyTorch allows creating tensors with values generated randomly from\n",
    "different distributions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7319, 0.6792, 0.0530, 0.1782],\n",
      "         [0.5746, 0.8844, 0.1616, 0.1229],\n",
      "         [0.5827, 0.4093, 0.8628, 0.5392]],\n",
      "\n",
      "        [[0.6035, 0.9491, 0.9427, 0.3746],\n",
      "         [0.9816, 0.9623, 0.2767, 0.1142],\n",
      "         [0.7656, 0.7438, 0.4404, 0.3085]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random tensors\n",
    "random_tensor = torch.rand((2, 3, 4))\n",
    "print(random_tensor)\n",
    "random_tensor.ndim, random_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a tensor with shape `(2, 3, 4)` is created whose elements are uniformly distributed\n",
    "in the interval `[0, 1)`. This type of tensor is useful for:\n",
    "\n",
    "1. Verifying that input and output dimensions of a model are coherent between layers.\n",
    "2. Checking that internal operations are performed without errors before using real data.\n",
    "3. Exploring model behavior in unit tests or quick experiments.\n",
    "\n",
    "In addition to random tensors, it is common to use tensors initialized with zeros or\n",
    "ones, for example, to define masks, templates, or initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_tensors = torch.zeros((3, 4))\n",
    "zero_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensors = torch.ones((3, 4))\n",
    "ones_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create tensors containing sequences of equally spaced values using\n",
    "`torch.arange`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
      "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n",
      "        72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98])\n",
      "Shape of 'range_tensor': torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "range_tensor = torch.arange(start=0, end=100, step=2)\n",
    "print(range_tensor)\n",
    "print(f\"Shape of 'range_tensor': {range_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor contains values from 0 to 98 with a step of 2. From it, other tensors that\n",
    "inherit its shape can be constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'range_copy': torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with the same dimension as another tensor\n",
    "range_copy = torch.zeros_like(input=range_tensor)\n",
    "print(f\"Shape of 'range_copy': {range_copy.shape}\")\n",
    "range_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of functions like `zeros_like` or `ones_like` facilitates creating tensors\n",
    "compatible in shape and data type with existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion Between NumPy and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors and NumPy arrays are closely related, and conversion between them is\n",
    "straightforward and efficient. This interoperability is crucial when integrating PyTorch\n",
    "with other scientific computing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NumPy array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Converted tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Tensor dtype: torch.int64\n",
      "\n",
      "Original tensor:\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Converted NumPy array:\n",
      "[[ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# NumPy array to PyTorch tensor\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"Original NumPy array:\\n{numpy_array}\")\n",
    "print(f\"Converted tensor:\\n{tensor_from_numpy}\")\n",
    "print(f\"Tensor dtype: {tensor_from_numpy.dtype}\")\n",
    "\n",
    "# PyTorch tensor to NumPy array\n",
    "tensor = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "numpy_from_tensor = tensor.numpy()\n",
    "print(f\"\\nOriginal tensor:\\n{tensor}\")\n",
    "print(f\"Converted NumPy array:\\n{numpy_from_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important considerations:**\n",
    "\n",
    "1. **Memory sharing**: By default, `torch.from_numpy()` creates a tensor that shares\n",
    "   memory with the original NumPy array. Modifications to one will affect the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified NumPy array: [999   2   3]\n",
      "Tensor (shares memory): tensor([999,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate memory sharing\n",
    "np_array = np.array([1, 2, 3])\n",
    "tensor = torch.from_numpy(np_array)\n",
    "np_array[0] = 999\n",
    "print(f\"Modified NumPy array: {np_array}\")\n",
    "print(f\"Tensor (shares memory): {tensor}\")  # Also shows 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **GPU tensors**: Tensors on GPU must be moved to CPU before converting to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = torch.tensor([1, 2, 3], device=\"cuda\")\n",
    "    # This would raise an error: gpu_tensor.numpy()\n",
    "    numpy_from_gpu = gpu_tensor.cpu().numpy()  # Correct approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Gradient tracking**: Tensors with gradients enabled must have gradients detached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_with_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "# This would raise an error: tensor_with_grad.numpy()\n",
    "numpy_array = tensor_with_grad.detach().numpy()  # Correct approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types, Devices, and Tensor Compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor in PyTorch is characterized, among other aspects, by its data type (`dtype`),\n",
    "its shape (`shape`), and the computing device on which it resides (`device`). These\n",
    "attributes influence operation compatibility and computational performance. When\n",
    "operations are performed between tensors whose data types do not match, whose dimensions\n",
    "are not compatible for the defined operation, or that are on different devices (for\n",
    "example, one on CPU and another on GPU), conflicts and errors can occur during execution.\n",
    "\n",
    "The following snippet shows how to inspect these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3310, 0.7286, 0.5154],\n",
       "         [0.7631, 0.1078, 0.0951]],\n",
       "\n",
       "        [[0.1155, 0.5658, 0.4222],\n",
       "         [0.8116, 0.3659, 0.8505]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(size=(2, 2, 3))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: torch.float32\n",
      "Shape: torch.Size([2, 2, 3])\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data type: {tensor.dtype}\")\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, floating-point tensors are created with type `torch.float32`. However, it is\n",
    "possible to specify a different data type, such as `float16` or `float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9014, 0.6221, 0.9561],\n",
       "         [0.8164, 0.3057, 0.8794]],\n",
       "\n",
       "        [[0.3350, 0.0645, 0.3784],\n",
       "         [0.6958, 0.1509, 0.9800]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(size=(2, 2, 3), dtype=torch.float16)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: torch.float16\n",
      "Shape: torch.Size([2, 2, 3])\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data type: {tensor.dtype}\")\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of numerical precision involves a trade-off between computational cost, memory\n",
    "consumption, training stability, and result accuracy. For example, `float16` allows\n",
    "significantly accelerating training and inference on appropriate hardware, but may\n",
    "increase the risk of numerical stability problems in certain models.\n",
    "\n",
    "In general, it is important that tensors involved in the same operation share a\n",
    "compatible data type and are on the same device. Otherwise, it is necessary to explicitly\n",
    "convert the type (`tensor.to(torch.float32)`, `tensor.int()`, etc.) or move the tensor to\n",
    "the corresponding device (`tensor.to(\"cuda\")`, `tensor.to(\"cpu\")`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Errors and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with tensors, certain errors frequently arise. Understanding these common\n",
    "pitfalls helps prevent debugging headaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Incompatible Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1\n",
      "Correct result shape: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Error: dimension mismatch\n",
    "try:\n",
    "    a = torch.rand(2, 3)\n",
    "    b = torch.rand(3, 5)\n",
    "    c = a + b  # Raises error: shapes [2, 3] and [3, 5] are incompatible\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Correct: use matrix multiplication for different shapes\n",
    "c = torch.matmul(a, b)  # Results in shape [2, 5]\n",
    "print(f\"Correct result shape: {c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Device Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error: tensors on different devices\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        cpu_tensor = torch.rand(2, 3)\n",
    "        gpu_tensor = torch.rand(2, 3, device=\"cuda\")\n",
    "        result = cpu_tensor + gpu_tensor  # Raises error\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Correct: ensure both tensors are on the same device\n",
    "    result = cpu_tensor.to(\"cuda\") + gpu_tensor\n",
    "    # Or: result = cpu_tensor + gpu_tensor.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Type Incompatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Error: incompatible data types\n",
    "try:\n",
    "    int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "    float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "    # Some operations may fail or produce unexpected results\n",
    "    result = int_tensor / float_tensor  # Works but with implicit conversion\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Best practice: explicit type conversion\n",
    "result = int_tensor.float() / float_tensor\n",
    "print(f\"Result dtype: {result.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient-Related Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: a view of a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "# Error: modifying tensors with gradients in-place\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "try:\n",
    "    x[0] = 10.0  # In-place modification raises error\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Correct: use operations that don't modify tensors in-place\n",
    "x_new = torch.tensor([10.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x_new**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Memory Leaks with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential memory leak: accumulating gradients in a loop\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    x = torch.randn(1000, 1000, requires_grad=True)\n",
    "    y = x.sum()\n",
    "    # losses.append(y)  # BAD: keeps entire computational graph\n",
    "    losses.append(y.item())  # GOOD: only stores the scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tensors are created, PyTorch allows applying various reduction and aggregation\n",
    "operations on them. For example, you can calculate maximums, means, or maximum indices\n",
    "along specific dimensions. Consider the following two-dimensional tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5802, 0.1192, 0.8978],\n",
       "        [0.7366, 0.8610, 0.3174]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(size=(2, 3))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max` method allows obtaining the maximum value along a dimension and its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7366, 0.8610, 0.8978]),\n",
       "indices=tensor([1, 1, 0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum by columns (dimension 0)\n",
    "tensor.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.8978, 0.8610]),\n",
       "indices=tensor([2, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum by rows (dimension 1)\n",
    "tensor.max(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the convention is adopted that columns correspond to axis or dimension\n",
    "`0`, while rows are associated with axis or dimension `1`. Similarly, the mean can be\n",
    "calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6584, 0.4901, 0.6076]), tensor([0.5324, 0.6384]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mean(dim=0), tensor.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6584, 0.4901, 0.6076]), tensor([0.5324, 0.6384]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(tensor, dim=0), torch.mean(tensor, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `argmax` function returns the indices of maximum values along a determined dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 0]), tensor([1, 1, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(tensor, dim=0), tensor.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices can be used to select elements or substructures of the tensor. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5802, 0.1192, 0.8978],\n",
       "        [0.7366, 0.8610, 0.3174]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8978, 0.3174])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:, tensor.argmax(dim=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all rows and the column corresponding to the largest value of the first row are\n",
    "selected, illustrating how to combine reduction operations with indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Operations Between Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports a comprehensive set of mathematical operations between tensors.\n",
    "Understanding the differences between element-wise operations, matrix multiplication, and\n",
    "broadcasting is fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations apply to corresponding elements of tensors with compatible\n",
    "shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition (a + b):\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "Subtraction (a - b):\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "\n",
      "Element-wise multiplication (a * b):\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n",
      "Element-wise division (a / b):\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "\n",
      "Power (a ** 2):\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Element-wise addition\n",
    "print(\"Addition (a + b):\")\n",
    "print(a + b)\n",
    "\n",
    "# Element-wise subtraction\n",
    "print(\"\\nSubtraction (a - b):\")\n",
    "print(a - b)\n",
    "\n",
    "# Element-wise multiplication\n",
    "print(\"\\nElement-wise multiplication (a * b):\")\n",
    "print(a * b)\n",
    "\n",
    "# Element-wise division\n",
    "print(\"\\nElement-wise division (a / b):\")\n",
    "print(a / b)\n",
    "\n",
    "# Element-wise power\n",
    "print(\"\\nPower (a ** 2):\")\n",
    "print(a**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides multiple ways to perform matrix multiplication, each with specific use\n",
    "cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.matmul(a, b):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "a @ b:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "torch.mm(a, b):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "All methods equivalent: True\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Method 1: torch.matmul (recommended, works with batched matrices)\n",
    "result1 = torch.matmul(a, b)\n",
    "print(\"torch.matmul(a, b):\")\n",
    "print(result1)\n",
    "\n",
    "# Method 2: @ operator (syntactic sugar for matmul)\n",
    "result2 = a @ b\n",
    "print(\"\\na @ b:\")\n",
    "print(result2)\n",
    "\n",
    "# Method 3: torch.mm (only for 2D matrices)\n",
    "result3 = torch.mm(a, b)\n",
    "print(\"\\ntorch.mm(a, b):\")\n",
    "print(result3)\n",
    "\n",
    "# Verify all methods give the same result\n",
    "print(\n",
    "    f\"\\nAll methods equivalent: {torch.equal(result1, result2) and torch.equal(result2, result3)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences:**\n",
    "\n",
    "- `torch.matmul` (or `@`): Most versatile, handles broadcasting and batched operations\n",
    "- `torch.mm`: Only for 2D matrices, slightly faster but less flexible\n",
    "- `*`: Element-wise multiplication (NOT matrix multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting allows operations between tensors of different shapes by automatically\n",
    "expanding dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor + Scalar:\n",
      "tensor([[11, 12, 13],\n",
      "        [14, 15, 16]])\n",
      "\n",
      "Matrix + Vector:\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "\n",
      "Broadcasting with different dimensions:\n",
      "tensor([[11, 21],\n",
      "        [12, 22],\n",
      "        [13, 23]])\n",
      "Result shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Scalar and tensor\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = 10\n",
    "result = a + b  # b is broadcasted to match a's shape\n",
    "print(\"Tensor + Scalar:\")\n",
    "print(result)\n",
    "\n",
    "# Example 2: Vector and matrix\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "b = torch.tensor([10, 20, 30])  # Shape: (3,)\n",
    "result = a + b  # b is broadcasted along dim 0\n",
    "print(\"\\nMatrix + Vector:\")\n",
    "print(result)\n",
    "\n",
    "# Example 3: Broadcasting with unsqueeze\n",
    "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
    "b = torch.tensor([10, 20])  # Shape: (2,)\n",
    "result = a + b  # Broadcasted to (3, 2)\n",
    "print(\"\\nBroadcasting with different dimensions:\")\n",
    "print(result)\n",
    "print(f\"Result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcasting rules:**\n",
    "\n",
    "1. Dimensions are aligned from right to left\n",
    "2. Dimensions must be equal, one of them must be 1, or one doesn't exist\n",
    "3. The result has the maximum size along each dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-place Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations ending with an underscore (`_`) modify tensors in-place, saving memory but\n",
    "potentially causing issues with gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([1., 2., 3.])\n",
      "Memory address: 360139200\n",
      "After add_(5): tensor([6., 7., 8.])\n",
      "Memory address: 360139200\n",
      "After mul_(2): tensor([12., 14., 16.])\n",
      "\n",
      "Original y: tensor([1., 2., 3.])\n",
      "Memory address: 360110272\n",
      "After y = y + 5: tensor([6., 7., 8.])\n",
      "Memory address: 360141760\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(f\"Original x: {x}\")\n",
    "print(f\"Memory address: {x.data_ptr()}\")\n",
    "\n",
    "# In-place addition\n",
    "x.add_(5)\n",
    "print(f\"After add_(5): {x}\")\n",
    "print(f\"Memory address: {x.data_ptr()}\")  # Same address\n",
    "\n",
    "# In-place multiplication\n",
    "x.mul_(2)\n",
    "print(f\"After mul_(2): {x}\")\n",
    "\n",
    "# Compare with non-in-place operations\n",
    "y = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(f\"\\nOriginal y: {y}\")\n",
    "print(f\"Memory address: {y.data_ptr()}\")\n",
    "\n",
    "y = y + 5  # Creates new tensor\n",
    "print(f\"After y = y + 5: {y}\")\n",
    "print(f\"Memory address: {y.data_ptr()}\")  # Different address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Avoid in-place operations on tensors with `requires_grad=True` as they can\n",
    "cause errors during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Reshaping and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to reshape and manipulate tensors is crucial for building neural\n",
    "networks, as layer outputs often need to be reshaped to match subsequent layer inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View, Reshape, and Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Shape: torch.Size([12])\n",
      "\n",
      "View as (3, 4):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Reshape as (2, 6):\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "\n",
      "View as (3, -1):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Flattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Flatten method: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample tensor\n",
    "x = torch.arange(12)\n",
    "print(f\"Original tensor: {x}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "\n",
    "# Using view (shares memory with original tensor)\n",
    "x_view = x.view(3, 4)\n",
    "print(f\"\\nView as (3, 4):\\n{x_view}\")\n",
    "\n",
    "# Using reshape (may create a copy if necessary)\n",
    "x_reshape = x.reshape(2, 6)\n",
    "print(f\"\\nReshape as (2, 6):\\n{x_reshape}\")\n",
    "\n",
    "# Using -1 for automatic dimension calculation\n",
    "x_auto = x.view(3, -1)  # -1 automatically becomes 4\n",
    "print(f\"\\nView as (3, -1):\\n{x_auto}\")\n",
    "\n",
    "# Flatten: convert to 1D\n",
    "x_flat = x.view(-1)\n",
    "print(f\"\\nFlattened: {x_flat}\")\n",
    "\n",
    "# Flatten with method\n",
    "x_flat2 = x.flatten()\n",
    "print(f\"Flatten method: {x_flat2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences:**\n",
    "\n",
    "- `view()`: Returns a view of the original tensor (shares memory), requires contiguous\n",
    "  memory\n",
    "- `reshape()`: May return a view or copy, more flexible but potentially less efficient\n",
    "- `flatten()`: Always returns a 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Contiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Is contiguous: True\n",
      "\n",
      "Transposed:\n",
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "Is contiguous: False\n",
      "\n",
      "Error with view(): view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "\n",
      "Reshape works: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n",
      "After contiguous(), view works: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the difference between view and reshape with transpose\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Original:\\n{x}\")\n",
    "print(f\"Is contiguous: {x.is_contiguous()}\")\n",
    "\n",
    "# Transpose makes tensor non-contiguous\n",
    "x_t = x.t()\n",
    "print(f\"\\nTransposed:\\n{x_t}\")\n",
    "print(f\"Is contiguous: {x_t.is_contiguous()}\")\n",
    "\n",
    "# view() fails on non-contiguous tensor\n",
    "try:\n",
    "    x_t.view(12)\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError with view(): {e}\")\n",
    "\n",
    "# reshape() works (creates a copy)\n",
    "x_t_reshaped = x_t.reshape(12)\n",
    "print(f\"\\nReshape works: {x_t_reshaped}\")\n",
    "\n",
    "# Make contiguous explicitly\n",
    "x_t_cont = x_t.contiguous()\n",
    "x_t_view = x_t_cont.view(12)\n",
    "print(f\"After contiguous(), view works: {x_t_view}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squeeze and Unsqueeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations add or remove dimensions of size 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 3, 1, 4])\n",
      "After squeeze(): torch.Size([3, 4])\n",
      "After squeeze(dim=0): torch.Size([3, 1, 4])\n",
      "After unsqueeze(dim=0): torch.Size([1, 3, 4])\n",
      "\n",
      "Single image shape: torch.Size([3, 224, 224])\n",
      "Batched image shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with dimensions of size 1\n",
    "x = torch.rand(1, 3, 1, 4)\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "x_squeezed = x.squeeze()\n",
    "print(f\"After squeeze(): {x_squeezed.shape}\")\n",
    "\n",
    "# Remove specific dimension\n",
    "x_squeezed_dim = x.squeeze(dim=0)\n",
    "print(f\"After squeeze(dim=0): {x_squeezed_dim.shape}\")\n",
    "\n",
    "# Add dimension\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"After unsqueeze(dim=0): {x_unsqueezed.shape}\")\n",
    "\n",
    "# Practical example: preparing batch dimension\n",
    "single_image = torch.rand(3, 224, 224)  # C, H, W\n",
    "batched_image = single_image.unsqueeze(0)  # Add batch dimension: B, C, H, W\n",
    "print(f\"\\nSingle image shape: {single_image.shape}\")\n",
    "print(f\"Batched image shape: {batched_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining multiple tensors is a common operation in deep learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Tensor b:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Tensor c:\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]])\n",
      "\n",
      "Concatenate along dim=0:\n",
      "tensor([[1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]])\n",
      "Shape: torch.Size([6, 3])\n",
      "\n",
      "Concatenate along dim=1:\n",
      "tensor([[1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000]])\n",
      "Shape: torch.Size([2, 9])\n",
      "\n",
      "Stack along dim=0:\n",
      "tensor([[[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000]]])\n",
      "Shape: torch.Size([3, 2, 3])\n",
      "\n",
      "Stack along dim=1 shape: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create sample tensors\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(2, 3)\n",
    "c = torch.full((2, 3), 0.5)\n",
    "\n",
    "print(\"Tensor a:\")\n",
    "print(a)\n",
    "print(\"\\nTensor b:\")\n",
    "print(b)\n",
    "print(\"\\nTensor c:\")\n",
    "print(c)\n",
    "\n",
    "# Concatenation along dimension 0 (rows)\n",
    "cat_dim0 = torch.cat([a, b, c], dim=0)\n",
    "print(f\"\\nConcatenate along dim=0:\")\n",
    "print(cat_dim0)\n",
    "print(f\"Shape: {cat_dim0.shape}\")\n",
    "\n",
    "# Concatenation along dimension 1 (columns)\n",
    "cat_dim1 = torch.cat([a, b, c], dim=1)\n",
    "print(f\"\\nConcatenate along dim=1:\")\n",
    "print(cat_dim1)\n",
    "print(f\"Shape: {cat_dim1.shape}\")\n",
    "\n",
    "# Stacking creates a new dimension\n",
    "stacked = torch.stack([a, b, c], dim=0)\n",
    "print(f\"\\nStack along dim=0:\")\n",
    "print(stacked)\n",
    "print(f\"Shape: {stacked.shape}\")\n",
    "\n",
    "# Stack along different dimension\n",
    "stacked_dim1 = torch.stack([a, b, c], dim=1)\n",
    "print(f\"\\nStack along dim=1 shape: {stacked_dim1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences:**\n",
    "\n",
    "- `torch.cat()`: Concatenates along an existing dimension\n",
    "- `torch.stack()`: Creates a new dimension and stacks along it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "Shape: torch.Size([4, 6])\n",
      "\n",
      "Split into chunks of size 2 along dim=0:\n",
      "Chunk 0 shape: torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "Chunk 1 shape: torch.Size([2, 6])\n",
      "tensor([[12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "\n",
      "Split into custom sizes [1, 2, 3] along dim=1:\n",
      "Chunk 0 shape: torch.Size([4, 1])\n",
      "Chunk 1 shape: torch.Size([4, 2])\n",
      "Chunk 2 shape: torch.Size([4, 3])\n",
      "\n",
      "Chunk into 2 parts:\n",
      "Chunk 0 shape: torch.Size([2, 6])\n",
      "Chunk 1 shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor to split\n",
    "x = torch.arange(24).reshape(4, 6)\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}\")\n",
    "\n",
    "# Split into equal chunks\n",
    "chunks = torch.split(x, split_size_or_sections=2, dim=0)\n",
    "print(f\"\\nSplit into chunks of size 2 along dim=0:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i} shape: {chunk.shape}\")\n",
    "    print(chunk)\n",
    "\n",
    "# Split into specific sizes\n",
    "split_sizes = [1, 2, 3]\n",
    "custom_chunks = torch.split(x, split_sizes, dim=1)\n",
    "print(f\"\\nSplit into custom sizes {split_sizes} along dim=1:\")\n",
    "for i, chunk in enumerate(custom_chunks):\n",
    "    print(f\"Chunk {i} shape: {chunk.shape}\")\n",
    "\n",
    "# Chunk: split into specified number of chunks\n",
    "num_chunks = 2\n",
    "chunked = torch.chunk(x, chunks=num_chunks, dim=0)\n",
    "print(f\"\\nChunk into {num_chunks} parts:\")\n",
    "for i, chunk in enumerate(chunked):\n",
    "    print(f\"Chunk {i} shape: {chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also allows performing advanced indexing operations and constructing submatrices\n",
    "through slicing techniques. Consider a random matrix of size `(4, 4)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2717, 0.6154, 0.4069, 0.8359],\n",
       "        [0.1893, 0.2178, 0.6230, 0.2607],\n",
       "        [0.5701, 0.5434, 0.6506, 0.3638],\n",
       "        [0.9924, 0.3657, 0.8248, 0.6186]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.rand((4, 4))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to extract submatrices taking one element out of every two in both\n",
    "dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2717, 0.4069],\n",
       "         [0.5701, 0.6506]]),\n",
       " tensor([[0.6154, 0.8359],\n",
       "         [0.5434, 0.3638]]),\n",
       " tensor([[0.1893, 0.6230],\n",
       "         [0.9924, 0.8248]]),\n",
       " tensor([[0.2178, 0.2607],\n",
       "         [0.3657, 0.6186]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submatrix_1 = matrix[0::2, 0::2]\n",
    "submatrix_2 = matrix[0::2, 1::2]\n",
    "submatrix_3 = matrix[1::2, 0::2]\n",
    "submatrix_4 = matrix[1::2, 1::2]\n",
    "\n",
    "submatrix_1, submatrix_2, submatrix_3, submatrix_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These submatrices can be stacked along a new dimension using `torch.stack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2717, 0.4069],\n",
       "         [0.5701, 0.6506]],\n",
       "\n",
       "        [[0.6154, 0.8359],\n",
       "         [0.5434, 0.3638]],\n",
       "\n",
       "        [[0.1893, 0.6230],\n",
       "         [0.9924, 0.8248]],\n",
       "\n",
       "        [[0.2178, 0.2607],\n",
       "         [0.3657, 0.6186]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submatrices = torch.stack([submatrix_1, submatrix_2, submatrix_3, submatrix_4])\n",
    "submatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submatrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tensor in which each submatrix occupies a position along the first\n",
    "dimension. If you want to add an additional dimension, you can use `unsqueeze`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submatrices.shape)\n",
    "submatrices = submatrices.unsqueeze(dim=0)\n",
    "submatrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this tensor, different operations can be performed. For example, we will calculate\n",
    "the Frobenius matrix norm (the Frobenius norm is equivalent to the square root of the sum\n",
    "of squares of all matrix elements) of each submatrix using `torch.linalg.matrix_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9938, 1.2269, 1.4454, 0.7948]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = torch.linalg.matrix_norm(submatrices, ord=\"fro\", dim=(-2, -1))\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the norms are calculated, you can select the submatrix with the highest norm using\n",
    "`argmax` on the `norm` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1893, 0.6230],\n",
       "         [0.9924, 0.8248]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submatrices[:, torch.argmax(norm), :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates how to combine indexing, stacking, dimension insertion, and\n",
    "linear algebra operations to analyze and manipulate complex matrix structures within a\n",
    "tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility constitutes a fundamental requirement in the development and evaluation\n",
    "of machine learning models. In this context, reproducibility is understood as the ability\n",
    "to obtain the same results when repeatedly executing an experiment under the same\n",
    "conditions: same code, same data, same hyperparameter configuration and, especially\n",
    "relevant, same random initialization.\n",
    "\n",
    "In PyTorch, an important part of model behavior depends on random processes, such as the\n",
    "initialization of neural network weights, the generation of tensors with random values,\n",
    "or random data sampling during training. If these processes are not controlled, small\n",
    "variations in initializations can produce different results in each execution, making it\n",
    "difficult to compare experiments and debug errors.\n",
    "\n",
    "To mitigate this problem, PyTorch provides mechanisms that allow fixing the seed of the\n",
    "random number generator. One of the most used is the instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x739fa9b387b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call initializes PyTorch's random number generator with a fixed seed, in this case\n",
    "the value `42`. From that moment on, all random operations that depend on this generator\n",
    "will produce the same sequence of values in successive executions, as long as the rest of\n",
    "the conditions (PyTorch version, hardware, operation order, etc.) remain constant. In\n",
    "this way, the creation of random tensors, the initialization of model parameters, and\n",
    "other stochastic processes associated with PyTorch become deterministic.\n",
    "\n",
    "The use of a fixed seed is especially important in experimentation and teaching\n",
    "environments. In a teaching context, it allows all students to obtain the same results\n",
    "when executing example notebooks, facilitating the follow-up of explanations and the\n",
    "detection of possible conceptual or implementation errors. In a research and development\n",
    "context, fixing the seed favors rigorous comparison between different models or\n",
    "configurations, as it reduces variability attributable solely to chance.\n",
    "\n",
    "It should be noted that, to achieve more complete reproducibility, it is often necessary\n",
    "to also fix the seeds of other random number generators used in the same environment,\n",
    "such as those from Python's standard libraries or NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Reproducibility Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complete reproducibility across all components of a PyTorch project, it is\n",
    "recommended to set seeds for all relevant libraries and configure deterministic behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility settings applied\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "\n",
    "# 3pps\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seeds for reproducibility across all random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for all RNGs\n",
    "    \"\"\"\n",
    "    # Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy random number generator\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch random number generators\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # PyTorch CUDA random number generator (for GPU operations)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "    # Configure PyTorch to use deterministic algorithms\n",
    "    # Note: This may impact performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # For complete reproducibility in PyTorch >= 1.8\n",
    "    # torch.use_deterministic_algorithms(True)  # Uncomment if needed\n",
    "\n",
    "\n",
    "# Apply reproducibility settings\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Reproducibility settings applied\")\n",
    "print(f\"Random seed set to: 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run:\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]])\n",
      "\n",
      "Second run:\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]])\n",
      "\n",
      "Tensors are identical: True\n"
     ]
    }
   ],
   "source": [
    "# Test reproducibility with random tensor generation\n",
    "\n",
    "\n",
    "def test_reproducibility():\n",
    "    \"\"\"Verify that setting the seed produces identical results\"\"\"\n",
    "\n",
    "    # First run\n",
    "    set_seed(42)\n",
    "    tensor1 = torch.rand(3, 3)\n",
    "    print(\"First run:\")\n",
    "    print(tensor1)\n",
    "\n",
    "    # Second run with same seed\n",
    "    set_seed(42)\n",
    "    tensor2 = torch.rand(3, 3)\n",
    "    print(\"\\nSecond run:\")\n",
    "    print(tensor2)\n",
    "\n",
    "    # Verify they are identical\n",
    "    print(f\"\\nTensors are identical: {torch.equal(tensor1, tensor2)}\")\n",
    "\n",
    "\n",
    "test_reproducibility()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
