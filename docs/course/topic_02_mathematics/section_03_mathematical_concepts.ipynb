{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Mathematical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, mathematical operations frequently used in machine learning and deep\n",
    "learning are introduced: distance metrics, cosine similarity, and the Hadamard product. \n",
    "All are formulated in matrix form and illustrated with practical examples that allow \n",
    "understanding their usefulness in the treatment of numerical data and, in particular, \n",
    "vector representations (_embeddings_) and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metrics quantify the dissimilarity between vectors in a feature space. Unlike \n",
    "similarity measures (which increase with greater resemblance), distance measures decrease \n",
    "as vectors become more alike. These metrics are fundamental in many machine learning \n",
    "algorithms such as k-Nearest Neighbors (k-NN), clustering (K-Means, DBSCAN), and \n",
    "dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidean distance is the most intuitive metric, corresponding to the straight-line \n",
    "distance between two points in Euclidean space. For two vectors $u, v \\in \\mathbb{R}^d$, \n",
    "it is defined as:\n",
    "\n",
    "$$\n",
    "d_{\\text{Euclidean}}(u, v) = \\|u - v\\| = \\sqrt{\\sum_{i=1}^{d} (u_i - v_i)^2}\n",
    "$$\n",
    "\n",
    "This metric is sensitive to the magnitude of the vectors and assumes that all dimensions \n",
    "contribute equally to the distance. It is particularly useful when the scale of features \n",
    "is meaningful and when dealing with continuous data in geometric spaces.\n",
    "\n",
    "**Properties:**\n",
    "- Always non-negative: $d(u, v) \\geq 0$\n",
    "- Symmetric: $d(u, v) = d(v, u)$\n",
    "- Satisfies the triangle inequality: $d(u, w) \\leq d(u, v) + d(v, w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "The Manhattan distance (also known as L1 distance or taxicab distance) measures the \n",
    "distance between two points by summing the absolute differences of their coordinates:\n",
    "\n",
    "$$\n",
    "d_{\\text{Manhattan}}(u, v) = \\|u - v\\|_1 = \\sum_{i=1}^{d} |u_i - v_i|\n",
    "$$\n",
    "\n",
    "The name comes from the analogy of navigating a grid-like street pattern, where movement \n",
    "is restricted to orthogonal directions. This metric is less sensitive to outliers than \n",
    "Euclidean distance and is often preferred when working with high-dimensional sparse data \n",
    "or when features have different units.\n",
    "\n",
    "**Use cases:**\n",
    "- Recommendation systems with categorical features\n",
    "- Image processing (pixel-wise comparison)\n",
    "- Feature spaces where diagonal movement is not meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "\n",
    "The Minkowski distance is a generalization that encompasses both Euclidean and Manhattan \n",
    "distances as special cases. It is defined by a parameter $p \\geq 1$:\n",
    "\n",
    "$$\n",
    "d_{\\text{Minkowski}}(u, v) = \\left(\\sum_{i=1}^{d} |u_i - v_i|^p\\right)^{1/p}\n",
    "$$\n",
    "\n",
    "**Special cases:**\n",
    "- $p = 1$: Manhattan distance\n",
    "- $p = 2$: Euclidean distance\n",
    "- $p \\to \\infty$: Chebyshev distance (maximum absolute difference across any dimension)\n",
    "\n",
    "The choice of $p$ determines how much the metric emphasizes large differences between \n",
    "coordinates. Larger values of $p$ make the distance more sensitive to the largest \n",
    "coordinate difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Distance Matrix\n",
    "\n",
    "In practice, it is often necessary to compute distances between all pairs of vectors in a \n",
    "dataset. For a matrix $X \\in \\mathbb{R}^{n \\times d}$ where each row represents a vector, \n",
    "the pairwise distance matrix $D \\in \\mathbb{R}^{n \\times n}$ contains the distance between \n",
    "every pair of vectors.\n",
    "\n",
    "For Euclidean distance, an efficient matrix formulation uses the algebraic identity:\n",
    "\n",
    "$$\n",
    "\\|u - v\\|^2 = \\|u\\|^2 + \\|v\\|^2 - 2u \\cdot v\n",
    "$$\n",
    "\n",
    "This allows computing all pairwise distances using matrix operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pairwise_euclidean_distance(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the pairwise Euclidean distance matrix efficiently.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of shape (n, d) where each row is a vector\n",
    "\n",
    "    Returns:\n",
    "        Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||\n",
    "    \"\"\"\n",
    "    # Compute squared norms for each vector (n, 1)\n",
    "    X_squared = np.sum(X**2, axis=1, keepdims=True)\n",
    "\n",
    "    # Use the identity: ||x-y||² = ||x||² + ||y||² - 2x·y\n",
    "    distances_squared = X_squared + X_squared.T - 2 * (X @ X.T)\n",
    "\n",
    "    # Clip to avoid numerical errors with negative values\n",
    "    distances_squared = np.maximum(distances_squared, 0)\n",
    "\n",
    "    return np.sqrt(distances_squared)\n",
    "\n",
    "\n",
    "def pairwise_manhattan_distance(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the pairwise Manhattan distance matrix.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of shape (n, d) where each row is a vector\n",
    "\n",
    "    Returns:\n",
    "        Distance matrix of shape (n, n) where D[i,j] = ||X[i] - X[j]||_1\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    distances = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        # Broadcasting: (1, d) - (n, d) = (n, d)\n",
    "        diff = np.abs(X[i : i + 1] - X)\n",
    "        distances[i] = np.sum(diff, axis=1)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "# Example usage\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "print(\"Original vectors:\\n\", X)\n",
    "\n",
    "euclidean_dist = pairwise_euclidean_distance(X)\n",
    "print(\"\\nPairwise Euclidean distances:\\n\", euclidean_dist)\n",
    "\n",
    "manhattan_dist = pairwise_manhattan_distance(X)\n",
    "print(\"\\nPairwise Manhattan distances:\\n\", manhattan_dist)\n",
    "\n",
    "# Compare distances for first two vectors\n",
    "print(f\"\\nDistance between vectors 0 and 1:\")\n",
    "print(f\"  Euclidean: {euclidean_dist[0, 1]:.3f}\")\n",
    "print(f\"  Manhattan: {manhattan_dist[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Distance vs. Similarity\n",
    "\n",
    "While distance metrics measure dissimilarity, cosine similarity (discussed in the next \n",
    "section) measures angular similarity. The key differences are:\n",
    "\n",
    "| Aspect | Distance Metrics | Cosine Similarity |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Range** | $[0, \\infty)$ | $[-1, 1]$ |\n",
    "| **Magnitude sensitivity** | Sensitive to vector magnitude | Invariant to magnitude |\n",
    "| **Interpretation** | Geometric separation | Angular alignment |\n",
    "| **Best for** | Continuous features, clustering | Text embeddings, semantic similarity |\n",
    "\n",
    "Distance metrics are preferred when the absolute magnitudes of features matter (e.g., \n",
    "physical measurements), while cosine similarity is preferred when only the direction or \n",
    "relative proportions matter (e.g., document term frequencies, word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity measures the degree of similarity between two vectors based on the\n",
    "angle they form, rather than their magnitude. Given a pair of vectors\n",
    "$u, v \\in \\mathbb{R}^d$, cosine similarity is defined as:\n",
    "\n",
    "$$\n",
    "\\text{sim}(u, v) = \\frac{u \\cdot v}{\\|u\\|\\,\\|v\\|},\n",
    "$$\n",
    "\n",
    "where $u \\cdot v$ denotes the dot product between $u$ and $v$, and $\\|\\cdot\\|$ is the\n",
    "Euclidean norm. The similarity value is in the range $[-1, 1]$, although in many\n",
    "practical contexts (for example, with non-negative embeddings) it usually takes values\n",
    "between 0 and 1. When the similarity is close to 1, the vectors point approximately in\n",
    "the same direction; when it is close to 0, they are almost orthogonal; and when it is\n",
    "negative, they point in opposite directions.\n",
    "\n",
    "In machine learning tasks, cosine similarity is routinely used to compare vector\n",
    "representations of elements such as words, documents, images, or users in recommendation\n",
    "systems. The idea is that vectors that are angularly close represent semantically similar\n",
    "entities.\n",
    "\n",
    "Below is an example in NumPy where the cosine similarity matrix is calculated for a set\n",
    "of vectors (embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_matrix(matrix: np.ndarray) -> np.ndarray:\n",
    "    # Normalizes each row by dividing it by its Euclidean norm\n",
    "    return matrix / np.expand_dims(\n",
    "        np.sqrt(np.sum(np.power(matrix, 2), axis=1)), axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def cosine_similarity(matrix: np.ndarray) -> np.ndarray:\n",
    "    # Assumes normalized rows: cosine similarity coincides with dot product\n",
    "    return matrix @ matrix.T\n",
    "\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "print(\"Original embeddings:\\n\", X)\n",
    "\n",
    "X_normalized = normalize_matrix(matrix=X)\n",
    "print(\"\\nNormalized embeddings:\\n\", X_normalized)\n",
    "\n",
    "similarity_matrix = cosine_similarity(matrix=X_normalized)\n",
    "print(\"\\nSimilarity matrix:\\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, each row of `X` represents a feature vector. The `normalize_matrix`\n",
    "function normalizes each vector to have norm 1. Once normalized, the matrix product\n",
    "`matrix @ matrix.T` produces a square matrix in which entry $(i, j)$ coincides with the\n",
    "cosine similarity between the $i$-th vector and the $j$-th vector. The diagonal of this\n",
    "matrix contains values equal to 1, since each vector has maximum similarity with itself.\n",
    "\n",
    "This matrix formulation allows efficiently calculating all pairwise similarities between\n",
    "a set of vectors, which is especially useful in tasks such as nearest neighbor search,\n",
    "clustering, or similarity graph construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadamard Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadamard product is an operation defined between matrices or tensors of equal shape,\n",
    "consisting of element-wise multiplication. If $A$ and $B$ are matrices of the same\n",
    "dimension, their Hadamard product $C = A \\odot B$ is defined by:\n",
    "\n",
    "$$C_{ij} = A_{ij} \\cdot B_{ij}, $$\n",
    "\n",
    "for all indices $i, j$. Unlike standard matrix multiplication, the Hadamard product does\n",
    "not combine rows and columns, but acts independently at each position.\n",
    "\n",
    "In machine learning and deep learning, the Hadamard product is frequently used to apply\n",
    "masks, combine features locally, or implement certain attention and gating mechanisms,\n",
    "where the contribution of each element of a tensor is modulated by multiplicative\n",
    "factors.\n",
    "\n",
    "The following example in PyTorch illustrates the use of the Hadamard product to apply a\n",
    "mask over a data matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "print(X)\n",
    "\n",
    "# Create a mask of the same size as X\n",
    "mask = torch.zeros_like(X)\n",
    "mask[0::2, 0::2] = 1\n",
    "print(mask)\n",
    "\n",
    "# Hadamard product (element-wise)\n",
    "output_tensor = X * mask\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `X` contains the original data and `mask` is a tensor of the same shape,\n",
    "initialized to zeros, in which ones are assigned at certain positions (rows and columns\n",
    "with step 2). The product `X * mask` applies the mask over the data: where the mask\n",
    "equals 1, the original value is preserved; where it equals 0, the result is nullified.\n",
    "This pattern is very common for selecting or attenuating specific regions in matrices or\n",
    "higher-dimensional tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Example: Segmentation and \"Pseudo-Attention\" in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadamard product becomes especially interesting when working with images, which can\n",
    "be represented as tensors of size $(\\text{height}, \\text{width}, \\text{channels})$. A\n",
    "typical application consists of segmenting an image to isolate a region of interest and,\n",
    "subsequently, applying a mask over the original image to highlight or suppress that\n",
    "region.\n",
    "\n",
    "The following example illustrates a workflow that combines several techniques:\n",
    "\n",
    "1. Loading an image in RGB format and visualization.\n",
    "2. Conversion to grayscale and intensity normalization.\n",
    "3. Image segmentation using clustering (K-Means) with two classes.\n",
    "4. Refinement of the resulting mask using morphological operations (hole filling and\n",
    "   closing).\n",
    "5. Application of the refined mask over the original image using a Hadamard product,\n",
    "   generating a \"pseudo-attention mechanism\" effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image = np.asarray(\n",
    "    Image.open(\n",
    "        \"/home/dani/Repositorios/unie-deep-learning/docs/assets/course/topic_02_mathematics/cat_image.jpg\"\n",
    "    )\n",
    ")\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(image.max(), image.min())\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Convert to grayscale and normalize to [0, 1]\n",
    "gray_scale_image = np.mean(image / 255.0, axis=-1)\n",
    "plt.imshow(gray_scale_image, vmin=0, vmax=1, cmap=\"Grays\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(f\"gray_scale_image shape: {gray_scale_image.shape}\")\n",
    "gray_scale_image.max(), gray_scale_image.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, a grayscale image is available represented as a 2D matrix of values in the\n",
    "range $[0, 1]$. Next, K-Means is applied to group pixels into two clusters, which can be\n",
    "interpreted as background and object, depending on intensity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for K-Means: vectorize the image\n",
    "X = np.expand_dims(gray_scale_image.flatten(), axis=-1)\n",
    "\n",
    "# Fit a K-Means model with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "\n",
    "# Reconstruct the cluster mask with the image shape\n",
    "kmeans_mask = kmeans.labels_.reshape(gray_scale_image.shape)\n",
    "kmeans_mask.max(), kmeans_mask.min()\n",
    "\n",
    "plt.imshow(kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting mask may contain fragmented regions or small holes. To improve its spatial\n",
    "coherence, morphological operations are applied using `scipy.ndimage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill holes in the mask\n",
    "filled_kmeans_mask = ndimage.binary_fill_holes(kmeans_mask)\n",
    "plt.imshow(filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Closing operation to smooth the mask and eliminate imperfections\n",
    "structure = np.ones((5, 5))  # A larger kernel implies more interpolation\n",
    "closed_filled_kmeans_mask = ndimage.binary_closing(\n",
    "    filled_kmeans_mask,\n",
    "    structure=structure,\n",
    "    iterations=10,\n",
    ")\n",
    "plt.imshow(closed_filled_kmeans_mask, vmin=0, vmax=1, cmap=\"Grays\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the mask is refined, it is applied over the original image using a Hadamard product\n",
    "extended to the three color channels. To do this, the binary mask is replicated along the\n",
    "channel axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the binary mask in the 3 color channels\n",
    "mask_3channels = np.repeat(\n",
    "    np.expand_dims(closed_filled_kmeans_mask, -1),\n",
    "    3,\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "# Apply the mask to the original image (pseudo-attention mechanism)\n",
    "pseudo_attention_mechanism = image * mask_3channels\n",
    "plt.imshow(pseudo_attention_mechanism)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure acts as a \"pseudo-attention mechanism\" because the binary mask highlights\n",
    "a region of the image (for example, the main figure, such as a cat) and attenuates the\n",
    "rest. Although this is not a learned attention mechanism, like those used in modern\n",
    "architectures (for example, Transformers), it illustrates how an operation as simple as\n",
    "the Hadamard product can be used to focus processing on specific areas of an input\n",
    "tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [How to Convert an RGB Image to Grayscale](https://brandonrohrer.com/convert_rgb_to_grayscale.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
