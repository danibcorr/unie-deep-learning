# Introduction

In this second topic of the course, we delve deeper into the practice of deep learning,
moving from a mainly theoretical perspective to one oriented toward implementation. The
objective is for students to acquire the ability to build, understand, and experiment
with machine learning models using specialized libraries, as well as to establish the
mathematical foundations that support them.

Throughout the topic, different sections and content are introduced, some of which may
not be covered in the main course presentation. It is recommended to review all available
sections and materials, including those marked as optional or additional. When there are
doubts about the status or update of certain content, it is advisable to consult the
instructors, who can provide guidance on its validity and relevance. Exploring these
complementary materials allows discovering new methods and approaches, and constitutes a
good opportunity to expand knowledge beyond the mandatory minimums.

In this block, the fundamental concepts of machine learning closely related to
mathematics and their implementation in code are worked on with special intensity. We
start from the data structures previously seen theoretically and move to their practical
application through Jupyter notebooks, using deep learning libraries, with special
emphasis on PyTorch. It shows how to define and manipulate tensors and matrices, as well
as how to perform basic and advanced operations between these tensors, understanding
their mathematical and computational interpretation.

Additionally, essential mathematical concepts for machine learning are introduced and
consolidated. Among them, gradients and automatic differentiation are studied, a central
feature of PyTorch and other deep learning libraries, which allows calculating
derivatives efficiently and accurately during model training. Classic models such as
linear regression and logistic regression are also addressed, analyzing their
mathematical formulation, their interpretation, and the role they play as the basis and
foundation of many modern machine learning algorithms.

The topic also includes the study of other mathematical tools useful in practice, such as
cosine similarity, the Hadamard product, and other related matrix operations. These
operations are especially relevant in various machine learning and deep learning
algorithms and architectures, for example in recommendation models, natural language
processing, or deep neural networks, where the relationships and combinations between
vectors and matrices constitute the core of computation.

From these foundations, the gradient descent method and its variants are presented,
explaining how it is used to adjust a model's parameters to the available data.
Subsequently, it shows how to combine the basic elements involved in learning a single
neuron or perceptron to build complete layers and, from these, multi-layer neural
networks (multilayer perceptrons). This step is key to understanding the transition from
simple linear models to what is now known as deep learning.

Finally, regularization techniques aimed at avoiding overfitting are introduced, that is,
the excessive adaptation of model parameters to training data. Different types of
optimizers designed to accelerate and stabilize the convergence process during training
are studied, as well as the most common metrics for evaluating model performance. In this
way, the topic offers an integrated vision that connects mathematical foundations, data
structures, training strategies, and evaluation, providing a solid framework for the
development and understanding of deep learning systems.
