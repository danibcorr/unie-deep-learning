{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders constitute a family of neural network architectures designed to learn\n",
    "compressed representations of data in an unsupervised manner. The fundamental structure\n",
    "of an autoencoder is organized into two main blocks: An encoder, which transforms the\n",
    "original input into a lower-dimensional latent representation, and a decoder, which takes\n",
    "this latent representation and reconstructs from it an approximation of the original\n",
    "input. The training objective consists of minimizing the discrepancy between the\n",
    "reconstructed output and the input, so that the model is forced to capture the most\n",
    "relevant characteristics of the data in the latent space.\n",
    "\n",
    "This document presents several variants of autoencoders, from basic dense architectures\n",
    "to more advanced models such as variational autoencoders (VAE), Beta-VAE, and VQ-VAE. All\n",
    "implementations are developed on the MNIST dataset and are provided as fully functional\n",
    "code, ready to be executed from start to finish in a Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Autoencoder with Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla autoencoder uses exclusively dense (fully connected) layers to encode and\n",
    "decode MNIST images. Each image of size $28 \\times 28$ is flattened into a vector of\n",
    "dimension 784 and projected into a lower-dimensional latent space. The encoder applies a\n",
    "sequence of linear transformations and nonlinear activation functions until it reaches\n",
    "the latent space, whereas the decoder performs the inverse process to reconstruct the\n",
    "image.\n",
    "\n",
    "This configuration introduces the central idea of autoencoders but exhibits clear\n",
    "limitations. Dense layers do not explicitly exploit the spatial structure of the image,\n",
    "which leads to a large number of parameters due to the full connectivity between neurons.\n",
    "In addition, since local relationships between pixels are not modeled explicitly,\n",
    "reconstructions tend to be blurrier and less detailed.\n",
    "\n",
    "The following code presents a basic functional implementation on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class VanillaAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int = 784, latent_dim: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        # Encoder: Progressively reduces dimensionality\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        # Decoder: Reconstructs from the latent space\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid(),  # Output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Flatten the image\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Encode\n",
    "        latent = self.encoder(x)\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(latent)\n",
    "        # Return to image shape\n",
    "        return reconstructed.view(-1, 1, 28, 28)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist_data(batch_size: int = 128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    device: str = \"cuda\",\n",
    ") -> nn.Module:\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(data)\n",
    "            loss = criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    num_images: int = 10,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "\n",
    "    model.eval()\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:num_images].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(data)\n",
    "\n",
    "    data = data.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "\n",
    "        axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla autoencoder execution\n",
    "train_loader, test_loader = prepare_mnist_data()\n",
    "vanilla_ae = VanillaAutoencoder(input_dim=784, latent_dim=32)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vanilla_ae = train_autoencoder(vanilla_ae, train_loader, num_epochs=10, device=device)\n",
    "visualize_reconstructions(vanilla_ae, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denoising autoencoder extends the previous approach by introducing noise into the\n",
    "input during training. In this case, the encoder receives a corrupted version of the\n",
    "image, while the loss function compares the decoder output with the clean original image.\n",
    "This mechanism forces the model to learn robust latent representations that capture the\n",
    "underlying structure of the data, rather than merely approximating the identity function.\n",
    "\n",
    "Noise is usually introduced as additive Gaussian noise, and values are subsequently\n",
    "clipped to keep them in the range $[0, 1]$. In this way, the model learns to \"undo\" the\n",
    "corruption, acting as a filter that preserves relevant content and discards spurious\n",
    "details.\n",
    "\n",
    "The following code illustrates an implementation of this variant on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int = 784, latent_dim: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images: torch.Tensor, noise_factor: float = 0.3) -> torch.Tensor:\n",
    "    noisy = images + noise_factor * torch.randn_like(images)\n",
    "    noisy = torch.clip(noisy, 0.0, 1.0)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_denoising_ae(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    device: str = \"cuda\",\n",
    "    noise_factor: float = 0.3,\n",
    ") -> nn.Module:\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for data, _ in train_loader:\n",
    "            clean_data = data.to(device)\n",
    "            noisy_data = add_noise(clean_data, noise_factor)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(noisy_data)\n",
    "            loss = criterion(reconstructed, clean_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_denoising(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    noise_factor: float = 0.3,\n",
    "    num_images: int = 10,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "\n",
    "    model.eval()\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:num_images].to(device)\n",
    "    noisy_data = add_noise(data, noise_factor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(noisy_data)\n",
    "\n",
    "    data = data.cpu()\n",
    "    noisy_data = noisy_data.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(3, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Original\", rotation=0, labelpad=40)\n",
    "\n",
    "        axes[1, i].imshow(noisy_data[i].squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Noisy\", rotation=0, labelpad=40)\n",
    "\n",
    "        axes[2, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n",
    "        axes[2, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[2, i].set_ylabel(\"Denoised\", rotation=0, labelpad=40)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising autoencoder execution\n",
    "denoising_ae = DenoisingAutoencoder(input_dim=784, latent_dim=32)\n",
    "denoising_ae = train_denoising_ae(\n",
    "    denoising_ae, train_loader, num_epochs=10, device=device\n",
    ")\n",
    "visualize_denoising(denoising_ae, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional autoencoders are better suited to image data because they explicitly\n",
    "exploit spatial structure. The encoder applies convolutions with shared weights and local\n",
    "filters; spatial dimensionality is reduced through stride and the stacking of layers. The\n",
    "decoder uses transposed convolutions to perform upsampling and reconstruct the original\n",
    "resolution.\n",
    "\n",
    "In this context, convolutions provide several advantages. They significantly reduce the\n",
    "number of parameters compared with dense layers, due to weight sharing across different\n",
    "spatial positions. They also capture local patterns and hierarchical structures (edges,\n",
    "digit parts, whole digits), which leads to sharper reconstructions that are more\n",
    "consistent with image content.\n",
    "\n",
    "The following implementation illustrates a convolutional autoencoder with a linear\n",
    "bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        # Convolutional encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 28x28 -> 14x14\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # 14x14 -> 7x7\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 7x7 -> 4x4 (slight additional reduction)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # Linear bottleneck\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.unflatten = nn.Unflatten(1, (128, 4, 4))\n",
    "\n",
    "        # Decoder with transposed convolutions\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 4x4 -> 7x7\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 7x7 -> 14x14\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # 14x14 -> 28x28\n",
    "            nn.ConvTranspose2d(\n",
    "                32, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        latent = self.fc_encode(x)\n",
    "        # Decode\n",
    "        x = self.fc_decode(latent)\n",
    "        x = self.unflatten(x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return reconstructed\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc_encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the convolutional autoencoder\n",
    "conv_ae = ConvAutoencoder(latent_dim=128)\n",
    "conv_ae = train_autoencoder(conv_ae, train_loader, num_epochs=10, device=device)\n",
    "visualize_reconstructions(conv_ae, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposed convolutions can introduce characteristic artifacts known as checkerboard\n",
    "artifacts, which arise when the combination of kernel size and stride produces uneven\n",
    "overlaps during the upsampling operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder with Interpolation-Based Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mitigate checkerboard artifacts, it is common to replace transposed convolutions with\n",
    "an upsampling strategy based on interpolation followed by standard convolutions. In this\n",
    "configuration, the spatial resolution is first increased by interpolation (bilinear,\n",
    "bicubic, etc.), and then a convolution is applied to refine the result and learn filters\n",
    "over the rescaled image.\n",
    "\n",
    "This procedure tends to produce smoother and visually more coherent reconstructions,\n",
    "significantly reducing undesired patterns at the cost of some additional computational\n",
    "cost.\n",
    "\n",
    "The following model preserves the same convolutional encoder as the previous autoencoder\n",
    "but replaces the `ConvTranspose2d`-based decoder with a decoder that combines `Upsample`\n",
    "and `Conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplingAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        # Encoder identical to the convolutional autoencoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_encode = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.unflatten = nn.Unflatten(1, (128, 4, 4))\n",
    "\n",
    "        # Decoder with upsampling + convolution\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 4x4 -> 7x7\n",
    "            nn.Upsample(size=(7, 7), mode=\"bilinear\", align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 7x7 -> 14x14\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # 14x14 -> 28x28\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        latent = self.fc_encode(x)\n",
    "        x = self.fc_decode(latent)\n",
    "        x = self.unflatten(x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and visualization\n",
    "upsampling_ae = UpsamplingAutoencoder(latent_dim=128)\n",
    "upsampling_ae = train_autoencoder(\n",
    "    upsampling_ae, train_loader, num_epochs=10, device=device\n",
    ")\n",
    "visualize_reconstructions(upsampling_ae, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of bilinear or bicubic interpolation followed by standard convolutions generally\n",
    "produces visually more pleasant reconstructions and significantly reduces checkerboard\n",
    "artifacts, while preserving the model's ability to capture high-level patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE) introduces an important conceptual change with respect\n",
    "to deterministic autoencoders. Instead of learning a direct mapping from the input to a\n",
    "fixed latent vector, the encoder learns the parameters of a probability distribution over\n",
    "the latent space. It is usually assumed that each latent dimension follows an independent\n",
    "Gaussian distribution, so the encoder produces a mean $\\mu$ and a logarithm of the\n",
    "variance $\\log \\sigma^2$ for each dimension.\n",
    "\n",
    "During training, a sample $z$ is drawn from the latent space using the reparameterization\n",
    "trick:\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon \\sim \\mathcal{N}(0, I)$ and\n",
    "\n",
    "$$\\sigma = \\exp\\left(\\tfrac{1}{2} \\log\\sigma^2\\right)$$\n",
    "\n",
    "This formulation allows gradients to be backpropagated through the sampling operation.\n",
    "\n",
    "The VAE loss function includes two terms. The first is the reconstruction loss, which\n",
    "measures the discrepancy between the original and reconstructed images (for example,\n",
    "using binary cross-entropy). The second is a regularization term based on the\n",
    "Kullback–Leibler (KL) divergence between the learned latent distribution and a standard\n",
    "normal distribution $\\mathcal{N}(0, I)$:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2}\\sum_{i} \\left(1 + \\log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2\\right)$$\n",
    "\n",
    "This term forces the latent space to adopt a well-structured distribution, facilitating\n",
    "sampling and the generation of new examples.\n",
    "\n",
    "The following code presents a convolutional VAE implementation for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim: int = 20) -> None:\n",
    "        super().__init__()\n",
    "        # Convolutional encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                32, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),  # 14x14 -> 28x28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc_decode(z)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(\n",
    "    reconstructed: torch.Tensor,\n",
    "    original: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    recon_loss = nn.functional.binary_cross_entropy(\n",
    "        reconstructed, original, reduction=\"sum\"\n",
    "    )\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    device: str = \"cuda\",\n",
    ") -> nn.Module:\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, mu, logvar = model(data)\n",
    "            loss = vae_loss(reconstructed, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Loss: {total_loss / len(train_loader.dataset):.4f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space_tsne(\n",
    "    model: VAE, data_loader: DataLoader, device: str = \"cuda\", n_samples: int = 5000\n",
    ") -> None:\n",
    "    \"\"\"Visualize the latent space using t-SNE.\"\"\"\n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            latent_vectors.append(mu.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            if len(latent_vectors) * data.size(0) >= n_samples:\n",
    "                break\n",
    "\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)[:n_samples]\n",
    "    labels = np.concatenate(labels, axis=0)[:n_samples]\n",
    "\n",
    "    print(\"Applying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    latent_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap=\"tab10\", alpha=0.6, s=5\n",
    "    )\n",
    "    plt.colorbar(scatter, label=\"Digit\")\n",
    "    plt.title(\"t-SNE Visualization of the VAE Latent Space\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(\n",
    "    model: VAE, num_samples: int = 16, latent_dim: int = 20, device: str = \"cuda\"\n",
    ") -> None:\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i].squeeze(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist_data(batch_size: int = 128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_loader, test_loader = prepare_mnist_data()\n",
    "\n",
    "# Train VAE\n",
    "vae = VAE(latent_dim=20)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae = train_vae(vae, train_loader, num_epochs=20, device=device)\n",
    "\n",
    "# Visualize latent space with t-SNE\n",
    "visualize_latent_space_tsne(vae, test_loader, device=device)\n",
    "\n",
    "# Generate synthetic samples\n",
    "generate_samples(vae, latent_dim=20, device=device)\n",
    "\n",
    "# Visualize reconstructions\n",
    "with torch.no_grad():\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:10].to(device)\n",
    "    reconstructed, _, _ = vae(data)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(data[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    axes[0, 0].set_ylabel(\"Original\", size=12)\n",
    "    axes[1, 0].set_ylabel(\"Reconstructed\", size=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEs are particularly useful for generating synthetic data by direct sampling in the\n",
    "latent space and for anomaly detection by analyzing out-of-distribution examples.\n",
    "However, they can suffer from the posterior collapse phenomenon, in which the decoder\n",
    "largely ignores latent information and learns to reconstruct from local patterns alone,\n",
    "reducing the quality and informativeness of latent representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta-VAE introduces a hyperparameter $\\beta$ in the VAE loss function to weight the\n",
    "KL divergence term:\n",
    "\n",
    "$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathcal{L}_{\\text{recon}} + \\beta \\,\\mathcal{L}_{\\text{KL}}$$\n",
    "\n",
    "When $\\beta > 1$, the model is forced to align the latent distribution more strongly with\n",
    "the standard normal distribution, which tends to produce more disentangled\n",
    "representations. In a disentangled latent space, each dimension preferentially captures\n",
    "an independent factor of variation in the data (for example, stroke thickness, slant, or\n",
    "size), improving interpretability and control over generated samples.\n",
    "\n",
    "Excessively high values of $\\beta$ can degrade reconstruction quality by penalizing\n",
    "latent code complexity too strongly.\n",
    "\n",
    "The following code shows how to adapt the loss and training procedure for a Beta-VAE\n",
    "using the VAE architecture defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_vae_loss(\n",
    "    reconstructed: torch.Tensor,\n",
    "    original: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    "    beta: float = 4.0,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    recon_loss = nn.functional.binary_cross_entropy(\n",
    "        reconstructed, original, reduction=\"sum\"\n",
    "    )\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_beta_vae(\n",
    "    model: VAE,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    beta: float = 4.0,\n",
    "    device: str = \"cuda\",\n",
    ") -> VAE:\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, mu, logvar = model(data)\n",
    "            loss = beta_vae_loss(reconstructed, data, mu, logvar, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the effect of controlled variations along individual latent dimensions, the\n",
    "latent traversal technique is used. It consists of systematically modifying a single\n",
    "latent coordinate while keeping the remaining ones fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_traversal(\n",
    "    model: VAE,\n",
    "    test_loader: DataLoader,\n",
    "    latent_dim: int = 20,\n",
    "    dim_to_vary: int = 0,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "\n",
    "    model.eval()\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[0:1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(data)\n",
    "        values = torch.linspace(-3, 3, 10)\n",
    "        samples = []\n",
    "\n",
    "        for val in values:\n",
    "            z = mu.clone()\n",
    "            z[0, dim_to_vary] = val\n",
    "            reconstructed = model.decode(z)\n",
    "            samples.append(reconstructed)\n",
    "\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "\n",
    "    samples = samples.cpu()\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i].squeeze(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"{values[i]:.1f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Beta-VAE\n",
    "beta_vae = VAE(latent_dim=20)\n",
    "beta_vae = train_beta_vae(\n",
    "    beta_vae, train_loader, num_epochs=20, beta=4.0, device=device\n",
    ")\n",
    "\n",
    "# Visualize variation of some latent dimensions\n",
    "for dim in range(5):\n",
    "    visualize_latent_traversal(beta_vae, test_loader, dim_to_vary=dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent traversal enables inspection of the influence of each latent dimension on\n",
    "generated samples, facilitating the interpretation of disentangled representations and\n",
    "the design of controlled manipulations over specific attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ-VAE (Vector Quantized VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQ-VAE introduces a fundamental modification in the treatment of the latent space.\n",
    "Instead of continuous codes, it uses a discrete representation based on a learned\n",
    "codebook of embeddings. The encoder projects the input into a continuous latent tensor of\n",
    "dimension $C$; each latent vector is then quantized by selecting the closest embedding\n",
    "from the codebook, that is, by assigning a discrete index. The decoder receives the\n",
    "quantized embeddings and reconstructs the input.\n",
    "\n",
    "This discretization offers several advantages. It avoids the posterior collapse problem\n",
    "typical of some VAEs, as quantization forces the model to actively use the latent space.\n",
    "Moreover, the discrete representation is particularly well suited to be modeled later\n",
    "using autoregressive models (for example, transformers), which has been crucial in\n",
    "generative architectures such as DALL·E. In this context, latent indices act as tokens on\n",
    "which language-modeling techniques can be applied.\n",
    "\n",
    "The following code presents a simple VQ-VAE implementation for MNIST, including the\n",
    "vector quantization module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VQ-VAE (Vector Quantized Variational Autoencoder) Implementation\"\"\"\n",
    "\n",
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantizer layer for VQ-VAE.\n",
    "    Converts continuous latent vectors into discrete codes from the codebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_embeddings: int, embedding_dim: int, commitment_cost: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        # Codebook of embeddings\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            quantized: Quantized tensor (B, C, H, W)\n",
    "            loss: Quantization loss (codebook + commitment)\n",
    "            encoding_indices: Indices of selected codebook vectors\n",
    "        \"\"\"\n",
    "        # Reorder to (B, H, W, C)\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten to (B*H*W, C)\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "\n",
    "        # L2 distances to each codebook embedding\n",
    "        distances = (\n",
    "            torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self.embeddings.weight.t())\n",
    "        )\n",
    "\n",
    "        # Index of nearest embedding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "\n",
    "        # One-hot encoding\n",
    "        encodings = torch.zeros(\n",
    "            encoding_indices.shape[0], self.num_embeddings, device=inputs.device\n",
    "        )\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantization via codebook\n",
    "        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)\n",
    "\n",
    "        # VQ losses\n",
    "        e_latent_loss = nn.functional.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = nn.functional.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        # Back to (B, C, H, W)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return quantized, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    VQ-VAE model with encoder, vector quantizer, and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int = 512, embedding_dim: int = 64) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: (1, 28, 28) -> (embedding_dim, 7, 7)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, embedding_dim, kernel_size=1),  # 7x7, C=embedding_dim\n",
    "        )\n",
    "\n",
    "        # Vector Quantizer\n",
    "        self.vq = VectorQuantizer(num_embeddings, embedding_dim)\n",
    "\n",
    "        # Decoder: (embedding_dim, 7, 7) -> (1, 28, 28)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                embedding_dim, 64, kernel_size=4, stride=2, padding=1\n",
    "            ),  # 7x7 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=4, stride=2, padding=1\n",
    "            ),  # 14x14 -> 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, 1, 28, 28)\n",
    "        Returns:\n",
    "            reconstructed: Reconstruction (B, 1, 28, 28)\n",
    "            vq_loss: Vector quantization loss\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq(z)\n",
    "        reconstructed = self.decoder(quantized)\n",
    "        return reconstructed, vq_loss\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Encode and quantize the input.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        quantized, _, indices = self.vq(z)\n",
    "        return quantized, indices\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode a quantized latent tensor.\"\"\"\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(\n",
    "    model: VQVAE,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\",\n",
    ") -> VQVAE:\n",
    "    \"\"\"\n",
    "    Train the VQ-VAE model.\n",
    "\n",
    "    Args:\n",
    "        model: VQVAE model.\n",
    "        train_loader: Training DataLoader.\n",
    "        num_epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "        device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_recon_loss = 0.0\n",
    "        total_vq_loss = 0.0\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            reconstructed, vq_loss = model(data)\n",
    "            recon_loss = nn.functional.mse_loss(reconstructed, data)\n",
    "            loss = recon_loss + vq_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_vq_loss += vq_loss.item()\n",
    "\n",
    "        avg_recon = total_recon_loss / len(train_loader)\n",
    "        avg_vq = total_vq_loss / len(train_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "            f\"Recon Loss: {avg_recon:.6f} | \"\n",
    "            f\"VQ Loss: {avg_vq:.6f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vqvae_reconstructions(\n",
    "    model: VQVAE, test_loader: DataLoader, device: str = \"cuda\", num_images: int = 8\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original and VQ-VAE reconstructed images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:num_images].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed, _ = model(data)\n",
    "\n",
    "    data = data.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(12, 3))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(data[i].squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    axes[0, 0].set_ylabel(\"Original\", size=12)\n",
    "    axes[1, 0].set_ylabel(\"Reconstructed\", size=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main VQ-VAE execution\n",
    "NUM_EMBEDDINGS = 512\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_EPOCHS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vqvae = VQVAE(num_embeddings=NUM_EMBEDDINGS, embedding_dim=EMBEDDING_DIM)\n",
    "vqvae = train_vqvae(vqvae, train_loader, num_epochs=NUM_EPOCHS, device=DEVICE)\n",
    "visualize_vqvae_reconstructions(vqvae, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQ-VAE provides a discrete latent space that is particularly suitable for integration\n",
    "into multimodal systems and complex generative models, in which tokenization of data is\n",
    "essential. Vector quantization offers a robust foundation for applying advanced\n",
    "sequential modeling techniques to image representations and facilitates integration with\n",
    "language architectures that operate on discrete sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
