{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    " La idea aquí es ver cómo utilizamos autoencoders, utilizando por ejemplo en Mist, que va a ser uno de los ejemplos más extendidos. La idea es utilizar un autoencoder vanilla, muy sencillito, basado en capas densas, ver las ventajas y desventajas que tiene, luego pasar a utilizar convoluciones y hablar del... Luego también de la convolución transpuesta, las ventajas y desventajas que presentan y cómo podemos, por ejemplo, utilizar mecanismos de upsampling, utilizando técnicas como la interpolación o interpolación lineal, bicúbica o similares para reducir artefactos que se producen durante la reconstrucción de las imágenes. Qué ventajas o desventajas presentan aquellos, ya que se producen artefactos conocidos como el patrón del ajedrez. Luego también veremos cómo estos autoencoders han ido evolucionando, tratando de los autoencoders variacionales, que tienden a aproximar lo que hacen es obtener una distribución de probabilidades de cada uno de los datos, y que se utilizan sobre todo, por ejemplo, para detectar sistemas fuera de distribución, por ejemplo. Y cómo se han también desarrollado diferentes alternativas a los autoencoders variacionales, como pueden ser bqbae, que se sigue utilizando a día... Bueno, todas estas técnicas se siguen utilizando, pero por ejemplo, bqbae lo que te permite es una representación discreta de representaciones en bebidas de los datos de entrada, y es muy utilizado, por ejemplo, en sistemas generativos, o luego también para tokenizar de manera discreta los datos de entrada e introducirlos a un LLM o cosas así. O luego también los beta-wise, porque los bytes y los autoencoders, aunque también las capas convolucionales, sufren de un problema que se conoce como el colapso de las representaciones, es decir, el modelo tiene... Todos los filtros o canales que aprenden los modelos pueden llegar a ser tan grandes que pueden llegar a aprender los mismos conceptos. Por lo tanto, hay pesos que se parecen mucho y que realmente no están aprendiendo, no hay tanta diferenciación. Por lo tanto, hay un gran coste computacional y de consumo que no se está aprovechando, porque están obteniendo una misma representación. Eso sería un poco una definición del colapso. Y los autoencoders variacionales sufren bastante de este colapso, porque empiezan a perder mucho, a no fijarse tanto, a perder mucha información. Sobre todo con la profundidad y si no se utilizan sistemas de regularizaciones muy fuertes. Entonces se implementa sistemas como Beta-By, que permite hacer un disentanglement del espacio embebido.\n",
    " \n",
    "- Implementar un autoencoder para MNIST normal, un Denoise AE, aqui entra el juego las convoluciones transpuestas y/o el uso de métodos de Upsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Auto Encoders basados en CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Variational Auto Encoders (VAE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
