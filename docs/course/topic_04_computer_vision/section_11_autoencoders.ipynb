{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de3306",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972707bc",
   "metadata": {},
   "source": [
    "## Auto Encoders basados en CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f3eb6",
   "metadata": {},
   "source": [
    "## Variational Auto Encoders (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d29fb",
   "metadata": {},
   "source": [
    "## Vector Quantization Variational Auto Encoders (VQ-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_size: int = 256) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a residual block that applies two convolutional\n",
    "        layers and ReLU activations.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels for the block.\n",
    "            hidden_size: Number of channels in the hidden layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.hidden_size,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.hidden_size,\n",
    "                out_channels=self.in_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the residual block.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: The input tensor to the block.\n",
    "\n",
    "        Returns:\n",
    "            A tensor that is the sum of the input tensor and the\n",
    "            block's output.\n",
    "        \"\"\"\n",
    "\n",
    "        return input_tensor + self.res_block(input_tensor)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_residuals: int,\n",
    "        hidden_size: int = 256,\n",
    "        kernel_size: int = 4,\n",
    "        stride: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes an encoder with convolutional layers and residual\n",
    "        blocks.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels to the encoder.\n",
    "            num_residuals: Number of residual blocks in the encoder.\n",
    "            hidden_size: Number of channels in hidden layers.\n",
    "            kernel_size: Size of the convolutional kernels.\n",
    "            stride: Stride of the convolutional kernels.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_residuals = num_residuals\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBlock(in_channels=hidden_size, hidden_size=hidden_size)\n",
    "                for _ in range(self.num_residuals)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: The input tensor to the encoder.\n",
    "\n",
    "        Returns:\n",
    "            A tensor processed by convolutional layers and residual\n",
    "            blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_output = self.model(input_tensor)\n",
    "        for res_block in self.residual_blocks:\n",
    "            encoder_output = res_block(encoder_output)\n",
    "        return encoder_output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_residuals: int,\n",
    "        out_channels: int = 3,  # Channel output (RGB)\n",
    "        hidden_size: int = 256,\n",
    "        kernel_size: int = 4,\n",
    "        stride: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a decoder with residual blocks and transpose\n",
    "        convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels to the decoder.\n",
    "            num_residuals: Number of residual blocks in the decoder.\n",
    "            out_channels: Number of output channels, e.g., RGB.\n",
    "            hidden_size: Number of channels in hidden layers.\n",
    "            kernel_size: Size of the convolutional kernels.\n",
    "            stride: Stride of the convolutional kernels.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_residuals = num_residuals\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    in_channels=self.in_channels, hidden_size=self.hidden_size\n",
    "                )\n",
    "                for _ in range(self.num_residuals)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.hidden_size,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.hidden_size,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: The input tensor to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            A tensor processed by residual blocks and transpose\n",
    "            convolutional layers.\n",
    "        \"\"\"\n",
    "\n",
    "        decoder_output = input_tensor\n",
    "        for res_block in self.residual_blocks:\n",
    "            decoder_output = res_block(decoder_output)\n",
    "\n",
    "        return self.model(decoder_output)\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, size_discrete_space: int, size_embeddings: int, beta: float = 0.25\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a vector quantizer with a learnable codebook.\n",
    "\n",
    "        Args:\n",
    "            size_discrete_space: Number of discrete embeddings.\n",
    "            size_embeddings: Size of each embedding vector.\n",
    "            beta: Weighting factor for the commitment loss.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.size_discrete_space = size_discrete_space\n",
    "        self.size_embeddings = size_embeddings\n",
    "        self.beta = beta\n",
    "\n",
    "        # Definimos el codebook como una matriz de K embeddings x D tamaño de embeddings\n",
    "        # Ha de ser una matriz aprendible\n",
    "        self.codebook = nn.Embedding(\n",
    "            num_embeddings=self.size_discrete_space, embedding_dim=self.size_embeddings\n",
    "        )\n",
    "        # Initialize weights uniformly\n",
    "        self.codebook.weight.data.uniform_(\n",
    "            -1 / self.size_discrete_space, 1 / self.size_discrete_space\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, encoder_output: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantizes the encoder output using the codebook.\n",
    "\n",
    "        Args:\n",
    "            encoder_output: Tensor of encoder outputs.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing VQ loss, quantized tensor, perplexity,\n",
    "            and encodings.\n",
    "        \"\"\"\n",
    "\n",
    "        # Comentario de otras implementaciones: The channels are used as the space\n",
    "        # in which to quantize.\n",
    "        # Encoder output ->  (B, C, H, W) -> (0, 1, 2, 3) -> (0, 2, 3, 1) -> (0*2*3, 1)\n",
    "        encoder_output = encoder_output.permute(0, 2, 3, 1).contiguous()\n",
    "        b, h, w, c = encoder_output.size()\n",
    "        encoder_output_flat = encoder_output.reshape(-1, c)\n",
    "\n",
    "        # Calculamos la distancia entre ambos vectores\n",
    "        distances = (\n",
    "            torch.sum(encoder_output_flat**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.codebook.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(encoder_output_flat, self.codebook.weight.t())\n",
    "        )\n",
    "\n",
    "        # Realizamos el encoding y extendemos una dimension (B*H*W, 1)\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "\n",
    "        # Matriz de ceros de (indices, size_discrete_space)\n",
    "        encodings = torch.zeros(\n",
    "            encoding_indices.shape[0],\n",
    "            self.size_discrete_space,\n",
    "            device=encoder_output.device,\n",
    "        )\n",
    "        # Colocamos un 1 en los indices de los encodings con el\n",
    "        # valor mínimo de distancia creando un vector one-hot\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Se cuantiza colocando un cero en los pesos no relevantes (distancias grandes)\n",
    "        # del codebook y le damos formato de nuevo al tensor\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight).view(b, h, w, c)\n",
    "\n",
    "        # VQ-VAE loss terms\n",
    "        # L = ||sg[z_e] - e||^2 + β||z_e - sg[e]||^2\n",
    "        # FIX: Corrected variable names and loss calculation\n",
    "        commitment_loss = F.mse_loss(\n",
    "            quantized.detach(), encoder_output\n",
    "        )  # ||sg[z_e] - e||^2\n",
    "        embedding_loss = F.mse_loss(\n",
    "            quantized, encoder_output.detach()\n",
    "        )  # ||z_e - sg[e]||^2\n",
    "        vq_loss = commitment_loss + self.beta * embedding_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = encoder_output + (quantized - encoder_output).detach()\n",
    "\n",
    "        # Calculate perplexity\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return (\n",
    "            vq_loss,\n",
    "            quantized.permute(0, 3, 1, 2).contiguous(),\n",
    "            perplexity,\n",
    "            encodings,\n",
    "        )\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        size_discrete_space: int,\n",
    "        size_embeddings: int,\n",
    "        num_residuals: int,\n",
    "        hidden_size: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        beta: float = 0.25,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a VQ-VAE model with encoder, decoder, and quantizer.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels for the model.\n",
    "            size_discrete_space: Number of discrete embeddings.\n",
    "            size_embeddings: Size of each embedding vector.\n",
    "            num_residuals: Number of residual blocks in encoder/decoder.\n",
    "            hidden_size: Number of channels in hidden layers.\n",
    "            kernel_size: Size of convolutional kernels.\n",
    "            stride: Stride of convolutional kernels.\n",
    "            beta: Weighting factor for the commitment loss.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.size_discrete_space = size_discrete_space\n",
    "        self.size_embeddings = size_embeddings\n",
    "        self.num_residuals = num_residuals\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.beta = beta\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=self.in_channels,\n",
    "            num_residuals=self.num_residuals,\n",
    "            hidden_size=self.hidden_size,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            in_channels=self.hidden_size,\n",
    "            num_residuals=self.num_residuals,\n",
    "            out_channels=self.in_channels,\n",
    "            hidden_size=self.hidden_size,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "\n",
    "        self.vector_quantizer = VectorQuantizer(\n",
    "            size_discrete_space=self.size_discrete_space,\n",
    "            size_embeddings=self.hidden_size,\n",
    "            beta=self.beta,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through VQ-VAE model.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Input tensor to the model.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing VQ loss, reconstructed tensor,\n",
    "            and perplexity.\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_output = self.encoder(input_tensor)\n",
    "        vq_loss, quantized, perplexity, _ = self.vector_quantizer(encoder_output)\n",
    "        decoder_output = self.decoder(quantized)\n",
    "        return vq_loss, decoder_output, perplexity\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = VQVAE(\n",
    "        in_channels=3,\n",
    "        size_discrete_space=512,\n",
    "        size_embeddings=64,\n",
    "        num_residuals=2,\n",
    "        hidden_size=128,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        beta=0.25,\n",
    "    )\n",
    "\n",
    "    x = torch.randn(4, 3, 64, 64)\n",
    "    vq_loss, reconstruction, perplexity = model(x)\n",
    "\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Reconstruction shape: {reconstruction.shape}\")\n",
    "    print(f\"VQ Loss: {vq_loss.item():.4f}\")\n",
    "    print(f\"Perplexity: {perplexity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5133e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
