{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Context and Relevance of LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **LeNet-5** architecture, developed by **Yann LeCun** and collaborators between 1988\n",
    "and 1998, represents one of the foundational early architectures in the field of\n",
    "convolutional neural networks. Its design was aimed at efficiently solving the problem\n",
    "of automatic handwritten character recognition. This architecture was successfully\n",
    "implemented in real-world systems for automatic check processing in the United States,\n",
    "making it one of the first industrial applications of deep learning.\n",
    "\n",
    "LeNet-5 demonstrates that it is possible to learn hierarchical representations directly\n",
    "from images while preserving the spatial structure of the data and significantly\n",
    "reducing the number of parameters compared to traditional fully connected networks.\n",
    "Unlike the previously used fully connected multilayer perceptrons, which treat each\n",
    "pixel as an independent feature and lose spatial layout information, LeNet-5 explicitly\n",
    "exploits the two-dimensional structure of images and the local correlation between\n",
    "nearby pixels.\n",
    "\n",
    "This architecture illustrates how the combination of convolutions, subsampling, and\n",
    "nonlinear activation functions allows the construction of systems robust to variations\n",
    "in position, scale, or moderate deformations of handwritten patterns, while maintaining\n",
    "manageable computational complexity. In this way, LeNet-5 serves as a direct predecessor\n",
    "of many modern computer vision architectures, establishing design principles that\n",
    "persist to this day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Foundations of LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the introduction of LeNet-5, image recognition was mainly approached using fully\n",
    "connected multilayer perceptrons. This approach has a fundamental structural limitation:\n",
    "the image is flattened into a one-dimensional vector, and the model completely loses\n",
    "information about the relative spatial arrangement of pixels. As a result, the system\n",
    "becomes extremely sensitive to small translations, local deformations, or changes in the\n",
    "object's position within the image. Additionally, the number of parameters grows rapidly\n",
    "with image size, making training difficult and favoring overfitting, especially with\n",
    "limited datasets.\n",
    "\n",
    "LeNet-5 introduced a decisive conceptual change through the combined use of\n",
    "convolutions, subsampling, and weight sharing. Convolutional layers detect local\n",
    "patterns, such as edges, corners, or strokes, while preserving the image's\n",
    "two-dimensional structure. Each convolutional filter slides over the image and acts as a\n",
    "specialized detector for a certain type of visual pattern. Subsampling, implemented\n",
    "using _average pooling_ operations, progressively reduces the spatial resolution of\n",
    "feature maps. This reduction provides approximate invariance to small translations and\n",
    "deformations while reducing computational cost and the number of parameters in\n",
    "subsequent layers.\n",
    "\n",
    "Weight sharing is another key design element: the same convolutional filter is applied\n",
    "across all positions in the image. Instead of learning a separate set of weights for\n",
    "each input pixel, a reduced set of filters is learned and reused throughout the image.\n",
    "This mechanism drastically reduces the number of parameters and enhances generalization,\n",
    "since the same pattern (e.g., a vertical stroke) may appear in different regions of the\n",
    "image and should be recognized consistently in all of them.\n",
    "\n",
    "Together, these mechanisms allow LeNet-5 to learn hierarchical representations\n",
    "progressively. Early layers capture simple, local patterns, while deeper layers combine\n",
    "these patterns to form higher-level features, increasingly abstract and task-specific.\n",
    "This hierarchical feature organization remains a fundamental principle in most modern\n",
    "convolutional neural network architectures, from AlexNet to transformer-based vision\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original LeNet-5 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original LeNet-5 architecture consists of seven trainable layers combining\n",
    "convolutions, subsampling, and fully connected layers. The input consists of grayscale\n",
    "images of size $32 \\times 32$, slightly larger than the standard MNIST image size of\n",
    "$28 \\times 28$. This enlargement introduces a margin around the digit, facilitating the\n",
    "application of convolutional filters and handling small translations without losing\n",
    "relevant edge information.\n",
    "\n",
    "Simplified, the architecture is organized into a convolutional part followed by a fully\n",
    "connected part. The convolutional part alternates convolutional layers and _average\n",
    "pooling_ layers, while the final part consists of dense layers that perform\n",
    "classification. The characteristic dimensions of each layer can be summarized as\n",
    "follows:\n",
    "\n",
    "| Layer  | Type            | Input                    | Output                   |\n",
    "| ------ | --------------- | ------------------------ | ------------------------ |\n",
    "| C1     | Convolution     | $32 \\times 32 \\times 1$  | $28 \\times 28 \\times 6$  |\n",
    "| S2     | Average Pooling | $28 \\times 28 \\times 6$  | $14 \\times 14 \\times 6$  |\n",
    "| C3     | Convolution     | $14 \\times 14 \\times 6$  | $10 \\times 10 \\times 16$ |\n",
    "| S4     | Average Pooling | $10 \\times 10 \\times 16$ | $5 \\times 5 \\times 16$   |\n",
    "| C5     | Convolution     | $5 \\times 5 \\times 16$   | $1 \\times 1 \\times 120$  |\n",
    "| F6     | Fully Connected | 120                      | 84                       |\n",
    "| Output | Fully Connected | 84                       | 10                       |\n",
    "\n",
    "The original architecture uses sigmoid or $\\tanh$ activation functions and applies\n",
    "_average pooling_ instead of the now more common _max pooling_. The total number of\n",
    "trainable parameters is around 60,000, relatively small compared to fully connected\n",
    "networks of similar size. Despite its apparent simplicity, LeNet-5 establishes a\n",
    "structural pattern still present in many contemporary architectures: a convolutional\n",
    "part for extracting spatial features followed by one or more dense layers that perform\n",
    "final classification using linear combinations of the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents a modern, functional implementation inspired by LeNet using\n",
    "PyTorch. The goal is to have a complete workflow, executable end-to-end and directly\n",
    "convertible into a Jupyter Notebook. The MNIST dataset is used as the reference dataset\n",
    "for handwritten digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the necessary libraries from Python’s standard library and third-party packages\n",
    "are imported, including modules for model construction, data handling, visualization,\n",
    "and embedding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "from typing import Any\n",
    "\n",
    "# 3pps\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device is automatically selected based on GPU availability. If CUDA is available,\n",
    "the GPU is used; otherwise, the model runs on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Visualization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function is defined to visually inspect examples from the dataset, displaying a\n",
    "set of images with their corresponding labels. This helps quickly verify that\n",
    "preprocessing is correct and samples are interpreted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(f\"Digit: {label}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains grayscale images of handwritten digits sized $28 \\times 28$.\n",
    "Preprocessing includes normalization using the mean $\\mu = 0.1307$ and standard\n",
    "deviation $\\sigma = 0.3081$, estimated from the dataset itself. Normalization is defined\n",
    "as:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "This centers the data and scales it, facilitating and stabilizing the training of deep\n",
    "networks by improving the numerical conditioning of optimization operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders are created from the training and test sets to batch samples, shuffle\n",
    "training examples, and efficiently handle data transfer to the computation device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, it is useful to inspect some training samples. The mean and standard\n",
    "deviation of a batch are also computed to verify proper normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "show_images(images[:10], labels[:10])\n",
    "\n",
    "print(f\"Batch mean: {images.mean():.3f}\")\n",
    "print(f\"Batch standard deviation: {images.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Modern LeNet Version in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modern, simplified version of LeNet is defined, adapted to MNIST and current deep\n",
    "learning practices. While not exactly replicating the original LeNet-5, it preserves the\n",
    "design spirit: a convolutional part for spatial feature extraction and a fully connected\n",
    "part for classification. Batch normalization and ReLU activation are included, standard\n",
    "in contemporary architectures, improving convergence speed and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_channels: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(32, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features` block applies two convolutional layers with spatial resolution reduction\n",
    "via `stride=2`, followed by batch normalization and ReLU activation.\n",
    "`nn.AdaptiveAvgPool2d((1, 1))` then adaptively reduces each feature map to size\n",
    "$1 \\times 1$ per channel, making the architecture robust to small spatial input\n",
    "variations. The `classifier` block flattens the features and applies a linear layer to\n",
    "produce logits for the 10 digit classes, later interpreted by `CrossEntropyLoss`, which\n",
    "internally applies softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is instantiated, moved to the selected device, and `torchinfo.summary`\n",
    "provides a structured architecture overview, including input/output dimensions and\n",
    "parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28), device=str(device))\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyperparameters, optimizer, and loss function are defined. AdamW is used,\n",
    "combining Adam’s advantages with explicit weight decay regularization. The chosen loss\n",
    "is `CrossEntropyLoss`, suitable for multi-class classification with integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is organized in epochs. Each epoch updates model parameters on the training\n",
    "set, then evaluates performance on the test set without updating parameters. Loss and\n",
    "accuracy are recorded for both sets to analyze learning progression and detect issues\n",
    "such as overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(\n",
    "        train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\"\n",
    "    ):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_dataloader))\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(\n",
    "            test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TEST]\"\n",
    "        ):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_dataloader))\n",
    "    test_accuracies.append(100 * correct / total)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train → Loss: {train_losses[-1]:.4f} | Acc: {train_accuracies[-1]:.2f}%\")\n",
    "    print(f\"  Test  → Loss: {test_losses[-1]:.4f} | Acc: {test_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.train()` activates training-specific behaviors, such as updating batch\n",
    "normalization statistics and applying dropout if present. `model.eval()` disables these\n",
    "behaviors for deterministic evaluation, and `torch.no_grad()` during validation avoids\n",
    "gradient computation, reducing memory usage and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Metric Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, loss and accuracy evolution for training and testing is plotted. This\n",
    "visual analysis helps identify overfitting, underfitting, or learning stagnation,\n",
    "guiding potential architecture or hyperparameter adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Evolution\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Evolution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing training and testing curves provides insights into model generalization. For\n",
    "example, increasing training accuracy with stagnant or decreasing test accuracy usually\n",
    "indicates overfitting, while high loss on both sets suggests insufficient model capacity\n",
    "or training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the structure of embeddings produced by the model is analyzed using t-SNE\n",
    "(_t-distributed Stochastic Neighbor Embedding_). The model’s linear outputs (logits) are\n",
    "extracted as example representations. t-SNE projects these high-dimensional vectors into\n",
    "2D space while preserving local neighborhood relations. This projection visually shows\n",
    "how the model separates different classes in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "max_samples = 1000\n",
    "embeddings, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        if len(all_labels) * train_dataloader.batch_size >= max_samples:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        embeddings.append(outputs.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "embeddings = torch.cat(embeddings).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    learning_rate=200,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "X_embedded = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap=\"tab10\", alpha=0.6, s=10\n",
    ")\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"t-SNE of embeddings learned by LeNet\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has learned a good data representation, points corresponding to different\n",
    "classes tend to cluster relatively separately in 2D space. This visualization provides\n",
    "an intuitive perspective on how the model internally organizes information and\n",
    "distinguishes handwritten digit classes. Clear separation indicates that the\n",
    "network-induced feature space facilitates linear classification in the final layer,\n",
    "confirming that learned representations are discriminative and semantically meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
