{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Context and Relevance of LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **LeNet-5** architecture, developed by **Yann LeCun** and his collaborators between 1988 and 1998, constitutes one of the earliest and most influential convolutional neural network architectures. It was specifically designed to address the problem of automatic handwritten character recognition, a task of considerable practical importance at the time. LeNet-5 was not merely a theoretical contribution, but was successfully deployed in real-world industrial systems, most notably for automatic check processing in the United States. For this reason, it is widely regarded as one of the first concrete and large-scale applications of deep learning in an operational setting.\n",
    "\n",
    "A central contribution of LeNet-5 lies in its demonstration that hierarchical feature representations can be learned directly from raw image data. By progressively transforming the input through multiple layers, the network captures increasingly abstract patterns while preserving the underlying spatial organization of the image. This approach leads to a significant reduction in the number of trainable parameters when compared to traditional fully connected multilayer perceptrons. In earlier architectures, each pixel was treated as an independent input feature, resulting in a loss of spatial information and a rapid growth in the number of parameters as input resolution increased. LeNet-5 overcomes these limitations by explicitly leveraging the two-dimensional structure of images and the strong local correlations that exist between neighboring pixels.\n",
    "\n",
    "The architecture illustrates how the coordinated use of convolutional layers, subsampling operations, and nonlinear activation functions enables the construction of models that are both expressive and computationally efficient. Convolutional layers enforce local connectivity and parameter sharing, subsampling layers progressively reduce spatial resolution while increasing robustness, and nonlinearities allow the network to model complex decision boundaries. Together, these components yield systems that are resilient to variations in position, scale, and moderate geometric deformations of handwritten characters, without incurring prohibitive computational costs.\n",
    "\n",
    "Through these design choices, LeNet-5 established a set of architectural principles that continue to underpin modern convolutional neural networks used in computer vision today. Its emphasis on spatial locality, hierarchical feature learning, and efficiency makes it a direct conceptual ancestor of many contemporary deep learning models, and a foundational milestone in the historical development of neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Foundations of LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the introduction of LeNet-5, image recognition tasks were predominantly addressed using fully connected multilayer perceptrons. This approach exhibits a fundamental structural limitation: the two-dimensional image is flattened into a one-dimensional vector before being processed by the network. As a consequence, all information about the relative spatial arrangement of pixels is lost. Pixels that are neighbors in the original image are treated in the same way as pixels that are far apart, preventing the model from exploiting spatial locality. This representation makes the system highly sensitive to small translations, local deformations, or changes in the position of the object within the image. Moreover, as image resolution increases, the number of parameters grows rapidly, leading to high computational costs, difficulties during training, and a strong tendency toward overfitting, particularly when only limited training data are available.\n",
    "\n",
    "LeNet-5 introduced a decisive conceptual shift by combining convolutional operations, subsampling mechanisms, and systematic weight sharing. Convolutional layers are designed to detect local patterns, such as edges, corners, or elementary strokes, while explicitly preserving the two-dimensional structure of the input image. Each convolutional filter is applied by sliding it across the image, acting as a specialized detector that responds strongly to a specific visual pattern wherever it appears. In this way, local features are extracted consistently across the entire spatial extent of the image.\n",
    "\n",
    "Subsampling layers, implemented in LeNet-5 through average pooling operations, further transform these feature maps by progressively reducing their spatial resolution. This dimensionality reduction introduces a degree of invariance to small translations and minor geometric deformations, as the precise location of a feature becomes less critical at coarser scales. At the same time, subsampling significantly decreases computational complexity and reduces the number of parameters required in subsequent layers, thereby improving efficiency and stability during training.\n",
    "\n",
    "A further key design principle is weight sharing. Instead of learning a distinct set of weights for each spatial position, the same convolutional filter is reused across all locations in the image. This approach drastically reduces the total number of parameters and enforces the idea that a meaningful visual pattern, such as a vertical stroke or an edge, should be recognized consistently regardless of its position. As a result, weight sharing enhances generalization and contributes to the robustness of the learned representations.\n",
    "\n",
    "The combined effect of convolution, subsampling, and weight sharing enables LeNet-5 to learn hierarchical representations in a progressive and structured manner. Early layers capture simple, localized features, while deeper layers integrate these elements into increasingly abstract and task-specific representations. This hierarchical organization of features remains a fundamental principle in contemporary convolutional neural network architectures, forming the conceptual foundation for models ranging from AlexNet to more recent vision systems, including transformer-based architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original LeNet-5 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Organization of the LeNet-5 Architecture\n",
    "\n",
    "The original LeNet-5 architecture is composed of seven trainable layers that combine convolutional operations, subsampling mechanisms, and fully connected transformations. The network is designed to process grayscale images of size $32 \\times 32$, a resolution that is slightly larger than the standard MNIST digit images of $28 \\times 28$. This deliberate enlargement introduces a uniform margin around the digit, which facilitates the application of convolutional filters near image boundaries and allows the model to handle small translations without discarding relevant edge information.\n",
    "\n",
    "From a structural perspective, the architecture can be divided into two main components: a convolutional feature extraction stage followed by a fully connected classification stage. In the convolutional part, convolutional layers and average pooling layers alternate, progressively transforming the input image into a set of compact and informative feature representations. The final part of the network consists of dense layers that operate on these extracted features to perform classification.\n",
    "\n",
    "The characteristic dimensions of the layers in the original LeNet-5 architecture are summarized in the following table, which illustrates how spatial resolution decreases while the number of feature channels increases as the data propagate through the network:\n",
    "\n",
    "| Layer  | Type            | Input                    | Output                   |\n",
    "| ------ | --------------- | ------------------------ | ------------------------ |\n",
    "| C1     | Convolution     | $32 \\times 32 \\times 1$  | $28 \\times 28 \\times 6$  |\n",
    "| S2     | Average Pooling | $28 \\times 28 \\times 6$  | $14 \\times 14 \\times 6$  |\n",
    "| C3     | Convolution     | $14 \\times 14 \\times 6$  | $10 \\times 10 \\times 16$ |\n",
    "| S4     | Average Pooling | $10 \\times 10 \\times 16$ | $5 \\times 5 \\times 16$   |\n",
    "| C5     | Convolution     | $5 \\times 5 \\times 16$   | $1 \\times 1 \\times 120$  |\n",
    "| F6     | Fully Connected | 120                      | 84                       |\n",
    "| Output | Fully Connected | 84                       | 10                       |\n",
    "\n",
    "In the original implementation, LeNet-5 employs sigmoid or $\\tanh$ activation functions rather than the rectified linear units commonly used in modern architectures. Furthermore, subsampling is performed using average pooling instead of max pooling, reflecting both the theoretical preferences and computational constraints of the period in which the model was developed. Despite these differences, the total number of trainable parameters is approximately 60,000, which is relatively small when compared to fully connected networks designed to process inputs of similar dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents a modern, functional implementation inspired by LeNet using\n",
    "PyTorch. The goal is to have a complete workflow, executable end-to-end and directly\n",
    "convertible into a Jupyter Notebook. The MNIST dataset is used as the reference dataset\n",
    "for handwritten digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the necessary libraries from Python’s standard library and third-party packages\n",
    "are imported, including modules for model construction, data handling, visualization,\n",
    "and embedding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "from typing import Any\n",
    "\n",
    "# 3pps\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device is automatically selected based on GPU availability. If CUDA is available,\n",
    "the GPU is used; otherwise, the model runs on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Visualization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function is defined to visually inspect examples from the dataset, displaying a\n",
    "set of images with their corresponding labels. This helps quickly verify that\n",
    "preprocessing is correct and samples are interpreted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(f\"Digit: {label}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains grayscale images of handwritten digits sized $28 \\times 28$.\n",
    "Preprocessing includes normalization using the mean $\\mu = 0.1307$ and standard\n",
    "deviation $\\sigma = 0.3081$, estimated from the dataset itself. Normalization is defined\n",
    "as:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "This centers the data and scales it, facilitating and stabilizing the training of deep\n",
    "networks by improving the numerical conditioning of optimization operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders are created from the training and test sets to batch samples, shuffle\n",
    "training examples, and efficiently handle data transfer to the computation device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, it is useful to inspect some training samples. The mean and standard\n",
    "deviation of a batch are also computed to verify proper normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "show_images(images[:10], labels[:10])\n",
    "\n",
    "print(f\"Batch mean: {images.mean():.3f}\")\n",
    "print(f\"Batch standard deviation: {images.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Modern LeNet Version in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modern, simplified version of LeNet is defined, adapted to MNIST and current deep\n",
    "learning practices. While not exactly replicating the original LeNet-5, it preserves the\n",
    "design spirit: a convolutional part for spatial feature extraction and a fully connected\n",
    "part for classification. Batch normalization and ReLU activation are included, standard\n",
    "in contemporary architectures, improving convergence speed and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_channels: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(32, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features` block applies two convolutional layers with spatial resolution reduction\n",
    "via `stride=2`, followed by batch normalization and ReLU activation.\n",
    "`nn.AdaptiveAvgPool2d((1, 1))` then adaptively reduces each feature map to size\n",
    "$1 \\times 1$ per channel, making the architecture robust to small spatial input\n",
    "variations. The `classifier` block flattens the features and applies a linear layer to\n",
    "produce logits for the 10 digit classes, later interpreted by `CrossEntropyLoss`, which\n",
    "internally applies softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is instantiated, moved to the selected device, and `torchinfo.summary`\n",
    "provides a structured architecture overview, including input/output dimensions and\n",
    "parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28), device=str(device))\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyperparameters, optimizer, and loss function are defined. AdamW is used,\n",
    "combining Adam’s advantages with explicit weight decay regularization. The chosen loss\n",
    "is `CrossEntropyLoss`, suitable for multi-class classification with integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is organized in epochs. Each epoch updates model parameters on the training\n",
    "set, then evaluates performance on the test set without updating parameters. Loss and\n",
    "accuracy are recorded for both sets to analyze learning progression and detect issues\n",
    "such as overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(\n",
    "        train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\"\n",
    "    ):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_dataloader))\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(\n",
    "            test_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TEST]\"\n",
    "        ):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_dataloader))\n",
    "    test_accuracies.append(100 * correct / total)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train → Loss: {train_losses[-1]:.4f} | Acc: {train_accuracies[-1]:.2f}%\")\n",
    "    print(f\"  Test  → Loss: {test_losses[-1]:.4f} | Acc: {test_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.train()` activates training-specific behaviors, such as updating batch\n",
    "normalization statistics and applying dropout if present. `model.eval()` disables these\n",
    "behaviors for deterministic evaluation, and `torch.no_grad()` during validation avoids\n",
    "gradient computation, reducing memory usage and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Metric Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, loss and accuracy evolution for training and testing is plotted. This\n",
    "visual analysis helps identify overfitting, underfitting, or learning stagnation,\n",
    "guiding potential architecture or hyperparameter adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Evolution\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Evolution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing training and testing curves provides insights into model generalization. For\n",
    "example, increasing training accuracy with stagnant or decreasing test accuracy usually\n",
    "indicates overfitting, while high loss on both sets suggests insufficient model capacity\n",
    "or training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the structure of embeddings produced by the model is analyzed using t-SNE\n",
    "(_t-distributed Stochastic Neighbor Embedding_). The model’s linear outputs (logits) are\n",
    "extracted as example representations. t-SNE projects these high-dimensional vectors into\n",
    "2D space while preserving local neighborhood relations. This projection visually shows\n",
    "how the model separates different classes in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "max_samples = 1000\n",
    "embeddings, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        if len(all_labels) * train_dataloader.batch_size >= max_samples:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        embeddings.append(outputs.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "embeddings = torch.cat(embeddings).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    learning_rate=200,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "X_embedded = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap=\"tab10\", alpha=0.6, s=10\n",
    ")\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"t-SNE of embeddings learned by LeNet\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has learned a good data representation, points corresponding to different\n",
    "classes tend to cluster relatively separately in 2D space. This visualization provides\n",
    "an intuitive perspective on how the model internally organizes information and\n",
    "distinguishes handwritten digit classes. Clear separation indicates that the\n",
    "network-induced feature space facilitates linear classification in the final layer,\n",
    "confirming that learned representations are discriminative and semantically meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
