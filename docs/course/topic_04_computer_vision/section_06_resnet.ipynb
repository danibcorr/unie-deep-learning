{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ResNet** (Residual Network) architecture, introduced in 2015 by **Kaiming He** and his collaborators at Microsoft Research, represents a decisive turning point in the design of deep neural networks for computer vision. While LeNet-5 established the systematic use of convolutional layers and architectures such as VGG demonstrated that increasing depth can significantly improve performance, ResNet introduces a fundamentally new structural mechanism that enables the effective and stable training of extremely deep networks, including models with more than one hundred layers.\n",
    "\n",
    "The importance of ResNet lies in its ability to overcome the optimization challenges that arise as network depth increases. Prior to its introduction, very deep architectures often suffered from severe training difficulties, including slow convergence, numerical instability, and degradation of performance as additional layers were added. These issues made it impractical to exploit the full representational power theoretically offered by deep models. ResNet addresses these limitations by reformulating how layers learn transformations, allowing information and gradients to propagate more effectively through the network.\n",
    "\n",
    "The practical impact of this innovation was clearly demonstrated when ResNet won the **ImageNet 2015** Large Scale Visual Recognition Challenge with a **152-layer** variant. This depth was previously considered unattainable from a training perspective, given the known difficulties associated with optimizing such deep architectures. The success of ResNet not only established a new state of the art in image recognition performance, but also reshaped prevailing assumptions about the feasible depth of neural networks, paving the way for subsequent generations of very deep models in computer vision and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Degradation Problem in Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to the introduction of ResNet, it was widely assumed that increasing the number of layers in a neural network should, at least in principle, enhance its representational capacity and improve performance. Deeper networks can theoretically capture increasingly complex hierarchical features, enabling more sophisticated modeling of input data. However, empirical studies revealed a surprising and counterintuitive phenomenon: beyond a certain depth—typically around twenty to thirty layers—adding additional layers often **degrades performance**, even on the training set itself. This effect is distinct from overfitting, as it occurs during training rather than being a consequence of limited generalization.\n",
    "\n",
    "This phenomenon is referred to as **degradation** and arises from structural optimization difficulties inherent to very deep networks. During backpropagation, the error signal—used to update network parameters—tends to either vanish or become numerically unstable as it propagates backward through many layers. Consequently, layers that are close to the input receive gradients that are extremely small or dominated by noise. As a result, these early layers fail to update their parameters effectively, preventing the network from fully exploiting its representational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections and Shortcut Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central innovation of ResNet lies in the introduction of **shortcut connections**, also referred to as **skip connections**, which form the fundamental building block known as the **residual block**. The underlying idea is conceptually straightforward yet profoundly impactful: rather than requiring each group of layers to learn a complete input-to-output mapping, the network is allowed to learn only the **residual**—that is, the difference between the input to the block and the desired output.\n",
    "\n",
    "Mathematically, if the desired mapping is denoted by $H(x)$ and the input to a residual block is $x$, the block is designed to learn a function $F(x) = H(x) - x$. The output of the block is then expressed as:\n",
    "\n",
    "$$\n",
    "y = F(x) + x\n",
    "$$\n",
    "\n",
    "This formulation offers two key advantages. First, it simplifies the learning problem: it is often easier for a block to learn small deviations from the identity mapping than to approximate a completely new transformation. Second, the shortcut connection provides a direct pathway for both the forward signal and the backward gradients to propagate through the network. This mechanism mitigates the vanishing gradient problem and enhances numerical stability, allowing extremely deep networks to be trained effectively.\n",
    "\n",
    "By enabling layers to focus on learning residual functions instead of full transformations, ResNet overcomes the degradation problem observed in very deep networks and establishes a structural principle that has become foundational in modern deep learning architectures. Residual blocks can be stacked to create networks with hundreds of layers, achieving remarkable performance while maintaining stable and efficient training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Residual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual learning provides several fundamental advantages. First, it facilitates optimization. If the optimal transformation in a certain block is close to the identity, it is easier for the network to learn a residual function with $F(x) \\approx 0$ than to learn a complete transformation $H(x) \\approx x$ from scratch. The function space of residuals tends to be closer to the origin and is therefore more accessible to gradient-based optimization methods.\n",
    "\n",
    "Second, gradient propagation improves significantly. During backpropagation, the gradient of the loss $L$ with respect to the input $x$ of a residual block satisfies, in simplified form,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\left( \\frac{\\partial F}{\\partial x} + I \\right),\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix. This expression guarantees that, even in the limiting case where $\\frac{\\partial F}{\\partial x}$ tends to zero, there is always a direct gradient path through the identity term $I$. In practice, this prevents the signal from vanishing completely and contributes to stabilizing the training of very deep networks.\n",
    "\n",
    "Third, residual connections introduce a form of adaptive depth. If a block is not necessary for the task, it can approximate $F(x) \\approx 0$ and effectively behave as an identity transformation, so that $y \\approx x$. The network thus retains the ability to neutralize blocks that do not provide improvements, without compromising the information flow. Taken together, these mechanisms make it possible to train networks with more than one hundred layers without suffering the severe degradation that affected previous architectures. The input signal can traverse the network without being distorted, and the gradient has alternative paths that mitigate vanishing effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Variants of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet family includes several configurations that differ mainly in depth and in the type of residual block used. Summarizing:\n",
    "\n",
    "|      Model | Layers | Parameters | Blocks per stage | Block type |\n",
    "| ---------: | -----: | ---------: | ---------------- | ---------- |\n",
    "|  ResNet-18 |     18 |      ~11 M | [2, 2, 2, 2]     | Basic      |\n",
    "|  ResNet-34 |     34 |      ~21 M | [3, 4, 6, 3]     | Basic      |\n",
    "|  ResNet-50 |     50 |      ~25 M | [3, 4, 6, 3]     | Bottleneck |\n",
    "| ResNet-101 |    101 |      ~44 M | [3, 4, 23, 3]    | Bottleneck |\n",
    "| ResNet-152 |    152 |      ~60 M | [3, 8, 36, 3]    | Bottleneck |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Block versus Bottleneck Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shallower versions, such as ResNet-18 and ResNet-34, the basic block is used. This block consists of two $3 \\times 3$ convolutions followed by batch normalization and ReLU activation, and a residual sum with the identity branch. The number of output channels matches that of the input, with an expansion factor of 1.\n",
    "\n",
    "In the deeper variants, such as ResNet-50, ResNet-101, and ResNet-152, the bottleneck block is used, whose purpose is to reduce computational cost while preserving representational capacity. This block combines three consecutive convolutions. The first, of size $1 \\times 1$, reduces the channel dimensionality, for example from 256 to 64 channels. The second, of size $3 \\times 3$, performs the main processing on a reduced number of channels. The third, again of size $1 \\times 1$, restores the original dimensionality, for example from 64 back to 256 channels. The typical expansion factor is 4: the number of output channels is four times the number of intermediate channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Impact and Importance of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet is currently regarded as a reference architecture in both academic and industrial contexts. The balance between depth, training stability, and efficiency makes it the backbone of numerous systems for face recognition, autonomous driving, medical imaging diagnosis, and large-scale visual analysis in multiple domains.\n",
    "\n",
    "Its advantages include efficient parameter usage, for example ResNet-50 uses approximately five times fewer parameters than VGG-16, the ability to train networks with more than one hundred layers without severe gradient degradation, its suitability as a base structure for transfer learning, numerical stability during training, and versatility, which has inspired variants in vision, natural language processing, and other modalities. Beyond solving a specific technical problem, ResNet redefines deep architecture design by explicitly incorporating identity paths that facilitate the flow of information and gradients through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation of ResNet for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a complete and fully functional implementation of **ResNet** (ResNet-18, ResNet-34, and ResNet-50) for the **CIFAR-10** dataset using PyTorch. The code is structured for easy conversion into a Jupyter Notebook and can be executed sequentially—from data loading to training, evaluation, and result visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the necessary modules for defining the network architecture, handling data, performing training, and visualizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "from typing import Any, List, Type, Union\n",
    "\n",
    "# 3pps\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants and hyperparameters that will be used throughout the experiment are defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "BATCH_SIZE: int = 128\n",
    "NUM_EPOCHS: int = 1\n",
    "LEARNING_RATE: float = 0.1\n",
    "WEIGHT_DECAY: float = 1e-4\n",
    "MOMENTUM: float = 0.9\n",
    "NUM_CLASSES: int = 10\n",
    "INPUT_SIZE: int = 32\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Initial learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Momentum: {MOMENTUM}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_images` function allows visualization of CIFAR-10 images together with theirground-truth labels and, optionally, model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):\n",
    "    \"\"\"\n",
    "    Visualize a set of images with their labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        images: Image tensor [N, C, H, W].\n",
    "        labels: Label tensor [N].\n",
    "        predictions: Optional tensor of predictions [N].\n",
    "        classes: List of class names.\n",
    "    \"\"\"\n",
    "    n_images = min(len(images), 8)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))\n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx in range(n_images):\n",
    "        img = images[idx]\n",
    "        label = labels[idx]\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Denormalize image (assuming standard normalization)\n",
    "        img = img / 2 + 0.5\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img)\n",
    "\n",
    "        title = f\"True: {classes[label]}\"\n",
    "        if predictions is not None:\n",
    "            pred = predictions[idx]\n",
    "            color = \"green\" if pred == label else \"red\"\n",
    "            title += f\"\\nPred: {classes[pred]}\"\n",
    "            ax.set_title(title, fontsize=9, color=color, fontweight=\"bold\")\n",
    "        else:\n",
    "            ax.set_title(title, fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization function defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 is then loaded and the preprocessing and data augmentation transformations for training and validation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization statistics\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Training transformations with data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation/test transformations\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]\n",
    ")\n",
    "\n",
    "print(\"Downloading CIFAR-10 dataset...\")\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "print(f\"  Test samples: {len(test_dataset):,}\")\n",
    "print(f\"  Number of classes: {len(train_dataset.classes)}\")\n",
    "print(\"  Image size: 32×32 pixels (RGB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation introduces random cropping and horizontal flipping to enhance generalization. Normalizing each channel using the CIFAR-10 mean and standard deviation centers and scales the data, promoting faster and more stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test `DataLoader` objects are then defined, configuring the number ofworker processes and other performance-oriented options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "print(\"DataLoaders configured:\")\n",
    "print(f\"  Training batches: {len(train_dataloader)}\")\n",
    "print(f\"  Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Exploration of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, it is helpful to inspect a batch of images to ensure that data loading and preprocessing are correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one training batch\n",
    "data_iter = iter(train_dataloader)\n",
    "train_images, train_labels = next(data_iter)\n",
    "\n",
    "print(\"\\nBatch dimensions:\")\n",
    "print(f\"  Images: {train_images.shape}\")\n",
    "print(f\"  Labels: {train_labels.shape}\")\n",
    "\n",
    "print(\"\\nDisplaying first 8 samples...\")\n",
    "show_images(train_images[:8], train_labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BasicBlock class implements the basic residual block used in ResNet-18 and ResNet-34. This block consists of two $3 \\times 3$ convolutions with batch normalization and a shortcut connection that adds the input to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic residual block for ResNet-18 and ResNet-34.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # First 3×3 convolution\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second 3×3 convolution\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut branch (dimensionality adjustment if needed)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bottleneck class implements the bottleneck block used in deeper ResNet variants such as ResNet-50, ResNet-101, and ResNet-152. This block uses three convolutions with an expansion factor of 4 to reduce computational cost while maintaining representational capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck block for ResNet-50, ResNet-101, and ResNet-152.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 1×1 conv to reduce dimensionality\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # 3×3 conv for main processing\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # 1×1 conv to restore dimensionality\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels, out_channels * self.expansion, kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet class implements the complete ResNet architecture adapted for CIFAR-10. The main differences from the original ImageNet version include a smaller initial convolution, no initial max pooling, and a final classification layer adapted to 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet implementation adapted for CIFAR-10.\n",
    "\n",
    "    Differences with respect to the original ImageNet version:\n",
    "      - First layer: Conv 3×3 instead of Conv 7×7.\n",
    "      - No initial MaxPooling (images are 32×32).\n",
    "      - Final classification layer adapted to CIFAR-10.\n",
    "\n",
    "    Args:\n",
    "        block: Block type (BasicBlock or Bottleneck).\n",
    "        layers: List with the number of blocks per stage.\n",
    "        num_classes: Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial layer adapted to CIFAR-10 (32×32)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Four stages of residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # Global pooling and final classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        stride: int = 1,\n",
    "    ) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Build one stage of residual blocks.\n",
    "\n",
    "        Args:\n",
    "            block: Residual block type.\n",
    "            out_channels: Number of output channels.\n",
    "            num_blocks: Number of blocks in the stage.\n",
    "            stride: Stride of the first block (downsampling).\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential containing the stage blocks.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "\n",
    "        # Dimensionality adjustment in the shortcut branch\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    out_channels * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize weights using He (Kaiming) initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ResNet forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, 3, 32, 32].\n",
    "\n",
    "        Returns:\n",
    "            Classification logits [B, num_classes].\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)  # 32×32 → 32×32\n",
    "        x = self.layer2(x)  # 32×32 → 16×16\n",
    "        x = self.layer3(x)  # 16×16 →  8×8\n",
    "        x = self.layer4(x)  #  8×8 →  4×4\n",
    "\n",
    "        x = self.avgpool(x)  # 4×4 → 1×1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract features before the classification layer.\n",
    "        Useful for visualization of embeddings and transfer learning.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory functions are defined to instantiate the different ResNet variants with the appropriate block types and layer configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Construct a ResNet-18 for the given number of classes.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def resnet34(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Construct a ResNet-34 for the given number of classes.\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def resnet50(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"Construct a ResNet-50 for the given number of classes.\"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "print(\"ResNet architecture defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation clearly separates the fundamental components: `BasicBlock` forResNet-18/34, `Bottleneck` for ResNet-50/101/152, and the `ResNet` class, which assemblesthe stages, manages downsampling, and applies global average pooling before the finalclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ResNet-18 instance adapted to CIFAR-10 is created and its structure and parameter countare examined using `torchinfo.summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet-18\n",
    "model = resnet18(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESNET-18 ARCHITECTURE SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))\n",
    "\n",
    "\n",
    "def count_parameters(module: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PARAMETER ANALYSIS BY COMPONENT\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Initial conv:     {count_parameters(model.conv1):>12,} parameters\")\n",
    "print(f\"  Layer 1 (64 ch.): {count_parameters(model.layer1):>12,} parameters\")\n",
    "print(f\"  Layer 2 (128 ch.):{count_parameters(model.layer2):>12,} parameters\")\n",
    "print(f\"  Layer 3 (256 ch.):{count_parameters(model.layer3):>12,} parameters\")\n",
    "print(f\"  Layer 4 (512 ch.):{count_parameters(model.layer4):>12,} parameters\")\n",
    "print(f\"  FC classifier:    {count_parameters(model.fc):>12,} parameters\")\n",
    "print(f\"  {'-'*66}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  TOTAL:            {total_params:>12,} parameters\")\n",
    "print(f\"  Trainable:        {trainable_params:>12,} parameters\")\n",
    "print(f\"  Memory (float32): {total_params * 4 / (1024**2):>10.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer, learning rate scheduler, and loss function are set up. Stochastic Gradient Descent (SGD) with Nesterov momentum is used, along with a MultiStepLR scheduler that decreases the learning rate at predefined epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Initial learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Momentum: {MOMENTUM}\")\n",
    "print(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Optimizer: SGD with Nesterov momentum\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    nesterov=True,\n",
    ")\n",
    "\n",
    "# Scheduler: MultiStepLR\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[60, 80], gamma=0.1\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Optimizer: SGD with Nesterov momentum\")\n",
    "print(\"  Nesterov momentum adds a 'look-ahead' in the descent direction\")\n",
    "print(\"\\nScheduler: MultiStepLR\")\n",
    "print(\"  Reduces learning rate ×0.1 at epochs 60 and 80\")\n",
    "print(\"  Classic strategy for training ResNet on CIFAR-10\")\n",
    "print(\"\\nLoss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is implemented with metric tracking and includes saving the model that achieves the highest test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric storage\n",
    "train_losses, train_accuracies = [], []\n",
    "test_losses, test_accuracies = [], []\n",
    "learning_rates = []\n",
    "\n",
    "# Variables for saving the best model\n",
    "best_test_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct, total\n",
    "\n",
    "\n",
    "print(\"STARTING TRAINING\\n\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    train_loop = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False\n",
    "    )\n",
    "\n",
    "    for batch_image, batch_label in train_loop:\n",
    "        batch_image = batch_image.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_image)\n",
    "        loss = loss_function(outputs, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_correct, batch_total = calculate_accuracy(outputs, batch_label)\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "        train_loop.set_postfix(\n",
    "            {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n",
    "        )\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_dataloader)\n",
    "    epoch_train_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "\n",
    "    test_loop = tqdm(\n",
    "        test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loop:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            batch_correct, batch_total = calculate_accuracy(outputs, labels)\n",
    "            correct_test += batch_correct\n",
    "            total_test += batch_total\n",
    "\n",
    "            test_loop.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"acc\": f\"{100 * correct_test / total_test:.2f}%\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    epoch_test_loss = test_loss / len(test_dataloader)\n",
    "    epoch_test_acc = 100 * correct_test / total_test\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_accuracies.append(epoch_test_acc)\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    # Save best model according to test accuracy\n",
    "    if epoch_test_acc > best_test_acc:\n",
    "        best_test_acc = epoch_test_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"test_acc\": best_test_acc,\n",
    "            },\n",
    "            \"resnet18_cifar10_best.pth\",\n",
    "        )\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")\n",
    "    print(f\"  Train → Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")\n",
    "    print(f\"  Test  → Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")\n",
    "    print(\n",
    "        f\"  LR: {current_lr:.6f} | Best test acc: {best_test_acc:.2f}% (epoch {best_epoch})\"\n",
    "    )\n",
    "    print(f\"  {'─'*66}\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Total time: {total_time / 60:.2f} minutes\")\n",
    "print(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\")\n",
    "print(f\"  Final test accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Best test accuracy: {best_test_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "# Save final model and metrics\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "        \"best_test_acc\": best_test_acc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "    },\n",
    "    \"resnet18_cifar10_final.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\nSaved models:\")\n",
    "print(\"  - resnet18_cifar10_best.pth (best model)\")\n",
    "print(\"  - resnet18_cifar10_final.pth (final model + metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to architectures like VGG, ResNet training on CIFAR-10 is generally more stable and efficient at similar depths, thanks to residual connections and a more moderate number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training metrics are visualized using four plots—loss progression, accuracy progression, learning rate schedule, and the train-test accuracy gap—to analyze model performance and identify potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(\n",
    "    epochs_range,\n",
    "    train_losses,\n",
    "    \"o-\",\n",
    "    label=\"Train Loss\",\n",
    "    linewidth=2,\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax1.plot(\n",
    "    epochs_range,\n",
    "    test_losses,\n",
    "    \"s-\",\n",
    "    label=\"Test Loss\",\n",
    "    linewidth=2,\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax1.axvline(\n",
    "    x=best_epoch,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Best epoch ({best_epoch})\",\n",
    ")\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(\n",
    "    epochs_range,\n",
    "    train_accuracies,\n",
    "    \"o-\",\n",
    "    label=\"Train Accuracy\",\n",
    "    linewidth=2,\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax2.plot(\n",
    "    epochs_range,\n",
    "    test_accuracies,\n",
    "    \"s-\",\n",
    "    label=\"Test Accuracy\",\n",
    "    linewidth=2,\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax2.axvline(\n",
    "    x=best_epoch,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Best epoch ({best_epoch})\",\n",
    ")\n",
    "ax2.axhline(\n",
    "    y=best_test_acc,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Best acc: {best_test_acc:.2f}%\",\n",
    ")\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax3.plot(\n",
    "    epochs_range,\n",
    "    learning_rates,\n",
    "    \"o-\",\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axvline(x=60, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"LR decay\")\n",
    "ax3.axvline(x=80, color=\"orange\", linestyle=\"--\", alpha=0.5)\n",
    "ax3.legend(fontsize=10)\n",
    "\n",
    "# Train–test gap\n",
    "gap = np.array(train_accuracies) - np.array(test_accuracies)\n",
    "ax4.plot(epochs_range, gap, \"o-\", color=\"purple\", linewidth=2, markersize=3, alpha=0.7)\n",
    "ax4.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax4.axhline(\n",
    "    y=5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Overfitting threshold (5%)\"\n",
    ")\n",
    "ax4.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax4.set_ylabel(\"Train–Test Gap (%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax4.set_title(\"Train–Test Accuracy Difference\", fontsize=14, fontweight=\"bold\")\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"resnet18_training_history.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResult analysis:\")\n",
    "final_gap = train_accuracies[-1] - test_accuracies[-1]\n",
    "print(f\"  Overfitting detected: {'YES' if final_gap > 10 else 'NO'}\")\n",
    "print(f\"  Final train–test gap: {final_gap:.2f}%\")\n",
    "print(f\"  Best epoch: {best_epoch}\")\n",
    "print(f\"  Improvement from epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderate train–test accuracy gaps indicate a healthy balance between fitting the data and generalizing to new samples. Learning rate reductions at epochs 60 and 80 often coincide with shifts in network behavior and corresponding improvements in test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, model predictions on the test set are visualized, including both correctlyclassified examples and some errors, allowing qualitative inspection of model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVisualizing predictions of the best model...\")\n",
    "\n",
    "# Get one test batch\n",
    "data_iter = iter(test_dataloader)\n",
    "test_images, test_labels = next(data_iter)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_images_device = test_images.to(device)\n",
    "    outputs = model(test_images_device)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    predictions = predictions.cpu()\n",
    "\n",
    "print(\"\\nFirst 8 predictions:\")\n",
    "show_images(test_images[:8], test_labels[:8], predictions[:8])\n",
    "\n",
    "# Examples of misclassifications\n",
    "incorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]\n",
    "\n",
    "if len(incorrect_indices) >= 8:\n",
    "    print(\"\\nExamples of incorrect predictions:\")\n",
    "    error_indices = incorrect_indices[:8]\n",
    "    show_images(\n",
    "        test_images[error_indices],\n",
    "        test_labels[error_indices],\n",
    "        predictions[error_indices],\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nOnly {len(incorrect_indices)} misclassifications in this batch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
