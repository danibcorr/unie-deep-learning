{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Attention Mechanism in Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Este clase implementa la capa SE de este paper: https://arxiv.org/abs/1709.01507\n",
    "\"\"\"\n",
    "\n",
    "# 3pps\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, channel_size: int, ratio: int) -> None:\n",
    "        \"\"\"\n",
    "        Implements Squeeze-and-Excitation (SE) block.\n",
    "\n",
    "        Args:\n",
    "            channel_size: Number of channels in the input tensor.\n",
    "            ratio: Reduction factor for the compression layer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Vamos a crear un modelo Sequential\n",
    "        self.se_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # (B, C, 1, 1)\n",
    "            nn.Flatten(),  # (B, C)\n",
    "            nn.Linear(\n",
    "                in_features=channel_size, out_features=channel_size // ratio\n",
    "            ),  # (B, C//ratio)\n",
    "            nn.ReLU(),  # (B, C//ratio)\n",
    "            nn.Linear(\n",
    "                in_features=channel_size // ratio, out_features=channel_size\n",
    "            ),  # (B, C)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies attention mechanism to input tensor.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Input tensor with shape (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor with attention applied, same shape as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Primero podemos obtener el tamaño del tensor de entrada\n",
    "        b, c, _, _ = input_tensor.size()\n",
    "\n",
    "        # Obtenemos el tensor de aplicar SE\n",
    "        x = self.se_block(input_tensor)\n",
    "\n",
    "        # Modificamos el shape del tensor para ajustarlo al input\n",
    "        x = x.view(b, c, 1, 1)\n",
    "\n",
    "        # Aplicamos el producto como mecanismo de atención\n",
    "        return x * input_tensor\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SqueezeExcitation(channel_size=3, ratio=16)\n",
    "\n",
    "    x = torch.randn(1, 3, 4, 4)\n",
    "    output_model = model(x)\n",
    "\n",
    "    print(output_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
