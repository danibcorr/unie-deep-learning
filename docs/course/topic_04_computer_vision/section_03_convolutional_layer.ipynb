{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers constitute the core of Convolutional Neural Networks (CNNs) used in\n",
    "computer vision. A convolution applies a filter or _kernel_ over an image to extract\n",
    "local features such as edges, textures, or shapes. This process is performed in a sliding\n",
    "manner over the image, generating an activation map that highlights those regions where\n",
    "the pattern defined by the filter is present with greater intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Dimensional Convolution from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how a convolution works, it is useful to start from an explicit\n",
    "implementation in NumPy on a grayscale image (2D matrix) and a two-dimensional kernel as\n",
    "well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def convolve2d(image, kernel, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Applies a 2D convolution.\n",
    "\n",
    "    image: Input image of shape (H, W).\n",
    "    kernel: Filter of shape (K_H, K_W).\n",
    "    padding: Zero padding around the image.\n",
    "    stride: Step with which the filter is displaced.\n",
    "    \"\"\"\n",
    "    # Apply padding if necessary\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode=\"constant\")\n",
    "\n",
    "    h, w = image.shape\n",
    "    kh, kw = kernel.shape\n",
    "\n",
    "    # Calculate output size\n",
    "    out_h = (h - kh) // stride + 1\n",
    "    out_w = (w - kw) // stride + 1\n",
    "\n",
    "    output = np.zeros((out_h, out_w))\n",
    "\n",
    "    # Apply convolution\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            h_start = i * stride\n",
    "            w_start = j * stride\n",
    "            region = image[h_start : h_start + kh, w_start : w_start + kw]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Usage example\n",
    "image = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "\n",
    "kernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
    "\n",
    "resultado = convolve2d(image, kernel)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the kernel slides over the image from left to right and from top\n",
    "to bottom. At each position, a local region of the same size as the kernel is taken,\n",
    "element-wise multiplication is performed, and the results are summed, producing a value\n",
    "in the output map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Filters for Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training neural networks, image processing used manually designed filters to\n",
    "detect specific patterns. Many of these filters remain useful for illustrating the effect\n",
    "of a convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "# Predefined filters\n",
    "filtros = {\n",
    "    # Detects vertical edges\n",
    "    \"vertical\": np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n",
    "    # Detects horizontal edges\n",
    "    \"horizontal\": np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),\n",
    "    # Sobel X (enhanced vertical edges)\n",
    "    \"sobel_x\": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
    "    # Sobel Y (enhanced horizontal edges)\n",
    "    \"sobel_y\": np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
    "    # Blur\n",
    "    \"blur\": np.ones((3, 3)) / 9.0,\n",
    "    # Edge detection (Laplacian)\n",
    "    \"laplacian\": np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "}\n",
    "\n",
    "# Load image\n",
    "image_pil = Image.open(\n",
    "    \"/home/dani/Repositorios/unie-deep-learning/docs/assets/course/topic_02_mathematics/cat_image.jpg\"\n",
    ").convert(\"L\")\n",
    "imagen = np.array(image_pil)\n",
    "\n",
    "# Apply all filters\n",
    "resultados = {}\n",
    "for nombre, filtro in filtros.items():\n",
    "    resultados[nombre] = convolve2d(imagen, filtro, mode=\"same\", boundary=\"symm\")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(imagen, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Filtered images\n",
    "for i, (nombre, resultado) in enumerate(resultados.items(), 1):\n",
    "    axes[i].imshow(resultado, cmap=\"gray\")\n",
    "    axes[i].set_title(nombre.capitalize())\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters like Sobel or Laplacian enhance abrupt intensity transitions, that is, edges.\n",
    "Blur filters average local values, smoothing noise and fine details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Components of Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Padding_ adds rows and columns of zeros around the input image. Its main purpose is to\n",
    "control the size of the output map and preserve information at the edges.\n",
    "\n",
    "Without padding, the size is reduced. For example:\n",
    "\n",
    "- Input: $5 \\times 5$, Kernel: $3 \\times 3$, Stride $= 1$  \n",
    "  Output: $3 \\times 3$\n",
    "\n",
    "With `padding = 1`, the effective input becomes $7 \\times 7$, so that:\n",
    "\n",
    "- Extended input: $7 \\times 7$, Kernel: $3 \\times 3$, Stride $= 1$  \n",
    "  Output: $5 \\times 5$\n",
    "\n",
    "In practice:\n",
    "\n",
    "- `padding = 0`: Used when one wishes to progressively reduce spatial size.\n",
    "- `padding = (kernel_size - 1) // 2`: Used to maintain the same input and output size\n",
    "  when `stride = 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _stride_ controls the displacement of the kernel over the image. A stride of 1\n",
    "traverses all adjacent pixels; a larger stride skips positions, reducing the spatial\n",
    "resolution of the output.\n",
    "\n",
    "Examples (with padding $=0$):\n",
    "\n",
    "- `stride = 1`: Input $8 \\times 8$, Kernel $3 \\times 3$ → Output $6 \\times 6$.\n",
    "- `stride = 2`: Input $8 \\times 8$, Kernel $3 \\times 3$ → Output $3 \\times 3$.\n",
    "\n",
    "Typical usage:\n",
    "\n",
    "- `stride = 1`: Used to capture all spatial details.\n",
    "- `stride = 2`: Used to reduce spatial size and computational cost, acting similarly to\n",
    "  pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel size determines the local field of view of the convolution:\n",
    "\n",
    "- Small kernel, such as $3 \\times 3$: Most common, efficient, and sufficient in most\n",
    "  modern architectures.\n",
    "- Large kernel, such as $5 \\times 5$ or $7 \\times 7$: Covers more context in a single\n",
    "  operation but introduces many more parameters.\n",
    "\n",
    "It is more efficient to stack several $3 \\times 3$ convolutions than to use a single\n",
    "convolution with a large kernel. For example:\n",
    "\n",
    "- A $5 \\times 5$ layer uses $25$ parameters per channel.\n",
    "- Two consecutive $3 \\times 3$ layers use $18$ parameters per channel and achieve a\n",
    "  similar or superior effective receptive field, while also introducing more\n",
    "  nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling: Spatial Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pooling_ reduces the spatial resolution of activation maps by aggregating information in\n",
    "local regions. It is used to decrease the size of intermediate tensors, control\n",
    "overfitting, and gain invariance to small translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling takes the maximum value within each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def max_pool2d(image, pool_size=2):\n",
    "    h, w = image.shape\n",
    "    out_h, out_w = h // pool_size, w // pool_size\n",
    "    output = np.zeros((out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[\n",
    "                i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size\n",
    "            ]\n",
    "            output[i, j] = np.max(region)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Simple example with a 4x4 matrix\n",
    "simple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])\n",
    "\n",
    "print(\"Original image (4x4):\")\n",
    "print(simple_image)\n",
    "print()\n",
    "\n",
    "pooled = max_pool2d(simple_image, pool_size=2)\n",
    "\n",
    "print(\"After max pooling (2x2):\")\n",
    "print(pooled)\n",
    "print()\n",
    "\n",
    "# Step by step explanation\n",
    "print(\"Step by step:\")\n",
    "print(\n",
    "    f\"Top-left region: {simple_image[0:2, 0:2].flatten()} → max = {np.max(simple_image[0:2, 0:2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Top-right region: {simple_image[0:2, 2:4].flatten()} → max = {np.max(simple_image[0:2, 2:4])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} → max = {np.max(simple_image[2:4, 0:2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} → max = {np.max(simple_image[2:4, 2:4])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling preserves the strongest responses in each region, emphasizing the presence of\n",
    "prominent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average pooling calculates the average in each region, smoothing the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def avg_pool2d(image, pool_size=2):\n",
    "    h, w = image.shape\n",
    "    out_h, out_w = h // pool_size, w // pool_size\n",
    "    output = np.zeros((out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[\n",
    "                i * pool_size : (i + 1) * pool_size, j * pool_size : (j + 1) * pool_size\n",
    "            ]\n",
    "            output[i, j] = np.mean(region)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Simple example with a 4x4 matrix\n",
    "simple_image = np.array([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14], [11, 12, 15, 16]])\n",
    "\n",
    "print(\"Original image (4x4):\")\n",
    "print(simple_image)\n",
    "print()\n",
    "\n",
    "pooled = avg_pool2d(simple_image, pool_size=2)\n",
    "\n",
    "print(\"After average pooling (2x2):\")\n",
    "print(pooled)\n",
    "print()\n",
    "\n",
    "# Step by step explanation\n",
    "print(\"Step by step:\")\n",
    "print(\n",
    "    f\"Top-left region: {simple_image[0:2, 0:2].flatten()} → mean = {np.mean(simple_image[0:2, 0:2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Top-right region: {simple_image[0:2, 2:4].flatten()} → mean = {np.mean(simple_image[0:2, 2:4])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Bottom-left region: {simple_image[2:4, 0:2].flatten()} → mean = {np.mean(simple_image[2:4, 0:2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Bottom-right region: {simple_image[2:4, 2:4].flatten()} → mean = {np.mean(simple_image[2:4, 2:4])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparatively, max pooling is more aggressive and focused on prominent features, while\n",
    "average pooling provides a more smoothed representation, which can be useful in tasks\n",
    "where a more stable global response is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Efficient Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modern efficiency-oriented architectures, especially on mobile devices, variants of\n",
    "standard convolution are used to drastically reduce the number of parameters and\n",
    "computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $1 \\times 1$ Convolution (Pointwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $1 \\times 1$ convolution acts on each spatial position independently and only mixes\n",
    "channels. It does not change the spatial size $(H, W)$, but it does change the number of\n",
    "channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "# In PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "conv1x1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1)\n",
    "# Input: (B, 64, H, W) → Output: (B, 32, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used to reduce or increase the number of channels (bottleneck blocks in ResNet, for\n",
    "example) and to introduce nonlinearity between linear combinations of channels at very\n",
    "low cost. The number of parameters is:\n",
    "\n",
    "$$ \\text{parameters} = 64 \\times 32 \\times 1 \\times 1 = 2048. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Depthwise_ convolution applies one filter per channel independently, without mixing\n",
    "them. In PyTorch, it is implemented using the `groups` parameter equal to the number of\n",
    "channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthwise = nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=64,  # Same number of channels\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    groups=64,  # One group per channel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a standard $3 \\times 3$ convolution from 64 to 64 channels, the number of parameters\n",
    "would be:\n",
    "\n",
    "$$ 64 \\times 64 \\times 3 \\times 3 = 36{,}864. $$\n",
    "\n",
    "In a depthwise convolution, only:\n",
    "\n",
    "$$ 64 \\times 3 \\times 3 = 576. $$\n",
    "\n",
    "This represents a massive cost reduction, although by itself it does not mix information\n",
    "between channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise-Separable Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Depthwise-separable_ convolution combines two steps:\n",
    "\n",
    "1. Depthwise convolution: Spatially filters each channel independently.\n",
    "2. Pointwise convolution ($1 \\times 1$): Mixes channels and adjusts the number of output\n",
    "   channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthwise_separable = nn.Sequential(\n",
    "    # Step 1: Depthwise (spatial filtering per channel)\n",
    "    nn.Conv2d(64, 64, kernel_size=3, groups=64, padding=1),\n",
    "    # Step 2: Pointwise (mix channels)\n",
    "    nn.Conv2d(64, 128, kernel_size=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the number of parameters is approximately 8–9 times smaller than that of an\n",
    "equivalent standard convolution, while maintaining competitive performance. This approach\n",
    "is widely used in efficient architectures such as MobileNet and EfficientNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupwise Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Groupwise_ convolution divides channels into several groups. Each group is processed\n",
    "independently, but within each group channels are mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupwise = nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=128,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    groups=4,  # Divides channels into 4 groups\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each group processes 16 input channels and produces 32 output channels.\n",
    "This technique allows balancing efficiency and modeling capacity and appears in\n",
    "architectures such as ResNeXt and ShuffleNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Convolutional Blocks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, convolutional and pooling layers are typically combined with normalization\n",
    "and activation functions to form basic blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "standard = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block applies a standard $3 \\times 3$ convolution, normalizes activations with Batch\n",
    "Normalization, and applies a ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Depthwise-Separable Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient = nn.Sequential(\n",
    "    # Depthwise\n",
    "    nn.Conv2d(3, 3, kernel_size=3, groups=3, padding=1),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # Pointwise\n",
    "    nn.Conv2d(3, 64, kernel_size=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    ")\n",
    "\n",
    "# Pooling\n",
    "pooling = nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block reduces computational cost while maintaining good representational power and\n",
    "is complemented with a pooling layer to reduce spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2-Type Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 uses inverted blocks with expansion and contraction of channels, combining\n",
    "$1 \\times 1$ and depthwise convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Expansion (1x1 conv)\n",
    "        self.expand = nn.Conv2d(in_ch, in_ch * 6, kernel_size=1)\n",
    "\n",
    "        # 2. Depthwise (3x3)\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_ch * 6,\n",
    "            in_ch * 6,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            groups=in_ch * 6,\n",
    "        )\n",
    "\n",
    "        # 3. Projection (1x1 conv)\n",
    "        self.project = nn.Conv2d(in_ch * 6, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expand(x)\n",
    "        out = self.depthwise(out)\n",
    "        out = self.project(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of block allows building highly efficient deep networks on limited hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Convolutions and Output Size Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2D convolution with input size $\\text{in\\_size}$, kernel size $k$, padding $p$, and stride $s$, the unidimensional output size (per axis) is given by:\n",
    "\n",
    "$$\n",
    "\\text{out\\_size} = \\left\\lfloor \\frac{\\text{in\\_size} + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: $32$, Kernel: $3$, Padding: $1$, Stride: $1$  \n",
    "  $$\n",
    "  \\text{out\\_size} = \\left\\lfloor \\frac{32 + 2 \\times 1 - 3}{1} \\right\\rfloor + 1 = 32\n",
    "  $$\n",
    "  that is, spatial size is maintained.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
