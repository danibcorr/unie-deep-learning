{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization constitutes a key stage both in input data preprocessing and in the\n",
    "internal design of neural network architectures. Its primary objective is to control the\n",
    "scale of numerical values, ensuring that different features are in comparable ranges and\n",
    "that training is stable, efficient, and less sensitive to initialization or\n",
    "hyperparameter choices.\n",
    "\n",
    "In the context of images, normalization can be divided into two major conceptual blocks.\n",
    "On one hand, input data normalization, which is applied before introducing images into\n",
    "the network. On the other hand, layer normalization, which is applied to the internal\n",
    "activations of the network during training. Although both categories pursue similar\n",
    "objectives, they are implemented at different stages of the data flow and with different\n",
    "mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data normalization is applied directly to images before they are processed by the\n",
    "network layers. In the case of images, one works with tensors or arrays where each pixel\n",
    "can be represented with raw values in the range $[0, 255]$ or, after prior conversion,\n",
    "with floating-point values.\n",
    "\n",
    "The purpose of this normalization is threefold. First, it provides numerical stability by\n",
    "avoiding excessively large or small values, which can cause uncontrolled or practically\n",
    "null gradients. Second, it accelerates training, as gradients propagate more uniformly\n",
    "through the network. Finally, it prevents one feature from dominating others simply due\n",
    "to its scale, favoring that all dimensions of the feature space contribute comparably to\n",
    "model learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for Normalizing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input normalization fulfills several essential objectives. In terms of numerical\n",
    "stability, it prevents activations from reaching magnitudes that hinder the convergence\n",
    "of optimization algorithms. Additionally, input homogenization facilitates that gradients\n",
    "calculated during backpropagation have reasonable orders of magnitude, which allows using\n",
    "more aggressive learning rates without compromising convergence. Finally, by adjusting\n",
    "all features to similar ranges, a balancing effect is produced, so that the model is not\n",
    "biased toward those components with larger numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Normalization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various standard techniques exist for normalizing images, each suitable for certain\n",
    "scenarios and architectures. A first technique consists of Min-Max normalization to the\n",
    "range $[0, 1]$. In this case, the image is linearly rescaled using its minimum and\n",
    "maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_min_max(image):\n",
    "    \"\"\"Brings the image to the range [0, 1].\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    normalized = (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is useful when one wants to work with values bounded between 0 and 1, for\n",
    "example in simple models or when one wishes to visualize or combine different data\n",
    "sources normalized to the same range.\n",
    "\n",
    "A widely used variant in deep neural networks consists of bringing values to the range\n",
    "$[-1, 1]$. For typical 8-bit images, a direct way to achieve this is to first divide by\n",
    "255 and then apply a linear transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minus_one_to_one(image):\n",
    "    \"\"\"Brings the image to the range [-1, 1].\"\"\"\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    normalized = 2.0 * image - 1.0\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of normalization is common in architectures such as Generative Adversarial\n",
    "Networks (GANs), where it is preferable for input data to be centered around zero.\n",
    "\n",
    "Another fundamental approach is standardization or $z$-score normalization. In this case,\n",
    "the mean is subtracted and divided by the standard deviation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(image):\n",
    "    \"\"\"Standardizes: (x - mean) / standard deviation.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    standardized = (image - mean) / (std + 1e-8)\n",
    "    return standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique transforms data so that it has approximately zero mean and unit variance.\n",
    "In computer vision, it is frequently used at the channel level, utilizing precomputed\n",
    "means and standard deviations over large datasets, such as ImageNet.\n",
    "\n",
    "In practice, frameworks like PyTorch facilitate input normalization through predefined\n",
    "transformations. A typical example for models pretrained on ImageNet is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Standard transformation for pretrained models (ImageNet)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Converts to tensor and scales to [0, 1]\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean per channel\n",
    "            std=[0.229, 0.224, 0.225],  # ImageNet standard deviation per channel\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, the `ToTensor` function converts the image to a floating-point tensor\n",
    "and scales values to the range $[0, 1]$. Subsequently, `Normalize` applies channel-wise\n",
    "standardization using global statistics from the original training set. This practice\n",
    "ensures that pretrained models receive inputs in the same statistical regime for which\n",
    "they were optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is performed within the network architecture, on the intermediate\n",
    "activations that are generated as data advances through different layers. Unlike input\n",
    "data normalization, which is fixed preprocessing, layer normalization is implemented as\n",
    "differentiable blocks that form part of the model and that, in many cases, contain\n",
    "learnable parameters.\n",
    "\n",
    "The general idea consists of normalizing activations according to certain dimensions (for\n",
    "example, over the batch, over channels, or over all elements of a sample), and then\n",
    "applying a linear transformation with scale and shift parameters that are learned during\n",
    "training. In this way, the so-called \"internal covariate shift\" is corrected and the\n",
    "distribution of activations is stabilized, which facilitates the training of deep\n",
    "networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Response Normalization (LRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Response Normalization (LRN) is a technique introduced in early networks such as\n",
    "AlexNet. Its purpose is to perform normalization based on the response of neighboring\n",
    "channels, mimicking certain lateral inhibition mechanisms observed in the biological\n",
    "visual system. Although it is included here for historical completeness, in practice its\n",
    "current use is residual, as it has been widely displaced by more effective methods such\n",
    "as Batch Normalization or Layer Normalization.\n",
    "\n",
    "A schematic implementation of LRN in PyTorch can be structured as a class that receives\n",
    "parameters such as neighborhood size $n$, coefficients $\\alpha$ and $\\beta$, and a\n",
    "constant $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LocalResponseNormalization(nn.Module):\n",
    "    def __init__(self, k=2.0, n=5, alpha=1e-4, beta=0.75):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    # The complete implementation would include the calculation of normalization\n",
    "    # over neighboring channels according to the above parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although LRN had relevance in early works with deep CNNs, its current impact is very\n",
    "limited and it is not considered a recommendable choice for modern architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Response Normalization (GRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Response Normalization (GRN) is proposed as a more recent alternative to local\n",
    "normalization. Instead of normalizing with respect to neighboring channels, GRN considers\n",
    "the global response of all channels for each spatial position and regulates the magnitude\n",
    "of activations per channel from that global information. The objective is to prevent\n",
    "certain channels from becoming redundant or systematically dominating the representation,\n",
    "promoting a more balanced distribution of energy across channels.\n",
    "\n",
    "A typical GRN implementation in PyTorch can take the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalResponseNormalization(nn.Module):\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate global norm per channel (p=2 over spatial dimensions)\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n",
    "        # Normalize with respect to the mean of global norms\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n",
    "        # Rescale and add residual component\n",
    "        return self.gamma * (x * nx) + self.beta + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, an $L^2$ norm per channel is first calculated by aggregating over spatial\n",
    "dimensions. Subsequently, this norm is normalized with respect to its mean and used to\n",
    "rescale the original activations through the learnable parameters $\\gamma$ and $\\beta$,\n",
    "to which the input itself is also added as a residual term. This type of normalization\n",
    "has been explored in modern convolutional architectures and in masked autoencoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization (BN) is one of the most influential internal normalization techniques\n",
    "in deep networks. Its central idea consists of normalizing activations using statistics\n",
    "(mean and variance) calculated over the training batch itself for each channel.\n",
    "\n",
    "For an activation tensor $x$ of size $(N, C, H, W)$, where $N$ is the batch size, $C$ is\n",
    "the number of channels, and $(H, W)$ is the spatial dimension, the mean and variance per\n",
    "channel are calculated in training mode:\n",
    "\n",
    "$$ \\mu*c = \\frac{1}{N H W} \\sum*{n,h,w} x*{n,c,h,w}, \\quad \\sigma_c^2 = \\frac{1}{N H W}\n",
    "\\sum*{n,h,w} (x\\_{n,c,h,w} - \\mu_c)^2. $$\n",
    "\n",
    "Next, normalization is performed:\n",
    "\n",
    "$$ \\hat{x}_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}, $$\n",
    "\n",
    "and an affine transformation is applied with learnable parameters $\\gamma_c$ and\n",
    "$\\beta_c$:\n",
    "\n",
    "$$ y*{n,c,h,w} = \\gamma_c \\hat{x}*{n,c,h,w} + \\beta_c. $$\n",
    "\n",
    "A simplified implementation of two-dimensional Batch Normalization can be expressed in\n",
    "PyTorch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization2D(nn.Module):\n",
    "    def __init__(self, num_channels, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Accumulated statistics for inference\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(1, num_channels, 1, 1))\n",
    "        self.register_buffer(\"running_var\", torch.ones(1, num_channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = x.var(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "            # Update accumulated statistics\n",
    "            self.running_mean = (\n",
    "                1 - self.momentum\n",
    "            ) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (\n",
    "                1 - self.momentum\n",
    "            ) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, batch statistics are used and accumulated means and variances are\n",
    "updated with a certain momentum. During inference, batch statistics are no longer used\n",
    "and accumulated means and variances are employed instead, which guarantees deterministic\n",
    "behavior.\n",
    "\n",
    "Among the main advantages of Batch Normalization are training acceleration, the\n",
    "possibility of using higher learning rates, and reduced dependence on weight\n",
    "initialization. In many architectures, BN also contributes to reducing the need for\n",
    "additional regularization techniques such as Dropout. However, it also presents\n",
    "limitations. In particular, its performance degrades when the batch size is very small,\n",
    "as mean and variance estimates become noisy, and its behavior differs between training\n",
    "and inference modes, which requires careful management of `train` and `eval` modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization (LN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization (LN) is designed to overcome some limitations of BN, especially in\n",
    "contexts where batch size is small or where the model structure does not adapt well to\n",
    "batch normalization, such as in recurrent networks or Transformers. In LN, normalization\n",
    "is performed independently for each sample, aggregating over all its feature dimensions.\n",
    "\n",
    "If one considers an input tensor $x$ associated with an individual sample, LN calculates\n",
    "the mean and variance over all relevant dimensions (for example, over channels and\n",
    "spatial positions) for each batch element, and normalizes analogously to BN but without\n",
    "depending on other samples in the batch. Thus, the normalization behavior is identical in\n",
    "training and inference, and does not depend on batch size.\n",
    "\n",
    "A schematic implementation of Layer Normalization for tensors of type $(N, C, H, W)$ can\n",
    "be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization2D(nn.Module):\n",
    "    def __init__(self, num_channels=None, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        if num_channels is not None:\n",
    "            self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
    "            self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=(1, 2, 3), keepdim=True)\n",
    "        var = x.var(dim=(1, 2, 3), keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * x_norm + self.beta\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This normalization is especially suitable for attention-based architectures, such as\n",
    "Transformers, and for recurrent networks, where dependence on batch statistics could\n",
    "introduce undesired noise. Additionally, by not differentiating between training and\n",
    "inference modes, it simplifies the operational flow of the model and facilitates the use\n",
    "of very small batch sizes, even equal to one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
