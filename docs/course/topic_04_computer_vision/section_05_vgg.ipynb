{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth and Simplicity in Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **VGG** architecture, developed by the _Visual Geometry Group_ at the University of\n",
    "Oxford and presented in 2014, constitutes a milestone in the evolution of deep learning\n",
    "applied to computer vision. While LeNet-5 establishes the conceptual foundations of\n",
    "convolutional neural networks, VGG systematically demonstrates that increasing the\n",
    "**depth** of the network, combined with an extremely simple and homogeneous structure,\n",
    "leads to significant improvements in performance. The most influential variants of this\n",
    "family are **VGG-16** and **VGG-19**, named according to the number of trainable layers\n",
    "that compose them.\n",
    "\n",
    "The design of VGG is characterized by its structural simplicity: a sequence of\n",
    "convolutions of fixed size, periodically separated by pooling operations, followed by a\n",
    "block of fully connected layers. This minimalist philosophy contrasts with later, more\n",
    "complex architectures, such as Inception, and turns VGG into a fundamental didactic\n",
    "reference for understanding deep convolutional networks. The architecture shows that\n",
    "highly discriminative visual representations can be obtained through the systematic\n",
    "repetition of basic components, without the need to introduce complex operations or\n",
    "specialized modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinctive feature of the VGG architecture lies in its deliberately conservative and homogeneous design philosophy. Unlike other architectures that combine convolutional filters of different sizes (for example, $5 \\times 5$ or $7 \\times 7$), VGG strictly standardizes its components and maintains a highly regular structure throughout the network. \n",
    "\n",
    "The architecture employs only $3 \\times 3$ convolutional filters, which are applied repeatedly in sequence. As the network depth increases and the spatial resolution of the feature maps decreases, the number of filters is progressively increased, typically doubling this number after each _max pooling_ operation. This strategy allows the network to incrementally enrich the representational capacity of the feature maps while controlling the computational cost in terms of spatial dimensions.\n",
    "\n",
    "In addition, VGG makes systematic use of convolutions with appropriate padding in order to preserve the spatial resolution within each block. At the end of each block, it applies $2 \\times 2$ _max pooling_ operations with stride 2 to reduce the spatial dimensions of the feature maps. This alternation between groups of convolutions with preserved resolution and pooling layers with downsampling produces a hierarchical representation of the input.\n",
    "\n",
    "Within this hierarchy, the early layers capture local and low-level information, such as edges, corners, and simple textures. As the signal propagates through deeper layers, the successive application of $3 \\times 3$ convolutions over increasingly abstract feature maps enables the network to aggregate broader spatial context and encode more complex and semantically rich patterns. Consequently, the deeper layers represent higher-level, more abstract features, such as object parts or entire object configurations, which are especially useful for tasks like image classification and object recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for the Use of Small Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of $3 \\times 3$ convolutional filters is justified by a combination of theoretical and practical arguments. Although LeNet-5 employs $5 \\times 5$ filters, the designers of VGG demonstrate that several consecutive layers of $3 \\times 3$ filters can emulate the receptive field of larger filters while requiring fewer parameters and incorporating a greater number of intermediate nonlinearities.\n",
    "\n",
    "From the perspective of the receptive field, a single $k \\times k$ convolution can be approximated by a sequence of smaller convolutions, such as $3 \\times 3$, provided that padding is chosen appropriately. In particular, two $3 \\times 3$ layers possess an effective receptive field equivalent to a $5 \\times 5$ filter, and three $3 \\times 3$ layers approximate a $7 \\times 7$ receptive field. This decomposition proves advantageous both in terms of parameter efficiency and in terms of the expressive capacity of the model.\n",
    "\n",
    "To understand this equivalence, it is useful to consider the notion of effective receptive field of a neuron in a convolutional network. Intuitively, the receptive field indicates the number of pixels from the original image that influence the activation of a neuron in a given layer. In the simplest case, with stride 1 and appropriate padding, a convolutional layer with filters of size $5 \\times 5$ possesses a receptive field of $5 \\times 5 = 25$ pixels. In contrast, two consecutive $3 \\times 3$ layers produce an effective receptive field of $5 \\times 5$: the first layer observes $3 \\times 3$ pixels of the image, and the second layer observes $3 \\times 3$ neurons of the previous layer, whose receptive fields overlap in such a manner that, combined, they cover $5 \\times 5$ pixels of the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Organization of VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG-16 variant receives color (RGB) input images of size $224 \\times 224$ pixels and is organized as a hierarchical sequence of five convolutional blocks, followed by a set of fully connected layers responsible for the final classification. Each block groups several $3 \\times 3$ convolutions followed by a $2 \\times 2$ max pooling operation.\n",
    "\n",
    "In its original configuration for ImageNet, the architecture is organized as follows:\n",
    "\n",
    "| Block   | Conv Layers | Filters | Typical Output Size          | Pooling               |\n",
    "| ------- | ----------: | ------: | ---------------------------- | --------------------- |\n",
    "| Block 1 |           2 |      64 | $112 \\times 112 \\times 64$ | MaxPool $2 \\times 2$ |\n",
    "| Block 2 |           2 |     128 | $56 \\times 56 \\times 128$  | MaxPool $2 \\times 2$ |\n",
    "| Block 3 |           3 |     256 | $28 \\times 28 \\times 256$  | MaxPool $2 \\times 2$ |\n",
    "| Block 4 |           3 |     512 | $14 \\times 14 \\times 512$  | MaxPool $2 \\times 2$ |\n",
    "| Block 5 |           3 |     512 | $7 \\times 7 \\times 512$    | MaxPool $2 \\times 2$ |\n",
    "| FC6     |           - |    4096 | 4096                         | -                     |\n",
    "| FC7     |           - |    4096 | 4096                         | -                     |\n",
    "| FC8     |           - |    1000 | 1000                         | Softmax               |\n",
    "\n",
    "The first two convolutional blocks are composed of two convolutional layers each. The first block employs 64 filters, while the second employs 128 filters, always with filter size $3 \\times 3$. At the end of each block, a $2 \\times 2$ max pooling operation is applied, whose role is to halve the spatial resolution and concentrate the most relevant information. The third, fourth, and fifth blocks increase the effective depth of the network by means of three consecutive convolutional layers per block. The number of filters increases to 256 in the third block and to 512 in the last two blocks. At these deeper stages, the network learns highly abstract features, such as object parts, complex textures, and high-level visual configurations, which are crucial for class discrimination.\n",
    "\n",
    "After the convolutional blocks, the resulting feature maps are transformed into a one-dimensional vector that feeds the classifier layers. The final segment consists of two dense layers with 4096 neurons each, followed by an output layer with 1000 neurons, corresponding to the thousand categories of the ImageNet dataset. The Softmax activation makes it possible to interpret the output as a probability distribution over classes. Throughout the architecture, the ReLU activation function is used, which accelerates training and helps mitigate the vanishing gradient problem. The combination of these design decisions—moderate depth, small filters, homogeneous structure—positions VGG-16 as a high-performance model on ImageNet, though at the cost of a very large number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact, Advantages, and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16 achieves second place in the 2014 ImageNet challenge, but its impact extends far\n",
    "beyond the competition. The scientific and technical community adopts this architecture\n",
    "as a reference due to its clear, regular, and easily interpretable design. This clarity\n",
    "makes it a fundamental tool both for research and teaching on deep convolutional\n",
    "networks, as well as for the development of numerous subsequent works in transfer\n",
    "learning and feature extraction.\n",
    "\n",
    "Among its **advantages**, it is worth highlighting its homogeneous and modular\n",
    "architecture, which facilitates implementation and experimentation; its excellent\n",
    "capacity for learning hierarchical features in images; and its suitability for **transfer\n",
    "learning**. The initial layers of VGG learn generic and robust representations focused on\n",
    "edges, textures, and local patterns that can be reused effectively in a wide variety of\n",
    "computer vision tasks by adapting only the final layers.\n",
    "\n",
    "However, VGG also presents **significant limitations**. The very large number of\n",
    "parameters (on the order of 138 million in VGG-16) implies considerable memory\n",
    "consumption (around 500 MB in 32-bit precision) and a high computational cost both in\n",
    "training and inference. These characteristics make the architecture unsuitable for\n",
    "devices with limited resources and increase the cost of large-scale production\n",
    "deployment. In addition, a substantial portion of the parameters is concentrated in the\n",
    "final fully connected layers, which has motivated later architectures to replace these\n",
    "layers with more efficient mechanisms such as _global average pooling_.\n",
    "\n",
    "These constraints have driven the development of subsequent architectures such as\n",
    "Inception, ResNet, or MobileNet, which aim to maintain or improve performance while\n",
    "reducing computational cost, facilitating the training of deeper networks, and adapting\n",
    "to resource-constrained environments. Despite this, VGG remains a classic reference model\n",
    "due to its conceptual transparency and its capacity to serve as a starting point in\n",
    "numerous practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation of VGG-16 on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements a variant of VGG-16 adapted to the CIFAR-10 dataset, which contains color images of $32 \\times 32$ pixels belonging to 10 categories. The original architecture, designed for ImageNet ($224 \\times 224$), is modified to accommodate the smaller input size and the reduced number of classes in CIFAR-10, while preserving the VGG design philosophy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modules are imported to implement VGG-16 and train it on CIFAR-10. The goal\n",
    "is to have at our disposal the PyTorch tools required to define the model, manage data,\n",
    "train the network, and analyze the results in a systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "from typing import Any, List\n",
    "\n",
    "# 3pps\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block verifies GPU compatibility when available and provides basic information about\n",
    "the execution environment, which is useful for reproducing experiments and diagnosing\n",
    "configuration issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants used throughout the implementation are defined next, such as batch size, number\n",
    "of epochs, learning rate, and number of classes. Centralizing these parameters\n",
    "facilitates experimentation and model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "BATCH_SIZE: int = 128\n",
    "NUM_EPOCHS: int = 1\n",
    "LEARNING_RATE: float = 1e-3\n",
    "WEIGHT_DECAY: float = 5e-4\n",
    "NUM_CLASSES: int = 10  # CIFAR-10 has 10 classes\n",
    "INPUT_SIZE: int = 32  # CIFAR-10: 32×32 images\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration provides a reasonable starting point for training VGG-16 on CIFAR-10,\n",
    "establishing a compromise between performance and computational cost that can be adjusted\n",
    "according to available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Visualization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual exploration of the data helps to better understand the problem and to verify that\n",
    "preprocessing is applied correctly. A function is defined to visualize images with their\n",
    "labels and, optionally, with model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels, predictions=None, classes=CIFAR10_CLASSES):\n",
    "    \"\"\"\n",
    "    Visualizes a set of images with their labels.\n",
    "\n",
    "    Args:\n",
    "        images: Image tensor [N, C, H, W]\n",
    "        labels: Label tensor [N]\n",
    "        predictions: Optional prediction tensor [N]\n",
    "        classes: List of class names\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(2 * n_images, 3))\n",
    "\n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (img, label, ax) in enumerate(zip(images, labels, axes)):\n",
    "        # Denormalize image for visualization\n",
    "        img = img / 2 + 0.5  # Revert normalization [-1, 1] -> [0, 1]\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "\n",
    "        title = f\"True: {classes[label]}\"\n",
    "        if predictions is not None:\n",
    "            pred = predictions[idx]\n",
    "            color = \"green\" if pred == label else \"red\"\n",
    "            title += f\"\\nPred: {classes[pred]}\"\n",
    "            ax.set_title(title, fontsize=10, color=color, fontweight=\"bold\")\n",
    "        else:\n",
    "            ax.set_title(title, fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization function defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used later to inspect both samples from the dataset and correct and\n",
    "incorrect model predictions, providing an essential visual diagnostic tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 data are loaded and the data augmentation and normalization transformations applied to the images are defined. CIFAR-10 contains 60,000 color images of $32 \\times 32$ pixels distributed across 10 classes and presents greater variability and complexity than MNIST due to the diversity of objects, backgrounds, and capture conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations with data augmentation for training\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # Random crop with padding\n",
    "        transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)  # CIFAR-10 RGB mean\n",
    "        ),  # CIFAR-10 RGB std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformations for validation (no augmentation)\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Downloading CIFAR-10 dataset...\")\n",
    "\n",
    "# Training set\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "\n",
    "# Test set\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "print(f\"  Test samples: {len(test_dataset):,}\")\n",
    "print(f\"  Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"  Classes: {', '.join(train_dataset.classes)}\")\n",
    "print(\"  Image size: 32×32 pixels (RGB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is introduced to increase the generalization capacity of the model. RandomCrop with padding simulates variations in framing and object position, while RandomHorizontalFlip increases robustness to horizontal symmetries. Both mechanisms reduce overfitting by generating slightly different versions of each image at each epoch, effectively expanding the training set without requiring additional data.\n",
    "\n",
    "Normalization is performed using the mean and standard deviation of the complete CIFAR-10 dataset for each RGB channel:\n",
    "\n",
    "$$\n",
    "\\mu = (0.4914, 0.4822, 0.4465), \\quad \\sigma = (0.2470, 0.2435, 0.2616).\n",
    "$$\n",
    "\n",
    "This operation centers and scales the values of each channel using the transformation:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma},\n",
    "$$\n",
    "\n",
    "which facilitates optimization and stabilizes training by improving the numerical conditioning of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datasets are defined, `DataLoader` objects are created to manage batch iteration\n",
    "during training and evaluation, including optimizations to accelerate data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Configuring DataLoaders:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Training DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Test DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"  Training batches: {len(train_dataloader)}\")\n",
    "print(f\"  Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common PyTorch optimizations are applied. The parameter `num_workers=4` allows data\n",
    "loading in parallel via auxiliary processes, taking advantage of multiple CPU cores. The\n",
    "use of `pin_memory=True` improves the speed of data transfer to the GPU by using pinned\n",
    "(non-pageable) memory. Finally, `persistent_workers=True` avoids recreating the worker\n",
    "processes at each epoch, reducing initialization overhead and accelerating the data\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Visual Exploration of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the model, it is useful to inspect some images from the training set and\n",
    "verify the tensor dimensions and the effect of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain one data batch\n",
    "data_iter = iter(train_dataloader)\n",
    "train_images, train_labels = next(data_iter)\n",
    "\n",
    "print(\"Batch dimensions:\")\n",
    "print(f\"  Images: {train_images.shape}\")\n",
    "print(f\"  Labels: {train_labels.shape}\")\n",
    "print(f\"\\n  Interpretation: {BATCH_SIZE} RGB images of size 32×32 pixels\")\n",
    "\n",
    "# Visualize first 8 examples\n",
    "print(\"\\nVisualizing first 8 samples...\")\n",
    "show_images(train_images[:8], train_labels[:8])\n",
    "\n",
    "# Statistics of normalized images\n",
    "print(\"\\nStatistics after normalization:\")\n",
    "print(f\"  Min value: {train_images.min():.3f}\")\n",
    "print(f\"  Max value: {train_images.max():.3f}\")\n",
    "print(\"  Mean per channel:\")\n",
    "for i, channel in enumerate([\"R\", \"G\", \"B\"]):\n",
    "    print(f\"    {channel}: {train_images[:, i, :, :].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis allows the verification that data are correctly loaded, preprocessing is\n",
    "applied appropriately, and data augmentation transformations produce reasonable\n",
    "variations without excessively distorting the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the VGG-16 Architecture for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VGG-16 variant adapted to $32 \\times 32$ images is implemented next. The original architecture for ImageNet is designed for $224 \\times 224$, so adjustments in the final layers are required to adapt to the reduced input size and the smaller number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of VGG-16 adapted for CIFAR-10 (32×32 pixels).\n",
    "\n",
    "    Architecture:\n",
    "    - 5 convolutional blocks with configuration [64, 128, 256, 512, 512]\n",
    "    - All filters are 3×3\n",
    "    - 2×2 MaxPooling after each block\n",
    "    - 3 fully connected layers at the end\n",
    "    - BatchNorm to stabilize training\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Block 1: 2 conv layers with 64 filters\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32×32 -> 16×16\n",
    "        )\n",
    "\n",
    "        # Block 2: 2 conv layers with 128 filters\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16×16 -> 8×8\n",
    "        )\n",
    "\n",
    "        # Block 3: 3 conv layers with 256 filters\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8×8 -> 4×4\n",
    "        )\n",
    "\n",
    "        # Block 4: 3 conv layers with 512 filters\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 4×4 -> 2×2\n",
    "        )\n",
    "\n",
    "        # Block 5: 3 conv layers with 512 filters\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 2×2 -> 1×1\n",
    "        )\n",
    "\n",
    "        # Classifier layers\n",
    "        # For CIFAR-10 (32×32), after 5 poolings: 32 / 2^5 = 1\n",
    "        # Therefore: 512 × 1 × 1 = 512 features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 1 * 1, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights using He initialization for layers with ReLU.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of VGG-16.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, 3, 32, 32]\n",
    "\n",
    "        Returns:\n",
    "            Classification logits [B, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extracts features before the final classification layer.\n",
    "        Useful for embedding visualization.\n",
    "        \"\"\"\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"VGG-16 architecture defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation and Complexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the architecture is defined, the model is instantiated, moved to the appropriate\n",
    "device (CPU or GPU), and its structure and parameter count are analyzed to verify that\n",
    "the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = VGG16(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Determine available device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"VGG-16 ARCHITECTURE SUMMARY\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "# Detailed architecture summary\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=str(device))\n",
    "\n",
    "# Parameter count per block\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"PARAMETER ANALYSIS PER BLOCK\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "\n",
    "def count_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "print(f\"  Block 1: {count_parameters(model.block1):>12,} parameters\")\n",
    "print(f\"  Block 2: {count_parameters(model.block2):>12,} parameters\")\n",
    "print(f\"  Block 3: {count_parameters(model.block3):>12,} parameters\")\n",
    "print(f\"  Block 4: {count_parameters(model.block4):>12,} parameters\")\n",
    "print(f\"  Block 5: {count_parameters(model.block5):>12,} parameters\")\n",
    "print(f\"  Classifier: {count_parameters(model.classifier):>12,} parameters\")\n",
    "print(f\"  {'-' * 66}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  TOTAL: {total_params:>12,} parameters\")\n",
    "print(f\"  Trainable: {trainable_params:>12,} parameters\")\n",
    "print(f\"  Memory (float32): {total_params * 4 / (1024 ** 2):>10.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original VGG-16 for ImageNet, about 14.7 million parameters correspond to the\n",
    "convolutional layers and around 123 million to the fully connected layers (approximately\n",
    "89 % of the total). The CIFAR-10 variant drastically reduces the parameters of the dense\n",
    "layers by going from 4096 to 512 neurons, resulting in a more manageable model suited to\n",
    "this dataset, although it remains considerably large compared with more modern, efficient\n",
    "architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer, loss function, and a learning rate scheduler are now defined to improve\n",
    "convergence and adapt dynamically to the evolution of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Initial learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "# Optimizer: SGD with momentum\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: reduces LR when progress plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",  # Monitor accuracy (to be maximized)\n",
    "    factor=0.1,  # Reduce LR to 10 % of current value\n",
    "    patience=3,  # After 3 epochs without improvement\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "# Loss function: Cross-Entropy\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Optimizer: SGD with momentum=0.9\")\n",
    "print(\"  SGD with momentum accumulates gradients with exponential decay\")\n",
    "print(\"  This helps escape local minima and accelerates convergence\")\n",
    "print(\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(\"  Reduces the learning rate when accuracy stops improving\")\n",
    "print(\"  Reduction factor: 0.1 (LR → 0.1 × LR)\")\n",
    "print(\"  Patience: 3 epochs\")\n",
    "print(\"\\nLoss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, for architectures such as VGG, SGD with momentum has been very effective, especially when sufficient training time is available and the learning rate is carefully tuned. The momentum term accumulates past gradients as follows:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla L(\\theta_{t-1}), \\quad \\theta_t = \\theta_{t-1} - \\text{lr} \\cdot v_t,\n",
    "$$\n",
    "\n",
    "where $0 < \\beta < 1$ is the momentum coefficient, typically 0.9. This mechanism accelerates descent in directions of consistent gradient and damps oscillations in directions of high curvature, improving both the speed of convergence and training stability.\n",
    "\n",
    "The ReduceLROnPlateau scheduler adjusts the learning rate dynamically based on the evolution of performance on the validation set. When the monitored metric, in this case test accuracy, stops improving for a given number of epochs, the patience parameter, the scheduler reduces the learning rate, allowing a finer adjustment of parameters near a local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop consists of two phases per epoch: a training phase, during which the\n",
    "model parameters are updated, and a validation phase, during which performance is\n",
    "evaluated without modifying the weights. Losses and accuracies on both phases are\n",
    "recorded, as well as the evolution of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics\n",
    "train_losses, train_accuracies = [], []\n",
    "test_losses, test_accuracies = [], []\n",
    "learning_rates = []\n",
    "\n",
    "\n",
    "# Auxiliary function to compute accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct, total\n",
    "\n",
    "\n",
    "print(\"STARTING TRAINING\\n\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # ============ TRAINING PHASE ============\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    train_loop = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TRAIN]\", leave=False\n",
    "    )\n",
    "\n",
    "    for batch_image, batch_label in train_loop:\n",
    "        batch_image = batch_image.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_image)\n",
    "        loss = loss_function(outputs, batch_label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_correct, batch_total = calculate_accuracy(outputs, batch_label)\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "        train_loop.set_postfix(\n",
    "            {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n",
    "        )\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_dataloader)\n",
    "    epoch_train_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # ============ VALIDATION PHASE ============\n",
    "    model.eval()\n",
    "    test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "\n",
    "    test_loop = tqdm(\n",
    "        test_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [TEST]\", leave=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loop:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            batch_correct, batch_total = calculate_accuracy(outputs, labels)\n",
    "            correct_test += batch_correct\n",
    "            total_test += batch_total\n",
    "\n",
    "            test_loop.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"acc\": f\"{100 * correct_test / total_test:.2f}%\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    epoch_test_loss = test_loss / len(test_dataloader)\n",
    "    epoch_test_acc = 100 * correct_test / total_test\n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_accuracies.append(epoch_test_acc)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(epoch_test_acc)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    # Epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    # Epoch report\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Time: {epoch_time:.2f}s\")\n",
    "    print(f\"  Train → Loss: {epoch_train_loss:.4f} | Acc: {epoch_train_acc:.2f}%\")\n",
    "    print(f\"  Test  → Loss: {epoch_test_loss:.4f} | Acc: {epoch_test_acc:.2f}%\")\n",
    "    print(f\"  LR: {current_lr:.6f}\")\n",
    "    print(f\"  {'─' * 66}\\n\")\n",
    "\n",
    "# Total time\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"  Total time: {total_time / 60:.2f} minutes\")\n",
    "print(f\"  Average time per epoch: {total_time / NUM_EPOCHS:.2f} seconds\")\n",
    "print(f\"  Final accuracy (train): {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Final accuracy (test): {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Best accuracy (test): {max(test_accuracies):.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "    },\n",
    "    \"vgg16_cifar10.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\nModel saved as 'vgg16_cifar10.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG is computationally intensive due to the number of convolution operations and the\n",
    "large number of channels in the deeper layers. On a modern GPU (for example, an RTX\n",
    "3080), each epoch may require on the order of tens of seconds with the described\n",
    "configuration, whereas on CPU the process can be an order of magnitude slower. The\n",
    "instruction `model.train()` activates training-specific behaviors, such as updating\n",
    "`BatchNorm` statistics and applying `Dropout`, while `model.eval()` deactivates these\n",
    "behaviors to ensure deterministic and reproducible evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the evolution of loss, accuracy, and learning rate across epochs makes it\n",
    "possible to identify potential issues such as overfitting, training stagnation, or\n",
    "inadequate learning rate schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "# Create figure with three subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(\n",
    "    epochs_range, train_losses, \"o-\", label=\"Train Loss\", linewidth=2, markersize=6\n",
    ")\n",
    "ax1.plot(epochs_range, test_losses, \"s-\", label=\"Test Loss\", linewidth=2, markersize=6)\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Loss\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\"Loss Evolution\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(list(epochs_range))\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(\n",
    "    epochs_range,\n",
    "    train_accuracies,\n",
    "    \"o-\",\n",
    "    label=\"Train Accuracy\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    ")\n",
    "ax2.plot(\n",
    "    epochs_range,\n",
    "    test_accuracies,\n",
    "    \"s-\",\n",
    "    label=\"Test Accuracy\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    ")\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\"Accuracy Evolution\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(list(epochs_range))\n",
    "\n",
    "# Learning rate plot\n",
    "ax3.plot(epochs_range, learning_rates, \"o-\", color=\"red\", linewidth=2, markersize=6)\n",
    "ax3.set_xlabel(\"Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_ylabel(\"Learning Rate\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xticks(list(epochs_range))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vgg16_training_history.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Quantitative analysis\n",
    "print(\"\\nResult analysis:\")\n",
    "diff = train_accuracies[-1] - test_accuracies[-1]\n",
    "print(f\"  Overfitting detected: {'YES' if diff > 10 else 'NO'}\")\n",
    "print(f\"  Train-test gap: {diff:.2f}%\")\n",
    "print(f\"  Best epoch (test acc): {np.argmax(test_accuracies) + 1}\")\n",
    "print(f\"  Gain since epoch 1: {test_accuracies[-1] - test_accuracies[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy curves help detect typical behaviors: if training loss decreases\n",
    "while validation loss increases, overfitting is evident; if both remain high, the model\n",
    "may be underfitting or the learning rate may be inadequate. The learning-rate plot on a\n",
    "logarithmic scale shows the stepwise decreases produced by the scheduler, which usually\n",
    "coincide with refinement phases during which the model parameters are adjusted more\n",
    "precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Per-Class Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix provides a detailed view of per-class performance and helps identify\n",
    "systematic error patterns, thereby clarifying which categories are more difficult for the\n",
    "model to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "print(\"Generating confusion matrix...\")\n",
    "\n",
    "# Obtain all predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=CIFAR10_CLASSES,\n",
    "    yticklabels=CIFAR10_CLASSES,\n",
    "    cbar_kws={\"label\": \"Number of samples\"},\n",
    ")\n",
    "plt.xlabel(\"Prediction\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Confusion Matrix - VGG16 on CIFAR-10\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vgg16_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    classification_report(\n",
    "        all_labels, all_predictions, target_names=CIFAR10_CLASSES, digits=3\n",
    "    )\n",
    ")\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(\"\\nPer-Class Accuracy Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "class_correct = cm.diagonal()\n",
    "class_total = cm.sum(axis=1)\n",
    "class_accuracy = 100 * class_correct / class_total\n",
    "\n",
    "for idx, class_name in enumerate(CIFAR10_CLASSES):\n",
    "    print(\n",
    "        f\"  {class_name:12s}: {class_accuracy[idx]:6.2f}% \"\n",
    "        f\"({class_correct[idx]:4d}/{class_total[idx]:4d})\"\n",
    "    )\n",
    "\n",
    "# Most confused class pairs\n",
    "print(\"\\nMost Confused Class Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "confusion_pairs = []\n",
    "for i in range(len(CIFAR10_CLASSES)):\n",
    "    for j in range(len(CIFAR10_CLASSES)):\n",
    "        if i != j:\n",
    "            confusion_pairs.append((cm[i, j], CIFAR10_CLASSES[i], CIFAR10_CLASSES[j]))\n",
    "\n",
    "confusion_pairs.sort(reverse=True)\n",
    "for count, true_class, pred_class in confusion_pairs[:5]:\n",
    "    print(f\"  {true_class:12s} → {pred_class:12s}: {count:4d} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main diagonal of the matrix reflects correct classifications, while off-diagonal\n",
    "elements quantify confusions between class pairs. On CIFAR-10 it is common to see\n",
    "confusions between `cat` and `dog`, `automobile` and `truck`, or `bird` and `airplane`,\n",
    "indicating that certain categories share similar visual patterns from the model’s\n",
    "perspective. This analysis is valuable for identifying model limitations and guiding\n",
    "potential improvements, such as collecting additional data for problematic classes or\n",
    "applying class-balancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Correct and Incorrect Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand model behavior, it is useful to visualize some correct predictions\n",
    "and some errors, providing a qualitative perspective that complements quantitative\n",
    "metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing model predictions...\\n\")\n",
    "\n",
    "# Obtain one test batch\n",
    "data_iter = iter(test_dataloader)\n",
    "test_images, test_labels = next(data_iter)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_images_device = test_images.to(device)\n",
    "    outputs = model(test_images_device)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    predictions = predictions.cpu()\n",
    "\n",
    "# Visualize first 8 predictions\n",
    "print(\"First 8 predictions:\")\n",
    "show_images(test_images[:8], test_labels[:8], predictions[:8])\n",
    "\n",
    "# Find misclassified examples\n",
    "incorrect_indices = (predictions != test_labels).nonzero(as_tuple=True)[0]\n",
    "\n",
    "if len(incorrect_indices) >= 8:\n",
    "    print(\"\\nExamples of incorrect predictions:\")\n",
    "    error_indices = incorrect_indices[:8]\n",
    "    show_images(\n",
    "        test_images[error_indices],\n",
    "        test_labels[error_indices],\n",
    "        predictions[error_indices],\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nOnly {len(incorrect_indices)} errors in this batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This qualitative analysis helps detect systematic error patterns, such as consistently\n",
    "confusing one type of vehicle with another or certain animal classes with one another.\n",
    "Visual inspection of errors can also reveal issues in the data, such as incorrect labels\n",
    "or ambiguous images that would be difficult for a human to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction and Visualization of Intermediate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of VGG’s strengths is its ability to learn hierarchical features in depth.\n",
    "Activations of intermediate layers can be inspected to better understand which types of\n",
    "patterns the network captures at each convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_maps(model, image, layer_name):\n",
    "    \"\"\"\n",
    "    Extracts activation maps from a specific layer.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations[\"output\"] = output\n",
    "\n",
    "    # Register hook\n",
    "    layer = dict([*model.named_modules()])[layer_name]\n",
    "    hook = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "\n",
    "    hook.remove()\n",
    "    return activations[\"output\"].squeeze().cpu()\n",
    "\n",
    "\n",
    "# Select one test image\n",
    "test_image, test_label = test_dataset[0]\n",
    "print(f\"Analyzing image of class: {CIFAR10_CLASSES[test_label]}\\n\")\n",
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "img_display = test_image / 2 + 0.5  # Denormalize\n",
    "plt.imshow(img_display.permute(1, 2, 0))\n",
    "plt.title(f\"Original Image: {CIFAR10_CLASSES[test_label]}\", fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select and visualize activations from different blocks\n",
    "layers_to_visualize = {\n",
    "    \"block1\": \"block1.0\",  # First conv of block1\n",
    "    \"block2\": \"block2.0\",  # First conv of block2\n",
    "    \"block3\": \"block3.0\",  # First conv of block3\n",
    "    \"block5\": \"block5.0\",  # First conv of block5\n",
    "}\n",
    "\n",
    "for block_name, layer_name in layers_to_visualize.items():\n",
    "    print(f\"Visualizing activations of {block_name}...\")\n",
    "\n",
    "    activations = get_activation_maps(model, test_image, layer_name)\n",
    "\n",
    "    # Select first 16 filters for visualization\n",
    "    num_filters = min(16, activations.shape[0])\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    fig.suptitle(\n",
    "        f\"Activations of {block_name.upper()} - \" f\"{CIFAR10_CLASSES[test_label]}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < num_filters:\n",
    "            activation = activations[idx]\n",
    "            ax.imshow(activation, cmap=\"viridis\")\n",
    "            ax.set_title(f\"Filter {idx}\", fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"vgg16_activations_{block_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nInterpretation of block-wise activations:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Block 1: Detects low-level features\")\n",
    "print(\"           (edges, corners, simple color variations)\")\n",
    "print(\"  Block 2-3: Detect mid-level patterns\")\n",
    "print(\"             (textures, more complex shapes, repetitive patterns)\")\n",
    "print(\"  Block 4-5: Detect high-level features\")\n",
    "print(\"             (object parts, combinations of textures and shapes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
