{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Transfer learning_ consists of reusing a previously trained model, typically on a large\n",
    "generic dataset (for example, ImageNet, with millions of images), as a starting point for\n",
    "solving a specific task. Instead of training a network from scratch, the representations\n",
    "already learned by the model are leveraged, which typically capture basic patterns such\n",
    "as edges, textures, shapes, and more complex compositions.\n",
    "\n",
    "This approach notably reduces the amount of data required, accelerates training, and\n",
    "typically provides better performance when working with small or medium-sized datasets.\n",
    "The network already \"knows\" general visual features, and it is only necessary to\n",
    "specialize it for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transfer Learning Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the use of pretrained models is articulated around three main strategies,\n",
    "which differ in which parts of the model are updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the _feature extraction_ strategy, all model parameters are frozen except the last\n",
    "classification layer. In this way, the pretrained model acts as a fixed feature extractor\n",
    "and only a lightweight classifier is trained on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze ALL layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer (classifier)\n",
    "num_classes = 2  # Example: dogs vs cats\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Only model.fc parameters are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option is especially suitable when the dataset is very small (less than about 1000\n",
    "images) and the images are relatively similar to those in ImageNet (natural scenes,\n",
    "everyday objects). Training is fast and the risk of overfitting is reduced, since most\n",
    "weights remain fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Fine-Tuning (Fine-Tuning Upper Layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _partial fine-tuning_, the deepest layers (close to the input) are frozen and the last\n",
    "convolutional layers are unfrozen together with the classifier. The idea is to preserve\n",
    "the most generic features (edges, textures) and adapt high-level representations to the\n",
    "new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze first layers (for example, all except layer4 and fc)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" not in name and \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Optimizer with different learning rates\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.layer4.parameters(),\n",
    "            \"lr\": 1e-4,\n",
    "        },  # Low LR for pretrained layers\n",
    "        {\"params\": model.fc.parameters(), \"lr\": 1e-3},  # Higher LR for new layer\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is appropriate for medium-sized datasets (on the order of 1,000 to 10,000\n",
    "images) and when the domain is moderately different from ImageNet (for example, medical\n",
    "images of structures that still share certain types of visual patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _full fine-tuning_, all parameters of the pretrained model are retrained, albeit with\n",
    "a relatively low learning rate to avoid abruptly destroying prior knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace classifier\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Entire model is trainable (low LR)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is recommended when a large dataset is available (more than 10,000 images)\n",
    "or when the domain is very different from the original training domain (for example, very\n",
    "specific medical images, satellite images, or industrial data with unusual textures). In\n",
    "exchange for higher computational cost, the maximum possible accuracy can be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: Dogs vs Cats Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents a simplified workflow based on _feature extraction_ for a binary\n",
    "classification problem, for example dogs versus cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary transformations are defined, including normalization with ImageNet means\n",
    "and standard deviations, a key requirement to correctly reuse pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Transformations (ImageNet normalization)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225],  # ImageNet standard deviation\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load dataset organized in folders by class\n",
    "train_dataset = datasets.ImageFolder(\"data/train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18 is used as a feature extractor and only the last layer is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Feature extraction: freeze everything except the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 2)  # 2 classes: dogs and cats\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this training, the model is specialized in distinguishing between dogs and cats\n",
    "based on the general representations it had already learned on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Embeddings_ are high-dimensional numerical vectors that compactly represent the content\n",
    "of an image. To obtain them, the pretrained model is reused by removing the last\n",
    "classification layer and using only the part that acts as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model):\n",
    "        # Remove the last layer (classifier) and keep the convolutional trunk\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        self.features.eval()\n",
    "\n",
    "    def extract(self, image):\n",
    "        \"\"\"Extracts the embedding of one or more images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.features(image)  # (B, C, 1, 1) in ResNet\n",
    "            embedding = embedding.view(embedding.size(0), -1)  # Flatten to (B, C)\n",
    "        return embedding.numpy()\n",
    "\n",
    "\n",
    "# Usage\n",
    "extractor = FeatureExtractor(model)\n",
    "image = torch.randn(1, 3, 224, 224)  # Example image\n",
    "embedding = extractor.extract(image)\n",
    "print(f\"Embedding shape: {embedding.shape}\")  # (1, 512) in ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings can be used for additional tasks, such as clustering, visualization,\n",
    "similarity search, or as input to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization with PCA and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When embeddings of many images are available (for example, those in the training set),\n",
    "they can be projected to two dimensions to visualize how different classes cluster in the\n",
    "feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a fast linear technique that finds directions of\n",
    "maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Assume embeddings has shape (N, 512) and labels has shape (N,)\n",
    "embeddings = ...  # Embedding matrix\n",
    "labels = ...  # Corresponding labels\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Embeddings in 2D (PCA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a nonlinear technique that typically provides clearer visualizations of\n",
    "clusters, although it is more computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.title(\"Embeddings in 2D (t-SNE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is deterministic and fast; t-SNE better captures local structures and groups, at the\n",
    "cost of greater computation time and some variability between executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search via Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct application of embeddings is semantic search: given a query embedding, the most\n",
    "similar images are retrieved by comparing their representations using a measure such as\n",
    "cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity between two vectors $v_1$ and $v_2$ is defined as:\n",
    "\n",
    "$$ \\text{sim}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\; \\|v_2\\|}, $$\n",
    "\n",
    "and takes values between $-1$ and $1$, where 1 indicates identical vectors (same\n",
    "direction), 0 indicates absence of directional relationship, and âˆ’1 indicates opposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between two vectors.\n",
    "    Result between -1 and 1.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Semantic Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A searcher can be built that, given a query embedding, returns the $k$ most similar\n",
    "images in a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self, embeddings_db, labels_db):\n",
    "        \"\"\"\n",
    "        embeddings_db: Matrix (N, D) with database embeddings.\n",
    "        labels_db: Vector (N,) with labels or identifiers.\n",
    "        \"\"\"\n",
    "        norms = np.linalg.norm(embeddings_db, axis=1, keepdims=True)\n",
    "        self.embeddings_db = embeddings_db / norms  # Normalize for cosine\n",
    "        self.labels_db = labels_db\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        \"\"\"Returns the top_k entries most similar to the query.\"\"\"\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        similarities = np.dot(self.embeddings_db, query_norm)\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        top_similarities = similarities[top_indices]\n",
    "        return top_indices, top_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the search database, embeddings are calculated for all images in the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    embs = extractor.extract(images)\n",
    "    all_embeddings.append(embs)\n",
    "    all_labels.extend(labels.numpy())\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "searcher = SemanticSearch(all_embeddings, all_labels)\n",
    "\n",
    "# Query with a new image\n",
    "query_image = torch.randn(1, 3, 224, 224)  # Example query\n",
    "query_emb = extractor.extract(query_image).squeeze()\n",
    "\n",
    "indices, sims = searcher.search(query_emb, top_k=5)\n",
    "for i, (idx, sim) in enumerate(zip(indices, sims), 1):\n",
    "    print(f\"#{i}: Index {idx}, Similarity {sim:.3f}, Class {all_labels[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mechanism constitutes the basis of visual recommendation systems, similarity-based\n",
    "image search engines, and visual database exploration tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Simplified Transfer Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical workflow with transfer learning for images can be summarized in the following\n",
    "steps:\n",
    "\n",
    "1. Load a pretrained model.\n",
    "2. Choose the strategy (feature extraction, partial or full fine-tuning).\n",
    "3. Train according to the selected strategy.\n",
    "4. Build an embedding extractor from the trained model.\n",
    "5. Obtain embeddings from the dataset.\n",
    "6. Visualize the structure of the feature space (PCA, t-SNE).\n",
    "7. Build a semantic search system on those embeddings.\n",
    "\n",
    "In code, a simplified pipeline could take this form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load pretrained model and prepare for feature extraction\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 2)\n",
    "\n",
    "# STEP 2: Train only the classifier\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# STEP 3: Create embedding extractor\n",
    "extractor = FeatureExtractor(model)\n",
    "\n",
    "# STEP 4: Calculate dataset embeddings\n",
    "embeddings = []\n",
    "labels_list = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    emb = extractor.extract(images)\n",
    "    embeddings.append(emb)\n",
    "    labels_list.extend(labels.numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# STEP 5: Visualize (for example, with PCA)\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels_array, cmap=\"tab10\")\n",
    "plt.show()\n",
    "\n",
    "# STEP 6: Create semantic searcher\n",
    "searcher = SemanticSearch(embeddings, labels_array)\n",
    "\n",
    "# STEP 7: Search for images similar to a query image\n",
    "query_img = next(iter(train_loader))[0][0:1]\n",
    "query_emb = extractor.extract(query_img).squeeze()\n",
    "indices, sims = searcher.search(query_emb, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
