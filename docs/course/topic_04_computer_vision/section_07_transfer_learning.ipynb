{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning consists of reusing a previously trained model, generally obtained from\n",
    "a large and generic dataset (for example, ImageNet, which contains millions of images),\n",
    "as the starting point for solving a more specific task. Instead of training a neural\n",
    "network from scratch, the model's previously acquired representations are leveraged.\n",
    "These representations tend to capture basic patterns such as edges, textures, and shapes,\n",
    "as well as increasingly complex visual compositions.\n",
    "\n",
    "This approach notably reduces the amount of data required, accelerates training, and\n",
    "usually yields better performance when only small or medium-sized datasets are available.\n",
    "The network already encodes general visual features, and it only needs to be specialized\n",
    "for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Strategies in Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical applications, the use of pretrained models is structured around three main\n",
    "strategies, which differ in terms of which parts of the model are updated during\n",
    "training: Feature extraction, partial fine-tuning, and full fine-tuning. The choice among\n",
    "these strategies depends primarily on the size of the target dataset and the similarity\n",
    "between the pretraining domain and the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature extraction strategy, all parameters of the pretrained model are frozen\n",
    "except for the final classification layer. In this configuration, the pretrained network\n",
    "acts as a fixed feature extractor, and only a lightweight classifier is trained on top of\n",
    "those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze ALL layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer (classifier)\n",
    "num_classes = 2  # Example: Dogs vs. cats\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Only model.fc parameters will be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option is particularly suitable when the dataset is very small (for example, fewer\n",
    "than approximately 1,000 images) and when the images are relatively similar to those in\n",
    "ImageNet, such as natural scenes and everyday objects. Training is fast, and the risk of\n",
    "overfitting is reduced, since most weights remain fixed and only the last layer adapts to\n",
    "the new categories. However, the model's capacity for adapting to domains that differ\n",
    "substantially from the pretraining data is limited, because the internal representations\n",
    "are not modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In partial fine-tuning, the earliest layers of the network (those closest to the input)\n",
    "are kept frozen, whereas several of the last convolutional layers, together with the\n",
    "classifier, are unfrozen and updated during training. The underlying idea is to preserve\n",
    "the most generic features, such as edges and simple textures, and adapt the higher-level\n",
    "representations to the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze the first layers (for example, all except layer4 and fc)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" not in name and \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Optimizer with different learning rates\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.layer4.parameters(),\n",
    "            \"lr\": 1e-4,  # Low learning rate for pretrained layers\n",
    "        },\n",
    "        {\n",
    "            \"params\": model.fc.parameters(),\n",
    "            \"lr\": 1e-3,  # Higher learning rate for the new layer\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is appropriate for medium-sized datasets (on the order of 1,000 to 10,000\n",
    "images) and when the domain is moderately different from ImageNet. An example is a\n",
    "medical imaging task in which the structures still share certain visual patterns, such as\n",
    "textures or shapes, with natural images, but require adaptation of the high-level\n",
    "abstractions. In this context, partial fine-tuning offers a balance between flexibility\n",
    "and protection against overfitting, since only part of the network is modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In full fine-tuning, all parameters of the pretrained model are updated during training,\n",
    "typically using a relatively low learning rate. The objective is to adapt the entire\n",
    "network to the new domain without abruptly destroying the knowledge acquired during\n",
    "pretraining (a phenomenon known as catastrophic forgetting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace classifier\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Entire model is trainable (low learning rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is recommended when a large dataset is available (for example, more than\n",
    "10,000 images) or when the domain is very different from the original one (for instance,\n",
    "highly specific medical images, satellite images, or industrial data with unusual\n",
    "textures). Although it implies a higher computational cost and a greater risk of\n",
    "overfitting if the dataset is insufficient, it tends to provide the highest possible\n",
    "performance, as the entire architecture is adapted to the new problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: Binary Classification (Dogs vs. Cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section presents a simplified workflow based on the feature extraction\n",
    "strategy applied to a binary classification problem, such as distinguishing dogs from\n",
    "cats. This example illustrates the typical steps needed to reuse a pretrained\n",
    "convolutional network in a practical setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary transformations are defined first. These transformations include resizing,\n",
    "cropping, conversion to tensor, and normalization with the mean and standard deviation\n",
    "used in ImageNet. This normalization is important in order to correctly reuse pretrained\n",
    "models, since their parameters were optimized under these statistical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformations (ImageNet normalization)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225],  # ImageNet standard deviation\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load dataset organized in folders by class\n",
    "train_dataset = datasets.ImageFolder(\"data/train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is assumed to be organized in directories, where each subfolder corresponds\n",
    "to a class (for example, `dogs/` and `cats/`), following the typical structure used by\n",
    "`ImageFolder`. This organization enables the automatic assignment of labels according to\n",
    "the folder names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18 is used as a feature extractor, and only the last fully connected layer is\n",
    "trained for the specific classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Feature extraction: Freeze everything except the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classification layer: 2 classes (dogs and cats)\n",
    "model.fc = nn.Linear(512, 2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this training process, the model is specialized in distinguishing between dogs and\n",
    "cats. It reuses the general visual representations that it previously learned from\n",
    "ImageNet and adapts only the final decision layer. In a more complete setting, it is\n",
    "advisable to incorporate validation sets, early stopping, and possibly data augmentation\n",
    "in order to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are high-dimensional numerical vectors that compactly represent the content of\n",
    "an image. To obtain them, the pretrained model is reused by removing the final\n",
    "classification layer and retaining only the part that acts as a feature extractor. In\n",
    "convolutional architectures such as ResNet, this generally corresponds to the\n",
    "convolutional trunk and the global pooling operations, up to but excluding the final\n",
    "fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model):\n",
    "        # Remove the last layer (classifier) and keep the convolutional trunk\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        self.features.eval()\n",
    "\n",
    "    def extract(self, image):\n",
    "        \"\"\"Extracts the embedding of one or more images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.features(image)  # Shape (B, C, 1, 1) in ResNet\n",
    "            embedding = embedding.view(embedding.size(0), -1)  # Flatten to (B, C)\n",
    "        return embedding.numpy()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "extractor = FeatureExtractor(model)\n",
    "image = torch.randn(1, 3, 224, 224)  # Example image\n",
    "embedding = extractor.extract(image)\n",
    "print(f\"Embedding shape: {embedding.shape}\")  # (1, 512) in ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings can be used as input for subsequent tasks such as clustering,\n",
    "visualization, similarity search, or integration into other machine learning models that\n",
    "operate on vector representations instead of raw images. For example, these vectors may\n",
    "serve as input to classical algorithms such as support vector machines, logistic\n",
    "regression, or k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization with PCA and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When embeddings have been obtained for many images, for example for the entire training\n",
    "set, they can be projected into two dimensions to visualize how the different classes\n",
    "cluster in the feature space. This provides an intuitive understanding of how well the\n",
    "model separates the categories and whether additional processing or model adaptation\n",
    "might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that\n",
    "identifies the directions of maximum variance in the data. It is fast, deterministic, and\n",
    "offers a first approximation of the structure of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Embeddings in 2D (PCA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot allows observation of whether the embeddings form well-separated\n",
    "clusters according to the class labels or whether there is substantial overlap. If the\n",
    "classes are not clearly separated, it may be necessary to revise the training strategy,\n",
    "collect more data, or modify the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm is a nonlinear\n",
    "dimensionality reduction technique that often provides more visually interpretable\n",
    "clusters than PCA, especially in high-dimensional spaces. However, it is more\n",
    "computationally expensive and sensitive to hyperparameters such as perplexity and\n",
    "learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.title(\"Embeddings in 2D (t-SNE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is deterministic and efficient, which makes it suitable for quick analyses and large\n",
    "datasets. t-SNE, by contrast, tends to better preserve local neighborhood relationships\n",
    "and reveal fine-grained groupings, at the expense of higher computational cost and some\n",
    "variability between runs. In practice, it is common to use PCA as a preliminary step to\n",
    "reduce dimensionality before applying t-SNE on a smaller number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct application of embeddings is semantic search: Given a query embedding, the goal\n",
    "is to retrieve the most similar images in a database by comparing their vector\n",
    "representations using a similarity measure such as cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity between two vectors $v_1$ and $v_2$ is defined as:\n",
    "\n",
    "$$\\text{sim}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\lVert v_1 \\rVert \\; \\lVert v_2 \\rVert}$$\n",
    "\n",
    "and takes values between $-1$ and $1$. A value of $1$ indicates that the vectors are\n",
    "identical in direction, $0$ indicates orthogonality (absence of directional\n",
    "relationship), and $-1$ indicates that the vectors point in opposite directions. In the\n",
    "context of image embeddings, similarities are often positive and close to $1$ for\n",
    "semantically similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between two vectors.\n",
    "    Result is between -1 and 1.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Semantic Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on cosine similarity, a search mechanism can be constructed that, given a query\n",
    "embedding, returns the $k$ most similar images in a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self, embeddings_db, labels_db):\n",
    "        \"\"\"\n",
    "        embeddings_db: Matrix (N, D) with database embeddings.\n",
    "        labels_db: Vector (N,) with labels or identifiers.\n",
    "        \"\"\"\n",
    "        norms = np.linalg.norm(embeddings_db, axis=1, keepdims=True)\n",
    "        self.embeddings_db = embeddings_db / norms  # Normalize for cosine similarity\n",
    "        self.labels_db = labels_db\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        \"\"\"Returns the top_k most similar elements to the query.\"\"\"\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        similarities = np.dot(self.embeddings_db, query_norm)\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        top_similarities = similarities[top_indices]\n",
    "        return top_indices, top_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the search database, embeddings are computed for all images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    embs = extractor.extract(images)\n",
    "    all_embeddings.append(embs)\n",
    "    all_labels.extend(labels.numpy())\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "searcher = SemanticSearch(all_embeddings, all_labels)\n",
    "\n",
    "# Query with a new image\n",
    "query_image = torch.randn(1, 3, 224, 224)  # Example query\n",
    "query_emb = extractor.extract(query_image).squeeze()\n",
    "\n",
    "indices, sims = searcher.search(query_emb, top_k=5)\n",
    "\n",
    "for i, (idx, sim) in enumerate(zip(indices, sims), 1):\n",
    "    print(f\"#{i}: Index {idx}, Similarity {sim:.3f}, Class {all_labels[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mechanism constitutes the basis of visual recommendation systems, image search by\n",
    "similarity, and interactive tools for exploring visual databases. By adjusting the method\n",
    "used to compute or index embeddings (for example, using approximate nearest neighbors\n",
    "search with specialized libraries), the system can scale to large collections of images\n",
    "without prohibitive computational costs at query time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Complete Transfer Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical workflow for transfer learning in image classification can be conceptually\n",
    "structured in the following steps: A pretrained model is loaded; a training strategy is\n",
    "selected (feature extraction, partial fine-tuning, or full fine-tuning); the model is\n",
    "trained according to this strategy; an embedding extractor is built; embeddings are\n",
    "computed for the dataset; the structure of the feature space is explored via\n",
    "visualization techniques; and finally, a semantic search system is constructed using\n",
    "these embeddings.\n",
    "\n",
    "In code, a simplified pipeline oriented to feature extraction may take the following\n",
    "form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 1: Load pretrained model and prepare for feature extraction\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 2)  # Example: 2 classes\n",
    "\n",
    "# STEP 2: Train only the classifier\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# STEP 3: Create embedding extractor\n",
    "extractor = FeatureExtractor(model)\n",
    "\n",
    "# STEP 4: Compute dataset embeddings\n",
    "embeddings = []\n",
    "labels_list = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    emb = extractor.extract(images)\n",
    "    embeddings.append(emb)\n",
    "    labels_list.extend(labels.numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# STEP 5: Visualize embeddings (for example, with PCA)\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels_array, cmap=\"tab10\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Embeddings in 2D (PCA)\")\n",
    "plt.show()\n",
    "\n",
    "# STEP 6: Create semantic search engine\n",
    "searcher = SemanticSearch(embeddings, labels_array)\n",
    "\n",
    "# STEP 7: Search for images similar to a query image\n",
    "query_img = next(iter(train_loader))[0][0:1]\n",
    "query_emb = extractor.extract(query_img).squeeze()\n",
    "\n",
    "indices, sims = searcher.search(query_emb, top_k=5)\n",
    "\n",
    "for i, (idx, sim) in enumerate(zip(indices, sims), 1):\n",
    "    print(f\"#{i}: Index {idx}, Similarity {sim:.3f}, Class {labels_array[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline exemplifies how a pretrained model is reused to solve a classification\n",
    "problem, how embeddings derived from that model can be exploited, and how these\n",
    "embeddings feed into visualization and semantic search functionalities. The complete\n",
    "process illustrates the versatility of transfer learning as a foundational tool in modern\n",
    "computer vision, enabling efficient development of high-performance models even when\n",
    "labeled data are limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
