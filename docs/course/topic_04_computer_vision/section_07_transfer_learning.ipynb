{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning consists of reusing a previously trained model, generally obtained froma large and generic dataset (for example, ImageNet, which contains millions of images),as the starting point for solving a more specific task. Instead of training a neuralnetwork from scratch, the model’s previously acquired representations are leveraged.These representations tend to capture basic patterns such as edges, textures, and shapes,as well as increasingly complex visual compositions.This approach notably reduces the amount of data required, accelerates training, andusually yields better performance when only small or medium-sized datasets are available.The network already encodes general visual features, and it only needs to be specializedfor the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Strategies in Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical applications, the use of pretrained models is structured around three mainstrategies, which differ in terms of which parts of the model are updated duringtraining: Feature extraction, partial fine-tuning, and full fine-tuning. The choice amongthese strategies depends primarily on the size of the target dataset and the similaritybetween the pretraining domain and the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature extraction strategy, all parameters of the pretrained model are frozenexcept for the final classification layer. In this configuration, the pretrained networkacts as a fixed feature extractor, and only a lightweight classifier is trained on top ofthose features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport torchvision.models as models# Load pretrained modelmodel = models.resnet18(pretrained=True)# Freeze ALL layersfor param in model.parameters():    param.requires_grad = False# Replace the last layer (classifier)num_classes = 2  # Example: Dogs vs. catsmodel.fc = nn.Linear(model.fc.in_features, num_classes)# Only model.fc parameters will be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option is particularly suitable when the dataset is very small (for example, fewerthan approximately 1,000 images) and when the images are relatively similar to those inImageNet, such as natural scenes and everyday objects. Training is fast, and the risk ofoverfitting is reduced, since most weights remain fixed and only the last layer adapts tothe new categories. However, the model’s capacity for adapting to domains that differsubstantially from the pretraining data is limited, because the internal representationsare not modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In partial fine-tuning, the earliest layers of the network (those closest to the input)are kept frozen, whereas several of the last convolutional layers, together with theclassifier, are unfrozen and updated during training. The underlying idea is to preservethe most generic features, such as edges and simple textures, and adapt the higher-levelrepresentations to the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport torchvision.models as models# Load pretrained modelmodel = models.resnet18(pretrained=True)# Freeze the first layers (for example, all except layer4 and fc)for name, param in model.named_parameters():    if \"layer4\" not in name and \"fc\" not in name:        param.requires_grad = False# Replace classifiernum_classes = 2model.fc = nn.Linear(model.fc.in_features, num_classes)# Optimizer with different learning ratesoptimizer = torch.optim.Adam(    [        {            \"params\": model.layer4.parameters(),            \"lr\": 1e-4,  # Low learning rate for pretrained layers        },        {            \"params\": model.fc.parameters(),            \"lr\": 1e-3,  # Higher learning rate for the new layer        },    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is appropriate for medium-sized datasets (on the order of 1,000 to 10,000images) and when the domain is moderately different from ImageNet. An example is amedical imaging task in which the structures still share certain visual patterns, such astextures or shapes, with natural images, but require adaptation of the high-levelabstractions. In this context, partial fine-tuning offers a balance between flexibilityand protection against overfitting, since only part of the network is modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In full fine-tuning, all parameters of the pretrained model are updated during training,typically using a relatively low learning rate. The objective is to adapt the entirenetwork to the new domain without abruptly destroying the knowledge acquired duringpretraining (a phenomenon known as catastrophic forgetting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport torchvision.models as models# Load pretrained modelmodel = models.resnet18(pretrained=True)# Replace classifiernum_classes = 2model.fc = nn.Linear(model.fc.in_features, num_classes)# Entire model is trainable (low learning rate)optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is recommended when a large dataset is available (for example, more than10,000 images) or when the domain is very different from the original one (for instance,highly specific medical images, satellite images, or industrial data with unusualtextures). Although it implies a higher computational cost and a greater risk ofoverfitting if the dataset is insufficient, it tends to provide the highest possibleperformance, as the entire architecture is adapted to the new problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: Binary Classification (Dogs vs. Cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section presents a simplified workflow based on the feature extractionstrategy applied to a binary classification problem, such as distinguishing dogs fromcats. This example illustrates the typical steps needed to reuse a pretrainedconvolutional network in a practical setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary transformations are defined first. These transformations include resizing,cropping, conversion to tensor, and normalization with the mean and standard deviationused in ImageNet. This normalization is important in order to correctly reuse pretrainedmodels, since their parameters were optimized under these statistical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoaderfrom torchvision import datasets, transforms# Transformations (ImageNet normalization)transform = transforms.Compose(    [        transforms.Resize(256),        transforms.CenterCrop(224),        transforms.ToTensor(),        transforms.Normalize(            mean=[0.485, 0.456, 0.406],  # ImageNet mean            std=[0.229, 0.224, 0.225],   # ImageNet standard deviation        ),    ])# Load dataset organized in folders by classtrain_dataset = datasets.ImageFolder(\"data/train\", transform=transform)train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is assumed to be organized in directories, where each subfolder correspondsto a class (for example, `dogs/` and `cats/`), following the typical structure used by`ImageFolder`. This organization enables the automatic assignment of labels according tothe folder names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18 is used as a feature extractor, and only the last fully connected layer istrained for the specific classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport torchvision.models as models# Load pretrained modelmodel = models.resnet18(pretrained=True)# Feature extraction: Freeze everything except the last layerfor param in model.parameters():    param.requires_grad = False# Replace the classification layer: 2 classes (dogs and cats)model.fc = nn.Linear(512, 2)criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)for epoch in range(5):    model.train()    for images, labels in train_loader:        optimizer.zero_grad()        outputs = model(images)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this training process, the model is specialized in distinguishing between dogs andcats. It reuses the general visual representations that it previously learned fromImageNet and adapts only the final decision layer. In a more complete setting, it isadvisable to incorporate validation sets, early stopping, and possibly data augmentationin order to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are high-dimensional numerical vectors that compactly represent the content ofan image. To obtain them, the pretrained model is reused by removing the finalclassification layer and retaining only the part that acts as a feature extractor. Inconvolutional architectures such as ResNet, this generally corresponds to theconvolutional trunk and the global pooling operations, up to but excluding the finalfully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport numpy as npclass FeatureExtractor:    def __init__(self, model):        # Remove the last layer (classifier) and keep the convolutional trunk        self.features = nn.Sequential(*list(model.children())[:-1])        self.features.eval()    def extract(self, image):        \"\"\"Extracts the embedding of one or more images.\"\"\"        with torch.no_grad():            embedding = self.features(image)                    # Shape (B, C, 1, 1) in ResNet            embedding = embedding.view(embedding.size(0), -1)   # Flatten to (B, C)        return embedding.numpy()# Usage exampleextractor = FeatureExtractor(model)image = torch.randn(1, 3, 224, 224)  # Example imageembedding = extractor.extract(image)print(f\"Embedding shape: {embedding.shape}\")  # (1, 512) in ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings can be used as input for subsequent tasks such as clustering,visualization, similarity search, or integration into other machine learning models thatoperate on vector representations instead of raw images. For example, these vectors mayserve as input to classical algorithms such as support vector machines, logisticregression, or k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization with PCA and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When embeddings have been obtained for many images, for example for the entire trainingset, they can be projected into two dimensions to visualize how the different classescluster in the feature space. This provides an intuitive understanding of how well themodel separates the categories and whether additional processing or model adaptationmight be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a linear dimensionality reduction technique thatidentifies the directions of maximum variance in the data. It is fast, deterministic, andoffers a first approximation of the structure of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pltfrom sklearn.decomposition import PCA# Assume embeddings has shape (N, 512) and labels has shape (N,)embeddings = ...  # Embedding matrixlabels = ...      # Corresponding labelspca = PCA(n_components=2)embeddings_2d = pca.fit_transform(embeddings)plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")plt.xlabel(\"PC1\")plt.ylabel(\"PC2\")plt.title(\"Embeddings in 2D (PCA)\")plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot allows observation of whether the embeddings form well-separatedclusters according to the class labels or whether there is substantial overlap. If theclasses are not clearly separated, it may be necessary to revise the training strategy,collect more data, or modify the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm is a nonlineardimensionality reduction technique that often provides more visually interpretableclusters than PCA, especially in high-dimensional spaces. However, it is morecomputationally expensive and sensitive to hyperparameters such as perplexity andlearning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNEtsne = TSNE(n_components=2, perplexity=30)embeddings_2d = tsne.fit_transform(embeddings)plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=\"tab10\")plt.title(\"Embeddings in 2D (t-SNE)\")plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is deterministic and efficient, which makes it suitable for quick analyses and largedatasets. t-SNE, by contrast, tends to better preserve local neighborhood relationshipsand reveal fine-grained groupings, at the expense of higher computational cost and somevariability between runs. In practice, it is common to use PCA as a preliminary step toreduce dimensionality before applying t-SNE on a smaller number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct application of embeddings is semantic search: Given a query embedding, the goalis to retrieve the most similar images in a database by comparing their vectorrepresentations using a similarity measure such as cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity between two vectors $v_1$ and $v_2$ is defined as:\\[ \\text{sim}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\lVert v_1 \\rVert \\; \\lVert v_2 \\rVert},\\]and takes values between $-1$ and $1$. A value of $1$ indicates that the vectors areidentical in direction, $0$ indicates orthogonality (absence of directionalrelationship), and $-1$ indicates that the vectors point in opposite directions. In thecontext of image embeddings, similarities are often positive and close to $1$ forsemantically similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npdef cosine_similarity(vec1, vec2):    \"\"\"    Calculates cosine similarity between two vectors.    Result is between -1 and 1.    \"\"\"    dot = np.dot(vec1, vec2)    norm1 = np.linalg.norm(vec1)    norm2 = np.linalg.norm(vec2)    return dot / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Semantic Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on cosine similarity, a search mechanism can be constructed that, given a queryembedding, returns the $k$ most similar images in a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:    def __init__(self, embeddings_db, labels_db):        \"\"\"        embeddings_db: Matrix (N, D) with database embeddings.        labels_db: Vector (N,) with labels or identifiers.        \"\"\"        norms = np.linalg.norm(embeddings_db, axis=1, keepdims=True)        self.embeddings_db = embeddings_db / norms  # Normalize for cosine similarity        self.labels_db = labels_db    def search(self, query_embedding, top_k=5):        \"\"\"Returns the top_k most similar elements to the query.\"\"\"        query_norm = query_embedding / np.linalg.norm(query_embedding)        similarities = np.dot(self.embeddings_db, query_norm)        top_indices = np.argsort(similarities)[::-1][:top_k]        top_similarities = similarities[top_indices]        return top_indices, top_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the search database, embeddings are computed for all images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []all_labels = []for images, labels in train_loader:    embs = extractor.extract(images)    all_embeddings.append(embs)    all_labels.extend(labels.numpy())all_embeddings = np.vstack(all_embeddings)all_labels = np.array(all_labels)searcher = SemanticSearch(all_embeddings, all_labels)# Query with a new imagequery_image = torch.randn(1, 3, 224, 224)  # Example queryquery_emb = extractor.extract(query_image).squeeze()indices, sims = searcher.search(query_emb, top_k=5)for i, (idx, sim) in enumerate(zip(indices, sims), 1):    print(f\"#{i}: Index {idx}, Similarity {sim:.3f}, Class {all_labels[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mechanism constitutes the basis of visual recommendation systems, image search bysimilarity, and interactive tools for exploring visual databases. By adjusting the methodused to compute or index embeddings (for example, using approximate nearest neighborsearch with specialized libraries), the system can scale to large collections of imageswithout prohibitive computational costs at query time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Complete Transfer Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical workflow for transfer learning in image classification can be conceptuallystructured in the following steps: A pretrained model is loaded; a training strategy isselected (feature extraction, partial fine-tuning, or full fine-tuning); the model istrained according to this strategy; an embedding extractor is built; embeddings arecomputed for the dataset; the structure of the feature space is explored viavisualization techniques; and finally, a semantic search system is constructed usingthese embeddings.In code, a simplified pipeline oriented to feature extraction may take the followingform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchimport torch.nn as nnimport torchvision.models as modelsimport numpy as npfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt# STEP 1: Load pretrained model and prepare for feature extractionmodel = models.resnet18(pretrained=True)for param in model.parameters():    param.requires_grad = Falsemodel.fc = nn.Linear(512, 2)  # Example: 2 classes# STEP 2: Train only the classifieroptimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)criterion = nn.CrossEntropyLoss()for epoch in range(5):    model.train()    for images, labels in train_loader:        outputs = model(images)        loss = criterion(outputs, labels)        optimizer.zero_grad()        loss.backward()        optimizer.step()# STEP 3: Create embedding extractorextractor = FeatureExtractor(model)# STEP 4: Compute dataset embeddingsembeddings = []labels_list = []for images, labels in train_loader:    emb = extractor.extract(images)    embeddings.append(emb)    labels_list.extend(labels.numpy())embeddings = np.vstack(embeddings)labels_array = np.array(labels_list)# STEP 5: Visualize embeddings (for example, with PCA)pca = PCA(n_components=2)emb_2d = pca.fit_transform(embeddings)plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels_array, cmap=\"tab10\")plt.xlabel(\"PC1\")plt.ylabel(\"PC2\")plt.title(\"Embeddings in 2D (PCA)\")plt.show()# STEP 6: Create semantic search enginesearcher = SemanticSearch(embeddings, labels_array)# STEP 7: Search for images similar to a query imagequery_img = next(iter(train_loader))[0][0:1]query_emb = extractor.extract(query_img).squeeze()indices, sims = searcher.search(query_emb, top_k=5)for i, (idx, sim) in enumerate(zip(indices, sims), 1):    print(f\"#{i}: Index {idx}, Similarity {sim:.3f}, Class {labels_array[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline exemplifies how a pretrained model is reused to solve a classificationproblem, how embeddings derived from that model can be exploited, and how theseembeddings feed into visualization and semantic search functionalities. The completeprocess illustrates the versatility of transfer learning as a foundational tool in moderncomputer vision, enabling efficient development of high-performance models even whenlabeled data are limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
