{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability in Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability in Convolutional Neural Networks (CNNs) is essential to understand the decisions made by the model, detect potential biases, and improve robustness against perturbations or distributional shifts. Due to their depth and the combination of convolutions with nonlinear activation functions, CNNs behave as highly complex systems whose internal mechanisms are difficult to inspect directly. For this reason, specific techniques are developed to visualize which regions or features of the input signal contribute most significantly to the predictions.\n",
    "\n",
    "This document describes and implements several of the most widely used methodologies for interpreting CNNs: saliency maps, Grad-CAM, Guided Grad-CAM (based on Guided Backpropagation), occlusion analysis, and Integrated Gradients. A complete, functional implementation on CIFAR-10 using an adapted ResNet-18 model is then presented, organized linearly for step-by-step execution and easily convertible into a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saliency maps rely on computing the gradient of the model output with respect to each input pixel. Intuitively, if a small variation in a pixel produces a significant change in the output associated with a specific class, that pixel is considered important for the decision. The absolute value of this gradient is used as a local relevance measure.\n",
    "\n",
    "Given a model $f(\\cdot)$ and an input image $x$, the saliency map for a class $c$ is defined as\n",
    "\n",
    "$$S = \\left| \\frac{\\partial f_c(x)}{\\partial x} \\right|$$\n",
    "\n",
    "When the network processes inputs with multiple channels (for example, RGB images), it is common to aggregate the channel-wise information to construct a two-dimensional map. A simple strategy is to take the maximum over the channel dimension:\n",
    "\n",
    "$$S_{i,j} = \\max_{k} \\left| \\frac{\\partial f_c(x)}{\\partial x_{k,i,j}} \\right|$$\n",
    "\n",
    "This map provides, for each spatial position $(i,j)$, a sensitivity measure of the class score $f_c$ with respect to perturbations of the corresponding pixels. Saliency maps are conceptually simple and computationally efficient; however, the resulting visualizations are often noisy and do not always align clearly with semantically interpretable regions of the image.\n",
    "\n",
    "The following code initializes the environment, defines basic configuration, and implements a class that generates saliency maps based on gradients, together with a visualization function that overlays the resulting map on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interpretability in Convolutional Neural Networks\n",
    "\n",
    "Complete functional implementation with CIFAR-10\n",
    "\n",
    "Implemented techniques:\n",
    "1. Saliency Maps\n",
    "2. Grad-CAM\n",
    "3. Guided Grad-CAM (based on Guided Backpropagation)\n",
    "4. Occlusion Analysis\n",
    "5. Integrated Gradients\n",
    "\"\"\"\n",
    "\n",
    "# Standard libraries\n",
    "# IMPORTS\n",
    "import warnings\n",
    "\n",
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from scipy.ndimage import zoom\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\\n\")\n",
    "\n",
    "# GLOBAL CONFIGURATION\n",
    "CONFIG = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the interpretability techniques, the CIFAR-10 dataset is used. CIFAR-10 contains color images of size $32 \\times 32$ belonging to ten different classes. The following function downloads and prepares the test set, applying a standard normalization that is widely used for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cifar10_data():\n",
    "    \"\"\"\n",
    "    Downloads and prepares the CIFAR-10 test set\n",
    "    with standard normalization.\n",
    "    \"\"\"\n",
    "    print(\"Preparing CIFAR-10...\")\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    print(f\"Test: {len(test_dataset)} images\\n\")\n",
    "    return test_loader, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18 Model Adapted to CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ResNet-18 model pretrained on ImageNet is used as the base and adapted to the characteristics of CIFAR-10. The adaptation consists of modifying the first convolutional layer to work more appropriately with $32 \\times 32$ images and adjusting the final fully connected layer to the number of classes in CIFAR-10. Although the model is loaded with pretrained ImageNet weights, the final layer is initialized randomly, so the performance may not be optimal without fine-tuning. However, this limitation does not affect the main purpose of the code, which is to illustrate interpretability techniques in a functional manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "    \"\"\"\n",
    "    Loads a ResNet-18 pretrained on ImageNet and adapts it to CIFAR-10.\n",
    "    \"\"\"\n",
    "    print(\"Loading pretrained model...\")\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Adapt the first layer to 32x32 images (remove initial max-pooling)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    # Adapt final layer for 10 CIFAR-10 classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "    model = model.to(CONFIG[\"device\"])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded on {CONFIG['device']}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Maps: Implementation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implementation computes saliency maps via gradients and includes a visualization routine that facilitates the direct analysis of which image regions contribute most to the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"1. SALIENCY MAPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"\"\"Saliency maps compute the gradient of the output with respect\n",
    "to each image pixel, indicating which regions have the largest\n",
    "influence on the prediction.\n",
    "\n",
    "Advantages:\n",
    "- Simple and efficient computation.\n",
    "- Shows the direct influence of pixels.\n",
    "\n",
    "Limitations:\n",
    "- Visualizations are often noisy.\n",
    "- Do not always align with semantically clear regions.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class SaliencyMapGenerator:\n",
    "    \"\"\"Generates saliency maps using gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\") -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "\n",
    "    def generate_saliency(self, image: torch.Tensor, target_class: int | None = None):\n",
    "        \"\"\"\n",
    "        Computes the saliency map for a single image.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            target_class: Target class index; if None, the model prediction is used.\n",
    "\n",
    "        Returns:\n",
    "            2D saliency map (numpy array).\n",
    "        \"\"\"\n",
    "        image = image.to(self.device)\n",
    "        image.requires_grad = True\n",
    "\n",
    "        output = self.model(image)\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "\n",
    "        saliency = image.grad.data.abs()\n",
    "        # Channel aggregation: maximum along the channel axis\n",
    "        saliency, _ = torch.max(saliency, dim=1)\n",
    "\n",
    "        return saliency.squeeze().cpu().numpy()\n",
    "\n",
    "    def visualize_saliency(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        original_image: np.ndarray,\n",
    "        target_class: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes the saliency map and its overlay on the original image.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            original_image: Denormalized image [H, W, 3] in [0, 1].\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "        \"\"\"\n",
    "        saliency = self.generate_saliency(image, target_class)\n",
    "\n",
    "        # Normalize to [0, 1] for visualization\n",
    "        saliency = (saliency - saliency.min()) / (\n",
    "            saliency.max() - saliency.min() + 1e-8\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(saliency, cmap=\"hot\")\n",
    "        axes[1].set_title(\"Saliency Map\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(original_image)\n",
    "        axes[2].imshow(saliency, cmap=\"hot\", alpha=0.5)\n",
    "        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM (Gradient-weighted Class Activation Mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM generates heatmaps that localize the regions of an image that contribute most strongly to the prediction for a specific class. Instead of operating directly on the pixels, Grad-CAM works on the activation maps of an internal convolutional layer, which tends to produce spatial relevance maps that are more structured and semantically interpretable.\n",
    "\n",
    "Let $A^k \\in \\mathbb{R}^{H \\times W}$ denote the activation map associated with channel $k$ of a selected convolutional layer. For a class $c$, importance coefficients are computed by performing a global average pooling of the gradients over the spatial dimensions:\n",
    "\n",
    "$$\\alpha_k = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\frac{\\partial f_c}{\\partial A_{ij}^k}$$\n",
    "\n",
    "Using these coefficients, a class-specific weighted activation map is constructed as\n",
    "\n",
    "$$L_c^{\\text{Grad-CAM}} = \\mathrm{ReLU}\\left( \\sum_k \\alpha_k A^k \\right)$$\n",
    "\n",
    "The ReLU function is applied to retain only positive contributions, under the assumption that activations that increase the class score are those to be highlighted. The spatial resolution of the Grad-CAM map is limited by the size of the activation maps of the chosen layer; therefore, the resulting map is often interpolated to match the size of the original image.\n",
    "\n",
    "The implementation below uses hooks to capture activations and gradients at the target layer and generates the corresponding Grad-CAM map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"2. GRAD-CAM (Gradient-weighted Class Activation Mapping)\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"\"\"Grad-CAM generates heatmaps that highlight the regions of the image\n",
    "that are most important for a specific class, using gradients with\n",
    "respect to an internal convolutional layer.\n",
    "\n",
    "Advantages:\n",
    "- More interpretable maps than basic saliency maps.\n",
    "- Localizes relevant object regions.\n",
    "\n",
    "Limitations:\n",
    "- Depends on the choice of the target layer.\n",
    "- Resolution is limited by the resolution of that layer.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM implementation for a target layer of a CNN.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: nn.Module, target_layer: str, device: str = \"cuda\"\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.target_layer = target_layer\n",
    "        self.device = device\n",
    "        self.gradients: torch.Tensor | None = None\n",
    "        self.activations: torch.Tensor | None = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self) -> None:\n",
    "        \"\"\"\n",
    "        Registers hooks on the target layer to capture activations and gradients\n",
    "        during forward and backward passes.\n",
    "        \"\"\"\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == self.target_layer:\n",
    "                module.register_forward_hook(forward_hook)\n",
    "                module.register_full_backward_hook(backward_hook)\n",
    "                break\n",
    "\n",
    "    def generate_cam(self, image: torch.Tensor, target_class: int | None = None):\n",
    "        \"\"\"\n",
    "        Generates the Grad-CAM map for an image and a target class.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "\n",
    "        Returns:\n",
    "            cam: Normalized 2D Grad-CAM map (numpy array).\n",
    "            target_class: Class used for the explanation.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        image = image.to(self.device)\n",
    "\n",
    "        output = self.model(image)\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "\n",
    "        # Weights: global average of gradients over H x W\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "        return cam, target_class\n",
    "\n",
    "    def visualize_cam(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        original_image: np.ndarray,\n",
    "        target_class: int | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Visualizes the Grad-CAM map and its overlay on the original image.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            original_image: Denormalized image [H, W, 3] in [0, 1].\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "        \"\"\"\n",
    "        cam, pred_class = self.generate_cam(image, target_class)\n",
    "\n",
    "        # Resize the map to the original image size via interpolation\n",
    "        cam_resized = zoom(\n",
    "            cam,\n",
    "            (\n",
    "                original_image.shape[0] / cam.shape[0],\n",
    "                original_image.shape[1] / cam.shape[1],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(cam_resized, cmap=\"jet\")\n",
    "        axes[1].set_title(\n",
    "            f\"Grad-CAM (Class: {CIFAR10_CLASSES[pred_class]})\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(original_image)\n",
    "        axes[2].imshow(cam_resized, cmap=\"jet\", alpha=0.5)\n",
    "        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return cam_resized, pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backpropagation and Guided Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guided Backpropagation modifies the gradient flow through ReLU units by forcing to zero those gradients that are negative both in the activation and in the incoming gradient. This filtering yields sharper gradient maps that focus on features considered relevant.\n",
    "\n",
    "Guided Grad-CAM combines the global localization capability of Grad-CAM with the pixel-level detail of Guided Backpropagation. The usual procedure consists of three sequential steps: first, a Grad-CAM map is computed for the target class; second, guided gradients with respect to the input image are obtained; finally, the Grad-CAM map is upsampled to the input resolution and multiplied elementwise by the guided gradients. The result is a high-resolution visualization in which edges and fine details inside the Grad-CAM-relevant regions are emphasized.\n",
    "\n",
    "The following code implements Guided Backpropagation. This implementation integrates naturally with the `GradCAM` class to build Guided Grad-CAM by multiplying the resized Grad-CAM map by the guided gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"3. GUIDED GRAD-CAM\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"\"\"Guided Grad-CAM combines Grad-CAM with Guided Backpropagation\n",
    "to obtain high-resolution visualizations that are both\n",
    "spatially precise and detailed at the pixel level.\n",
    "\n",
    "This script implements Guided Backpropagation,\n",
    "which can be combined with Grad-CAM maps.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class GuidedBackprop:\n",
    "    \"\"\"Guided Backpropagation implementation for a CNN.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\") -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self) -> None:\n",
    "        \"\"\"\n",
    "        Registers hooks on ReLU layers to filter negative gradients\n",
    "        during the backward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            if len(grad_input) > 0 and grad_input[0] is not None:\n",
    "                return (torch.clamp(grad_input[0], min=0.0),)\n",
    "            return grad_input\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_gradients(self, image: torch.Tensor, target_class: int | None = None):\n",
    "        \"\"\"\n",
    "        Generates guided gradients with respect to the input image.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "\n",
    "        Returns:\n",
    "            Guided gradients as a numpy array [3, H, W].\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        image = image.to(self.device)\n",
    "        image.requires_grad = True\n",
    "\n",
    "        output = self.model(image)\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "\n",
    "        gradients = image.grad.data.cpu().numpy()[0]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occlusion analysis adopts a complementary viewpoint to gradient-based methods. Instead of exploring the internal sensitivity of the model, it modifies the input explicitly. Small regions (patches) of the image are systematically occluded, and the effect on the probability assigned to a given class is measured. When occluding a region significantly decreases the probability, that region is interpreted as important for the prediction.\n",
    "\n",
    "Formally, for each position $(i,j)$ of a sliding window, an occluded version of the image $x^{(i,j)}$ is constructed, and the difference\n",
    "\n",
    "$$\\Delta p_c^{(i,j)} = p_c(x) - p_c\\bigl(x^{(i,j)}\\bigr)$$\n",
    "\n",
    "is evaluated, where $p_c(x)$ denotes the model probability assigned to class $c$. The resulting sensitivity map directly quantifies the importance of each region in terms of its impact on the model's confidence. This technique is independent of gradients and specific architectural details, although its computational cost increases with image resolution, due to the large number of model evaluations required.\n",
    "\n",
    "The class below implements a simple occlusion analysis, allowing the patch size and stride of the sliding window to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"4. OCCLUSION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"\"\"Systematically occludes regions of the image to observe\n",
    "how the prediction changes, revealing which areas are critical.\n",
    "\n",
    "Advantages:\n",
    "- Direct interpretation at the input level.\n",
    "- Does not require gradients or internal access to the architecture.\n",
    "\n",
    "Limitations:\n",
    "- High computational cost.\n",
    "- Sensitive to patch size and stride.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class OcclusionAnalysis:\n",
    "    \"\"\"Occlusion analysis for obtaining sensitivity maps.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\") -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "\n",
    "    def analyze(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        target_class: int | None = None,\n",
    "        patch_size: int = 4,\n",
    "        stride: int = 2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes a sensitivity map via systematic occlusion.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "            patch_size: Side length of the square occlusion patch in pixels.\n",
    "            stride: Stride of the sliding occlusion window.\n",
    "\n",
    "        Returns:\n",
    "            2D sensitivity map (numpy array).\n",
    "        \"\"\"\n",
    "        image = image.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image)\n",
    "            if target_class is None:\n",
    "                target_class = output.argmax(dim=1).item()\n",
    "            baseline_prob = torch.softmax(output, dim=1)[0, target_class].item()\n",
    "\n",
    "        _, _, h, w = image.shape\n",
    "        sensitivity_map = np.zeros((h, w))\n",
    "\n",
    "        for i in range(0, h - patch_size + 1, stride):\n",
    "            for j in range(0, w - patch_size + 1, stride):\n",
    "                occluded_image = image.clone()\n",
    "                occluded_image[:, :, i : i + patch_size, j : j + patch_size] = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(occluded_image)\n",
    "                    prob = torch.softmax(output, dim=1)[0, target_class].item()\n",
    "\n",
    "                sensitivity = baseline_prob - prob\n",
    "                current = sensitivity_map[i : i + patch_size, j : j + patch_size].mean()\n",
    "                sensitivity_map[i : i + patch_size, j : j + patch_size] = max(\n",
    "                    current, sensitivity\n",
    "                )\n",
    "\n",
    "        return sensitivity_map\n",
    "\n",
    "    def visualize(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        original_image: np.ndarray,\n",
    "        target_class: int | None = None,\n",
    "        patch_size: int = 4,\n",
    "        stride: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes the sensitivity map obtained via occlusion.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            original_image: Denormalized image [H, W, 3] in [0, 1].\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "            patch_size: Occlusion patch size.\n",
    "            stride: Sliding window stride.\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing with patch_size={patch_size}, stride={stride}...\")\n",
    "        sensitivity = self.analyze(image, target_class, patch_size, stride)\n",
    "        sensitivity = (sensitivity - sensitivity.min()) / (\n",
    "            sensitivity.max() - sensitivity.min() + 1e-8\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(sensitivity, cmap=\"hot\")\n",
    "        axes[1].set_title(\"Sensitivity Map\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(original_image)\n",
    "        axes[2].imshow(sensitivity, cmap=\"hot\", alpha=0.5)\n",
    "        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients is a theoretically grounded method to attribute a model prediction to input features. Instead of considering the gradient only at the point $x$, this method integrates gradients along a continuous path that connects a baseline $x'$ (for example, a completely black image) to the actual image $x$. This approach mitigates gradient saturation issues and satisfies desirable attribution axioms such as sensitivity and implementation invariance.\n",
    "\n",
    "Let $f_c$ denote the score for class $c$ (for example, the pre-softmax output). Integrated Gradients for dimension $i$ is defined as\n",
    "\n",
    "$$\\mathrm{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1}\\frac{\\partial f_c\\bigl(x' + \\alpha (x - x')\\bigr)}{\\partial x_i} \\, d\\alpha$$\n",
    "\n",
    "In practice, the integral is approximated by a discrete sum over $m$ uniformly spaced steps:\n",
    "\n",
    "$$\\mathrm{IG}_i(x) \\approx (x_i - x'_i) \\cdot \\frac{1}{m} \\sum_{k=1}^{m}\\frac{\\partial f_c\\bigl(x' + \\tfrac{k}{m}(x - x')\\bigr)}{\\partial x_i}$$\n",
    "\n",
    "Aggregating the absolute attributions over channels yields a spatial relevance map that is typically smoother and more stable than basic saliency maps, at the cost of requiring multiple model evaluations along the path between the baseline and the original image.\n",
    "\n",
    "The following implementation computes Integrated Gradients for a single image, allowing the baseline and the number of integration steps to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"5. INTEGRATED GRADIENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"\"\"Method that attributes the prediction to input features by\n",
    "integrating gradients along a path from a baseline\n",
    "to the actual image.\n",
    "\n",
    "Advantages:\n",
    "- Strong theoretical foundation.\n",
    "- Mitigates gradient saturation issues.\n",
    "\n",
    "Limitations:\n",
    "- Requires multiple model evaluations.\n",
    "- Depends on the choice of baseline.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class IntegratedGradients:\n",
    "    \"\"\"Integrated Gradients implementation for PyTorch models.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\") -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        target_class: int | None = None,\n",
    "        baseline: torch.Tensor | None = None,\n",
    "        steps: int = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes Integrated Gradients for an image and target class.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "            baseline: Tensor [1, 3, H, W] used as reference. If None, a zero tensor is used.\n",
    "            steps: Number of points along the integration path.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array [C, H, W] with per-channel attributions.\n",
    "        \"\"\"\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(image)\n",
    "\n",
    "        baseline = baseline.to(self.device)\n",
    "        image = image.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image)\n",
    "            if target_class is None:\n",
    "                target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Linear path between baseline and image\n",
    "        scaled_inputs = [\n",
    "            baseline + (float(i) / steps) * (image - baseline) for i in range(steps + 1)\n",
    "        ]\n",
    "        scaled_inputs = torch.cat(scaled_inputs, dim=0)\n",
    "        scaled_inputs.requires_grad = True\n",
    "\n",
    "        output = self.model(scaled_inputs)\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        target_output = output[:, target_class]\n",
    "        target_output.backward(torch.ones_like(target_output))\n",
    "\n",
    "        gradients = scaled_inputs.grad\n",
    "        avg_gradients = torch.mean(gradients, dim=0, keepdim=True)\n",
    "        integrated_grads = (image - baseline) * avg_gradients\n",
    "\n",
    "        return integrated_grads.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    def visualize(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        original_image: np.ndarray,\n",
    "        target_class: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes spatially aggregated Integrated Gradients and its overlay.\n",
    "\n",
    "        Args:\n",
    "            image: Tensor [1, 3, H, W] normalized.\n",
    "            original_image: Denormalized image [H, W, 3] in [0, 1].\n",
    "            target_class: Target class; if None, the model prediction is used.\n",
    "        \"\"\"\n",
    "        print(\"Computing Integrated Gradients (50 steps)...\")\n",
    "        ig = self.generate(image, target_class)\n",
    "\n",
    "        ig_aggregated = np.sum(np.abs(ig), axis=0)\n",
    "        ig_aggregated = (ig_aggregated - ig_aggregated.min()) / (\n",
    "            ig_aggregated.max() - ig_aggregated.min() + 1e-8\n",
    "        )\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(ig_aggregated, cmap=\"hot\")\n",
    "        axes[1].set_title(\"Integrated Gradients\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(original_image)\n",
    "        axes[2].imshow(ig_aggregated, cmap=\"hot\", alpha=0.5)\n",
    "        axes[2].set_title(\"Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the results properly, CIFAR-10 images should be denormalized before visualization. The function below reverses the standard normalization applied during preprocessing and returns an image in a format suitable for `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_cifar10(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Denormalizes a CIFAR-10 tensor for visualization.\n",
    "\n",
    "    Args:\n",
    "        tensor: Tensor [3, H, W] normalized with CIFAR-10 mean and std.\n",
    "\n",
    "    Returns:\n",
    "        Image as numpy array [H, W, 3] with values in [0, 1].\n",
    "    \"\"\"\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n",
    "    denorm = tensor * std + mean\n",
    "    denorm = torch.clamp(denorm, 0, 1)\n",
    "    return denorm.permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Interpretability Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all components are integrated into a coherent workflow that applies the different interpretability techniques to a test image from CIFAR-10. The pipeline includes data loading, model loading, sample selection, and the sequential execution of saliency maps, Grad-CAM, occlusion analysis, and Integrated Gradients. Guided Backpropagation is implemented and can be used to construct Guided Grad-CAM if one wishes to extend the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline() -> None:\n",
    "    \"\"\"\n",
    "    Executes all interpretability techniques in an integrated way\n",
    "    on a single CIFAR-10 image.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETE PIPELINE: INTERPRETABILITY IN CNNs\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    # Data\n",
    "    test_loader, _ = prepare_cifar10_data()\n",
    "\n",
    "    # Model\n",
    "    model = load_pretrained_model()\n",
    "\n",
    "    # Select a test image\n",
    "    print(\"Selecting test image...\")\n",
    "    images, labels = next(iter(test_loader))\n",
    "    image = images[0:1]\n",
    "    label = labels[0].item()\n",
    "    original_image = denormalize_cifar10(images[0].clone())\n",
    "    print(f\"True class: {CIFAR10_CLASSES[label]}\\n\")\n",
    "\n",
    "    # 1. Saliency Maps\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RUNNING: Saliency Maps\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    saliency_gen = SaliencyMapGenerator(model, CONFIG[\"device\"])\n",
    "    saliency_gen.visualize_saliency(image.clone(), original_image)\n",
    "\n",
    "    # 2. Grad-CAM\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RUNNING: Grad-CAM\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    grad_cam = GradCAM(model, target_layer=\"layer4\", device=CONFIG[\"device\"])\n",
    "    grad_cam.visualize_cam(image.clone(), original_image)\n",
    "\n",
    "    # 4. Occlusion Analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RUNNING: Occlusion Analysis\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    occlusion = OcclusionAnalysis(model, device=CONFIG[\"device\"])\n",
    "    occlusion.visualize(image.clone(), original_image, patch_size=4, stride=2)\n",
    "\n",
    "    # 5. Integrated Gradients\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RUNNING: Integrated Gradients\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    ig = IntegratedGradients(model, device=CONFIG[\"device\"])\n",
    "    ig.visualize(image.clone(), original_image)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This complete pipeline provides a practical framework for exploring interpretability in CNNs on CIFAR-10. Although the ResNet-18 model is not explicitly fine-tuned on this dataset within the script, the code structure allows the same analysis workflow to be reused with a model trained specifically on CIFAR-10 by simply replacing the model loading function with a version that retrieves weights adapted to the domain of interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
