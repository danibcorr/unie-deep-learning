{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8887d3",
   "metadata": {},
   "source": [
    "# Transformers in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb1e6f",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a26fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "\n",
    "# 3pps\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Patches(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size_height: int,\n",
    "        patch_size_width: int,\n",
    "        img_height: int,\n",
    "        img_width: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize patch extraction module.\n",
    "\n",
    "        Args:\n",
    "            patch_size_height: Height of each patch.\n",
    "            patch_size_width: Width of each patch.\n",
    "            img_height: Height of the input image.\n",
    "            img_width: Width of the input image.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If img_height not divisible by patch height.\n",
    "            ValueError: If img_width not divisible by patch width.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if img_height % patch_size_height != 0:\n",
    "            raise ValueError(\n",
    "                \"img_height tiene que se divisible entre el patch_size_height\"\n",
    "            )\n",
    "\n",
    "        if img_width % patch_size_width != 0:\n",
    "            raise ValueError(\n",
    "                \"img_width tiene que se divisible entre el patch_size_width\"\n",
    "            )\n",
    "\n",
    "        self.patch_size_height = patch_size_height\n",
    "        self.patch_size_width = patch_size_width\n",
    "        self.unfold = nn.Unfold(\n",
    "            kernel_size=(self.patch_size_height, self.patch_size_width),\n",
    "            stride=(self.patch_size_height, self.patch_size_width),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract patches from input tensor.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Batch of images as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with patches from input images.\n",
    "        \"\"\"\n",
    "\n",
    "        # unfold devuelve (b, c * patch_height * patch_width, num_patches)\n",
    "        patches = self.unfold(input_tensor)\n",
    "        # Necesitamos (B, NUM_PATCHES, C * patch_size_height * patch_size_width)\n",
    "        return patches.transpose(2, 1)\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size_height: int,\n",
    "        patch_size_width: int,\n",
    "        in_channels: int,\n",
    "        d_model: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize patch embedding module.\n",
    "\n",
    "        Args:\n",
    "            patch_size_height: Height of each patch.\n",
    "            patch_size_width: Width of each patch.\n",
    "            in_channels: Number of input channels.\n",
    "            d_model: Dimension of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.patch_size_height = patch_size_height\n",
    "        self.patch_size_width = patch_size_width\n",
    "        self.in_channels = in_channels\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Esta es una de las diferencias con usar transformers en el texto\n",
    "        # Aquí usamos FFN en vez de Embedding layer, es una proyección\n",
    "        # de los pixeles\n",
    "        self.embedding = nn.Linear(\n",
    "            in_features=self.in_channels\n",
    "            * self.patch_size_height\n",
    "            * self.patch_size_width,\n",
    "            out_features=self.d_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply linear projection to input tensor.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Batch of image patches as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor after linear projection of patches.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.embedding(input_tensor)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_length: int, dropout_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize positional encoding module.\n",
    "\n",
    "        Args:\n",
    "            d_model: Dimension of the model.\n",
    "            sequence_length: Max length of input sequences.\n",
    "            dropout_rate: Dropout rate applied on outputs.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Cuando le damos una secuencia de tokens, tenemos que saber\n",
    "        # la longitud máxima de la secuencia\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Creamos una matriz del positional embedding\n",
    "        # (sequence_length, d_model)\n",
    "        pe_matrix = torch.zeros(size=(self.sequence_length, self.d_model))\n",
    "\n",
    "        # Crear vector de posiciones\n",
    "        position = torch.arange(0, self.sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Crear vector de divisores\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Aplicar sin y cos\n",
    "        pe_matrix[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe_matrix[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Tenemos que convertirlo a (1, sequence_length, d_model) para\n",
    "        # procesarlo por lotes\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)\n",
    "\n",
    "        # Esta matriz no se aprende, es fija, la tenemos que guardar con el modelo\n",
    "        self.register_buffer(name=\"pe_matrix\", tensor=pe_matrix)\n",
    "\n",
    "    def forward(self, input_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_embedding: Batch of input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings with added positional encoding.\n",
    "        \"\"\"\n",
    "\n",
    "        # (B, ..., d_model) -> (B, sequence_length, d_model)\n",
    "        # Seleccionamos\n",
    "        x = input_embedding + (\n",
    "            self.pe_matrix[:, : input_embedding.shape[1], :]  # type: ignore\n",
    "        ).requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6) -> None:\n",
    "        \"\"\"\n",
    "        Initialize layer normalization module.\n",
    "\n",
    "        Args:\n",
    "            features: Number of features in input.\n",
    "            eps: Small value to avoid division by zero.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.features = features\n",
    "        self.eps = eps\n",
    "\n",
    "        # Utilizamos un factor alpha para multiplicar el valor de la normalización\n",
    "        self.alpha = nn.Parameter(torch.ones(self.features))\n",
    "        # Utilizamos un factor del sesgo para sumar\n",
    "        self.bias = nn.Parameter(torch.zeros(self.features))\n",
    "\n",
    "    def forward(self, input_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply layer normalization to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_embedding: Batch of input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Normalized embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        # (B, sequence_length, d_model)\n",
    "        mean = torch.mean(input=input_embedding, dim=-1, keepdim=True)\n",
    "        var = torch.var(input=input_embedding, dim=-1, keepdim=True, unbiased=False)\n",
    "        return (\n",
    "            self.alpha * ((input_embedding - mean) / (torch.sqrt(var + self.eps)))\n",
    "            + self.bias\n",
    "        )\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize feed-forward neural network.\n",
    "\n",
    "        Args:\n",
    "            d_model: Input and output feature dimensions.\n",
    "            d_ff: Hidden layer feature dimensions.\n",
    "            dropout_rate: Dropout rate applied on layers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Creamos el modelo secuencial\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_model, out_features=self.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features=self.d_ff, out_features=self.d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process input tensor through feed-forward layers.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Batch of input tensors.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after feed-forward processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # (B, sequence_length, d_model)\n",
    "        return self.ffn(input_tensor)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention module.\n",
    "\n",
    "        Args:\n",
    "            d_model: Number of features in input.\n",
    "            h: Number of attention heads.\n",
    "            dropout_rate: Dropout rate applied on scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # el tamalo de los embeddings debe ser proporcional al número de cabezas\n",
    "        # para realizar la división, por lo que es el resto ha de ser 0\n",
    "        if d_model % h != 0:\n",
    "            raise ValueError(\"d_model ha de ser divisible entre h\")\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Valore establecidos en el paper\n",
    "        self.d_k = self.d_model // self.h\n",
    "        self.d_v = self.d_model // self.h\n",
    "\n",
    "        # Parámetros\n",
    "        self.W_K = nn.Linear(\n",
    "            in_features=self.d_model, out_features=self.d_model, bias=False\n",
    "        )\n",
    "        self.W_Q = nn.Linear(\n",
    "            in_features=self.d_model, out_features=self.d_model, bias=False\n",
    "        )\n",
    "        self.W_V = nn.Linear(\n",
    "            in_features=self.d_model, out_features=self.d_model, bias=False\n",
    "        )\n",
    "        self.W_OUTPUT_CONCAT = nn.Linear(\n",
    "            in_features=self.d_model, out_features=self.d_model, bias=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(\n",
    "        k: torch.Tensor,\n",
    "        q: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "        dropout: nn.Dropout | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute attention scores and output.\n",
    "\n",
    "        Args:\n",
    "            k: Key tensor.\n",
    "            q: Query tensor.\n",
    "            v: Value tensor.\n",
    "            mask: Mask tensor, optional.\n",
    "            dropout: Dropout layer, optional.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of attention output and scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Primero realizamos el producto matricial con la transpuesta\n",
    "        # q = (Batch, h, seq_len, d_k)\n",
    "        # k.T = (Batch, h, d_k, seq_len)\n",
    "        # matmul_q_k = (Batch, h, seq_len, seq_len)\n",
    "        matmul_q_k = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Luego realizamos el escalado\n",
    "        d_k = k.shape[-1]\n",
    "        matmul_q_k_scaled = matmul_q_k / math.sqrt(d_k)\n",
    "\n",
    "        # El enmascarado es para el decoder, relleno de infinitos\n",
    "        if mask is not None:\n",
    "            matmul_q_k_scaled.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        # Obtenemos los scores/puntuación de la atención\n",
    "        attention_scores = F.softmax(input=matmul_q_k_scaled, dim=-1)\n",
    "\n",
    "        # Aplicamos dropout\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # Multiplicamos por el valor\n",
    "        # attention_scores = (Batch, h, seq_len, seq_len)\n",
    "        # v = (Batch, h, seq_len, d_k)\n",
    "        # Output = (Batch, h, seq_len, d_k)\n",
    "        return (attention_scores @ v), attention_scores\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        k: torch.Tensor,\n",
    "        q: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process input tensors through multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            k: Key tensor.\n",
    "            q: Query tensor.\n",
    "            v: Value tensor.\n",
    "            mask: Mask tensor, optional.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after attention processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # k -> (Batch, seq_len, d_model) igual para el resto\n",
    "        key_prima = self.W_K(k)\n",
    "        query_prima = self.W_Q(q)\n",
    "        value_prima = self.W_V(v)\n",
    "\n",
    "        # Cambiamos las dimensiones y hacemos el split de los embedding para cada head\n",
    "        # Pasando de (Batch, seq_len, d_model) a (Batch, seq_len, h, d_k)\n",
    "        # Para luego pasar de (Batch, seq_len, h, d_k) a (Batch, h, seq_len, d_k)\n",
    "        key_prima = key_prima.view(\n",
    "            key_prima.shape[0], key_prima.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "        query_prima = query_prima.view(\n",
    "            query_prima.shape[0], query_prima.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "        value_prima = value_prima.view(\n",
    "            value_prima.shape[0], value_prima.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # Obtenemos la matriz de atencion y la puntuación\n",
    "        # attention = (Batch, h, seq_len, d_k)\n",
    "        # attention_scores = (Batch, h, seq_len, seq_len)\n",
    "        attention, attention_scores = MultiHeadAttention.attention(\n",
    "            k=key_prima,\n",
    "            q=query_prima,\n",
    "            v=value_prima,\n",
    "            mask=mask,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        # Tenemos que concatenar la información de todas las cabezas\n",
    "        # Queremos (Batch, seq_len, d_model)\n",
    "        # self.d_k = self.d_model // self.h; d_model = d_k * h\n",
    "        attention = attention.transpose(1, 2)  # (Batch, seq_len, h, d_k)\n",
    "        b, seq_len, h, d_k = attention.size()\n",
    "        # Al parecer, contiguous permite evitar errores de memoria\n",
    "        attention_concat = attention.contiguous().view(\n",
    "            b, seq_len, h * d_k\n",
    "        )  # (Batch, seq_len, h * d_k) = (Batch, seq_len, d_model)\n",
    "\n",
    "        return self.W_OUTPUT_CONCAT(attention_concat)\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize residual connection module.\n",
    "\n",
    "        Args:\n",
    "            features: Number of features in input.\n",
    "            dropout_rate: Dropout rate for sublayer output.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = LayerNormalization(features=features)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply residual connection to sublayer output.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Original input tensor.\n",
    "            sublayer: Sublayer module to apply.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with residual connection applied.\n",
    "        \"\"\"\n",
    "\n",
    "        return input_tensor + self.dropout(sublayer(self.layer_norm(input_tensor)))\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, h: int, dropout_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize encoder block module.\n",
    "\n",
    "        Args:\n",
    "            d_model: Number of features in input.\n",
    "            d_ff: Hidden layer feature dimensions.\n",
    "            h: Number of attention heads.\n",
    "            dropout_rate: Dropout rate for layers.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Parametros\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.h = h\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Definicion de las capas\n",
    "        self.multi_head_attention_layer = MultiHeadAttention(\n",
    "            d_model=self.d_model, h=self.h, dropout_rate=self.dropout_rate\n",
    "        )\n",
    "        self.residual_layer_1 = ResidualConnection(\n",
    "            features=d_model, dropout_rate=self.dropout_rate\n",
    "        )\n",
    "        self.feed_forward_layer = FeedForward(\n",
    "            d_model=self.d_model, d_ff=self.d_ff, dropout_rate=self.dropout_rate\n",
    "        )\n",
    "        self.residual_layer_2 = ResidualConnection(\n",
    "            features=d_model, dropout_rate=self.dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process input tensor through encoder block.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Batch of input tensors.\n",
    "            mask: Mask tensor, optional.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after encoder block processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Utilizamos self-attention, por lo que k, q, v son del mismo vector de entrada\n",
    "        input_tensor = self.residual_layer_1(\n",
    "            input_tensor,\n",
    "            lambda x: self.multi_head_attention_layer(k=x, q=x, v=x, mask=mask),\n",
    "        )\n",
    "\n",
    "        # Segunda conexión residual con feed-forward\n",
    "        input_tensor = self.residual_layer_2(\n",
    "            input_tensor, lambda x: self.feed_forward_layer(x)\n",
    "        )\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size_height: int,\n",
    "        patch_size_width: int,\n",
    "        img_height: int,\n",
    "        img_width: int,\n",
    "        in_channels: int,\n",
    "        num_encoders: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        h: int,\n",
    "        num_classes: int,\n",
    "        dropout_rate: float,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Vision Transformer (VIT).\n",
    "\n",
    "        Args:\n",
    "            patch_size_height: Height of each patch.\n",
    "            patch_size_width: Width of each patch.\n",
    "            img_height: Height of input images.\n",
    "            img_width: Width of input images.\n",
    "            in_channels: Number of input channels.\n",
    "            num_encoders: Number of encoder blocks.\n",
    "            d_model: Dimension of the model.\n",
    "            d_ff: Dimension of feed-forward layers.\n",
    "            h: Number of attention heads.\n",
    "            num_classes: Number of output classes.\n",
    "            dropout_rate: Dropout rate for layers.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size_height = patch_size_height\n",
    "        self.patch_size_width = patch_size_width\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.in_channels = in_channels\n",
    "        self.num_encoders = num_encoders\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.h = h\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Número de patches\n",
    "        self.num_patches = (img_height // patch_size_height) * (\n",
    "            img_width // patch_size_width\n",
    "        )\n",
    "\n",
    "        # CLS token permite tener una representación global de todos los inputs\n",
    "        # de la imagen (de los diferentes embeddings de cada patch)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n",
    "\n",
    "        self.patch_layer = Patches(\n",
    "            patch_size_height=self.patch_size_height,\n",
    "            patch_size_width=self.patch_size_width,\n",
    "            img_height=self.img_height,\n",
    "            img_width=self.img_width,\n",
    "        )\n",
    "\n",
    "        self.embeddings = PatchEmbedding(\n",
    "            patch_size_height=self.patch_size_height,\n",
    "            patch_size_width=self.patch_size_width,\n",
    "            in_channels=self.in_channels,\n",
    "            d_model=self.d_model,\n",
    "        )\n",
    "\n",
    "        # Entiendo que la longitud de la secuencia coincide con el numero de patches\n",
    "        # y un embedding más de la clase,\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model=self.d_model,\n",
    "            sequence_length=self.num_patches + 1,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Capas del Encoder\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    d_model=self.d_model,\n",
    "                    d_ff=self.d_ff,\n",
    "                    h=self.h,\n",
    "                    dropout_rate=self.dropout_rate,\n",
    "                )\n",
    "                for _ in range(self.num_encoders)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNormalization(features=self.d_model)\n",
    "\n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_model, out_features=self.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(in_features=self.d_model, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process input tensor through VIT model.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Batch of input images.\n",
    "\n",
    "        Returns:\n",
    "            Classification output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extraemos los patches\n",
    "        input_patches = self.patch_layer(input_tensor)\n",
    "\n",
    "        # Convertimso a embeddings los patches\n",
    "        patch_embeddings = self.embeddings(input_patches)\n",
    "\n",
    "        # Tenemos que añadir el token de la clase al inicio de la secuencia\n",
    "        # (B, 1, d_model)\n",
    "        cls_tokens = self.cls_token.expand(input_tensor.shape[0], -1, -1)\n",
    "        # (B, num_patches+1, d_model)\n",
    "        embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)\n",
    "\n",
    "        # Añadir positional encoding\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "\n",
    "        # Encoders del transformer\n",
    "        encoder_output = embeddings\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output = encoder_layer(encoder_output)\n",
    "\n",
    "        # Usar solo el CLS token para clasificación\n",
    "        encoder_output = self.layer_norm(encoder_output)\n",
    "        cls_output = encoder_output[:, 0]\n",
    "\n",
    "        # Clasificación final\n",
    "        return self.mlp_classifier(cls_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = VisionTransformer(\n",
    "        patch_size_height=16,\n",
    "        patch_size_width=16,\n",
    "        img_height=224,\n",
    "        img_width=224,\n",
    "        in_channels=3,\n",
    "        num_encoders=12,\n",
    "        d_model=768,\n",
    "        d_ff=3072,\n",
    "        h=12,\n",
    "        num_classes=1000,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "\n",
    "    x = torch.randn(2, 3, 224, 224)\n",
    "    output = model(x)\n",
    "\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
