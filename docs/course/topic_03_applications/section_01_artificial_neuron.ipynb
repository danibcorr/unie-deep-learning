{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neuron and Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before addressing Deep Learning, it is essential to understand intelligence as the\n",
    "ability to process information and make goal-oriented decisions. This perspective serves\n",
    "as the foundation for **Artificial Intelligence (AI)**, understood as the development of\n",
    "computational systems capable of emulating aspects of human behavior: learning from\n",
    "experience, adapting to changes in the environment, and solving problems with minimal\n",
    "human intervention.\n",
    "\n",
    "Within AI, **Machine Learning** focuses on designing algorithms that learn from data.\n",
    "Instead of explicitly defining decision rules, an **objective function** is specified\n",
    "that quantifies model performance, and its parameters are optimized from labeled or\n",
    "unlabeled examples. This approach, often described as _software 2.0_, largely replaces\n",
    "manual programming with learning from data.\n",
    "\n",
    "**Deep Learning** constitutes a specialization of machine learning based on **deep neural\n",
    "networks**, capable of learning hierarchical representations of information and modeling\n",
    "highly nonlinear relationships. Thanks to these properties, deep learning has achieved\n",
    "outstanding results in computer vision, natural language processing, audio analysis, and,\n",
    "in general, in the treatment of unstructured or high-dimensional data.\n",
    "\n",
    "A key aspect in the recent advancement of Deep Learning is the so-called **scaling\n",
    "laws**, which show how model performance improves systematically by increasing data\n",
    "volume, computational capacity, and the number of parameters. This phenomenon has enabled\n",
    "training large-scale models, such as **large language models (LLM)**, which exhibit\n",
    "emergent capabilities of reasoning, transfer, and generalization beyond direct training\n",
    "data. In parallel, **computational efficiency** is actively researched through lighter\n",
    "architectures, specialized hardware (GPU, TPU), and low-level numerical optimizations.\n",
    "\n",
    "Neural networks store knowledge in the form of **implicit memory** in their parameters\n",
    "(weights and biases). This poses important challenges related to **generalization**\n",
    "capacity, particularly the difference between behavior on data from the same distribution\n",
    "as training (_in-distribution_) and data outside that distribution\n",
    "(_out-of-distribution_). Likewise, in continuous learning contexts, problems such as\n",
    "**catastrophic forgetting** appear, where the model loses performance on previously\n",
    "learned tasks when incorporating new information. These issues have driven the\n",
    "development of **foundation models**, trained generally on large data corpora and\n",
    "subsequently adapted to specific tasks through _fine-tuning_ or _prompting_ techniques.\n",
    "\n",
    "From a formal perspective, learning is modeled as an **optimization problem**: A **loss\n",
    "function** is defined that measures prediction error, and the parameters that minimize an\n",
    "aggregated cost function are sought. For this, gradient-based algorithms are used,\n",
    "supported by **automatic differentiation**, which allows efficiently calculating\n",
    "derivatives in neural networks with millions or billions of parameters. In this context,\n",
    "data is transformed into continuous representations through **embeddings**, vectors in\n",
    "high-dimensional spaces that capture semantic or structural relationships between\n",
    "represented entities (words, images, users, products, etc.).\n",
    "\n",
    "Deep Learning uses **specialized architectures** depending on data type and task: dense\n",
    "networks (_fully connected_) for tabular or moderate-dimensional data, convolutional\n",
    "networks (CNN) for spatial data and images, recurrent networks (RNN) and Transformers for\n",
    "sequences, as well as multimodal models capable of integrating information from multiple\n",
    "sources (text, image, audio, video). While many problems with **structured data** can be\n",
    "effectively addressed with classical Machine Learning methods, **unstructured data**\n",
    "usually requires deep networks that automatically learn complex and meaningful\n",
    "representations from raw data.\n",
    "\n",
    "In this conceptual framework, the **artificial neuron** emerges as a mathematical\n",
    "abstraction inspired by the biological neuron. In simplified form, a neuron receives an\n",
    "input vector $\\mathbf{x}$, applies a linear combination parameterized by weights\n",
    "$\\mathbf{w}$ and a bias $b$, and finally passes the result through a nonlinear\n",
    "**activation function** $\\sigma$:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b, \\\\ \\hat{y} = \\sigma(z).\n",
    "$$\n",
    "\n",
    "This structure constitutes the basic building block from which complete layers and deep\n",
    "neural networks are built. On this basis, classical models such as **linear regression**\n",
    "and **logistic regression** are developed, which can be interpreted as neurons with an\n",
    "appropriate activation (linear or sigmoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression and logistic regression** provide the conceptual foundation of deep\n",
    "learning by introducing the paradigm of **differentiable models**: models formed by\n",
    "linear transformations and differentiable nonlinear functions, which allows adjusting\n",
    "their parameters through gradient-based optimization algorithms. This principle is common\n",
    "to all modern neural network architectures.\n",
    "\n",
    "In both cases, the starting point is the calculation of a **logit** or linear combination\n",
    "of input features:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^n$ is the input vector (features),\n",
    "$\\mathbf{w} \\in \\mathbb{R}^n$ is the weight vector, and $b \\in \\mathbb{R}$ is the bias.\n",
    "This value $z$ constitutes the output of the linear part of the neuron.\n",
    "\n",
    "In a **linear model for regression**, the prediction is defined as\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b,\n",
    "$$\n",
    "\n",
    "and can take unbounded real values. This type of model is used for **regression**, that\n",
    "is, to predict continuous variables such as prices, temperatures, or physical quantities.\n",
    "\n",
    "In **classification** problems, logits are transformed into probabilities through\n",
    "activation functions. In **binary classification**, the **sigmoid function** is used:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\\\ \\hat{y} = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b),\n",
    "$$\n",
    "\n",
    "so that $\\hat{y} \\in (0, 1)$ can be interpreted as the probability of belonging to the\n",
    "positive class. In **multiclass classification**, the **Softmax** function is used, which\n",
    "from a vector of logits $\\mathbf{z} \\in \\mathbb{R}^K$ produces a probability distribution\n",
    "over $K$ classes:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}, \\\\ k = 1, \\dots, K.\n",
    "$$\n",
    "\n",
    "More generally, a neural network can be described as a **composition of differentiable\n",
    "layers**:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(\\mathbf{x}), \\\\\n",
    "f_\\ell(\\mathbf{x}) = \\sigma_\\ell(\\mathbf{W}_\\ell \\mathbf{x} + \\mathbf{b}_\\ell),\n",
    "$$\n",
    "\n",
    "where each layer applies a linear transformation\n",
    "$\\mathbf{W}_\\ell \\mathbf{x} + \\mathbf{b}_\\ell$ followed by a nonlinear activation\n",
    "function $\\sigma_\\ell$. This combination allows approximating highly complex and\n",
    "nonlinear functions, endowing the model with great expressive capacity.\n",
    "\n",
    "**Logistic regression** is a supervised method for **binary classification** that\n",
    "explicitly models the probability of class membership. Given labeled data\n",
    "$(\\mathbf{x}^{(i)}, y^{(i)})$, assumed independent and identically distributed, the model\n",
    "learns parameters $(\\mathbf{w}, b)$ that maximize the probability of observed labels. In\n",
    "applications such as image classification, inputs are represented as high-dimensional\n",
    "vectors obtained by **flattening** the pixel matrices. For example, an RGB image of\n",
    "$64 \\times 64$ pixels is represented as a vector in $\\mathbb{R}^{12288}$.\n",
    "\n",
    "Learning is formalized through a **loss function** $\\mathcal{L}(\\hat{y}, y)$, which\n",
    "measures the prediction error $\\hat{y}$ against the true label $y$, and a **cost\n",
    "function** defined as the average of losses over the training set:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}),\n",
    "$$\n",
    "\n",
    "where $M$ is the number of examples. In logistic regression, the **logarithmic loss** or\n",
    "**log-loss** is commonly used:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = - \\big( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\big),\n",
    "$$\n",
    "\n",
    "which provides well-behaved gradients and favors the convergence of optimization\n",
    "algorithms. In regression problems, other losses are used, such as **mean squared error\n",
    "(MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{M} \\sum_{i=1}^{M} \\big(\\hat{y}^{(i)} - y^{(i)}\\big)^2,\n",
    "$$\n",
    "\n",
    "**MAE** (mean absolute error), or **Huber loss**, depending on the desired trade-off\n",
    "between sensitivity to outliers and numerical stability.\n",
    "\n",
    "It is important to emphasize that low cost on the training set does not guarantee good\n",
    "performance on unseen data. **Overfitting** appears when the model memorizes training\n",
    "examples instead of learning generalizable patterns. This phenomenon is favored by small\n",
    "datasets, excessively complex architectures, or noisy data that poorly represents the\n",
    "distribution of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent** is one of the fundamental algorithms for training machine learning\n",
    "models. Its objective is to find parameter values that minimize a cost function, so that\n",
    "model predictions fit observed data as well as possible.\n",
    "\n",
    "In the case of logistic regression, the cost function $J(\\mathbf{w}, b)$ is defined from\n",
    "log-loss:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = -\\frac{1}{M} \\sum_{i=1}^{M} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big].\n",
    "$$\n",
    "\n",
    "To reduce the value of $J$, partial derivatives with respect to model parameters are\n",
    "calculated. These derivatives define the **gradient**, that is, the direction in which\n",
    "the cost function increases most rapidly. Since the objective is to minimize $J$, the\n",
    "algorithm adjusts parameters in the opposite direction to the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\mathbf{d w} = \\frac{1}{M} \\sum_{i=1}^{M} (\\hat{y}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}, \\\\\n",
    "\\frac{\\partial J}{\\partial b} = d b = \\frac{1}{M} \\sum_{i=1}^{M} (\\hat{y}^{(i)} - y^{(i)}).\n",
    "$$\n",
    "\n",
    "These terms indicate in what direction and with what magnitude $\\mathbf{w}$ and $b$\n",
    "should be modified to decrease error. The complete gradient descent procedure is\n",
    "developed iteratively and can be described as:\n",
    "\n",
    "1. **Parameter initialization**: Initial values are assigned to $\\mathbf{w}$ and $b$,\n",
    "   often small and random or zeros, depending on the problem.\n",
    "2. **Forward propagation**: $\\hat{y}$ is calculated from input data $X$ and the loss\n",
    "   function $\\mathcal{L}(\\hat{y}, y)$ and cost function $J(\\mathbf{w}, b)$ are evaluated.\n",
    "3. **Backward propagation**: Partial derivatives $\\mathbf{d w}$ and $d b$ are obtained\n",
    "   through automatic differentiation or analytical derivation, which indicate how to\n",
    "   adjust parameters.\n",
    "4. **Parameter update**: Values of $\\mathbf{w}$ and $b$ are updated according to the\n",
    "   rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\cdot \\mathbf{d w}, \\\\\n",
    "b := b - \\alpha \\cdot d b,\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**, a hyperparameter that controls step size at each\n",
    "iteration. If $\\alpha$ is too large, the algorithm may diverge; if it is too small,\n",
    "convergence will be very slow.\n",
    "\n",
    "This process is repeated until reaching an acceptable minimum of $J(\\mathbf{w}, b)$,\n",
    "which translates into more accurate predictions. In practice, gradient descent is\n",
    "implemented in a **vectorized** manner, leveraging matrix operations on all examples (or\n",
    "minibatches) in parallel, which simplifies code and allows exploiting GPU computational\n",
    "capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation functions** introduce nonlinearity into neural networks and allow successive\n",
    "layers to capture complex relationships between input variables. Without nonlinear\n",
    "activation functions, a composition of linear layers would be equivalent to a single\n",
    "linear transformation, severely limiting model capacity.\n",
    "\n",
    "Below, several common activation functions are defined and their curves are shown through\n",
    "simple Python code. NumPy is used for calculation and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "from typing import Callable\n",
    "\n",
    "# 3pps\n",
    "# 3rd party packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(input: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "\n",
    "def tanh(input: np.ndarray) -> np.ndarray:\n",
    "    return (np.exp(input) - np.exp(-input)) / (np.exp(input) + np.exp(-input))\n",
    "\n",
    "\n",
    "def relu(input: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "\n",
    "def leaky_relu(input: np.ndarray, alpha: float = 0.1) -> np.ndarray:\n",
    "    return np.maximum(alpha * input, input)\n",
    "\n",
    "\n",
    "def elu(input: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
    "    return np.where(input < 0, alpha * (np.exp(input) - 1), input)\n",
    "\n",
    "\n",
    "def swish(input: np.ndarray) -> np.ndarray:\n",
    "    return input * sigmoid(input)\n",
    "\n",
    "\n",
    "def gelu(input: np.ndarray) -> np.ndarray:\n",
    "    return (\n",
    "        0.5 * input * (1 + tanh(math.sqrt(2 / math.pi) * (input + 0.044715 * input**3)))\n",
    "    )\n",
    "\n",
    "\n",
    "steps = np.arange(-10, 10, 0.1)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle(\"Activation Functions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of functions and their names\n",
    "functions = [\n",
    "    (\"Sigmoid\", sigmoid),\n",
    "    (\"Tanh\", tanh),\n",
    "    (\"ReLU\", relu),\n",
    "    (\"LeakyReLU\", leaky_relu),\n",
    "    (\"ELU\", elu),\n",
    "    (\"Swish\", swish),\n",
    "    (\"GELU\", gelu),\n",
    "]\n",
    "\n",
    "# Plot each function\n",
    "for idx, (name, func) in enumerate(functions):\n",
    "    axes[idx].plot(steps, func(steps), linewidth=2)\n",
    "    axes[idx].set_title(f\"{name} function\")\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlabel(\"x\")\n",
    "    axes[idx].set_ylabel(\"f(x)\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(functions), len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these functions has particular properties regarding saturation, derivatives,\n",
    "symmetry, and numerical behavior:\n",
    "\n",
    "- **Sigmoid**: Compresses the input value to the interval $(0, 1)$. Suitable for\n",
    "  probabilistic outputs in binary classification, although it can suffer from gradient\n",
    "  saturation problems.\n",
    "- **Tanh**: Similar to sigmoid, but centered at zero, with range $(-1, 1)$. Usually\n",
    "  provides better gradients than pure sigmoid in intermediate layers.\n",
    "- **ReLU (Rectified Linear Unit)**: Defines $\\mathrm{ReLU}(x) = \\max(0, x)$. It is one of\n",
    "  the most used activations due to its simplicity and good behavior in deep networks.\n",
    "- **Leaky ReLU** and **ELU**: Introduce a small slope in the negative part to avoid\n",
    "  completely inactive neurons and improve gradient propagation.\n",
    "- **Swish** and **GELU**: Are smooth and nonlinear modern functions, used in recent\n",
    "  architectures (for example, Transformers), which often offer empirical performance\n",
    "  improvements over ReLU in certain contexts.\n",
    "\n",
    "These functions are implemented in a differentiable way, which allows PyTorch and other\n",
    "libraries to automatically calculate their gradients during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Example with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how all the previous elements combine—neurons, activation functions, loss\n",
    "functions, gradient descent, and automatic differentiation—a **binary classification**\n",
    "example with PyTorch on a synthetic dataset is presented.\n",
    "\n",
    "In this example, data is generated using the `make_circles` function from `scikit-learn`,\n",
    "which produces two classes in the shape of concentric circles, a nonlinearly separable\n",
    "problem. Next, a simple neural network is defined, trained using stochastic gradient\n",
    "descent, and its performance is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "\n",
    "# 3pps\n",
    "# 3rd party packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Sequential model: hidden layer + GELU activation + output layer + sigmoid\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(input_tensor)\n",
    "\n",
    "\n",
    "# Generate circle-shaped data\n",
    "n_samples = 1000\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)\n",
    "X.shape, y.shape\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = BinaryClassifier(num_classes=2)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "print(y_train.min(), y_train.max(), y_train.dtype)\n",
    "print(y_test.min(), y_test.max(), y_test.dtype)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training loop by epochs is defined, using minibatches and recording both loss and\n",
    "accuracy on training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "num_batches = math.ceil(len(X_train) / batch_size)\n",
    "num_batches_test = math.ceil(len(X_test) / batch_size)\n",
    "\n",
    "plot_loss_train = []\n",
    "plot_loss_test = []\n",
    "plot_acc_train = []\n",
    "plot_acc_test = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch_train = []\n",
    "    loss_epoch_test = []\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n",
    "        y_batch = y_train[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = loss_function(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch_train.append(loss.item())\n",
    "        pred_labels = (predictions >= 0.5).float()\n",
    "        acc = (pred_labels == y_batch).float().mean().item() * 100\n",
    "        accuracy_train.append(acc)\n",
    "\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i in range(num_batches_test):\n",
    "            X_test_batch = X_test[i * batch_size : (i + 1) * batch_size]\n",
    "            y_test_batch = y_test[i * batch_size : (i + 1) * batch_size].view(-1, 1)\n",
    "\n",
    "            predictions_inference = model(X_test_batch)\n",
    "            loss_test = loss_function(predictions_inference, y_test_batch)\n",
    "            loss_epoch_test.append(loss_test.item())\n",
    "\n",
    "            pred_labels_test = (predictions_inference >= 0.5).float()\n",
    "            acc_test = (pred_labels_test == y_test_batch).float().mean().item() * 100\n",
    "            accuracy_test.append(acc_test)\n",
    "\n",
    "    # Epoch averages\n",
    "    train_loss_mean = np.mean(loss_epoch_train)\n",
    "    test_loss_mean = np.mean(loss_epoch_test)\n",
    "    train_acc_mean = np.mean(accuracy_train)\n",
    "    test_acc_mean = np.mean(accuracy_test)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}, \"\n",
    "        f\"Train Loss: {train_loss_mean:.4f}, \"\n",
    "        f\"Test Loss: {test_loss_mean:.4f}, \"\n",
    "        f\"Train Acc: {train_acc_mean:.2f}%, \"\n",
    "        f\"Test Acc: {test_acc_mean:.2f}%\"\n",
    "    )\n",
    "\n",
    "    plot_loss_train.append(train_loss_mean)\n",
    "    plot_loss_test.append(test_loss_mean)\n",
    "    plot_acc_train.append(train_acc_mean)\n",
    "    plot_acc_test.append(test_acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, loss and accuracy curves throughout epochs are plotted and the model's\n",
    "ability to separate classes on the test set is visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss evolution\n",
    "plt.plot(range(num_epochs), plot_loss_train, label=\"Train Loss\")\n",
    "plt.plot(range(num_epochs), plot_loss_test, label=\"Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy evolution\n",
    "plt.plot(range(num_epochs), plot_acc_train, label=\"Train Acc\")\n",
    "plt.plot(range(num_epochs), plot_acc_test, label=\"Test Acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Original test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "plt.show()\n",
    "\n",
    "# Model predictions on test set\n",
    "with torch.inference_mode():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "predictions = np.where(predictions.numpy() >= 1e-1, 1, 0)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
