{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve computational efficiency, gradient-based optimization in deep\n",
    "learning does not usually rely on the full dataset at each update step. Instead, it\n",
    "**employs** stochastic gradient descent (SGD), which uses small subsets of the data known\n",
    "as mini-batches. This strategy introduces stochasticity into the optimization process,\n",
    "reduces computational cost per update, and helps escape problematic regions of the loss\n",
    "landscape, such as saddle points, where the gradient vanishes without corresponding to a\n",
    "true minimum.\n",
    "\n",
    "Basic gradient descent can be inefficient or unstable in certain scenarios, particularly\n",
    "when the loss surface exhibits strong anisotropy or narrow valleys. For this reason,\n",
    "several variants have been developed to improve convergence speed and robustness. Among\n",
    "the most widely used are Momentum, RMSprop, and Adam, all of which build on the idea of\n",
    "adapting the update step based on past gradient information.\n",
    "\n",
    "Momentum augments SGD with an inertia term that accumulates gradient information over\n",
    "time, thereby smoothing the updates. It is defined by the following equations:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta)\\,\\nabla_\\theta \\mathcal{L}(\\theta_t),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta\\,v_t,\n",
    "$$\n",
    "\n",
    "where $v_t$ denotes the accumulated \"velocity\", $\\beta \\in [0, 1)$ is a decay\n",
    "coefficient, and $\\eta$ is the learning rate. In practice, $\\beta$ is often set to $0.9$.\n",
    "This mechanism reduces oscillations in directions of high curvature and accelerates\n",
    "convergence along narrow valleys by integrating information across multiple steps instead\n",
    "of relying solely on the current gradient.\n",
    "\n",
    "RMSprop is an adaptive learning rate method that rescales the gradients by a moving\n",
    "average of their squared values. It is defined as:\n",
    "\n",
    "$$\n",
    "s_t = \\rho s_{t-1} + (1 - \\rho)\\left(\\nabla_\\theta \\mathcal{L}(\\theta_t)\\right)^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}}\\,\\nabla_\\theta \\mathcal{L}(\\theta_t),\n",
    "$$\n",
    "\n",
    "where $\\rho \\approx 0.9$ and $\\epsilon \\approx 10^{-8}$ is a small constant introduced to\n",
    "avoid division by zero. In this formulation, parameters associated with consistently\n",
    "large gradients are updated with smaller steps, while those associated with small\n",
    "gradients receive relatively larger steps. This adaptive behaviour improves the stability\n",
    "of training and makes the optimizer more robust to poorly scaled loss landscapes.\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines the advantages of Momentum and RMSprop by\n",
    "maintaining both an exponential moving average of the gradients (first moment) and an\n",
    "exponential moving average of their squared values (second moment). The algorithm\n",
    "proceeds in four stages.\n",
    "\n",
    "First, it computes the exponentially weighted moving average of the gradients (first\n",
    "moment):\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\,\\nabla_\\theta \\mathcal{L}(\\theta_t).\n",
    "$$\n",
    "\n",
    "Second, it computes the exponentially weighted moving average of the squared gradients\n",
    "(second moment):\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)\\,\\left(\\nabla_\\theta \\mathcal{L}(\\theta_t)\\right)^2.\n",
    "$$\n",
    "\n",
    "Third, it performs a bias correction step to compensate for the initialization of $m_t$\n",
    "and $v_t$ at zero:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}.\n",
    "$$\n",
    "\n",
    "Finally, it updates the parameters using the bias-corrected moments:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\,\\hat{m}_t.\n",
    "$$\n",
    "\n",
    "Typical recommended values are $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and\n",
    "$\\epsilon = 10^{-8}$. Adam is widely adopted in deep learning due to its fast\n",
    "convergence, numerical stability, and robustness to suboptimal hyperparameter\n",
    "configurations. It adapts the learning rate per parameter based on both the magnitude and\n",
    "the variance of the gradients, while also leveraging momentum-like smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple 1D Function Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustrative example, the following implementation compares these optimizers on the\n",
    "simple one-dimensional test function $f(\\theta) = \\theta^2$, whose global minimum is\n",
    "located at $\\theta = 0$. In this setting, all optimizers start from an initial value\n",
    "$\\theta = 5$ and attempt to reduce the loss. Although each algorithm follows a different\n",
    "trajectory through parameter space, they all tend toward the global minimum at\n",
    "$\\theta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Loss function and gradient\n",
    "loss = lambda theta: theta**2\n",
    "grad = lambda theta: 2 * theta\n",
    "\n",
    "# Initial value\n",
    "theta_init = 5.0\n",
    "\n",
    "\n",
    "# Stochastic Gradient Descent (SGD)\n",
    "def sgd(theta, grad, eta=0.1, steps=20):\n",
    "    for t in range(steps):\n",
    "        theta -= eta * grad(theta)\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Momentum\n",
    "def momentum(theta, grad, eta=0.1, beta=0.9, steps=20):\n",
    "    v = 0\n",
    "    for t in range(steps):\n",
    "        v = beta * v + (1 - beta) * grad(theta)\n",
    "        theta -= eta * v\n",
    "    return theta\n",
    "\n",
    "\n",
    "# RMSprop\n",
    "def rmsprop(theta, grad, eta=0.1, rho=0.9, eps=1e-8, steps=20):\n",
    "    s = 0\n",
    "    for t in range(steps):\n",
    "        g = grad(theta)\n",
    "        s = rho * s + (1 - rho) * g**2\n",
    "        theta -= eta / (np.sqrt(s) + eps) * g\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Adam\n",
    "def adam(theta, grad, eta=0.1, beta1=0.9, beta2=0.999, eps=1e-8, steps=20):\n",
    "    m, v = 0, 0\n",
    "    for t in range(1, steps + 1):\n",
    "        g = grad(theta)\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * g**2\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        theta -= eta / (np.sqrt(v_hat) + eps) * m_hat\n",
    "    return theta\n",
    "\n",
    "\n",
    "print(\"SGD:\", sgd(theta_init, grad))\n",
    "print(\"Momentum:\", momentum(theta_init, grad))\n",
    "print(\"RMSprop:\", rmsprop(theta_init, grad))\n",
    "print(\"Adam:\", adam(theta_init, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training with Different Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates how different optimizers perform when training a small\n",
    "neural network on the circle dataset used in the regularization examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Generate and prepare data\n",
    "n_samples = 1000\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "\n",
    "# Simple neural network\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_with_optimizer(optimizer_name, learning_rate=0.01, num_epochs=100):\n",
    "    model = BinaryClassifier()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Select optimizer\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"Momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_train)\n",
    "        loss = loss_fn(logits, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test)\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "# Compare optimizers\n",
    "optimizers = [\"SGD\", \"Momentum\", \"RMSprop\", \"Adam\"]\n",
    "results = {}\n",
    "\n",
    "for opt_name in optimizers:\n",
    "    print(f\"Training with {opt_name}...\")\n",
    "    train_loss, test_loss = train_with_optimizer(opt_name, learning_rate=0.01)\n",
    "    results[opt_name] = {\"train\": train_loss, \"test\": test_loss}\n",
    "    print(f\"  Final train loss: {train_loss[-1]:.4f}\")\n",
    "    print(f\"  Final test loss: {test_loss[-1]:.4f}\\n\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for opt_name in optimizers:\n",
    "    plt.plot(results[opt_name][\"train\"], label=opt_name)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for opt_name in optimizers:\n",
    "    plt.plot(results[opt_name][\"test\"], label=opt_name)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"Test Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
