{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document presents an integrated and progressively developed explanation of\n",
    "regularization and normalization techniques in machine learning and deep learning,\n",
    "combining both theoretical foundations and a practical implementation in PyTorch. The\n",
    "central example consists of a binary classification problem solved with a small neural\n",
    "network trained on a synthetic dataset with circular structure. The code illustrates how\n",
    "to implement and apply L1 (Lasso) and L2 (Ridge) regularization in practice, while the\n",
    "theoretical sections place these techniques in the broader context of linear regression,\n",
    "overfitting control, and model stability.\n",
    "\n",
    "The exposition follows a linear and didactic structure. It starts from the analytical\n",
    "solution of linear regression using the Moore–Penrose pseudoinverse and motivates the\n",
    "need for regularization from a numerical stability and generalization perspective.\n",
    "Subsequently, it formalizes L1 and L2 regularization, discusses other common\n",
    "regularization strategies (dropout, data augmentation, early stopping, input\n",
    "normalization), and explains activation normalization methods (Batch Normalization and\n",
    "Layer Normalization). Finally, it connects this theory with a complete PyTorch\n",
    "implementation, highlighting how the abstract concepts of Ridge and Lasso regularization\n",
    "translate into concrete training code.\n",
    "\n",
    "The text is intended to be accessible to readers with basic knowledge of linear algebra,\n",
    "probability, and Python programming, but it maintains a formal and technically precise\n",
    "tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and the Moore–Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of linear regression, the objective is to estimate a parameter vector\n",
    "$\\mathbf{w}$ that best fits a set of training examples. Given a design matrix\n",
    "$\\mathbf{X} \\in \\mathbb{R}^{N \\times n}$, where each row represents an example and each\n",
    "column a feature, and a target vector $\\mathbf{y} \\in \\mathbb{R}^N$, the classical\n",
    "least-squares formulation seeks to minimize the mean squared error between the\n",
    "predictions $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}$ and the true targets $\\mathbf{y}$.\n",
    "\n",
    "In the simplest case, when the matrix $\\mathbf{X}^\\top \\mathbf{X}$ is invertible and\n",
    "well-conditioned, it is possible to obtain an analytical solution for the model weights\n",
    "using the Moore–Penrose pseudoinverse. The closed-form expression for the optimal weights\n",
    "in the least-squares sense is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "This expression corresponds to the normal equations and provides a direct solution\n",
    "without iterative optimization. However, in many practical situations the matrix\n",
    "$\\mathbf{X}^\\top \\mathbf{X}$ is not well-behaved. When it is nearly singular, that is,\n",
    "when some of its eigenvalues are very small or close to zero, the inverse\n",
    "$(\\mathbf{X}^\\top \\mathbf{X})^{-1}$ becomes numerically unstable. Small perturbations in\n",
    "the data can then cause large variations in the estimated parameters $\\mathbf{w}$.\n",
    "\n",
    "This phenomenon has two main consequences. On the one hand, it produces highly sensitive\n",
    "models whose predictions change drastically in response to small input variations. On the\n",
    "other hand, it favours overfitting, that is, the model adapts too closely to the specific\n",
    "noise and peculiarities of the training data, displaying poor generalization when\n",
    "evaluated on new, unseen examples. To alleviate these issues, it is common to introduce\n",
    "regularization terms into the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization introduces additional terms into the loss function that penalize\n",
    "excessively large parameter values. From a statistical viewpoint, regularization reduces\n",
    "the variance of the estimator by constraining the hypothesis space. From a numerical\n",
    "viewpoint, it improves the conditioning of the optimization problem and stabilizes the\n",
    "solution.\n",
    "\n",
    "In the context of linear regression, the standard mean squared error loss is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2.\n",
    "$$\n",
    "\n",
    "Regularization augments this objective with a penalty on the weights. The two most widely\n",
    "used regularization schemes are L2 regularization (Ridge Regression) and L1\n",
    "regularization (Lasso Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization (Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization adds a penalty term proportional to the square of the Euclidean norm of\n",
    "the weights. The Ridge loss function takes the form:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Ridge}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_2^2,\n",
    "$$\n",
    "\n",
    "where $\\|\\mathbf{w}\\|_2^2 = \\sum_j w_j^2$ and $\\lambda \\ge 0$ is a hyperparameter that\n",
    "controls the intensity of the penalization. A larger $\\lambda$ enforces a stronger\n",
    "restriction on the magnitude of the weights.\n",
    "\n",
    "From a geometric perspective, the L2 penalty discourages solutions with large\n",
    "coefficients and favours weight vectors with smaller and more evenly distributed\n",
    "components. This behaviour leads to smoother and more stable models that are less\n",
    "sensitive to noise. In terms of generalization, L2 regularization reduces overfitting by\n",
    "penalizing complex models with large parameters. In the context of gradient-based\n",
    "optimization, this penalty manifests as a shrinkage of the weights towards zero at each\n",
    "update, a phenomenon commonly known as weight decay.\n",
    "\n",
    "In addition, L2 regularization improves the conditioning of the matrix\n",
    "$\\mathbf{X}^\\top \\mathbf{X}$ by effectively adding $\\lambda \\mathbf{I}$ to it in the\n",
    "normal equations, which mitigates the problems associated with small eigenvalues. The\n",
    "resulting solution is more robust to perturbations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization (Lasso Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization, on the other hand, adds a penalty term based on the sum of the\n",
    "absolute values of the weights. The Lasso loss function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Lasso}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_1,\n",
    "$$\n",
    "\n",
    "where $\\|\\mathbf{w}\\|_1 = \\sum_j |w_j|$. As in the Ridge case, $\\lambda$ determines the\n",
    "strength of the regularization.\n",
    "\n",
    "The key difference between L1 and L2 regularization lies in the geometry of the penalty.\n",
    "The L1 norm induces sparsity: it tends to drive some coefficients exactly to zero. This\n",
    "effect leads to models in which only a subset of features remains active, effectively\n",
    "performing implicit feature selection. As a result, L1-regularized models are often\n",
    "simpler and more interpretable, as they rely on fewer variables.\n",
    "\n",
    "However, in optimization problems that are already non-convex due to the presence of deep\n",
    "neural architectures, the interaction of the L1 penalty with gradient descent can be less\n",
    "straightforward than in linear regression. The absolute value introduces\n",
    "non-differentiability at zero, and although subgradient methods and approximate\n",
    "techniques are commonly used, L1 regularization is less widespread in deep learning than\n",
    "L2 regularization and other regularization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the capacity of neural networks to approximate highly complex functions\n",
    "substantially increases the risk of overfitting. Regularization and normalization\n",
    "techniques play a fundamental role in controlling this capacity, improving\n",
    "generalization, and stabilizing training dynamics.\n",
    "\n",
    "Regularization, in this broader sense, encompasses any technique designed to limit the\n",
    "model’s dependence on the specifics of the training data, promoting representations that\n",
    "remain robust when confronted with unseen examples. While L1 and L2 regularization\n",
    "continue to be important, deep learning also includes complementary strategies such as\n",
    "dropout, data augmentation, early stopping, and input normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several regularization techniques are widely used in practical deep learning systems.\n",
    "\n",
    "L2 regularization (Ridge) is implemented in neural networks as weight decay, typically\n",
    "through a term added to the loss or directly integrated into the optimizer. It penalizes\n",
    "the squared magnitude of the weights, preventing them from growing excessively and\n",
    "favouring smoother, more generalizable solutions. It stabilizes optimization and reduces\n",
    "the variance of the learned parameters.\n",
    "\n",
    "L1 regularization (Lasso) penalizes the absolute magnitude of the weights, driving many\n",
    "of them towards zero. In neural networks, this induces sparsity in the parameters and can\n",
    "be used as a form of implicit feature selection or connection pruning. Nonetheless, in\n",
    "deep models it is less common than L2 and often combined with other regularization\n",
    "methods.\n",
    "\n",
    "Dropout randomly deactivates a subset of neurons during training. At each forward pass, a\n",
    "random binary mask is applied to the activations of selected layers, forcing the network\n",
    "to distribute information and avoid strong co-adaptations between units. This mechanism\n",
    "acts as an ensemble of sub-models that share parameters and increases robustness by\n",
    "preventing the model from relying too heavily on any particular path through the network.\n",
    "\n",
    "However, dropout introduces stochasticity into the outputs. For a fixed input, two\n",
    "different forward passes can produce different predictions due to different sampled\n",
    "masks, and some mask realizations may be suboptimal (for example, with an unusually large\n",
    "number of deactivated units). After training, a deterministic behaviour is usually\n",
    "desired for inference. One strategy consists of estimating the expected output via Monte\n",
    "Carlo sampling, that is, performing multiple stochastic forward passes and averaging the\n",
    "results. This approach not only stabilizes predictions but also yields an empirical\n",
    "measure of predictive uncertainty. Nevertheless, it increases inference cost. A more\n",
    "common and efficient alternative is to replace the stochastic dropout operation with a\n",
    "deterministic scaling of the activations during inference, using the expected value of\n",
    "the random mask. Most deep learning libraries implement this approximation automatically\n",
    "when switching the model from training mode to evaluation mode.\n",
    "\n",
    "Data augmentation generates additional training examples from existing data through\n",
    "label-preserving transformations, such as rotations, translations, scaling, cropping, or\n",
    "changes in brightness and contrast in the case of images. This technique increases the\n",
    "diversity of the training set and encourages the model to learn features invariant to\n",
    "these transformations. In doing so, it reduces overfitting and improves generalization,\n",
    "especially when the original dataset is relatively small.\n",
    "\n",
    "Early stopping monitors the model’s performance on a validation set and stops training\n",
    "when the validation error ceases to improve or begins to deteriorate. This technique\n",
    "prevents the network from continuing to adapt to noise and idiosyncrasies in the training\n",
    "set beyond the point of minimal validation error. It can be interpreted as a form of\n",
    "implicit regularization because it limits the effective capacity of the model by\n",
    "terminating training early.\n",
    "\n",
    "Input normalization preprocesses the features so that they share similar scales and are\n",
    "centred around zero, for example by subtracting the mean and dividing by the standard\n",
    "deviation of each feature. This operation improves numerical stability, accelerates\n",
    "convergence in gradient-based optimization, and prevents certain features with large\n",
    "magnitudes from dominating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Normalization: Batch and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to regularizing the parameters, it is crucial to control the distribution of\n",
    "activations within a network. During training, the internal representations produced by\n",
    "intermediate layers can change significantly as earlier layers are updated, a phenomenon\n",
    "known as internal covariate shift. This variability complicates optimization, as each\n",
    "layer must constantly adapt to shifting input distributions.\n",
    "\n",
    "Activation normalization techniques address this issue by enforcing more stable and\n",
    "balanced distributions of activations across the network, thereby facilitating training\n",
    "and enabling the use of higher learning rates.\n",
    "\n",
    "Batch Normalization normalizes the activations of each layer using the mean and variance\n",
    "computed over the examples in a mini-batch. For each feature channel, it estimates the\n",
    "batch mean and variance, subtracts the mean from the activations, and divides by the\n",
    "standard deviation. Subsequently, it applies learnable scaling and shifting parameters\n",
    "that allow the network to recover any necessary distribution. Batch Normalization reduces\n",
    "internal covariate shift, speeds up convergence, and often reduces sensitivity to\n",
    "hyperparameters. However, its performance depends on the mini-batch size and composition.\n",
    "It can be less effective or unstable when batches are very small or when the data\n",
    "distribution varies strongly between batches, as in certain sequence modelling or\n",
    "streaming scenarios.\n",
    "\n",
    "Layer Normalization, in contrast, performs normalization at the level of individual\n",
    "samples rather than across the batch. It computes the mean and variance over the features\n",
    "of each sample and then normalizes and re-scales the activations. This property makes\n",
    "Layer Normalization especially suitable for architectures such as transformers and\n",
    "recurrent networks, as well as for distributed training settings, because it does not\n",
    "require sharing statistics across different examples in a mini-batch. It provides more\n",
    "consistent normalization in cases where batch composition can fluctuate substantially or\n",
    "where very small batch sizes are used.\n",
    "\n",
    "Both Batch Normalization and Layer Normalization can be interpreted as mechanisms that\n",
    "stabilize and regularize the training process by controlling the distribution of\n",
    "activations, although they are not regularization techniques in the same strict sense as\n",
    "L1 or L2 penalties on the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the preceding concepts concrete, this section describes a complete implementation\n",
    "of a binary classification task using PyTorch, in which L1 and L2 regularization are\n",
    "explicitly incorporated into the training process. Although the problem is formulated as\n",
    "classification rather than regression, the underlying idea of penalizing parameter norms\n",
    "to improve generalization is the same as in Ridge and Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation starts by importing the necessary libraries for numerical computation,\n",
    "visualization, data generation, and model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "\n",
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `math` module provides mathematical utilities such as `ceil`, which is used to\n",
    "compute the number of mini-batches. The `matplotlib.pyplot` and `numpy` libraries are\n",
    "used for data visualization and numerical operations, respectively. PyTorch (`torch`,\n",
    "`torch.nn`) supplies the infrastructure for defining and training neural networks.\n",
    "Finally, scikit-learn functions `make_circles` and `train_test_split` are used to\n",
    "generate a synthetic dataset and partition it into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple feed-forward neural network designed for binary classification with\n",
    "two-dimensional inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 16), nn.GELU(), nn.Linear(16, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture comprises an input layer, a hidden layer, and an output layer. The input\n",
    "layer (`nn.Linear(2, 16)`) maps two-dimensional feature vectors to a 16-dimensional\n",
    "hidden representation. This choice is consistent with the synthetic dataset, where each\n",
    "example has two features. A GELU (Gaussian Error Linear Unit) activation introduces\n",
    "non-linearity between the hidden layer and the output layer. This non-linear\n",
    "transformation allows the network to learn complex decision boundaries that cannot be\n",
    "captured by linear models.\n",
    "\n",
    "The output layer (`nn.Linear(16, 1)`) produces a single scalar per input sample, known as\n",
    "a logit. In binary classification, logits are typically transformed into probabilities\n",
    "using the sigmoid function. In this example, the logits are directly passed to a loss\n",
    "function (`BCEWithLogitsLoss`) that internally combines the sigmoid operation with binary\n",
    "cross-entropy in a numerically stable manner.\n",
    "\n",
    "The `forward` method defines the computation performed at each call. The input tensor `x`\n",
    "has shape $(N, 2)$, where $N$ is the batch size, and the output tensor has shape\n",
    "$(N, 1)$. This convention aligns with the shape of the label tensors used during\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two concentric circles, a classical example of a non-linearly\n",
    "separable problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `make_circles` generates $n_{\\text{samples}}$ two-dimensional points\n",
    "arranged in two circular clusters. The `noise` parameter introduces Gaussian noise into\n",
    "the data, making the classification task more realistic and preventing perfect\n",
    "separability. The `random_state` parameter fixes the random seed, ensuring reproducible\n",
    "data generation.\n",
    "\n",
    "The feature matrix `X` has shape $(1000, 2)$ and contains the coordinates of the points,\n",
    "while the label vector `y` has shape $(1000,)$ and encodes the class of each point (0 or\n",
    "1). A scatter plot allows visual inspection of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, points are coloured according to their class, and the concentric circular\n",
    "structure becomes evident. Because the classes are not linearly separable, a linear\n",
    "classifier cannot find a straight decision boundary that separates them, which motivates\n",
    "the use of a neural network with a non-linear hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the generalization performance of the model, the dataset is split into\n",
    "training and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `test_size=0.3` indicates that 30% of the data is reserved for testing,\n",
    "while 70% is used for training. The parameter `stratify=y` ensures that both subsets\n",
    "preserve the original class distribution, which is important to avoid biases in the\n",
    "evaluation. The `random_state` parameter ensures that the split is reproducible.\n",
    "\n",
    "Since PyTorch models operate on tensors, the NumPy arrays are converted to `torch.Tensor`\n",
    "objects with appropriate data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are cast to 32-bit floating-point numbers, the usual precision for neural\n",
    "network training. The labels are also converted to `float32` and reshaped from shape\n",
    "$(N,)$ to $(N, 1)$ using `unsqueeze(1)`, so that their shape matches the model outputs.\n",
    "This alignment simplifies the use of scalar-valued loss functions that operate on tensors\n",
    "of identical shapes.\n",
    "\n",
    "It is common to verify the ranges and types of the label tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.min(), y_train.max(), y_train.dtype)\n",
    "print(y_test.min(), y_test.max(), y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These checks confirm that labels are correctly encoded (typically 0 and 1) and use a\n",
    "floating-point type, as required by `BCEWithLogitsLoss`.\n",
    "\n",
    "Separate scatter plots of the training and test sets allow verification that both subsets\n",
    "are representative of the overall data distribution and preserve the circular structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mirror Ridge and Lasso regularization within the neural network setting, two functions\n",
    "are defined that augment the loss with L2 and L1 penalties applied to the model\n",
    "parameters.\n",
    "\n",
    "L2 regularization (Ridge) is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regularization(model, loss, alpha):\n",
    "    l2_penalty = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" not in name:\n",
    "            l2_penalty += torch.sum(param**2)\n",
    "    return loss + alpha * l2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over the named parameters of the model using\n",
    "`model.named_parameters()`. For each parameter whose name does not contain the substring\n",
    "`'bias'`, it adds the sum of the squared values to `l2_penalty`. Bias terms are typically\n",
    "excluded from regularization, because penalizing them rarely improves generalization and\n",
    "may introduce unnecessary constraints. The function returns the original loss plus\n",
    "`alpha` times the accumulated penalty. The coefficient `alpha` plays the role of\n",
    "$\\lambda$ in the theoretical expressions, controlling the strength of the L2\n",
    "regularization.\n",
    "\n",
    "L1 regularization (Lasso) is implemented analogously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regularization(model, loss, alpha):\n",
    "    l1_penalty = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" not in name:\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "    return loss + alpha * l1_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the penalty is the sum of the absolute values of the weights (excluding biases). As\n",
    "before, the final loss is the sum of the original loss and the scaled penalty. In theory,\n",
    "this L1 term encourages sparsity in the weights, although in small networks with dense\n",
    "fully connected layers, the degree of sparsity may be modest compared to linear models\n",
    "with large numbers of features.\n",
    "\n",
    "Conceptually, these functions transfer the idea of Ridge and Lasso from the linear\n",
    "regression setting (where they are applied to $\\mathbf{w}$) to a neural network setting\n",
    "(where they are applied to all or part of the learned parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete training procedure is encapsulated in the `train_model` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, optimizer, loss_fn, reg_fn=None, alpha=0.0, num_epochs=20, batch_size=32\n",
    "):\n",
    "\n",
    "    num_batches = math.ceil(len(X_train) / batch_size)\n",
    "    num_batches_test = math.ceil(len(X_test) / batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses, train_accs = [], []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n",
    "            y_batch = y_train[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "\n",
    "            if reg_fn is not None:\n",
    "                loss = reg_fn(model, loss, alpha)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            acc = (preds == y_batch).float().mean().item() * 100\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "\n",
    "        model.eval()\n",
    "        test_losses, test_accs = [], []\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for i in range(num_batches_test):\n",
    "                X_batch = X_test[i * batch_size : (i + 1) * batch_size]\n",
    "                y_batch = y_test[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "                logits = model(X_batch)\n",
    "                loss = loss_fn(logits, y_batch)\n",
    "\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "                acc = (preds == y_batch).float().mean().item() * 100\n",
    "\n",
    "                test_losses.append(loss.item())\n",
    "                test_accs.append(acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:02d} | \"\n",
    "            f\"Train Loss: {np.mean(train_losses):.4f} | \"\n",
    "            f\"Test Loss: {np.mean(test_losses):.4f} | \"\n",
    "            f\"Train Acc: {np.mean(train_accs):.2f}% | \"\n",
    "            f\"Test Acc: {np.mean(test_accs):.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function receives the model, an optimizer (for example, Adam), a loss function (here,\n",
    "`BCEWithLogitsLoss`), an optional regularization function (`reg_fn`), the corresponding\n",
    "regularization coefficient `alpha`, and the training hyperparameters `num_epochs` and\n",
    "`batch_size`.\n",
    "\n",
    "The training loop performs the following tasks. First, it computes the number of\n",
    "mini-batches for both training and test sets using `math.ceil`. For each epoch, the model\n",
    "is set to training mode with `model.train()`, and two lists are initialized to store\n",
    "batch-wise training losses and accuracies.\n",
    "\n",
    "The training dataset is then processed in mini-batches. For each batch, slices of\n",
    "`X_train` and `y_train` are selected. The gradients are reset via\n",
    "`optimizer.zero_grad()`, and the model produces logits for the batch. The primary loss is\n",
    "computed by applying `loss_fn` to the logits and labels. If a regularization function has\n",
    "been provided, the loss is augmented using `reg_fn(model, loss, alpha)`, effectively\n",
    "adding L1 or L2 penalties to the objective.\n",
    "\n",
    "After computing the final loss, `loss.backward()` is called to perform backpropagation,\n",
    "computing gradients with respect to all parameters. The optimizer then updates the model\n",
    "parameters via `optimizer.step()`.\n",
    "\n",
    "Accuracy for the batch is computed as the percentage of correctly classified examples.\n",
    "Both the loss and accuracy values are accumulated for later averaging.\n",
    "\n",
    "Once all training batches for the epoch have been processed, the model is switched to\n",
    "evaluation mode using `model.eval()`. In evaluation mode, certain layers such as dropout\n",
    "or batch normalization (if present) adjust their behaviour accordingly. The evaluation on\n",
    "the test set is carried out within a `with torch.inference_mode():` context, which\n",
    "disables gradient calculation, reducing memory consumption and computation time. The test\n",
    "data is also processed in mini-batches, and for each batch the loss and accuracy are\n",
    "computed in the same manner as during training, but without performing backpropagation or\n",
    "parameter updates.\n",
    "\n",
    "At the end of each epoch, the function prints a summary that includes the average\n",
    "training loss, test loss, training accuracy, and test accuracy. This information allows\n",
    "one to assess both how well the model fits the training data and how well it generalizes\n",
    "to unseen examples. The presence or absence of regularization will influence these\n",
    "metrics, typically reducing overfitting by sacrificing a small amount of training\n",
    "performance in exchange for better test performance.\n",
    "\n",
    "It is worth noting that, for simplicity, this implementation accesses `X_train`,\n",
    "`y_train`, `X_test`, and `y_test` as global variables. In more modular designs, data\n",
    "loaders (`torch.utils.data.DataLoader`) are usually employed to encapsulate batching,\n",
    "shuffling, and parallel loading of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the script demonstrates how to train the same model under two different\n",
    "regularization regimes.\n",
    "\n",
    "Training with L2 (Ridge) regularization is configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_model(model, optimizer, loss_fn, reg_fn=ridge_regularization, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fresh instance of `BinaryClassifier` is created, and the Adam optimizer is initialized\n",
    "with a relatively high learning rate of $3 \\times 10^{-2}$. The `BCEWithLogitsLoss` loss\n",
    "function is used because the model outputs logits. The training loop is invoked with\n",
    "`ridge_regularization` as the regularization function, and the regularization coefficient\n",
    "is set to `alpha = 0.001`. During training, this configuration adds an L2 penalty on the\n",
    "weights to the loss at each parameter update, analogous to Ridge regression. One\n",
    "typically observes smoother weight trajectories and improved generalization compared to\n",
    "an unregularized model.\n",
    "\n",
    "The model is then re-trained from scratch with L1 (Lasso) regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n",
    "\n",
    "train_model(model, optimizer, loss_fn, reg_fn=lasso_regularization, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reinitialization step ensures that both experiments start from comparable initial\n",
    "conditions, making the comparison between L2 and L1 regularization more meaningful. The\n",
    "optimizer and loss function remain the same, but the regularization function is now\n",
    "`lasso_regularization`. L1 regularization tends to promote sparsity in the parameter\n",
    "space, potentially zeroing out some weights. In small networks, this effect may be less\n",
    "visually apparent than in large linear models, but it still encourages simpler solutions.\n",
    "\n",
    "By comparing the printed training and test losses and accuracies for both configurations,\n",
    "one can empirically observe the influence of L2 and L1 regularization on the learning\n",
    "dynamics and generalization performance of the model. In particular, it is often the case\n",
    "that L2 regularization yields smooth and stable decision boundaries, while L1\n",
    "regularization, when appropriately tuned, can lead to slightly more compact or sparse\n",
    "parameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Dropout constitutes an extension of the traditional use of Dropout in neural\n",
    "networks. This technique is simultaneously interpreted as a regularization mechanism\n",
    "during training and as an approximate Bayesian inference method during the prediction\n",
    "phase. Both perspectives are complementary and are supported by a common theoretical\n",
    "framework that connects deep learning with probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its classical formulation, Dropout is introduced as a regularization strategy aimed at\n",
    "reducing overfitting in deep neural networks. During training, certain neurons—or, more\n",
    "precisely, their activations—are randomly deactivated with a preset probability. This\n",
    "procedure modifies the effective architecture of the network in each forward pass, which\n",
    "induces behavior similar to training an ensemble of smaller models that share parameters.\n",
    "\n",
    "From a functional perspective, the random deactivation of neurons forces the network not\n",
    "to depend excessively on specific units, which reduces co-adaptation between them. The\n",
    "model is forced to distribute relevant information across multiple pathways and to learn\n",
    "internal representations that are redundant and robust against the absence of certain\n",
    "nodes. This effect is particularly beneficial in scenarios with a high number of\n",
    "parameters and limited-size datasets, where the risk of overfitting is <SIGNUM>.\n",
    "\n",
    "In some cases, it has been demonstrated that the use of Dropout can be interpreted as an\n",
    "approximate form of L2 regularization on the network's weights. This equivalence is\n",
    "established under certain hypotheses about the architecture and type of layers employed,\n",
    "and allows understanding Dropout as a mechanism that implicitly penalizes excessively\n",
    "complex parameter configurations, favoring simpler and better generalizable solutions.\n",
    "\n",
    "A basic <SIGNUM> of implementing Dropout as a regularizer in a network defined with\n",
    "PyTorch is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Classic Dropout in training\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 50), nn.Dropout(p=0.5), nn.Linear(50, 10)  # Regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, Dropout is used in a standard manner: During training, the random\n",
    "shutdown of neurons is activated, while during inference it is deactivated, using the\n",
    "network deterministically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Dropout as Approximate Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Monte Carlo Dropout arises when one decides to keep Dropout active also\n",
    "during inference. Instead of making a single deterministic prediction with the complete\n",
    "network, multiple forward passes are executed over the same <SIGNUM> data, applying\n",
    "Dropout in each of them. This produces a set of stochastic predictions that can be\n",
    "interpreted as samples from an approximate predictive distribution.\n",
    "\n",
    "From a practical point of view, this procedure allows <SIGNUM> both an average prediction\n",
    "and an associated uncertainty measure. The mean of the stochastic predictions is used as\n",
    "the point output of the model, while the dispersion of said predictions (for <SIGNUM>,\n",
    "their standard deviation) offers an approximation to epistemic uncertainty, that is, the\n",
    "uncertainty derived from the model's lack of knowledge about the data.\n",
    "\n",
    "This <SIGNUM> can be implemented in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import torch\n",
    "\n",
    "\n",
    "# Monte Carlo Dropout in inference\n",
    "def mc_dropout_prediction(model, x, n_samples=100):\n",
    "    model.train()  # Keeps dropout active also in inference\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        with torch.no_grad():\n",
    "            predictions.append(model(x))\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "    mean = predictions.mean(dim=0)\n",
    "    uncertainty = predictions.std(dim=0)\n",
    "\n",
    "    return mean, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the call to `model.train()` during the inference phase is intentional.\n",
    "Although under usual conditions this instruction is exclusively associated with training,\n",
    "here it is used so that the Dropout layers remain active and continue eliminating units\n",
    "randomly in each pass. In this way, each evaluation of the model on the same <SIGNUM>\n",
    "generates a slightly different output, which allows constructing an empirical\n",
    "distribution of predictions.\n",
    "\n",
    "The mean of this distribution acts as a more robust point estimate, while the standard\n",
    "deviation provides a measure of dispersion that is interpreted as uncertainty. The\n",
    "greater this deviation, the greater the model's uncertainty regarding the prediction\n",
    "made.\n",
    "\n",
    "The relevance of Monte Carlo Dropout goes beyond its practical utility as a\n",
    "regularization tool or uncertainty estimation. Gal and Ghahramani (2016) demonstrate that\n",
    "training a neural network with Dropout and keeping it active during inference is\n",
    "mathematically equivalent to performing variational inference in a Bayesian model,\n",
    "specifically in an approximation of a Gaussian process over the functions represented by\n",
    "the network.\n",
    "\n",
    "In formal terms, the use of Dropout can be interpreted as the introduction of prior\n",
    "distributions over the network's weights, while the randomness induced in predictions\n",
    "during inference corresponds to the approximation of the posterior distribution over\n",
    "functions. The Monte Carlo procedure—that is, the repetition of stochastic\n",
    "predictions—allows approximately sampling from that posterior distribution, providing not\n",
    "only a point prediction, but also an explicit measure of the associated uncertainty.\n",
    "\n",
    "This equivalence situates Monte Carlo Dropout within the framework of variational\n",
    "Bayesian inference. The model ceases to be understood simply as a deterministic network\n",
    "with fixed weights and comes to be interpreted as a family of functions parameterized by\n",
    "stochastic latent variables. Inference is no longer limited to finding a single set of\n",
    "optimal parameters, but to approximating a distribution over said parameters or,\n",
    "equivalently, over the model's outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
