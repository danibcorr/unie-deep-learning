{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** constitutes one of the fundamental processes in natural language\n",
    "processing and deep learning applied to text. This process consists of the systematic\n",
    "decomposition of textual sequences into smaller units called \"tokens\", which can\n",
    "correspond to words, subwords, or even individual characters, depending on the strategy\n",
    "employed.\n",
    "\n",
    "The need for tokenization arises from an inherent limitation of computational systems:\n",
    "they operate exclusively with numerical representations. While humans process language\n",
    "naturally through linguistic symbols, neural network architectures require all\n",
    "information to be encoded in the form of numerical vectors. Tokenization therefore acts\n",
    "as a bridge between the linguistic domain and the mathematical domain, allowing machine\n",
    "learning models to process, analyze, and generate text effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic word-by-word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most intuitive approach to tokenization consists of segmenting text using whitespace\n",
    "as natural delimiters between words. This method, although simple, allows understanding\n",
    "the fundamental principles of the tokenization process and establishes the foundations\n",
    "for more sophisticated techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I like machine learning\n",
      "Tokens: ['I', 'like', 'machine', 'learning']\n",
      "Number of tokens: 4\n"
     ]
    }
   ],
   "source": [
    "# Basic tokenization example\n",
    "texto = \"I like machine learning\"\n",
    "\n",
    "# Method 1: Using Python's split()\n",
    "tokens = texto.split()\n",
    "print(\"Original text:\", texto)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Original text: I like machine learning\n",
    "Tokens: ['I', 'like', 'machine', 'learning']\n",
    "Number of tokens: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To advance beyond simple text splitting, it is necessary to build a system that not only\n",
    "segments words, but also establishes a one-to-one correspondence between each unique word\n",
    "and a numerical identifier. This mapping allows representing any text as a sequence of\n",
    "numbers, facilitating its processing by machine learning models.\n",
    "\n",
    "The implementation of a basic tokenizer requires maintaining two complementary data\n",
    "structures: a dictionary that maps words to numbers and another that performs the inverse\n",
    "transformation. Additionally, a mechanism is needed to assign unique identifiers to each\n",
    "new word found during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EXAMPLE 1: Simple Tokenizer\n",
      "==================================================\n",
      "Learned vocabulary: 6 words\n",
      "\n",
      "Complete vocabulary:\n",
      "----------------------------------------\n",
      "  0 -> i\n",
      "  1 -> like\n",
      "  2 -> programming\n",
      "  3 -> learning\n",
      "  4 -> is\n",
      "  5 -> fun\n",
      "\n",
      "Text to encode: 'i like learning programming'\n",
      "Encoded text: [0, 1, 3, 2]\n",
      "Decoded text: 'i like learning programming'\n",
      "\n",
      "Text with new word: 'i like cooking'\n",
      "Encoded: [0, 1, -1]\n",
      "Note: -1 indicates unknown word\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    A basic tokenizer that splits text into words\n",
    "    and assigns them unique numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Dictionary to store word -> number\n",
    "        self.word_to_number = {}\n",
    "        # Inverse dictionary: number -> word\n",
    "        self.number_to_word = {}\n",
    "        # Counter to assign numbers\n",
    "        self.next_number = 0\n",
    "\n",
    "    def train(self, texts):\n",
    "        \"\"\"\n",
    "        Learns which words exist in the texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of strings with training texts\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            # Convert to lowercase and split into words\n",
    "            words = text.lower().split()\n",
    "\n",
    "            # For each word, if we haven't seen it, assign it a number\n",
    "            for word in words:\n",
    "                if word not in self.word_to_number:\n",
    "                    self.word_to_number[word] = self.next_number\n",
    "                    self.number_to_word[self.next_number] = word\n",
    "                    self.next_number += 1\n",
    "\n",
    "        print(f\"Learned vocabulary: {len(self.word_to_number)} words\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Converts text into a list of numbers.\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        numbers = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.word_to_number:\n",
    "                numbers.append(self.word_to_number[word])\n",
    "            else:\n",
    "                # If we don't know the word, use -1\n",
    "                numbers.append(-1)\n",
    "\n",
    "        return numbers\n",
    "\n",
    "    def decode(self, numbers):\n",
    "        \"\"\"\n",
    "        Converts a list of numbers back to text.\n",
    "        \"\"\"\n",
    "        words = []\n",
    "\n",
    "        for number in numbers:\n",
    "            if number in self.number_to_word:\n",
    "                words.append(self.number_to_word[number])\n",
    "            else:\n",
    "                words.append(\"[UNKNOWN]\")\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def show_vocabulary(self):\n",
    "        \"\"\"Shows all words the tokenizer knows.\"\"\"\n",
    "        print(\"\\nComplete vocabulary:\")\n",
    "        print(\"-\" * 40)\n",
    "        for word, number in sorted(self.word_to_number.items(), key=lambda x: x[1]):\n",
    "            print(f\"{number:3d} -> {word}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "print(\"=\" * 50)\n",
    "print(\"EXAMPLE 1: Simple Tokenizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training texts\n",
    "training_texts = [\"i like programming\", \"i like learning\", \"programming is fun\"]\n",
    "\n",
    "# Create and train the tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.train(training_texts)\n",
    "\n",
    "# Show the learned vocabulary\n",
    "tokenizer.show_vocabulary()\n",
    "\n",
    "# Test encoding\n",
    "new_text = \"i like learning programming\"\n",
    "print(f\"\\nText to encode: '{new_text}'\")\n",
    "\n",
    "encoded = tokenizer.encode(new_text)\n",
    "print(f\"Encoded text: {encoded}\")\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded text: '{decoded}'\")\n",
    "\n",
    "# Test with unknown word\n",
    "unknown_text = \"i like cooking\"\n",
    "print(f\"\\nText with new word: '{unknown_text}'\")\n",
    "encoded_unk = tokenizer.encode(unknown_text)\n",
    "print(f\"Encoded: {encoded_unk}\")\n",
    "print(\"Note: -1 indicates unknown word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens and vocabulary management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in real-world natural language processing applications presents challenges\n",
    "that go beyond simple word-to-number conversion. Situations arise that require special\n",
    "treatment: words that did not appear during training, sequences of different lengths that\n",
    "must be processed in batches, and the need to explicitly mark the beginning and end of\n",
    "sequences.\n",
    "\n",
    "To address these issues, modern tokenization systems incorporate special tokens with\n",
    "specific functions. The padding token allows uniformizing sequence length, facilitating\n",
    "parallel processing. The unknown word token provides a consistent representation for\n",
    "terms not seen during training. The start and end of sequence tokens allow models to\n",
    "explicitly identify the boundaries of each input, which is especially relevant in text\n",
    "generation and machine translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXAMPLE 2: Special Tokens and Padding\n",
      "==================================================\n",
      "Vocabulary: 12 words\n",
      "  - Special words: 4\n",
      "  - Normal words: 8\n",
      "\n",
      "Text 1: 'hello python'\n",
      "Encoded: [2, 4, 6, 3]\n",
      "Length: 4\n",
      "\n",
      "Text 2: 'i like'\n",
      "Encoded (fixed length=10): [2, 9, 10, 3, 0, 0, 0, 0, 0, 0]\n",
      "Length: 10\n",
      "Decoded: 'i like'\n"
     ]
    }
   ],
   "source": [
    "class TokenizerWithSpecials:\n",
    "    \"\"\"\n",
    "    Tokenizer that handles unknown words and padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Special tokens\n",
    "        self.PAD = \"[PAD]\"  # For padding short sequences\n",
    "        self.UNK = \"[UNK]\"  # For unknown words\n",
    "        self.SOS = \"[SOS]\"  # Start of Sequence\n",
    "        self.EOS = \"[EOS]\"  # End of Sequence\n",
    "\n",
    "        # Initialize dictionaries with special tokens\n",
    "        self.word_to_number = {self.PAD: 0, self.UNK: 1, self.SOS: 2, self.EOS: 3}\n",
    "\n",
    "        self.number_to_word = {0: self.PAD, 1: self.UNK, 2: self.SOS, 3: self.EOS}\n",
    "\n",
    "        self.next_number = 4\n",
    "\n",
    "    def train(self, texts):\n",
    "        \"\"\"Learns the vocabulary from texts.\"\"\"\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "\n",
    "            for word in words:\n",
    "                if word not in self.word_to_number:\n",
    "                    self.word_to_number[word] = self.next_number\n",
    "                    self.number_to_word[self.next_number] = word\n",
    "                    self.next_number += 1\n",
    "\n",
    "        print(f\"Vocabulary: {len(self.word_to_number)} words\")\n",
    "        print(f\"  - Special words: 4\")\n",
    "        print(f\"  - Normal words: {len(self.word_to_number) - 4}\")\n",
    "\n",
    "    def encode(self, text, add_special=True, fixed_length=None):\n",
    "        \"\"\"\n",
    "        Encodes text with advanced options.\n",
    "\n",
    "        Args:\n",
    "            text: Text to encode\n",
    "            add_special: Whether to add [SOS] and [EOS]\n",
    "            fixed_length: If specified, adjusts to this length\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "\n",
    "        # Convert words to numbers\n",
    "        numbers = []\n",
    "        for word in words:\n",
    "            if word in self.word_to_number:\n",
    "                numbers.append(self.word_to_number[word])\n",
    "            else:\n",
    "                numbers.append(self.word_to_number[self.UNK])\n",
    "\n",
    "        # Add start and end tokens if requested\n",
    "        if add_special:\n",
    "            numbers = (\n",
    "                [self.word_to_number[self.SOS]]\n",
    "                + numbers\n",
    "                + [self.word_to_number[self.EOS]]\n",
    "            )\n",
    "\n",
    "        # Adjust to fixed length if specified\n",
    "        if fixed_length is not None:\n",
    "            if len(numbers) < fixed_length:\n",
    "                # Pad with PAD\n",
    "                numbers = numbers + [self.word_to_number[self.PAD]] * (\n",
    "                    fixed_length - len(numbers)\n",
    "                )\n",
    "            else:\n",
    "                # Truncate\n",
    "                numbers = numbers[:fixed_length]\n",
    "\n",
    "        return numbers\n",
    "\n",
    "    def decode(self, numbers, remove_special=True):\n",
    "        \"\"\"Decodes numbers to text.\"\"\"\n",
    "        words = []\n",
    "\n",
    "        for number in numbers:\n",
    "            if number in self.number_to_word:\n",
    "                word = self.number_to_word[number]\n",
    "\n",
    "                # Skip special tokens if requested\n",
    "                if remove_special and word in [self.PAD, self.UNK, self.SOS, self.EOS]:\n",
    "                    continue\n",
    "\n",
    "                words.append(word)\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# Usage examples\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXAMPLE 2: Special Tokens and Padding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train\n",
    "texts = [\"hello world\", \"python is great\", \"i like learning\"]\n",
    "\n",
    "tokenizer_v2 = TokenizerWithSpecials()\n",
    "tokenizer_v2.train(texts)\n",
    "\n",
    "# Encode without fixed length\n",
    "text1 = \"hello python\"\n",
    "print(f\"\\nText 1: '{text1}'\")\n",
    "cod1 = tokenizer_v2.encode(text1)\n",
    "print(f\"Encoded: {cod1}\")\n",
    "print(f\"Length: {len(cod1)}\")\n",
    "\n",
    "# Encode with fixed length\n",
    "text2 = \"i like\"\n",
    "print(f\"\\nText 2: '{text2}'\")\n",
    "cod2 = tokenizer_v2.encode(text2, fixed_length=10)\n",
    "print(f\"Encoded (fixed length=10): {cod2}\")\n",
    "print(f\"Length: {len(cod2)}\")\n",
    "\n",
    "# Decode\n",
    "print(f\"Decoded: '{tokenizer_v2.decode(cod2)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the tokenization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep understanding of the tokenization process is facilitated through explicit\n",
    "visualization of the transformations the text undergoes at each stage. Observing how a\n",
    "sequence of words is converted into a sequence of numerical identifiers, and can\n",
    "subsequently be recovered as original text, allows identifying potential problems and\n",
    "understanding the system's behavior with different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKENIZATION VISUALIZATION\n",
      "============================================================\n",
      "\n",
      "1. Original text:\n",
      "   'python is great'\n",
      "\n",
      "   Tokens (numbers):\n",
      "   [2, 6, 7, 8, 3]\n",
      "\n",
      "   Visual representation:\n",
      "   [SOS]           ->   2\n",
      "   python          ->   6\n",
      "   is              ->   7\n",
      "   great           ->   8\n",
      "   [EOS]           ->   3\n",
      "\n",
      "   Decoded:\n",
      "   'python is great'\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Original text:\n",
      "   'i like programming'\n",
      "\n",
      "   Tokens (numbers):\n",
      "   [2, 9, 10, 1, 3]\n",
      "\n",
      "   Visual representation:\n",
      "   [SOS]           ->   2\n",
      "   i               ->   9\n",
      "   like            ->  10\n",
      "   programming     ->   1\n",
      "   [EOS]           ->   3\n",
      "\n",
      "   Decoded:\n",
      "   'i like'\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Original text:\n",
      "   'hello artificial intelligence'\n",
      "\n",
      "   Tokens (numbers):\n",
      "   [2, 4, 1, 1, 3]\n",
      "\n",
      "   Visual representation:\n",
      "   [SOS]           ->   2\n",
      "   hello           ->   4\n",
      "   artificial      ->   1\n",
      "   intelligence    ->   1\n",
      "   [EOS]           ->   3\n",
      "\n",
      "   Decoded:\n",
      "   'hello'\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def visualize_tokenization(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Visually shows how each text is tokenized.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOKENIZATION VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"\\n{i}. Original text:\")\n",
    "        print(f\"   '{text}'\")\n",
    "\n",
    "        # Encode\n",
    "        encoded = tokenizer.encode(text, add_special=True)\n",
    "\n",
    "        print(f\"\\n   Tokens (numbers):\")\n",
    "        print(f\"   {encoded}\")\n",
    "\n",
    "        print(f\"\\n   Visual representation:\")\n",
    "        # Show each token with its word\n",
    "        words = [\"[SOS]\"] + text.lower().split() + [\"[EOS]\"]\n",
    "        for word, number in zip(words, encoded):\n",
    "            print(f\"   {word:15} -> {number:3}\")\n",
    "\n",
    "        print(f\"\\n   Decoded:\")\n",
    "        print(f\"   '{tokenizer.decode(encoded)}'\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# Visualization example\n",
    "example_texts = [\n",
    "    \"python is great\",\n",
    "    \"i like programming\",\n",
    "    \"hello artificial intelligence\",\n",
    "]\n",
    "\n",
    "visualize_tokenization(tokenizer_v2, example_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length normalization through padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most relevant technical aspects in textual sequence processing is the\n",
    "management of variable lengths. Natural texts present considerable diversity in terms of\n",
    "their length: from brief phrases of few words to extensive paragraphs with dozens or\n",
    "hundreds of tokens. However, neural network architectures, especially when processing\n",
    "multiple examples simultaneously in batches, require all input sequences to have uniform\n",
    "dimensions.\n",
    "\n",
    "Padding constitutes the standard solution to this problem. It consists of artificially\n",
    "extending shorter sequences until reaching a target length, typically determined by the\n",
    "longest sequence in the batch. This extension is performed through the insertion of\n",
    "special padding tokens that the model learns to ignore during processing. Alternatively,\n",
    "when a sequence exceeds the maximum allowed length, truncation is applied, preserving\n",
    "only the first tokens up to the established limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LENGTH COMPARISON\n",
      "============================================================\n",
      "\n",
      "Maximum length found: 8 tokens\n",
      "\n",
      "Comparison:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Text: 'hello'\n",
      "Without padding (length 3): [2, 4, 3]\n",
      "With padding (length 8): [2, 4, 3, 0, 0, 0, 0, 0]\n",
      "PADs added: 5\n",
      "\n",
      "Text: 'python is great'\n",
      "Without padding (length 5): [2, 6, 7, 8, 3]\n",
      "With padding (length 8): [2, 6, 7, 8, 3, 0, 0, 0]\n",
      "PADs added: 3\n",
      "\n",
      "Text: 'i like learning programming in python'\n",
      "Without padding (length 8): [2, 9, 10, 11, 1, 1, 6, 3]\n",
      "With padding (length 8): [2, 9, 10, 11, 1, 1, 6, 3]\n",
      "PADs added: 0\n"
     ]
    }
   ],
   "source": [
    "def compare_lengths(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Compares the lengths of different texts and shows\n",
    "    how padding uniformizes them.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LENGTH COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Find maximum length\n",
    "    lengths = []\n",
    "    for text in texts:\n",
    "        cod = tokenizer.encode(text, add_special=True)\n",
    "        lengths.append(len(cod))\n",
    "\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    print(f\"\\nMaximum length found: {max_length} tokens\")\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text in texts:\n",
    "        # Without padding\n",
    "        without_padding = tokenizer.encode(text, add_special=True)\n",
    "\n",
    "        # With padding\n",
    "        with_padding = tokenizer.encode(text, add_special=True, fixed_length=max_length)\n",
    "\n",
    "        print(f\"\\nText: '{text}'\")\n",
    "        print(f\"Without padding (length {len(without_padding)}): {without_padding}\")\n",
    "        print(f\"With padding (length {len(with_padding)}): {with_padding}\")\n",
    "\n",
    "        # Count how many PADs were added\n",
    "        num_pads = with_padding.count(0)\n",
    "        print(f\"PADs added: {num_pads}\")\n",
    "\n",
    "\n",
    "# Comparison example\n",
    "different_texts = [\"hello\", \"python is great\", \"i like learning programming in python\"]\n",
    "\n",
    "compare_lengths(tokenizer_v2, different_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical application: Review processing system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of all presented concepts materializes in the construction of a complete\n",
    "system capable of processing real-world text. A representative use case is product review\n",
    "analysis, where the objective consists of transforming opinions expressed in natural\n",
    "language into numerical representations that can subsequently feed sentiment\n",
    "classification models or other analysis tasks.\n",
    "\n",
    "This system integrates the tokenizer with special token management capabilities, length\n",
    "normalization, and vocabulary maintenance built from a training set. The resulting\n",
    "architecture allows processing new reviews consistently, applying the same\n",
    "transformations that will be used during machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MINI PROJECT: Review System\n",
      "============================================================\n",
      "\n",
      "Adding reviews to the system...\n",
      "  - [POSITIVE] this product is excellent\n",
      "  - [NEGATIVE] very bad quality do not recommend\n",
      "  - [POSITIVE] incredible i love it\n",
      "  - [NEGATIVE] terrible experience\n",
      "  - [POSITIVE] perfect product arrived fast\n",
      "  - [NEGATIVE] does not work properly\n",
      "Training tokenizer with reviews...\n",
      "Vocabulary: 26 words\n",
      "  - Special words: 4\n",
      "  - Normal words: 22\n",
      "\n",
      "============================================================\n",
      "SYSTEM STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total reviews: 6\n",
      "Positive reviews: 3\n",
      "Negative reviews: 3\n",
      "Vocabulary: 26 words\n",
      "\n",
      "Average length: 4.0 words\n",
      "Minimum length: 2 words\n",
      "Maximum length: 6 words\n",
      "\n",
      "============================================================\n",
      "PROCESSING NEW REVIEWS\n",
      "============================================================\n",
      "\n",
      "Processing: 'excellent product very good'\n",
      "--------------------------------------------------\n",
      "1. Split into words: ['excellent', 'product', 'very', 'good']\n",
      "2. Convert to numbers: [2, 7, 5, 8, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3. Decode: 'excellent product very'\n",
      "\n",
      "Processing: 'bad experience terrible'\n",
      "--------------------------------------------------\n",
      "1. Split into words: ['bad', 'experience', 'terrible']\n",
      "2. Convert to numbers: [2, 9, 19, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3. Decode: 'bad experience terrible'\n",
      "\n",
      "Processing: 'perfect recommend'\n",
      "--------------------------------------------------\n",
      "1. Split into words: ['perfect', 'recommend']\n",
      "2. Convert to numbers: [2, 20, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3. Decode: 'perfect recommend'\n"
     ]
    }
   ],
   "source": [
    "class ReviewSystem:\n",
    "    \"\"\"\n",
    "    Complete system for processing product reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TokenizerWithSpecials()\n",
    "        self.reviews = []\n",
    "        self.labels = []  # 1 = positive, 0 = negative\n",
    "\n",
    "    def add_review(self, text, is_positive):\n",
    "        \"\"\"Adds a review to the system.\"\"\"\n",
    "        self.reviews.append(text)\n",
    "        self.labels.append(1 if is_positive else 0)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the tokenizer with all reviews.\"\"\"\n",
    "        print(\"Training tokenizer with reviews...\")\n",
    "        self.tokenizer.train(self.reviews)\n",
    "\n",
    "    def process_review(self, text, length=15):\n",
    "        \"\"\"Processes a new review.\"\"\"\n",
    "        print(f\"\\nProcessing: '{text}'\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = text.lower().split()\n",
    "        print(f\"1. Split into words: {tokens}\")\n",
    "\n",
    "        # Encode\n",
    "        encoded = self.tokenizer.encode(text, add_special=True, fixed_length=length)\n",
    "        print(f\"2. Convert to numbers: {encoded}\")\n",
    "\n",
    "        # Decode\n",
    "        decoded = self.tokenizer.decode(encoded)\n",
    "        print(f\"3. Decode: '{decoded}'\")\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def show_statistics(self):\n",
    "        \"\"\"Shows dataset statistics.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SYSTEM STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(f\"\\nTotal reviews: {len(self.reviews)}\")\n",
    "        print(f\"Positive reviews: {sum(self.labels)}\")\n",
    "        print(f\"Negative reviews: {len(self.labels) - sum(self.labels)}\")\n",
    "        print(f\"Vocabulary: {len(self.tokenizer.word_to_number)} words\")\n",
    "\n",
    "        # Lengths\n",
    "        lengths = [len(r.split()) for r in self.reviews]\n",
    "        print(f\"\\nAverage length: {sum(lengths)/len(lengths):.1f} words\")\n",
    "        print(f\"Minimum length: {min(lengths)} words\")\n",
    "        print(f\"Maximum length: {max(lengths)} words\")\n",
    "\n",
    "\n",
    "# Create the system\n",
    "print(\"=\" * 60)\n",
    "print(\"MINI PROJECT: Review System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "system = ReviewSystem()\n",
    "\n",
    "# Add training reviews\n",
    "training_reviews = [\n",
    "    (\"this product is excellent\", True),\n",
    "    (\"very bad quality do not recommend\", False),\n",
    "    (\"incredible i love it\", True),\n",
    "    (\"terrible experience\", False),\n",
    "    (\"perfect product arrived fast\", True),\n",
    "    (\"does not work properly\", False),\n",
    "]\n",
    "\n",
    "print(\"\\nAdding reviews to the system...\")\n",
    "for text, is_positive in training_reviews:\n",
    "    system.add_review(text, is_positive)\n",
    "    sentiment = \"POSITIVE\" if is_positive else \"NEGATIVE\"\n",
    "    print(f\"  - [{sentiment}] {text}\")\n",
    "\n",
    "# Train\n",
    "system.train()\n",
    "\n",
    "# Show statistics\n",
    "system.show_statistics()\n",
    "\n",
    "# Process new reviews\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROCESSING NEW REVIEWS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_reviews = [\n",
    "    \"excellent product very good\",\n",
    "    \"bad experience terrible\",\n",
    "    \"perfect recommend\",\n",
    "]\n",
    "\n",
    "for review in new_reviews:\n",
    "    system.process_review(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unie-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
