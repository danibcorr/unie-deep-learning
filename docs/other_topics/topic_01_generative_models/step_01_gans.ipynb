{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7b1da3",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def show_generated_samples(\n",
    "    generator: nn.Module, noise, device: str, num_samples: int = 16\n",
    ") -> None:\n",
    "    \"\"\"Función auxiliar para mostrar muestras generadas\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = generator(noise[:num_samples]).cpu()\n",
    "        samples = (samples + 1) / 2  # Desnormalizar de [-1,1] a [0,1]\n",
    "\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "        for i in range(num_samples):\n",
    "            row, col = i // 4, i % 4\n",
    "            axes[row, col].imshow(samples[i, 0], cmap=\"gray\")\n",
    "            axes[row, col].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, hidden_size: int = 64, dropout_rate: float = 0.2\n",
    "    ) -> None:\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Creamos el modelo que lo ajustaremos para MNIST\n",
    "        self.model = nn.Sequential(\n",
    "            # Input MNIST = (B, C, H, W) = (B, 1, 28, 28) -> (B, H, 14, 14)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.hidden_size // 2,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(p=self.dropout_rate),\n",
    "            # (B, H, 14, 14) -> (B, H, 7, 7)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.hidden_size // 2,\n",
    "                out_channels=self.hidden_size,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(p=self.dropout_rate),\n",
    "            # (B, H, 7, 7) -> (B, 1, 7, 7)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.hidden_size,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # (B, 1, 7, 7) -> (B, 1, 1, 1)\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # (B, 1, 1, 1) -> (B, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            # El discriminador ha de devolver un escalar entre 0 y 1 (falso/real)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(input_tensor)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self, z_dim: int, data_shape: tuple[int, int, int], hidden_size: int\n",
    "    ) -> None:\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros de la clase\n",
    "        self.z_dim = z_dim\n",
    "        self.data_shape = data_shape\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Del ruido que es un tensor plano, vamos a crear una matriz inicial\n",
    "        # de (B, H, 7, 7) que es el tamaño antes de aplanar en el Discriminador\n",
    "        self.projection = nn.Sequential(\n",
    "            # (B, z_dim) -> (B, H * 7 * 7)\n",
    "            nn.Linear(\n",
    "                in_features=self.z_dim,\n",
    "                out_features=self.hidden_size * 7 * 7,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # (B, H, 7, 7) -> (B, H, 14, 14)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.hidden_size,\n",
    "                out_channels=self.hidden_size,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            # (B, H, 14, 14) -> (B, H, 28, 28)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.hidden_size,\n",
    "                out_channels=self.hidden_size,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            # (B, H, 28, 28) -> (B, 1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.hidden_size, out_channels=1, kernel_size=1, stride=1\n",
    "            ),\n",
    "            # Normalizamos los valores entre -1 y 1\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # (B, z_dim) -> (B, H * 7 * 7)\n",
    "        projection = self.projection(input_tensor)\n",
    "        # (B, H * 7 * 7) -> (B, H, 7, 7)\n",
    "        projection = projection.view(projection.size(0), self.hidden_size, 7, 7)\n",
    "\n",
    "        return self.model(projection)\n",
    "\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_size_discriminator: int,\n",
    "        dropout_rate: float,\n",
    "        z_dim: int,\n",
    "        data_shape: tuple[int, int, int],\n",
    "        hidden_size_generator: int,\n",
    "    ) -> None:\n",
    "        # Constructor de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        # Definimos los parámetros\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size_discriminator = hidden_size_discriminator\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.z_dim = z_dim\n",
    "        self.data_shape = data_shape\n",
    "        self.hidden_size_generator = hidden_size_generator\n",
    "\n",
    "        # Definimos los modelos\n",
    "        self.discriminator = Discriminator(\n",
    "            in_channels=self.in_channels,\n",
    "            hidden_size=self.hidden_size_discriminator,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n",
    "        self.generator = Generator(\n",
    "            z_dim=self.z_dim,\n",
    "            data_shape=self.data_shape,\n",
    "            hidden_size=self.hidden_size_generator,\n",
    "        )\n",
    "\n",
    "    def forward(self, real_data: torch.Tensor, batch_size: int | None = None) -> dict:\n",
    "        if batch_size is None:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "        # Generar ruido aleatorio\n",
    "        noise = torch.randn(batch_size, self.z_dim, device=real_data.device)\n",
    "\n",
    "        # Generar imágenes falsas\n",
    "        fake_data = self.generator(noise)\n",
    "\n",
    "        # Pasar datos reales por el discriminador\n",
    "        real_predictions = self.discriminator(real_data)\n",
    "\n",
    "        # Pasar datos falsos por el discriminador\n",
    "        fake_predictions = self.discriminator(fake_data)\n",
    "\n",
    "        return {\n",
    "            \"real_predictions\": real_predictions,\n",
    "            \"fake_predictions\": fake_predictions,\n",
    "            \"fake_data\": fake_data,\n",
    "            \"noise\": noise,\n",
    "        }\n",
    "\n",
    "    def generate_samples_inference(self, num_samples: int, device: str) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # (Num_samples, z_dim)\n",
    "            noise = torch.randn(num_samples, self.z_dim, device=device)\n",
    "            # (Num_samples, z_dim) -> MNIST: (Num_samples, 1, 28, 28)\n",
    "            samples = self.generator(input_tensor=noise)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def discriminator_loss(\n",
    "        self, real_predictions: torch.Tensor, fake_predictions: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        criterion = nn.BCELoss()\n",
    "        # Matriz de 1s\n",
    "        real_labels = torch.ones_like(real_predictions)\n",
    "        # Matriz de 0s\n",
    "        fake_labels = torch.zeros_like(fake_predictions)\n",
    "\n",
    "        real_loss = criterion(real_predictions, real_labels)\n",
    "        fake_loss = criterion(fake_predictions, fake_labels)\n",
    "\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def generator_loss(self, fake_predictions: torch.Tensor) -> torch.Tensor:\n",
    "        criterion = nn.BCELoss()\n",
    "        fake_real_labels = torch.ones_like(fake_predictions)\n",
    "        return criterion(fake_predictions, fake_real_labels)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Seleccionamos el dispositivo actual\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Las GANs son muy sensibles a los hiperparámetros\n",
    "    lr = 2e-4\n",
    "    # Dimensión de los datos de entrada (C, H, W)\n",
    "    data_dimension = (1, 28, 28)\n",
    "    # Esta es la dimension del ruido\n",
    "    z_dim = 100\n",
    "    batch_size = 128\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Definimos el modelo\n",
    "    model = GAN(\n",
    "        in_channels=data_dimension[0],\n",
    "        hidden_size_discriminator=64,\n",
    "        dropout_rate=0.2,\n",
    "        z_dim=z_dim,\n",
    "        data_shape=data_dimension,\n",
    "        hidden_size_generator=256,\n",
    "    ).to(device)\n",
    "\n",
    "    # Transformaciones que vamos a aplicar a MNIST\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                # Normalizar a [-1, 1] para coincidir con Tanh\n",
    "                (0.5,),\n",
    "                (0.5,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Descargamos MNIST\n",
    "    dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Vamos a utilizar un optimizador para cada modelo\n",
    "    opt_disc = optim.AdamW(params=model.discriminator.parameters(), lr=lr)\n",
    "    opt_gen = optim.AdamW(params=model.generator.parameters(), lr=lr)\n",
    "\n",
    "    # Ruido fijo para generar muestras durante el entrenamiento\n",
    "    fixed_noise = torch.randn(64, z_dim, device=device)\n",
    "\n",
    "    # Listas para guardar las pérdidas\n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "\n",
    "    print(\"Iniciando entrenamiento de la GAN...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_disc_loss: int | float = 0\n",
    "        epoch_gen_loss: int | float = 0\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Época {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for _, (real, _) in enumerate(pbar):\n",
    "            real = real.to(device)\n",
    "            batch_size_current = real.shape[0]\n",
    "\n",
    "            # ENTRENAR DISCRIMINADOR\n",
    "            opt_disc.zero_grad()\n",
    "\n",
    "            # Generar datos falsos\n",
    "            noise = torch.randn(batch_size_current, z_dim, device=device)\n",
    "            fake_data = model.generator(\n",
    "                noise\n",
    "            ).detach()  # Detach para no actualizar generador\n",
    "\n",
    "            # Predicciones del discriminador\n",
    "            real_preds = model.discriminator(real)\n",
    "            fake_preds = model.discriminator(fake_data)\n",
    "\n",
    "            # Pérdida del discriminador\n",
    "            lossD = model.discriminator_loss(real_preds, fake_preds)\n",
    "            lossD.backward()\n",
    "            opt_disc.step()\n",
    "\n",
    "            # ENTRENAR GENERADOR\n",
    "            opt_gen.zero_grad()\n",
    "\n",
    "            # Generar nuevos datos falsos (sin detach)\n",
    "            noise = torch.randn(batch_size_current, z_dim, device=device)\n",
    "            fake_data = model.generator(noise)\n",
    "            fake_preds_for_gen = model.discriminator(fake_data)\n",
    "\n",
    "            # Pérdida del generador\n",
    "            lossG = model.generator_loss(fake_preds_for_gen)\n",
    "            lossG.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "            # Acumular pérdidas\n",
    "            epoch_disc_loss += lossD.item()\n",
    "            epoch_gen_loss += lossG.item()\n",
    "\n",
    "            # Actualizar barra de progreso\n",
    "            pbar.set_postfix(\n",
    "                {\"D_loss\": f\"{lossD.item():.4f}\", \"G_loss\": f\"{lossG.item():.4f}\"}\n",
    "            )\n",
    "\n",
    "        # Pérdidas promedio de la época\n",
    "        avg_disc_loss = epoch_disc_loss / len(loader)\n",
    "        avg_gen_loss = epoch_gen_loss / len(loader)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Época {epoch + 1}, \"\n",
    "            f\"Pérdida D: {avg_disc_loss:.4f}, \"\n",
    "            f\"Pérdida G: {avg_gen_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Mostrar muestras cada 5 épocas\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"\\nMostrando muestras generadas en época {epoch + 1}...\")\n",
    "            show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n",
    "\n",
    "    print(\"\\n¡Entrenamiento completado!\")\n",
    "\n",
    "    # Mostrar gráfica de pérdidas\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(disc_losses, label=\"Discriminador\", color=\"red\")\n",
    "    plt.plot(gen_losses, label=\"Generador\", color=\"blue\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"Pérdida\")\n",
    "    plt.title(\"Evolución de las Pérdidas\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Mostrar muestras generadas finales\n",
    "    plt.subplot(1, 2, 2)\n",
    "    model.generator.eval()\n",
    "    with torch.no_grad():\n",
    "        final_samples = model.generator(fixed_noise[:16]).cpu()\n",
    "        final_samples = (final_samples + 1) / 2  # Desnormalizar\n",
    "        grid = torchvision.utils.make_grid(final_samples, nrow=4, padding=2)\n",
    "        plt.imshow(grid.permute(1, 2, 0), cmap=\"gray\")\n",
    "        plt.title(\"Muestras Finales Generadas\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nMostrando muestras generadas...\")\n",
    "    show_generated_samples(model.generator, fixed_noise, device, num_samples=16)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
